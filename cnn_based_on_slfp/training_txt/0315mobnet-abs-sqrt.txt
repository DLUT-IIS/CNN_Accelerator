Script started on 2024-03-15 00:12:45+08:00 [TERM="xterm-256color" TTY="/dev/pts/0" COLUMNS="214" LINES="21"]
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ bash bash_train.sh 
=> creating model mobilenet_m1 ...
SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
./imgnet_train_eval.py:177: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-15 00:12:53.427855 epoch: 1 step: 0 cls_loss= 2.43683 (19592 samples/sec)
2024-03-15 00:13:49.328067 epoch: 1 step: 500 cls_loss= 1.65096 (286 samples/sec)
2024-03-15 00:14:45.579237 epoch: 1 step: 1000 cls_loss= 2.05126 (284 samples/sec)
2024-03-15 00:15:41.829241 epoch: 1 step: 1500 cls_loss= 1.59973 (284 samples/sec)
2024-03-15 00:16:38.612212 epoch: 1 step: 2000 cls_loss= 1.07812 (281 samples/sec)
2024-03-15 00:17:36.333143 epoch: 1 step: 2500 cls_loss= 1.68316 (277 samples/sec)
2024-03-15 00:18:33.807153 epoch: 1 step: 3000 cls_loss= 1.93086 (278 samples/sec)
2024-03-15 00:21:32.869805------------------------------------------------------ Precision@1: 66.80%  Precision@1: 87.21%

top1: [66.8]
top5: [87.214]
2024-03-15 00:21:33.072483 epoch: 2 step: 0 cls_loss= 1.98899 (79354 samples/sec)
2024-03-15 00:22:29.002275 epoch: 2 step: 500 cls_loss= 1.73658 (286 samples/sec)
2024-03-15 00:23:25.105681 epoch: 2 step: 1000 cls_loss= 1.37716 (285 samples/sec)
2024-03-15 00:24:21.366423 epoch: 2 step: 1500 cls_loss= 1.93983 (284 samples/sec)
2024-03-15 00:25:18.142589 epoch: 2 step: 2000 cls_loss= 2.22033 (281 samples/sec)
2024-03-15 00:26:13.791600 epoch: 2 step: 2500 cls_loss= 2.20162 (287 samples/sec)
2024-03-15 00:27:10.255236 epoch: 2 step: 3000 cls_loss= 1.91101 (283 samples/sec)
2024-03-15 00:30:08.230037------------------------------------------------------ Precision@1: 67.16%  Precision@1: 87.69%

top1: [66.8, 67.156]
top5: [87.214, 87.69200000000001]
2024-03-15 00:30:08.429431 epoch: 3 step: 0 cls_loss= 1.88452 (80684 samples/sec)
2024-03-15 00:31:05.948045 epoch: 3 step: 500 cls_loss= 1.39343 (278 samples/sec)
2024-03-15 00:32:03.096752 epoch: 3 step: 1000 cls_loss= 2.19411 (280 samples/sec)
2024-03-15 00:33:00.431048 epoch: 3 step: 1500 cls_loss= 1.72663 (279 samples/sec)
2024-03-15 00:33:57.370678 epoch: 3 step: 2000 cls_loss= 1.62125 (281 samples/sec)
2024-03-15 00:34:54.494305 epoch: 3 step: 2500 cls_loss= 1.74746 (280 samples/sec)
2024-03-15 00:35:51.823632 epoch: 3 step: 3000 cls_loss= 1.60718 (279 samples/sec)
2024-03-15 00:38:50.681454------------------------------------------------------ Precision@1: 67.18%  Precision@1: 87.78%

top1: [66.8, 67.156, 67.18]
top5: [87.214, 87.69200000000001, 87.78]
2024-03-15 00:38:50.893774 epoch: 4 step: 0 cls_loss= 2.11777 (75785 samples/sec)
2024-03-15 00:39:47.805719 epoch: 4 step: 500 cls_loss= 2.43197 (281 samples/sec)
2024-03-15 00:40:44.390699 epoch: 4 step: 1000 cls_loss= 2.30036 (282 samples/sec)
2024-03-15 00:41:41.625698 epoch: 4 step: 1500 cls_loss= 2.35609 (279 samples/sec)
2024-03-15 00:42:39.383322 epoch: 4 step: 2000 cls_loss= 1.42861 (277 samples/sec)
2024-03-15 00:43:36.698054 epoch: 4 step: 2500 cls_loss= 1.88000 (279 samples/sec)
2024-03-15 00:44:33.423323 epoch: 4 step: 3000 cls_loss= 1.76797 (282 samples/sec)
2024-03-15 00:47:31.037015------------------------------------------------------ Precision@1: 67.28%  Precision@1: 87.56%

top1: [66.8, 67.156, 67.18, 67.284]
top5: [87.214, 87.69200000000001, 87.78, 87.56400000000001]
2024-03-15 00:47:31.244532 epoch: 5 step: 0 cls_loss= 1.90405 (77529 samples/sec)
2024-03-15 00:48:27.889590 epoch: 5 step: 500 cls_loss= 2.02458 (282 samples/sec)
2024-03-15 00:49:23.887780 epoch: 5 step: 1000 cls_loss= 1.93422 (285 samples/sec)
2024-03-15 00:50:19.971129 epoch: 5 step: 1500 cls_loss= 1.69959 (285 samples/sec)
2024-03-15 00:51:16.602315 epoch: 5 step: 2000 cls_loss= 1.30059 (282 samples/sec)
2024-03-15 00:52:11.944540 epoch: 5 step: 2500 cls_loss= 1.76663 (289 samples/sec)
2024-03-15 00:53:07.569868 epoch: 5 step: 3000 cls_loss= 2.56270 (287 samples/sec)
2024-03-15 00:56:07.179346------------------------------------------------------ Precision@1: 67.28%  Precision@1: 87.67%

top1: [66.8, 67.156, 67.18, 67.284, 67.278]
top5: [87.214, 87.69200000000001, 87.78, 87.56400000000001, 87.674]
2024-03-15 00:56:07.379025 epoch: 6 step: 0 cls_loss= 1.72916 (80582 samples/sec)
2024-03-15 00:57:04.316342 epoch: 6 step: 500 cls_loss= 1.32712 (281 samples/sec)
2024-03-15 00:58:00.712229 epoch: 6 step: 1000 cls_loss= 2.21445 (283 samples/sec)
2024-03-15 00:58:57.063242 epoch: 6 step: 1500 cls_loss= 1.85013 (284 samples/sec)
2024-03-15 00:59:53.824106 epoch: 6 step: 2000 cls_loss= 1.75161 (281 samples/sec)
2024-03-15 01:00:50.472491 epoch: 6 step: 2500 cls_loss= 1.31221 (282 samples/sec)
2024-03-15 01:01:47.339209 epoch: 6 step: 3000 cls_loss= 2.08188 (281 samples/sec)
2024-03-15 01:04:44.744468------------------------------------------------------ Precision@1: 66.80%  Precision@1: 87.45%

top1: [66.8, 67.156, 67.18, 67.284, 67.278, 66.802]
top5: [87.214, 87.69200000000001, 87.78, 87.56400000000001, 87.674, 87.446]
2024-03-15 01:04:44.946283 epoch: 7 step: 0 cls_loss= 2.13788 (79712 samples/sec)
2024-03-15 01:05:41.949091 epoch: 7 step: 500 cls_loss= 1.87704 (280 samples/sec)
2024-03-15 01:06:39.124316 epoch: 7 step: 1000 cls_loss= 1.62698 (279 samples/sec)
2024-03-15 01:07:35.666297 epoch: 7 step: 1500 cls_loss= 1.67860 (283 samples/sec)
2024-03-15 01:08:33.367915 epoch: 7 step: 2000 cls_loss= 1.73796 (277 samples/sec)
2024-03-15 01:09:31.054164 epoch: 7 step: 2500 cls_loss= 1.46476 (277 samples/sec)
2024-03-15 01:10:28.488678 epoch: 7 step: 3000 cls_loss= 1.45232 (278 samples/sec)
2024-03-15 01:13:29.111558------------------------------------------------------ Precision@1: 67.47%  Precision@1: 87.76%

top1: [66.8, 67.156, 67.18, 67.284, 67.278, 66.802, 67.46600000000001]
top5: [87.214, 87.69200000000001, 87.78, 87.56400000000001, 87.674, 87.446, 87.756]
2024-03-15 01:13:29.318280 epoch: 8 step: 0 cls_loss= 2.52198 (77768 samples/sec)
2024-03-15 01:14:25.795426 epoch: 8 step: 500 cls_loss= 1.70603 (283 samples/sec)
2024-03-15 01:15:22.285883 epoch: 8 step: 1000 cls_loss= 1.79614 (283 samples/sec)
2024-03-15 01:16:18.906873 epoch: 8 step: 1500 cls_loss= 1.46192 (282 samples/sec)
2024-03-15 01:17:15.670042 epoch: 8 step: 2000 cls_loss= 1.12086 (281 samples/sec)
2024-03-15 01:18:12.279450 epoch: 8 step: 2500 cls_loss= 1.61053 (282 samples/sec)
2024-03-15 01:19:09.079454 epoch: 8 step: 3000 cls_loss= 1.79007 (281 samples/sec)
2024-03-15 01:22:06.709387------------------------------------------------------ Precision@1: 67.13%  Precision@1: 87.55%

top1: [66.8, 67.156, 67.18, 67.284, 67.278, 66.802, 67.46600000000001, 67.126]
top5: [87.214, 87.69200000000001, 87.78, 87.56400000000001, 87.674, 87.446, 87.756, 87.55]
2024-03-15 01:22:06.911451 epoch: 9 step: 0 cls_loss= 2.27231 (79637 samples/sec)
2024-03-15 01:23:03.744483 epoch: 9 step: 500 cls_loss= 1.91706 (281 samples/sec)
2024-03-15 01:24:00.807582 epoch: 9 step: 1000 cls_loss= 1.83128 (280 samples/sec)
2024-03-15 01:24:58.779048 epoch: 9 step: 1500 cls_loss= 1.29832 (276 samples/sec)
2024-03-15 01:25:56.006886 epoch: 9 step: 2000 cls_loss= 1.87950 (279 samples/sec)
2024-03-15 01:26:53.834430 epoch: 9 step: 2500 cls_loss= 1.29726 (276 samples/sec)
2024-03-15 01:27:51.653631 epoch: 9 step: 3000 cls_loss= 0.94206 (276 samples/sec)
2024-03-15 01:30:51.643493------------------------------------------------------ Precision@1: 67.15%  Precision@1: 87.54%

top1: [66.8, 67.156, 67.18, 67.284, 67.278, 66.802, 67.46600000000001, 67.126, 67.15]
top5: [87.214, 87.69200000000001, 87.78, 87.56400000000001, 87.674, 87.446, 87.756, 87.55, 87.542]
2024-03-15 01:30:51.843301 epoch: 10 step: 0 cls_loss= 2.27308 (80542 samples/sec)
2024-03-15 01:31:48.750959 epoch: 10 step: 500 cls_loss= 1.83727 (281 samples/sec)
2024-03-15 01:32:45.920423 epoch: 10 step: 1000 cls_loss= 1.36022 (279 samples/sec)
2024-03-15 01:33:43.564704 epoch: 10 step: 1500 cls_loss= 1.79337 (277 samples/sec)
2024-03-15 01:34:40.463758 epoch: 10 step: 2000 cls_loss= 2.05179 (281 samples/sec)
2024-03-15 01:35:38.480050 epoch: 10 step: 2500 cls_loss= 1.53391 (275 samples/sec)
2024-03-15 01:36:36.073250 epoch: 10 step: 3000 cls_loss= 2.10162 (277 samples/sec)
2024-03-15 01:39:37.612805------------------------------------------------------ Precision@1: 67.29%  Precision@1: 87.59%

top1: [66.8, 67.156, 67.18, 67.284, 67.278, 66.802, 67.46600000000001, 67.126, 67.15, 67.288]
top5: [87.214, 87.69200000000001, 87.78, 87.56400000000001, 87.674, 87.446, 87.756, 87.55, 87.542, 87.592]
=> creating model mobilenet_m1 ...
CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
./imgnet_train_eval.py:119: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-15 01:39:39.879336 epoch: 1 step: 0 cls_loss= 2.40850 (19414 samples/sec)
2024-03-15 01:40:36.382622 epoch: 1 step: 500 cls_loss= 1.98206 (283 samples/sec)
2024-03-15 01:41:32.788944 epoch: 1 step: 1000 cls_loss= 1.29290 (283 samples/sec)
2024-03-15 01:42:29.408271 epoch: 1 step: 1500 cls_loss= 1.83781 (282 samples/sec)
2024-03-15 01:43:25.802838 epoch: 1 step: 2000 cls_loss= 1.96889 (283 samples/sec)
2024-03-15 01:44:23.129389 epoch: 1 step: 2500 cls_loss= 2.21398 (279 samples/sec)
2024-03-15 01:45:19.649562 epoch: 1 step: 3000 cls_loss= 1.92980 (283 samples/sec)
2024-03-15 01:48:20.131687------------------------------------------------------ Precision@1: 67.20%  Precision@1: 87.62%

top1: [67.19800000000001]
top5: [87.622]
2024-03-15 01:48:20.322595 epoch: 2 step: 0 cls_loss= 2.39878 (84305 samples/sec)
2024-03-15 01:49:16.560112 epoch: 2 step: 500 cls_loss= 1.30836 (284 samples/sec)
2024-03-15 01:50:13.659427 epoch: 2 step: 1000 cls_loss= 1.86901 (280 samples/sec)
2024-03-15 01:51:10.856080 epoch: 2 step: 1500 cls_loss= 2.25017 (279 samples/sec)
2024-03-15 01:52:08.512516 epoch: 2 step: 2000 cls_loss= 1.66335 (277 samples/sec)
2024-03-15 01:53:05.889789 epoch: 2 step: 2500 cls_loss= 1.61179 (278 samples/sec)
2024-03-15 01:54:02.600885 epoch: 2 step: 3000 cls_loss= 1.94960 (282 samples/sec)
2024-03-15 01:57:04.636176------------------------------------------------------ Precision@1: 66.93%  Precision@1: 87.41%

top1: [67.19800000000001, 66.934]
top5: [87.622, 87.414]
2024-03-15 01:57:04.835304 epoch: 3 step: 0 cls_loss= 2.34195 (80788 samples/sec)
2024-03-15 01:58:01.486378 epoch: 3 step: 500 cls_loss= 1.58596 (282 samples/sec)
2024-03-15 01:58:57.650076 epoch: 3 step: 1000 cls_loss= 1.46631 (284 samples/sec)
2024-03-15 01:59:53.604649 epoch: 3 step: 1500 cls_loss= 2.28893 (286 samples/sec)
2024-03-15 02:00:50.584815 epoch: 3 step: 2000 cls_loss= 1.36529 (280 samples/sec)
2024-03-15 02:01:48.281753 epoch: 3 step: 2500 cls_loss= 2.01494 (277 samples/sec)
2024-03-15 02:02:45.962597 epoch: 3 step: 3000 cls_loss= 2.54433 (277 samples/sec)
2024-03-15 02:05:44.953004------------------------------------------------------ Precision@1: 67.33%  Precision@1: 87.72%

top1: [67.19800000000001, 66.934, 67.33200000000001]
top5: [87.622, 87.414, 87.71600000000001]
2024-03-15 02:05:45.158674 epoch: 4 step: 0 cls_loss= 1.84340 (78197 samples/sec)
2024-03-15 02:06:42.058898 epoch: 4 step: 500 cls_loss= 1.06737 (281 samples/sec)
2024-03-15 02:07:39.120401 epoch: 4 step: 1000 cls_loss= 1.56488 (280 samples/sec)
2024-03-15 02:08:36.091178 epoch: 4 step: 1500 cls_loss= 2.31425 (280 samples/sec)
2024-03-15 02:09:33.196743 epoch: 4 step: 2000 cls_loss= 1.59684 (280 samples/sec)
2024-03-15 02:10:30.549503 epoch: 4 step: 2500 cls_loss= 1.81774 (279 samples/sec)
2024-03-15 02:11:27.440514 epoch: 4 step: 3000 cls_loss= 1.46470 (281 samples/sec)
2024-03-15 02:14:24.542312------------------------------------------------------ Precision@1: 67.29%  Precision@1: 87.73%

top1: [67.19800000000001, 66.934, 67.33200000000001, 67.288]
top5: [87.622, 87.414, 87.71600000000001, 87.73]
2024-03-15 02:14:24.750944 epoch: 5 step: 0 cls_loss= 1.47002 (77133 samples/sec)
2024-03-15 02:15:21.969443 epoch: 5 step: 500 cls_loss= 2.06420 (279 samples/sec)
2024-03-15 02:16:19.482041 epoch: 5 step: 1000 cls_loss= 1.99193 (278 samples/sec)
2024-03-15 02:17:16.098396 epoch: 5 step: 1500 cls_loss= 2.35475 (282 samples/sec)
2024-03-15 02:18:13.854067 epoch: 5 step: 2000 cls_loss= 1.77491 (277 samples/sec)
2024-03-15 02:19:10.857116 epoch: 5 step: 2500 cls_loss= 2.08761 (280 samples/sec)
2024-03-15 02:20:08.345280 epoch: 5 step: 3000 cls_loss= 1.87791 (278 samples/sec)
2024-03-15 02:23:06.306462------------------------------------------------------ Precision@1: 67.37%  Precision@1: 87.73%

top1: [67.19800000000001, 66.934, 67.33200000000001, 67.288, 67.374]
top5: [87.622, 87.414, 87.71600000000001, 87.73, 87.73400000000001]
2024-03-15 02:23:06.542184 epoch: 6 step: 0 cls_loss= 2.07339 (68196 samples/sec)
2024-03-15 02:24:03.556928 epoch: 6 step: 500 cls_loss= 1.49370 (280 samples/sec)
2024-03-15 02:24:59.867223 epoch: 6 step: 1000 cls_loss= 1.81881 (284 samples/sec)
2024-03-15 02:25:56.557134 epoch: 6 step: 1500 cls_loss= 1.51473 (282 samples/sec)
2024-03-15 02:26:53.908650 epoch: 6 step: 2000 cls_loss= 2.23735 (279 samples/sec)
2024-03-15 02:27:51.023298 epoch: 6 step: 2500 cls_loss= 1.86526 (280 samples/sec)
2024-03-15 02:28:47.850075 epoch: 6 step: 3000 cls_loss= 1.63440 (281 samples/sec)
2024-03-15 02:31:48.995862------------------------------------------------------ Precision@1: 67.22%  Precision@1: 87.64%

top1: [67.19800000000001, 66.934, 67.33200000000001, 67.288, 67.374, 67.22]
top5: [87.622, 87.414, 87.71600000000001, 87.73, 87.73400000000001, 87.644]
2024-03-15 02:31:49.210973 epoch: 7 step: 0 cls_loss= 1.97742 (74789 samples/sec)
2024-03-15 02:32:45.900147 epoch: 7 step: 500 cls_loss= 1.85107 (282 samples/sec)
2024-03-15 02:33:43.296513 epoch: 7 step: 1000 cls_loss= 1.87204 (278 samples/sec)
2024-03-15 02:34:41.191456 epoch: 7 step: 1500 cls_loss= 1.16798 (276 samples/sec)
2024-03-15 02:35:38.372530 epoch: 7 step: 2000 cls_loss= 2.69142 (279 samples/sec)
2024-03-15 02:36:35.518426 epoch: 7 step: 2500 cls_loss= 1.75956 (280 samples/sec)
2024-03-15 02:37:32.452576 epoch: 7 step: 3000 cls_loss= 1.68401 (281 samples/sec)
2024-03-15 02:40:32.364542------------------------------------------------------ Precision@1: 67.08%  Precision@1: 87.64%

top1: [67.19800000000001, 66.934, 67.33200000000001, 67.288, 67.374, 67.22, 67.08200000000001]
top5: [87.622, 87.414, 87.71600000000001, 87.73, 87.73400000000001, 87.644, 87.642]
2024-03-15 02:40:32.565202 epoch: 8 step: 0 cls_loss= 2.19468 (80184 samples/sec)
2024-03-15 02:41:29.454851 epoch: 8 step: 500 cls_loss= 1.83706 (281 samples/sec)
2024-03-15 02:42:26.014267 epoch: 8 step: 1000 cls_loss= 1.90436 (282 samples/sec)
2024-03-15 02:43:23.400741 epoch: 8 step: 1500 cls_loss= 1.90705 (278 samples/sec)
2024-03-15 02:44:20.588967 epoch: 8 step: 2000 cls_loss= 1.84458 (279 samples/sec)
2024-03-15 02:45:17.302166 epoch: 8 step: 2500 cls_loss= 1.72097 (282 samples/sec)
2024-03-15 02:46:14.193977 epoch: 8 step: 3000 cls_loss= 1.45007 (281 samples/sec)
2024-03-15 02:49:15.659689------------------------------------------------------ Precision@1: 67.03%  Precision@1: 87.66%

top1: [67.19800000000001, 66.934, 67.33200000000001, 67.288, 67.374, 67.22, 67.08200000000001, 67.028]
top5: [87.622, 87.414, 87.71600000000001, 87.73, 87.73400000000001, 87.644, 87.642, 87.664]
2024-03-15 02:49:15.854401 epoch: 9 step: 0 cls_loss= 2.07985 (82676 samples/sec)
2024-03-15 02:50:13.099256 epoch: 9 step: 500 cls_loss= 1.45765 (279 samples/sec)
2024-03-15 02:51:10.603019 epoch: 9 step: 1000 cls_loss= 1.88629 (278 samples/sec)
2024-03-15 02:52:08.001633 epoch: 9 step: 1500 cls_loss= 2.12451 (278 samples/sec)
2024-03-15 02:53:04.991132 epoch: 9 step: 2000 cls_loss= 1.26419 (280 samples/sec)
2024-03-15 02:54:02.258137 epoch: 9 step: 2500 cls_loss= 2.35627 (279 samples/sec)
2024-03-15 02:54:59.618847 epoch: 9 step: 3000 cls_loss= 1.98265 (279 samples/sec)
2024-03-15 02:57:58.056407------------------------------------------------------ Precision@1: 67.17%  Precision@1: 87.65%

top1: [67.19800000000001, 66.934, 67.33200000000001, 67.288, 67.374, 67.22, 67.08200000000001, 67.028, 67.166]
top5: [87.622, 87.414, 87.71600000000001, 87.73, 87.73400000000001, 87.644, 87.642, 87.664, 87.652]
2024-03-15 02:57:58.259609 epoch: 10 step: 0 cls_loss= 1.77841 (79178 samples/sec)
2024-03-15 02:58:54.894114 epoch: 10 step: 500 cls_loss= 1.98649 (282 samples/sec)
2024-03-15 02:59:52.051745 epoch: 10 step: 1000 cls_loss= 1.68023 (280 samples/sec)
2024-03-15 03:00:49.379876 epoch: 10 step: 1500 cls_loss= 1.70891 (279 samples/sec)
2024-03-15 03:01:46.423921 epoch: 10 step: 2000 cls_loss= 1.61305 (280 samples/sec)
2024-03-15 03:02:42.752559 epoch: 10 step: 2500 cls_loss= 1.60848 (284 samples/sec)
2024-03-15 03:03:39.226004 epoch: 10 step: 3000 cls_loss= 1.92964 (283 samples/sec)
2024-03-15 03:06:36.303229------------------------------------------------------ Precision@1: 67.24%  Precision@1: 87.70%

top1: [67.19800000000001, 66.934, 67.33200000000001, 67.288, 67.374, 67.22, 67.08200000000001, 67.028, 67.166, 67.244]
top5: [87.622, 87.414, 87.71600000000001, 87.73, 87.73400000000001, 87.644, 87.642, 87.664, 87.652, 87.702]
=> creating model mobilenet_m1 ...
SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
./imgnet_train_eval.py:177: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-15 03:06:38.574274 epoch: 1 step: 0 cls_loss= 1.41723 (19538 samples/sec)
2024-03-15 03:07:34.548235 epoch: 1 step: 500 cls_loss= 1.80726 (285 samples/sec)
2024-03-15 03:08:31.614368 epoch: 1 step: 1000 cls_loss= 1.49628 (280 samples/sec)
2024-03-15 03:09:28.442273 epoch: 1 step: 1500 cls_loss= 1.95523 (281 samples/sec)
2024-03-15 03:10:25.980705 epoch: 1 step: 2000 cls_loss= 2.88018 (278 samples/sec)
2024-03-15 03:11:23.272719 epoch: 1 step: 2500 cls_loss= 2.04367 (279 samples/sec)
2024-03-15 03:12:20.131102 epoch: 1 step: 3000 cls_loss= 1.91570 (281 samples/sec)
2024-03-15 03:15:20.262574------------------------------------------------------ Precision@1: 66.80%  Precision@1: 87.40%

top1: [66.804]
top5: [87.404]
2024-03-15 03:15:20.458918 epoch: 2 step: 0 cls_loss= 2.08020 (82015 samples/sec)
2024-03-15 03:16:17.102072 epoch: 2 step: 500 cls_loss= 1.98203 (282 samples/sec)
2024-03-15 03:17:13.427845 epoch: 2 step: 1000 cls_loss= 2.21498 (284 samples/sec)
2024-03-15 03:18:09.463305 epoch: 2 step: 1500 cls_loss= 1.94942 (285 samples/sec)
2024-03-15 03:19:05.679921 epoch: 2 step: 2000 cls_loss= 1.44420 (284 samples/sec)
2024-03-15 03:20:01.495672 epoch: 2 step: 2500 cls_loss= 2.03532 (286 samples/sec)
2024-03-15 03:20:57.785645 epoch: 2 step: 3000 cls_loss= 1.52354 (284 samples/sec)
2024-03-15 03:23:55.319688------------------------------------------------------ Precision@1: 67.15%  Precision@1: 87.64%

top1: [66.804, 67.148]
top5: [87.404, 87.644]
2024-03-15 03:23:55.510347 epoch: 3 step: 0 cls_loss= 1.78851 (84431 samples/sec)
2024-03-15 03:24:51.686207 epoch: 3 step: 500 cls_loss= 1.75630 (284 samples/sec)
2024-03-15 03:25:48.397423 epoch: 3 step: 1000 cls_loss= 1.81153 (282 samples/sec)
2024-03-15 03:26:45.341674 epoch: 3 step: 1500 cls_loss= 2.38439 (281 samples/sec)
2024-03-15 03:27:42.990320 epoch: 3 step: 2000 cls_loss= 2.43623 (277 samples/sec)
2024-03-15 03:28:40.176147 epoch: 3 step: 2500 cls_loss= 1.97204 (279 samples/sec)
2024-03-15 03:29:36.998740 epoch: 3 step: 3000 cls_loss= 1.65772 (281 samples/sec)
2024-03-15 03:32:36.421765------------------------------------------------------ Precision@1: 67.22%  Precision@1: 87.77%

top1: [66.804, 67.148, 67.21600000000001]
top5: [87.404, 87.644, 87.768]
2024-03-15 03:32:36.670924 epoch: 4 step: 0 cls_loss= 1.87068 (64491 samples/sec)
2024-03-15 03:33:33.084055 epoch: 4 step: 500 cls_loss= 1.87268 (283 samples/sec)
2024-03-15 03:34:29.581610 epoch: 4 step: 1000 cls_loss= 1.48958 (283 samples/sec)
2024-03-15 03:35:26.645502 epoch: 4 step: 1500 cls_loss= 1.70067 (280 samples/sec)
2024-03-15 03:36:22.633727 epoch: 4 step: 2000 cls_loss= 1.54109 (285 samples/sec)
2024-03-15 03:37:19.585812 epoch: 4 step: 2500 cls_loss= 1.37496 (281 samples/sec)
2024-03-15 03:38:16.462689 epoch: 4 step: 3000 cls_loss= 1.54791 (281 samples/sec)
2024-03-15 03:41:14.562584------------------------------------------------------ Precision@1: 67.08%  Precision@1: 87.64%

top1: [66.804, 67.148, 67.21600000000001, 67.07600000000001]
top5: [87.404, 87.644, 87.768, 87.64]
2024-03-15 03:41:14.776566 epoch: 5 step: 0 cls_loss= 1.62178 (75181 samples/sec)
2024-03-15 03:42:10.156725 epoch: 5 step: 500 cls_loss= 1.98990 (288 samples/sec)
2024-03-15 03:43:05.571559 epoch: 5 step: 1000 cls_loss= 1.91484 (288 samples/sec)
2024-03-15 03:44:02.359747 epoch: 5 step: 1500 cls_loss= 1.73278 (281 samples/sec)
2024-03-15 03:44:58.429235 epoch: 5 step: 2000 cls_loss= 1.95152 (285 samples/sec)
2024-03-15 03:45:55.546166 epoch: 5 step: 2500 cls_loss= 1.67390 (280 samples/sec)
2024-03-15 03:46:52.653523 epoch: 5 step: 3000 cls_loss= 1.40074 (280 samples/sec)
2024-03-15 03:49:54.027748------------------------------------------------------ Precision@1: 67.10%  Precision@1: 87.61%

top1: [66.804, 67.148, 67.21600000000001, 67.07600000000001, 67.098]
top5: [87.404, 87.644, 87.768, 87.64, 87.612]
2024-03-15 03:49:54.271984 epoch: 6 step: 0 cls_loss= 1.16194 (65807 samples/sec)
2024-03-15 03:50:50.721425 epoch: 6 step: 500 cls_loss= 1.95912 (283 samples/sec)
2024-03-15 03:51:48.133538 epoch: 6 step: 1000 cls_loss= 1.73521 (278 samples/sec)
2024-03-15 03:52:45.593674 epoch: 6 step: 1500 cls_loss= 2.03847 (278 samples/sec)
2024-03-15 03:53:42.671918 epoch: 6 step: 2000 cls_loss= 2.27362 (280 samples/sec)
2024-03-15 03:54:39.939656 epoch: 6 step: 2500 cls_loss= 1.74449 (279 samples/sec)
2024-03-15 03:55:37.444150 epoch: 6 step: 3000 cls_loss= 1.82777 (278 samples/sec)
2024-03-15 03:58:37.748403------------------------------------------------------ Precision@1: 67.19%  Precision@1: 87.66%

top1: [66.804, 67.148, 67.21600000000001, 67.07600000000001, 67.098, 67.188]
top5: [87.404, 87.644, 87.768, 87.64, 87.612, 87.664]
2024-03-15 03:58:37.948772 epoch: 7 step: 0 cls_loss= 1.72168 (80306 samples/sec)
2024-03-15 03:59:34.242867 epoch: 7 step: 500 cls_loss= 1.97851 (284 samples/sec)
2024-03-15 04:00:30.952528 epoch: 7 step: 1000 cls_loss= 2.43860 (282 samples/sec)
2024-03-15 04:01:27.824116 epoch: 7 step: 1500 cls_loss= 1.60599 (281 samples/sec)
2024-03-15 04:02:25.469186 epoch: 7 step: 2000 cls_loss= 2.24636 (277 samples/sec)
2024-03-15 04:03:21.956132 epoch: 7 step: 2500 cls_loss= 1.64264 (283 samples/sec)
2024-03-15 04:04:18.209022 epoch: 7 step: 3000 cls_loss= 1.68342 (284 samples/sec)
2024-03-15 04:07:17.048184------------------------------------------------------ Precision@1: 67.22%  Precision@1: 87.84%

top1: [66.804, 67.148, 67.21600000000001, 67.07600000000001, 67.098, 67.188, 67.22]
top5: [87.404, 87.644, 87.768, 87.64, 87.612, 87.664, 87.836]
2024-03-15 04:07:17.252330 epoch: 8 step: 0 cls_loss= 1.30378 (78796 samples/sec)
2024-03-15 04:08:13.548211 epoch: 8 step: 500 cls_loss= 1.06155 (284 samples/sec)
2024-03-15 04:09:10.854038 epoch: 8 step: 1000 cls_loss= 1.87560 (279 samples/sec)
2024-03-15 04:10:07.928730 epoch: 8 step: 1500 cls_loss= 1.86166 (280 samples/sec)
2024-03-15 04:11:05.638334 epoch: 8 step: 2000 cls_loss= 2.08708 (277 samples/sec)
2024-03-15 04:12:03.923776 epoch: 8 step: 2500 cls_loss= 1.74554 (274 samples/sec)
2024-03-15 04:13:01.717288 epoch: 8 step: 3000 cls_loss= 2.42030 (276 samples/sec)
2024-03-15 04:15:58.787090------------------------------------------------------ Precision@1: 67.20%  Precision@1: 87.71%

top1: [66.804, 67.148, 67.21600000000001, 67.07600000000001, 67.098, 67.188, 67.22, 67.20400000000001]
top5: [87.404, 87.644, 87.768, 87.64, 87.612, 87.664, 87.836, 87.714]
2024-03-15 04:15:58.989372 epoch: 9 step: 0 cls_loss= 2.39383 (79574 samples/sec)
2024-03-15 04:16:54.534165 epoch: 9 step: 500 cls_loss= 1.77267 (288 samples/sec)
2024-03-15 04:17:50.735227 epoch: 9 step: 1000 cls_loss= 2.01421 (284 samples/sec)
2024-03-15 04:18:47.328697 epoch: 9 step: 1500 cls_loss= 2.21103 (282 samples/sec)
2024-03-15 04:19:44.365415 epoch: 9 step: 2000 cls_loss= 1.97181 (280 samples/sec)
2024-03-15 04:20:41.380498 epoch: 9 step: 2500 cls_loss= 1.57490 (280 samples/sec)
2024-03-15 04:21:37.632850 epoch: 9 step: 3000 cls_loss= 1.73989 (284 samples/sec)
2024-03-15 04:24:36.473951------------------------------------------------------ Precision@1: 67.27%  Precision@1: 87.62%

top1: [66.804, 67.148, 67.21600000000001, 67.07600000000001, 67.098, 67.188, 67.22, 67.20400000000001, 67.266]
top5: [87.404, 87.644, 87.768, 87.64, 87.612, 87.664, 87.836, 87.714, 87.616]
2024-03-15 04:24:36.675949 epoch: 10 step: 0 cls_loss= 1.97030 (79652 samples/sec)
2024-03-15 04:25:32.936426 epoch: 10 step: 500 cls_loss= 1.54859 (284 samples/sec)
2024-03-15 04:26:29.266190 epoch: 10 step: 1000 cls_loss= 1.86820 (284 samples/sec)
2024-03-15 04:27:25.287977 epoch: 10 step: 1500 cls_loss= 1.80344 (285 samples/sec)
2024-03-15 04:28:21.395236 epoch: 10 step: 2000 cls_loss= 1.28041 (285 samples/sec)
2024-03-15 04:29:17.752910 epoch: 10 step: 2500 cls_loss= 2.37569 (283 samples/sec)
2024-03-15 04:30:13.969863 epoch: 10 step: 3000 cls_loss= 1.39031 (284 samples/sec)
2024-03-15 04:33:14.227562------------------------------------------------------ Precision@1: 67.26%  Precision@1: 87.75%

top1: [66.804, 67.148, 67.21600000000001, 67.07600000000001, 67.098, 67.188, 67.22, 67.20400000000001, 67.266, 67.26]
top5: [87.404, 87.644, 87.768, 87.64, 87.612, 87.664, 87.836, 87.714, 87.616, 87.748]
=> creating model mobilenet_m1 ...
CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
./imgnet_train_eval.py:119: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-15 04:33:16.515048 epoch: 1 step: 0 cls_loss= 1.46705 (18769 samples/sec)
2024-03-15 04:34:13.469742 epoch: 1 step: 500 cls_loss= 1.56523 (280 samples/sec)
2024-03-15 04:35:10.062482 epoch: 1 step: 1000 cls_loss= 1.96364 (282 samples/sec)
2024-03-15 04:36:06.871580 epoch: 1 step: 1500 cls_loss= 1.56548 (281 samples/sec)
2024-03-15 04:37:03.865157 epoch: 1 step: 2000 cls_loss= 2.25701 (280 samples/sec)
2024-03-15 04:38:01.309705 epoch: 1 step: 2500 cls_loss= 1.60711 (278 samples/sec)
2024-03-15 04:38:58.359794 epoch: 1 step: 3000 cls_loss= 1.40324 (280 samples/sec)
2024-03-15 04:41:55.448146------------------------------------------------------ Precision@1: 67.15%  Precision@1: 87.60%

top1: [67.146]
top5: [87.596]
2024-03-15 04:41:55.645414 epoch: 2 step: 0 cls_loss= 2.12154 (81562 samples/sec)
2024-03-15 04:42:52.701876 epoch: 2 step: 500 cls_loss= 1.73964 (280 samples/sec)
2024-03-15 04:43:50.193466 epoch: 2 step: 1000 cls_loss= 2.07494 (278 samples/sec)
2024-03-15 04:44:46.483614 epoch: 2 step: 1500 cls_loss= 1.48659 (284 samples/sec)
2024-03-15 04:45:42.955436 epoch: 2 step: 2000 cls_loss= 2.25562 (283 samples/sec)
2024-03-15 04:46:39.722459 epoch: 2 step: 2500 cls_loss= 1.74367 (281 samples/sec)
2024-03-15 04:47:35.548670 epoch: 2 step: 3000 cls_loss= 1.29390 (286 samples/sec)
2024-03-15 04:50:35.492460------------------------------------------------------ Precision@1: 67.22%  Precision@1: 87.65%

top1: [67.146, 67.224]
top5: [87.596, 87.646]
2024-03-15 04:50:35.695420 epoch: 3 step: 0 cls_loss= 1.76146 (79255 samples/sec)
2024-03-15 04:51:31.778028 epoch: 3 step: 500 cls_loss= 1.82337 (285 samples/sec)
2024-03-15 04:52:28.462912 epoch: 3 step: 1000 cls_loss= 2.45606 (282 samples/sec)
2024-03-15 04:53:26.259035 epoch: 3 step: 1500 cls_loss= 1.67742 (276 samples/sec)
2024-03-15 04:54:23.730883 epoch: 3 step: 2000 cls_loss= 1.84077 (278 samples/sec)
2024-03-15 04:55:21.862522 epoch: 3 step: 2500 cls_loss= 1.95732 (275 samples/sec)
2024-03-15 04:56:19.492104 epoch: 3 step: 3000 cls_loss= 1.84676 (277 samples/sec)
2024-03-15 04:59:18.802550------------------------------------------------------ Precision@1: 67.24%  Precision@1: 87.66%

top1: [67.146, 67.224, 67.236]
top5: [87.596, 87.646, 87.664]
2024-03-15 04:59:19.063693 epoch: 4 step: 0 cls_loss= 2.67508 (61498 samples/sec)
2024-03-15 05:00:15.187566 epoch: 4 step: 500 cls_loss= 1.44500 (285 samples/sec)
2024-03-15 05:01:12.468040 epoch: 4 step: 1000 cls_loss= 1.15999 (279 samples/sec)
2024-03-15 05:02:08.676019 epoch: 4 step: 1500 cls_loss= 2.33831 (284 samples/sec)
2024-03-15 05:03:05.055975 epoch: 4 step: 2000 cls_loss= 1.76409 (283 samples/sec)
2024-03-15 05:04:02.199213 epoch: 4 step: 2500 cls_loss= 2.05116 (280 samples/sec)
2024-03-15 05:04:59.539433 epoch: 4 step: 3000 cls_loss= 1.11180 (279 samples/sec)
2024-03-15 05:07:56.828344------------------------------------------------------ Precision@1: 66.80%  Precision@1: 87.47%

top1: [67.146, 67.224, 67.236, 66.796]
top5: [87.596, 87.646, 87.664, 87.468]
2024-03-15 05:07:57.054993 epoch: 5 step: 0 cls_loss= 2.59531 (70927 samples/sec)
2024-03-15 05:08:53.836813 epoch: 5 step: 500 cls_loss= 1.50041 (281 samples/sec)
2024-03-15 05:09:50.918576 epoch: 5 step: 1000 cls_loss= 1.42254 (280 samples/sec)
2024-03-15 05:10:47.930412 epoch: 5 step: 1500 cls_loss= 1.52537 (280 samples/sec)
2024-03-15 05:11:44.889732 epoch: 5 step: 2000 cls_loss= 1.58059 (280 samples/sec)
2024-03-15 05:12:41.822178 epoch: 5 step: 2500 cls_loss= 1.90954 (281 samples/sec)
2024-03-15 05:13:39.277100 epoch: 5 step: 3000 cls_loss= 1.48892 (278 samples/sec)
2024-03-15 05:16:40.422620------------------------------------------------------ Precision@1: 66.69%  Precision@1: 87.31%

top1: [67.146, 67.224, 67.236, 66.796, 66.686]
top5: [87.596, 87.646, 87.664, 87.468, 87.31400000000001]
2024-03-15 05:16:40.631030 epoch: 6 step: 0 cls_loss= 1.97385 (77203 samples/sec)
2024-03-15 05:17:37.236716 epoch: 6 step: 500 cls_loss= 1.32923 (282 samples/sec)
2024-03-15 05:18:33.602517 epoch: 6 step: 1000 cls_loss= 1.36778 (283 samples/sec)
2024-03-15 05:19:29.398143 epoch: 6 step: 1500 cls_loss= 2.59269 (286 samples/sec)
2024-03-15 05:20:25.756507 epoch: 6 step: 2000 cls_loss= 1.21736 (283 samples/sec)
2024-03-15 05:21:22.805757 epoch: 6 step: 2500 cls_loss= 1.95215 (280 samples/sec)
2024-03-15 05:22:19.702845 epoch: 6 step: 3000 cls_loss= 2.51585 (281 samples/sec)
2024-03-15 05:25:17.548343------------------------------------------------------ Precision@1: 66.59%  Precision@1: 87.10%

top1: [67.146, 67.224, 67.236, 66.796, 66.686, 66.59400000000001]
top5: [87.596, 87.646, 87.664, 87.468, 87.31400000000001, 87.096]
2024-03-15 05:25:17.750391 epoch: 7 step: 0 cls_loss= 1.73592 (79605 samples/sec)
2024-03-15 05:26:15.229178 epoch: 7 step: 500 cls_loss= 1.78625 (278 samples/sec)
2024-03-15 05:27:12.521542 epoch: 7 step: 1000 cls_loss= 1.83090 (279 samples/sec)
2024-03-15 05:28:10.321467 epoch: 7 step: 1500 cls_loss= 1.51221 (276 samples/sec)
2024-03-15 05:29:08.379859 epoch: 7 step: 2000 cls_loss= 1.53114 (275 samples/sec)
2024-03-15 05:30:06.071702 epoch: 7 step: 2500 cls_loss= 2.09864 (277 samples/sec)
2024-03-15 05:31:02.122868 epoch: 7 step: 3000 cls_loss= 1.77126 (285 samples/sec)
2024-03-15 05:33:59.271313------------------------------------------------------ Precision@1: 67.17%  Precision@1: 87.68%

top1: [67.146, 67.224, 67.236, 66.796, 66.686, 66.59400000000001, 67.17]
top5: [87.596, 87.646, 87.664, 87.468, 87.31400000000001, 87.096, 87.678]
2024-03-15 05:33:59.476510 epoch: 8 step: 0 cls_loss= 1.77627 (78374 samples/sec)
2024-03-15 05:34:56.399574 epoch: 8 step: 500 cls_loss= 2.58440 (281 samples/sec)
2024-03-15 05:35:53.772000 epoch: 8 step: 1000 cls_loss= 1.38003 (278 samples/sec)
2024-03-15 05:36:50.959409 epoch: 8 step: 1500 cls_loss= 2.21267 (279 samples/sec)
2024-03-15 05:37:48.167515 epoch: 8 step: 2000 cls_loss= 2.10309 (279 samples/sec)
2024-03-15 05:38:45.714062 epoch: 8 step: 2500 cls_loss= 1.67821 (278 samples/sec)
2024-03-15 05:39:42.422102 epoch: 8 step: 3000 cls_loss= 1.67059 (282 samples/sec)
2024-03-15 05:42:39.390609------------------------------------------------------ Precision@1: 67.12%  Precision@1: 87.67%

top1: [67.146, 67.224, 67.236, 66.796, 66.686, 66.59400000000001, 67.17, 67.122]
top5: [87.596, 87.646, 87.664, 87.468, 87.31400000000001, 87.096, 87.678, 87.668]
2024-03-15 05:42:39.587668 epoch: 9 step: 0 cls_loss= 1.94529 (81612 samples/sec)
2024-03-15 05:43:36.482929 epoch: 9 step: 500 cls_loss= 2.07217 (281 samples/sec)
2024-03-15 05:44:33.522652 epoch: 9 step: 1000 cls_loss= 1.45609 (280 samples/sec)
2024-03-15 05:45:30.417489 epoch: 9 step: 1500 cls_loss= 1.39690 (281 samples/sec)
2024-03-15 05:46:27.040100 epoch: 9 step: 2000 cls_loss= 2.03378 (282 samples/sec)
2024-03-15 05:47:23.802563 epoch: 9 step: 2500 cls_loss= 2.12306 (281 samples/sec)
2024-03-15 05:48:21.589173 epoch: 9 step: 3000 cls_loss= 1.66043 (276 samples/sec)
2024-03-15 05:51:20.005708------------------------------------------------------ Precision@1: 67.11%  Precision@1: 87.64%

top1: [67.146, 67.224, 67.236, 66.796, 66.686, 66.59400000000001, 67.17, 67.122, 67.114]
top5: [87.596, 87.646, 87.664, 87.468, 87.31400000000001, 87.096, 87.678, 87.668, 87.636]
2024-03-15 05:51:20.207646 epoch: 10 step: 0 cls_loss= 2.29910 (79703 samples/sec)
2024-03-15 05:52:17.852348 epoch: 10 step: 500 cls_loss= 2.28344 (277 samples/sec)
2024-03-15 05:53:15.698492 epoch: 10 step: 1000 cls_loss= 2.09852 (276 samples/sec)
2024-03-15 05:54:13.529257 epoch: 10 step: 1500 cls_loss= 1.45729 (276 samples/sec)
2024-03-15 05:55:11.197210 epoch: 10 step: 2000 cls_loss= 1.82429 (277 samples/sec)
2024-03-15 05:56:09.072189 epoch: 10 step: 2500 cls_loss= 2.20149 (276 samples/sec)
2024-03-15 05:57:06.118027 epoch: 10 step: 3000 cls_loss= 1.41498 (280 samples/sec)
2024-03-15 06:00:05.455156------------------------------------------------------ Precision@1: 66.48%  Precision@1: 87.02%

top1: [67.146, 67.224, 67.236, 66.796, 66.686, 66.59400000000001, 67.17, 67.122, 67.114, 66.48]
top5: [87.596, 87.646, 87.664, 87.468, 87.31400000000001, 87.096, 87.678, 87.668, 87.636, 87.024]
=> creating model mobilenet_m1 ...
SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
./imgnet_train_eval.py:177: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-15 06:00:07.713696 epoch: 1 step: 0 cls_loss= 2.34346 (19936 samples/sec)
2024-03-15 06:01:03.725583 epoch: 1 step: 500 cls_loss= 1.78533 (285 samples/sec)
2024-03-15 06:02:00.738941 epoch: 1 step: 1000 cls_loss= 2.15824 (280 samples/sec)
2024-03-15 06:02:57.150947 epoch: 1 step: 1500 cls_loss= 2.30715 (283 samples/sec)
2024-03-15 06:03:53.580393 epoch: 1 step: 2000 cls_loss= 1.47173 (283 samples/sec)
2024-03-15 06:04:50.352038 epoch: 1 step: 2500 cls_loss= 1.75342 (281 samples/sec)
2024-03-15 06:05:47.095954 epoch: 1 step: 3000 cls_loss= 2.81108 (282 samples/sec)
2024-03-15 06:08:43.959154------------------------------------------------------ Precision@1: 66.72%  Precision@1: 87.35%

top1: [66.718]
top5: [87.354]
2024-03-15 06:08:44.161722 epoch: 2 step: 0 cls_loss= 2.01234 (79444 samples/sec)
2024-03-15 06:09:41.046183 epoch: 2 step: 500 cls_loss= 1.63039 (281 samples/sec)
2024-03-15 06:10:38.012841 epoch: 2 step: 1000 cls_loss= 1.52091 (280 samples/sec)
2024-03-15 06:11:34.997625 epoch: 2 step: 1500 cls_loss= 1.47658 (280 samples/sec)
2024-03-15 06:12:31.913127 epoch: 2 step: 2000 cls_loss= 2.19118 (281 samples/sec)
2024-03-15 06:13:28.887794 epoch: 2 step: 2500 cls_loss= 2.21683 (280 samples/sec)
2024-03-15 06:14:25.389006 epoch: 2 step: 3000 cls_loss= 2.11277 (283 samples/sec)
2024-03-15 06:17:23.565408------------------------------------------------------ Precision@1: 67.04%  Precision@1: 87.38%

top1: [66.718, 67.036]
top5: [87.354, 87.384]
2024-03-15 06:17:23.812344 epoch: 3 step: 0 cls_loss= 1.64444 (65094 samples/sec)
2024-03-15 06:18:20.442988 epoch: 3 step: 500 cls_loss= 1.81442 (282 samples/sec)
2024-03-15 06:19:15.701830 epoch: 3 step: 1000 cls_loss= 2.62647 (289 samples/sec)
2024-03-15 06:20:12.802680 epoch: 3 step: 1500 cls_loss= 1.96435 (280 samples/sec)
2024-03-15 06:21:09.524191 epoch: 3 step: 2000 cls_loss= 2.03076 (282 samples/sec)
2024-03-15 06:22:06.432209 epoch: 3 step: 2500 cls_loss= 1.92675 (281 samples/sec)
2024-03-15 06:23:03.565737 epoch: 3 step: 3000 cls_loss= 1.50405 (280 samples/sec)
2024-03-15 06:26:05.031053------------------------------------------------------ Precision@1: 66.96%  Precision@1: 87.57%

top1: [66.718, 67.036, 66.958]
top5: [87.354, 87.384, 87.572]
2024-03-15 06:26:05.228056 epoch: 4 step: 0 cls_loss= 1.55797 (81665 samples/sec)
2024-03-15 06:27:02.461386 epoch: 4 step: 500 cls_loss= 2.05901 (279 samples/sec)
2024-03-15 06:27:58.371854 epoch: 4 step: 1000 cls_loss= 2.44109 (286 samples/sec)
2024-03-15 06:28:55.410430 epoch: 4 step: 1500 cls_loss= 1.57341 (280 samples/sec)
2024-03-15 06:29:52.967230 epoch: 4 step: 2000 cls_loss= 2.40741 (278 samples/sec)
2024-03-15 06:30:50.573257 epoch: 4 step: 2500 cls_loss= 2.09617 (277 samples/sec)
2024-03-15 06:31:48.482777 epoch: 4 step: 3000 cls_loss= 1.45527 (276 samples/sec)
2024-03-15 06:34:49.490713------------------------------------------------------ Precision@1: 67.23%  Precision@1: 87.65%

top1: [66.718, 67.036, 66.958, 67.232]
top5: [87.354, 87.384, 87.572, 87.65]
2024-03-15 06:34:49.699976 epoch: 5 step: 0 cls_loss= 1.54927 (76861 samples/sec)
2024-03-15 06:35:47.014770 epoch: 5 step: 500 cls_loss= 1.81791 (279 samples/sec)
2024-03-15 06:36:44.536570 epoch: 5 step: 1000 cls_loss= 2.21125 (278 samples/sec)
2024-03-15 06:37:41.575998 epoch: 5 step: 1500 cls_loss= 1.52819 (280 samples/sec)
2024-03-15 06:38:39.087462 epoch: 5 step: 2000 cls_loss= 2.16201 (278 samples/sec)
2024-03-15 06:39:36.350709 epoch: 5 step: 2500 cls_loss= 1.89972 (279 samples/sec)
2024-03-15 06:40:33.814812 epoch: 5 step: 3000 cls_loss= 2.16511 (278 samples/sec)
2024-03-15 06:43:31.176917------------------------------------------------------ Precision@1: 67.16%  Precision@1: 87.60%

top1: [66.718, 67.036, 66.958, 67.232, 67.158]
top5: [87.354, 87.384, 87.572, 87.65, 87.60000000000001]
2024-03-15 06:43:31.377491 epoch: 6 step: 0 cls_loss= 2.89756 (80146 samples/sec)
2024-03-15 06:44:28.574279 epoch: 6 step: 500 cls_loss= 2.12469 (279 samples/sec)
2024-03-15 06:45:25.598377 epoch: 6 step: 1000 cls_loss= 2.42969 (280 samples/sec)
2024-03-15 06:46:23.058390 epoch: 6 step: 1500 cls_loss= 1.86465 (278 samples/sec)
2024-03-15 06:47:19.410522 epoch: 6 step: 2000 cls_loss= 2.28566 (284 samples/sec)
2024-03-15 06:48:15.888308 epoch: 6 step: 2500 cls_loss= 2.28276 (283 samples/sec)
2024-03-15 06:49:12.587812 epoch: 6 step: 3000 cls_loss= 1.53868 (282 samples/sec)
2024-03-15 06:52:12.510685------------------------------------------------------ Precision@1: 67.26%  Precision@1: 87.71%

top1: [66.718, 67.036, 66.958, 67.232, 67.158, 67.26]
top5: [87.354, 87.384, 87.572, 87.65, 87.60000000000001, 87.714]
2024-03-15 06:52:12.707186 epoch: 7 step: 0 cls_loss= 1.20314 (81829 samples/sec)
2024-03-15 06:53:09.529408 epoch: 7 step: 500 cls_loss= 2.35071 (281 samples/sec)
2024-03-15 06:54:05.757563 epoch: 7 step: 1000 cls_loss= 1.14864 (284 samples/sec)
2024-03-15 06:55:02.523899 epoch: 7 step: 1500 cls_loss= 2.43417 (281 samples/sec)
2024-03-15 06:55:59.530557 epoch: 7 step: 2000 cls_loss= 1.76470 (280 samples/sec)
2024-03-15 06:56:56.843257 epoch: 7 step: 2500 cls_loss= 2.01742 (279 samples/sec)
2024-03-15 06:57:53.779635 epoch: 7 step: 3000 cls_loss= 1.81225 (281 samples/sec)
2024-03-15 07:00:51.903288------------------------------------------------------ Precision@1: 66.91%  Precision@1: 87.49%

top1: [66.718, 67.036, 66.958, 67.232, 67.158, 67.26, 66.91]
top5: [87.354, 87.384, 87.572, 87.65, 87.60000000000001, 87.714, 87.488]
2024-03-15 07:00:52.119865 epoch: 8 step: 0 cls_loss= 1.99683 (74284 samples/sec)
2024-03-15 07:01:48.606150 epoch: 8 step: 500 cls_loss= 2.03268 (283 samples/sec)
2024-03-15 07:02:44.751628 epoch: 8 step: 1000 cls_loss= 2.18993 (285 samples/sec)
2024-03-15 07:03:42.131067 epoch: 8 step: 1500 cls_loss= 1.61897 (278 samples/sec)
2024-03-15 07:04:40.330272 epoch: 8 step: 2000 cls_loss= 2.28217 (274 samples/sec)
2024-03-15 07:05:37.756950 epoch: 8 step: 2500 cls_loss= 1.99757 (278 samples/sec)
2024-03-15 07:06:34.522079 epoch: 8 step: 3000 cls_loss= 2.22342 (281 samples/sec)
2024-03-15 07:09:31.579930------------------------------------------------------ Precision@1: 67.30%  Precision@1: 87.82%

top1: [66.718, 67.036, 66.958, 67.232, 67.158, 67.26, 66.91, 67.296]
top5: [87.354, 87.384, 87.572, 87.65, 87.60000000000001, 87.714, 87.488, 87.816]
2024-03-15 07:09:31.781105 epoch: 9 step: 0 cls_loss= 1.99185 (79988 samples/sec)
2024-03-15 07:10:27.875436 epoch: 9 step: 500 cls_loss= 1.72807 (285 samples/sec)
2024-03-15 07:11:24.570664 epoch: 9 step: 1000 cls_loss= 1.45213 (282 samples/sec)
2024-03-15 07:12:21.379742 epoch: 9 step: 1500 cls_loss= 1.85634 (281 samples/sec)
2024-03-15 07:13:17.414584 epoch: 9 step: 2000 cls_loss= 1.69089 (285 samples/sec)
2024-03-15 07:14:14.297134 epoch: 9 step: 2500 cls_loss= 2.03824 (281 samples/sec)
2024-03-15 07:15:10.930549 epoch: 9 step: 3000 cls_loss= 2.83040 (282 samples/sec)
2024-03-15 07:18:09.039590------------------------------------------------------ Precision@1: 67.00%  Precision@1: 87.59%

top1: [66.718, 67.036, 66.958, 67.232, 67.158, 67.26, 66.91, 67.296, 67.004]
top5: [87.354, 87.384, 87.572, 87.65, 87.60000000000001, 87.714, 87.488, 87.816, 87.59400000000001]
2024-03-15 07:18:09.242533 epoch: 10 step: 0 cls_loss= 1.99570 (79269 samples/sec)
2024-03-15 07:19:06.162013 epoch: 10 step: 500 cls_loss= 1.73742 (281 samples/sec)
2024-03-15 07:20:02.930638 epoch: 10 step: 1000 cls_loss= 1.39032 (281 samples/sec)
2024-03-15 07:21:00.228576 epoch: 10 step: 1500 cls_loss= 1.19391 (279 samples/sec)
2024-03-15 07:21:56.859733 epoch: 10 step: 2000 cls_loss= 1.74831 (282 samples/sec)
2024-03-15 07:22:53.543953 epoch: 10 step: 2500 cls_loss= 2.76145 (282 samples/sec)
2024-03-15 07:23:50.208023 epoch: 10 step: 3000 cls_loss= 1.94825 (282 samples/sec)
2024-03-15 07:26:48.964607------------------------------------------------------ Precision@1: 67.47%  Precision@1: 87.63%

top1: [66.718, 67.036, 66.958, 67.232, 67.158, 67.26, 66.91, 67.296, 67.004, 67.47]
top5: [87.354, 87.384, 87.572, 87.65, 87.60000000000001, 87.714, 87.488, 87.816, 87.59400000000001, 87.632]
=> creating model mobilenet_m1 ...
CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
./imgnet_train_eval.py:119: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-15 07:26:51.232679 epoch: 1 step: 0 cls_loss= 2.24929 (19274 samples/sec)
2024-03-15 07:27:47.621976 epoch: 1 step: 500 cls_loss= 2.61483 (283 samples/sec)
2024-03-15 07:28:43.709585 epoch: 1 step: 1000 cls_loss= 2.09679 (285 samples/sec)
2024-03-15 07:29:40.129538 epoch: 1 step: 1500 cls_loss= 1.72208 (283 samples/sec)
2024-03-15 07:30:36.499959 epoch: 1 step: 2000 cls_loss= 2.68281 (283 samples/sec)
2024-03-15 07:31:33.308281 epoch: 1 step: 2500 cls_loss= 2.29702 (281 samples/sec)
2024-03-15 07:32:30.286042 epoch: 1 step: 3000 cls_loss= 1.85891 (280 samples/sec)
2024-03-15 07:35:26.562160------------------------------------------------------ Precision@1: 66.53%  Precision@1: 87.22%

top1: [66.534]
top5: [87.218]
2024-03-15 07:35:26.808259 epoch: 2 step: 0 cls_loss= 2.50069 (65302 samples/sec)
2024-03-15 07:36:23.471744 epoch: 2 step: 500 cls_loss= 1.98575 (282 samples/sec)
2024-03-15 07:37:20.302095 epoch: 2 step: 1000 cls_loss= 2.54273 (281 samples/sec)
2024-03-15 07:38:17.562340 epoch: 2 step: 1500 cls_loss= 2.03723 (279 samples/sec)
2024-03-15 07:39:14.663020 epoch: 2 step: 2000 cls_loss= 2.20436 (280 samples/sec)
2024-03-15 07:40:12.131069 epoch: 2 step: 2500 cls_loss= 1.67628 (278 samples/sec)
2024-03-15 07:41:09.516567 epoch: 2 step: 3000 cls_loss= 1.67928 (278 samples/sec)
2024-03-15 07:44:09.921353------------------------------------------------------ Precision@1: 66.70%  Precision@1: 87.18%

top1: [66.534, 66.7]
top5: [87.218, 87.178]
2024-03-15 07:44:10.135270 epoch: 3 step: 0 cls_loss= 1.96861 (75084 samples/sec)
2024-03-15 07:45:06.382206 epoch: 3 step: 500 cls_loss= 1.69432 (284 samples/sec)
2024-03-15 07:46:03.790311 epoch: 3 step: 1000 cls_loss= 1.67971 (278 samples/sec)
2024-03-15 07:47:00.501360 epoch: 3 step: 1500 cls_loss= 1.44653 (282 samples/sec)
2024-03-15 07:47:57.364044 epoch: 3 step: 2000 cls_loss= 2.09735 (281 samples/sec)
2024-03-15 07:48:54.267549 epoch: 3 step: 2500 cls_loss= 2.19711 (281 samples/sec)
2024-03-15 07:49:51.408191 epoch: 3 step: 3000 cls_loss= 2.00595 (280 samples/sec)
2024-03-15 07:52:51.449771------------------------------------------------------ Precision@1: 67.08%  Precision@1: 87.43%

top1: [66.534, 66.7, 67.078]
top5: [87.218, 87.178, 87.428]
2024-03-15 07:52:51.646138 epoch: 4 step: 0 cls_loss= 2.03514 (81947 samples/sec)
2024-03-15 07:53:48.668345 epoch: 4 step: 500 cls_loss= 2.18780 (280 samples/sec)
2024-03-15 07:54:45.434152 epoch: 4 step: 1000 cls_loss= 2.20917 (281 samples/sec)
2024-03-15 07:55:42.899133 epoch: 4 step: 1500 cls_loss= 2.27050 (278 samples/sec)
2024-03-15 07:56:40.390467 epoch: 4 step: 2000 cls_loss= 2.27629 (278 samples/sec)
2024-03-15 07:57:38.275666 epoch: 4 step: 2500 cls_loss= 1.47158 (276 samples/sec)
2024-03-15 07:58:35.855286 epoch: 4 step: 3000 cls_loss= 1.42181 (277 samples/sec)
2024-03-15 08:01:35.969860------------------------------------------------------ Precision@1: 67.00%  Precision@1: 87.58%

top1: [66.534, 66.7, 67.078, 66.998]
top5: [87.218, 87.178, 87.428, 87.58200000000001]
2024-03-15 08:01:36.172582 epoch: 5 step: 0 cls_loss= 2.77065 (79371 samples/sec)
2024-03-15 08:02:33.024336 epoch: 5 step: 500 cls_loss= 1.34004 (281 samples/sec)
2024-03-15 08:03:29.245207 epoch: 5 step: 1000 cls_loss= 1.62237 (284 samples/sec)
2024-03-15 08:04:26.402301 epoch: 5 step: 1500 cls_loss= 1.95586 (280 samples/sec)
2024-03-15 08:05:23.332162 epoch: 5 step: 2000 cls_loss= 2.57750 (281 samples/sec)
2024-03-15 08:06:19.978619 epoch: 5 step: 2500 cls_loss= 1.84403 (282 samples/sec)
2024-03-15 08:07:16.300896 epoch: 5 step: 3000 cls_loss= 1.74021 (284 samples/sec)
2024-03-15 08:10:16.791853------------------------------------------------------ Precision@1: 67.13%  Precision@1: 87.59%

top1: [66.534, 66.7, 67.078, 66.998, 67.132]
top5: [87.218, 87.178, 87.428, 87.58200000000001, 87.59400000000001]
2024-03-15 08:10:16.986289 epoch: 6 step: 0 cls_loss= 2.33043 (82774 samples/sec)
2024-03-15 08:11:13.304064 epoch: 6 step: 500 cls_loss= 1.95452 (284 samples/sec)
2024-03-15 08:12:09.532371 epoch: 6 step: 1000 cls_loss= 1.69602 (284 samples/sec)
2024-03-15 08:13:05.784303 epoch: 6 step: 1500 cls_loss= 2.27389 (284 samples/sec)
2024-03-15 08:14:02.632436 epoch: 6 step: 2000 cls_loss= 1.20512 (281 samples/sec)
2024-03-15 08:14:58.633674 epoch: 6 step: 2500 cls_loss= 2.13652 (285 samples/sec)
2024-03-15 08:15:56.317955 epoch: 6 step: 3000 cls_loss= 2.36387 (277 samples/sec)
2024-03-15 08:18:52.950617------------------------------------------------------ Precision@1: 67.02%  Precision@1: 87.58%

top1: [66.534, 66.7, 67.078, 66.998, 67.132, 67.016]
top5: [87.218, 87.178, 87.428, 87.58200000000001, 87.59400000000001, 87.57600000000001]
2024-03-15 08:18:53.151951 epoch: 7 step: 0 cls_loss= 2.31587 (79930 samples/sec)
2024-03-15 08:19:50.372746 epoch: 7 step: 500 cls_loss= 1.83841 (279 samples/sec)
2024-03-15 08:20:47.112281 epoch: 7 step: 1000 cls_loss= 1.82007 (282 samples/sec)
2024-03-15 08:21:43.262330 epoch: 7 step: 1500 cls_loss= 1.50269 (285 samples/sec)
2024-03-15 08:22:39.604086 epoch: 7 step: 2000 cls_loss= 1.61618 (284 samples/sec)
2024-03-15 08:23:36.215617 epoch: 7 step: 2500 cls_loss= 2.45502 (282 samples/sec)
2024-03-15 08:24:32.204039 epoch: 7 step: 3000 cls_loss= 1.80713 (285 samples/sec)
2024-03-15 08:27:29.663822------------------------------------------------------ Precision@1: 67.31%  Precision@1: 87.75%

top1: [66.534, 66.7, 67.078, 66.998, 67.132, 67.016, 67.306]
top5: [87.218, 87.178, 87.428, 87.58200000000001, 87.59400000000001, 87.57600000000001, 87.75]
2024-03-15 08:27:29.865041 epoch: 8 step: 0 cls_loss= 1.92611 (79896 samples/sec)
2024-03-15 08:28:26.631881 epoch: 8 step: 500 cls_loss= 2.02524 (281 samples/sec)
2024-03-15 08:29:23.374890 epoch: 8 step: 1000 cls_loss= 1.87827 (282 samples/sec)
2024-03-15 08:30:20.256394 epoch: 8 step: 1500 cls_loss= 1.72714 (281 samples/sec)
2024-03-15 08:31:16.751390 epoch: 8 step: 2000 cls_loss= 1.86794 (283 samples/sec)
2024-03-15 08:32:13.114173 epoch: 8 step: 2500 cls_loss= 2.00848 (283 samples/sec)
2024-03-15 08:33:10.165341 epoch: 8 step: 3000 cls_loss= 1.51546 (280 samples/sec)
2024-03-15 08:36:12.957928------------------------------------------------------ Precision@1: 66.99%  Precision@1: 87.60%

top1: [66.534, 66.7, 67.078, 66.998, 67.132, 67.016, 67.306, 66.988]
top5: [87.218, 87.178, 87.428, 87.58200000000001, 87.59400000000001, 87.57600000000001, 87.75, 87.60000000000001]
2024-03-15 08:36:13.160371 epoch: 9 step: 0 cls_loss= 2.21782 (79435 samples/sec)
2024-03-15 08:37:09.418691 epoch: 9 step: 500 cls_loss= 1.33747 (284 samples/sec)
2024-03-15 08:38:05.798273 epoch: 9 step: 1000 cls_loss= 1.75069 (283 samples/sec)
2024-03-15 08:39:01.776178 epoch: 9 step: 1500 cls_loss= 1.95060 (285 samples/sec)
2024-03-15 08:39:57.855712 epoch: 9 step: 2000 cls_loss= 1.69901 (285 samples/sec)
2024-03-15 08:40:53.919920 epoch: 9 step: 2500 cls_loss= 1.43543 (285 samples/sec)
2024-03-15 08:41:50.895914 epoch: 9 step: 3000 cls_loss= 2.38067 (280 samples/sec)
2024-03-15 08:44:49.320891------------------------------------------------------ Precision@1: 67.37%  Precision@1: 87.82%

top1: [66.534, 66.7, 67.078, 66.998, 67.132, 67.016, 67.306, 66.988, 67.37]
top5: [87.218, 87.178, 87.428, 87.58200000000001, 87.59400000000001, 87.57600000000001, 87.75, 87.60000000000001, 87.822]
2024-03-15 08:44:49.522795 epoch: 10 step: 0 cls_loss= 2.78043 (79697 samples/sec)
2024-03-15 08:45:46.590053 epoch: 10 step: 500 cls_loss= 1.74005 (280 samples/sec)
2024-03-15 08:46:43.526446 epoch: 10 step: 1000 cls_loss= 2.29259 (281 samples/sec)
2024-03-15 08:47:40.925951 epoch: 10 step: 1500 cls_loss= 1.29159 (278 samples/sec)
2024-03-15 08:48:38.380287 epoch: 10 step: 2000 cls_loss= 2.44134 (278 samples/sec)
2024-03-15 08:49:35.857925 epoch: 10 step: 2500 cls_loss= 1.71945 (278 samples/sec)
2024-03-15 08:50:33.362785 epoch: 10 step: 3000 cls_loss= 1.86669 (278 samples/sec)
2024-03-15 08:53:33.466289------------------------------------------------------ Precision@1: 67.26%  Precision@1: 87.72%

top1: [66.534, 66.7, 67.078, 66.998, 67.132, 67.016, 67.306, 66.988, 67.37, 67.26]
top5: [87.218, 87.178, 87.428, 87.58200000000001, 87.59400000000001, 87.57600000000001, 87.75, 87.60000000000001, 87.822, 87.724]
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ bash bash_train2.sh 
=> creating model mobilenet_m1 ...
SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
./imgnet_train_eval.py:177: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-15 09:20:37.089005 epoch: 1 step: 0 cls_loss= 2.44977 (19713 samples/sec)
2024-03-15 09:21:33.907458 epoch: 1 step: 500 cls_loss= 1.62306 (281 samples/sec)
2024-03-15 09:22:30.531037 epoch: 1 step: 1000 cls_loss= 2.47409 (282 samples/sec)
2024-03-15 09:23:28.092441 epoch: 1 step: 1500 cls_loss= 1.98552 (278 samples/sec)
2024-03-15 09:24:25.878609 epoch: 1 step: 2000 cls_loss= 1.88841 (276 samples/sec)
2024-03-15 09:25:22.921967 epoch: 1 step: 2500 cls_loss= 1.72848 (280 samples/sec)
2024-03-15 09:26:20.439584 epoch: 1 step: 3000 cls_loss= 2.41156 (278 samples/sec)
2024-03-15 09:29:21.736338------------------------------------------------------ Precision@1: 66.32%  Precision@1: 87.01%

top1: [66.322]
top5: [87.006]
2024-03-15 09:29:21.952725 epoch: 2 step: 0 cls_loss= 1.73950 (74346 samples/sec)
2024-03-15 09:30:19.438363 epoch: 2 step: 500 cls_loss= 1.59402 (278 samples/sec)
2024-03-15 09:31:16.817546 epoch: 2 step: 1000 cls_loss= 1.60214 (278 samples/sec)
2024-03-15 09:32:14.120100 epoch: 2 step: 1500 cls_loss= 1.46567 (279 samples/sec)
2024-03-15 09:33:11.468338 epoch: 2 step: 2000 cls_loss= 2.02540 (279 samples/sec)
2024-03-15 09:34:08.753350 epoch: 2 step: 2500 cls_loss= 2.52098 (279 samples/sec)
2024-03-15 09:35:06.990840 epoch: 2 step: 3000 cls_loss= 2.27138 (274 samples/sec)
2024-03-15 09:38:07.009050------------------------------------------------------ Precision@1: 66.38%  Precision@1: 87.06%

top1: [66.322, 66.38]
top5: [87.006, 87.062]
2024-03-15 09:38:07.212596 epoch: 3 step: 0 cls_loss= 2.31965 (79081 samples/sec)
2024-03-15 09:39:03.923765 epoch: 3 step: 500 cls_loss= 1.62369 (282 samples/sec)
2024-03-15 09:40:00.958300 epoch: 3 step: 1000 cls_loss= 2.48436 (280 samples/sec)
2024-03-15 09:40:57.901323 epoch: 3 step: 1500 cls_loss= 1.82155 (281 samples/sec)
2024-03-15 09:41:55.116799 epoch: 3 step: 2000 cls_loss= 1.81650 (279 samples/sec)
2024-03-15 09:42:51.970453 epoch: 3 step: 2500 cls_loss= 2.28029 (281 samples/sec)
2024-03-15 09:43:48.391349 epoch: 3 step: 3000 cls_loss= 1.92658 (283 samples/sec)
2024-03-15 09:46:49.649873------------------------------------------------------ Precision@1: 66.50%  Precision@1: 87.20%

top1: [66.322, 66.38, 66.504]
top5: [87.006, 87.062, 87.20400000000001]
2024-03-15 09:46:49.864159 epoch: 4 step: 0 cls_loss= 1.42000 (75066 samples/sec)
2024-03-15 09:47:47.835490 epoch: 4 step: 500 cls_loss= 1.31081 (276 samples/sec)
2024-03-15 09:48:45.828016 epoch: 4 step: 1000 cls_loss= 2.03345 (275 samples/sec)
2024-03-15 09:49:43.516174 epoch: 4 step: 1500 cls_loss= 1.91092 (277 samples/sec)
2024-03-15 09:50:41.127311 epoch: 4 step: 2000 cls_loss= 2.16791 (277 samples/sec)
2024-03-15 09:51:38.978645 epoch: 4 step: 2500 cls_loss= 1.93006 (276 samples/sec)
2024-03-15 09:52:36.786386 epoch: 4 step: 3000 cls_loss= 1.43614 (276 samples/sec)
2024-03-15 09:55:39.618637------------------------------------------------------ Precision@1: 66.71%  Precision@1: 87.28%

top1: [66.322, 66.38, 66.504, 66.712]
top5: [87.006, 87.062, 87.20400000000001, 87.278]
2024-03-15 09:55:39.817660 epoch: 5 step: 0 cls_loss= 2.67402 (80826 samples/sec)
2024-03-15 09:56:37.463954 epoch: 5 step: 500 cls_loss= 1.36740 (277 samples/sec)
2024-03-15 09:57:35.655097 epoch: 5 step: 1000 cls_loss= 1.86920 (275 samples/sec)
2024-03-15 09:58:32.876766 epoch: 5 step: 1500 cls_loss= 2.29262 (279 samples/sec)
2024-03-15 09:59:30.159357 epoch: 5 step: 2000 cls_loss= 1.72764 (279 samples/sec)
2024-03-15 10:00:27.852087 epoch: 5 step: 2500 cls_loss= 1.45916 (277 samples/sec)
2024-03-15 10:01:25.230103 epoch: 5 step: 3000 cls_loss= 2.59809 (278 samples/sec)
2024-03-15 10:04:30.400454------------------------------------------------------ Precision@1: 66.53%  Precision@1: 87.20%

top1: [66.322, 66.38, 66.504, 66.712, 66.53]
top5: [87.006, 87.062, 87.20400000000001, 87.278, 87.20400000000001]
2024-03-15 10:04:30.597798 epoch: 6 step: 0 cls_loss= 1.55917 (81567 samples/sec)
2024-03-15 10:05:28.540584 epoch: 6 step: 500 cls_loss= 1.79333 (276 samples/sec)
2024-03-15 10:06:26.399126 epoch: 6 step: 1000 cls_loss= 1.40930 (276 samples/sec)
2024-03-15 10:07:24.138346 epoch: 6 step: 1500 cls_loss= 1.93308 (277 samples/sec)
2024-03-15 10:08:21.938645 epoch: 6 step: 2000 cls_loss= 2.20446 (276 samples/sec)
2024-03-15 10:09:20.373899 epoch: 6 step: 2500 cls_loss= 1.65648 (273 samples/sec)
2024-03-15 10:10:18.375756 epoch: 6 step: 3000 cls_loss= 1.78842 (275 samples/sec)
2024-03-15 10:13:19.813220------------------------------------------------------ Precision@1: 67.02%  Precision@1: 87.43%

top1: [66.322, 66.38, 66.504, 66.712, 66.53, 67.016]
top5: [87.006, 87.062, 87.20400000000001, 87.278, 87.20400000000001, 87.426]
2024-03-15 10:13:20.011798 epoch: 7 step: 0 cls_loss= 1.70517 (81052 samples/sec)
2024-03-15 10:14:17.785695 epoch: 7 step: 500 cls_loss= 2.61458 (277 samples/sec)
2024-03-15 10:15:15.171792 epoch: 7 step: 1000 cls_loss= 2.05693 (278 samples/sec)
2024-03-15 10:16:13.326233 epoch: 7 step: 1500 cls_loss= 1.55214 (275 samples/sec)
2024-03-15 10:17:11.901411 epoch: 7 step: 2000 cls_loss= 1.00659 (273 samples/sec)
2024-03-15 10:18:10.465716 epoch: 7 step: 2500 cls_loss= 1.61878 (273 samples/sec)
2024-03-15 10:19:08.151276 epoch: 7 step: 3000 cls_loss= 2.09985 (277 samples/sec)
2024-03-15 10:22:12.829553------------------------------------------------------ Precision@1: 66.06%  Precision@1: 86.75%

top1: [66.322, 66.38, 66.504, 66.712, 66.53, 67.016, 66.06]
top5: [87.006, 87.062, 87.20400000000001, 87.278, 87.20400000000001, 87.426, 86.754]
2024-03-15 10:22:13.035643 epoch: 8 step: 0 cls_loss= 2.28736 (78055 samples/sec)
2024-03-15 10:23:10.453499 epoch: 8 step: 500 cls_loss= 1.76949 (278 samples/sec)
2024-03-15 10:24:08.127283 epoch: 8 step: 1000 cls_loss= 1.84121 (277 samples/sec)
2024-03-15 10:25:05.894052 epoch: 8 step: 1500 cls_loss= 1.83903 (277 samples/sec)
2024-03-15 10:26:04.127225 epoch: 8 step: 2000 cls_loss= 1.30247 (274 samples/sec)
2024-03-15 10:27:02.517192 epoch: 8 step: 2500 cls_loss= 1.50750 (274 samples/sec)
2024-03-15 10:28:01.405741 epoch: 8 step: 3000 cls_loss= 1.85855 (271 samples/sec)
2024-03-15 10:31:04.558022------------------------------------------------------ Precision@1: 66.97%  Precision@1: 87.41%

top1: [66.322, 66.38, 66.504, 66.712, 66.53, 67.016, 66.06, 66.968]
top5: [87.006, 87.062, 87.20400000000001, 87.278, 87.20400000000001, 87.426, 86.754, 87.406]
2024-03-15 10:31:04.765595 epoch: 9 step: 0 cls_loss= 1.74236 (77474 samples/sec)
2024-03-15 10:32:02.993385 epoch: 9 step: 500 cls_loss= 1.63006 (274 samples/sec)
2024-03-15 10:33:00.834963 epoch: 9 step: 1000 cls_loss= 2.09508 (276 samples/sec)
2024-03-15 10:33:58.577391 epoch: 9 step: 1500 cls_loss= 2.06294 (277 samples/sec)
2024-03-15 10:34:56.601419 epoch: 9 step: 2000 cls_loss= 2.56672 (275 samples/sec)
2024-03-15 10:35:53.680449 epoch: 9 step: 2500 cls_loss= 1.52692 (280 samples/sec)
2024-03-15 10:36:51.758222 epoch: 9 step: 3000 cls_loss= 1.93758 (275 samples/sec)
2024-03-15 10:39:54.548693------------------------------------------------------ Precision@1: 67.02%  Precision@1: 87.55%

top1: [66.322, 66.38, 66.504, 66.712, 66.53, 67.016, 66.06, 66.968, 67.024]
top5: [87.006, 87.062, 87.20400000000001, 87.278, 87.20400000000001, 87.426, 86.754, 87.406, 87.548]
2024-03-15 10:39:54.787418 epoch: 10 step: 0 cls_loss= 1.51788 (67378 samples/sec)
2024-03-15 10:40:52.486742 epoch: 10 step: 500 cls_loss= 1.91550 (277 samples/sec)
2024-03-15 10:41:49.441590 epoch: 10 step: 1000 cls_loss= 1.94531 (281 samples/sec)
2024-03-15 10:42:46.583098 epoch: 10 step: 1500 cls_loss= 2.65439 (280 samples/sec)
2024-03-15 10:43:44.719313 epoch: 10 step: 2000 cls_loss= 1.37427 (275 samples/sec)
2024-03-15 10:44:42.580140 epoch: 10 step: 2500 cls_loss= 1.90835 (276 samples/sec)
2024-03-15 10:45:40.479164 epoch: 10 step: 3000 cls_loss= 1.86486 (276 samples/sec)
2024-03-15 10:48:42.422250------------------------------------------------------ Precision@1: 66.97%  Precision@1: 87.48%

top1: [66.322, 66.38, 66.504, 66.712, 66.53, 67.016, 66.06, 66.968, 67.024, 66.97]
top5: [87.006, 87.062, 87.20400000000001, 87.278, 87.20400000000001, 87.426, 86.754, 87.406, 87.548, 87.476]
=> creating model mobilenet_m1 ...
CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
./imgnet_train_eval.py:119: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-15 10:48:44.712732 epoch: 1 step: 0 cls_loss= 2.18129 (20168 samples/sec)
2024-03-15 10:49:41.808577 epoch: 1 step: 500 cls_loss= 1.65074 (280 samples/sec)
2024-03-15 10:50:38.792903 epoch: 1 step: 1000 cls_loss= 2.03263 (280 samples/sec)
2024-03-15 10:51:36.244035 epoch: 1 step: 1500 cls_loss= 2.03812 (278 samples/sec)
2024-03-15 10:52:33.314731 epoch: 1 step: 2000 cls_loss= 1.50543 (280 samples/sec)
2024-03-15 10:53:30.766674 epoch: 1 step: 2500 cls_loss= 1.90354 (278 samples/sec)
2024-03-15 10:54:27.206274 epoch: 1 step: 3000 cls_loss= 2.04595 (283 samples/sec)
2024-03-15 10:57:26.422888------------------------------------------------------ Precision@1: 66.11%  Precision@1: 86.97%

top1: [66.108]
top5: [86.97200000000001]
2024-03-15 10:57:26.639051 epoch: 2 step: 0 cls_loss= 1.93902 (74405 samples/sec)
2024-03-15 10:58:23.912206 epoch: 2 step: 500 cls_loss= 2.29692 (279 samples/sec)
2024-03-15 10:59:21.185983 epoch: 2 step: 1000 cls_loss= 1.93349 (279 samples/sec)
2024-03-15 11:00:18.384034 epoch: 2 step: 1500 cls_loss= 2.05231 (279 samples/sec)
2024-03-15 11:01:15.492841 epoch: 2 step: 2000 cls_loss= 2.02031 (280 samples/sec)
2024-03-15 11:02:12.689946 epoch: 2 step: 2500 cls_loss= 2.66971 (279 samples/sec)
2024-03-15 11:03:09.582414 epoch: 2 step: 3000 cls_loss= 1.77793 (281 samples/sec)
2024-03-15 11:06:09.718931------------------------------------------------------ Precision@1: 66.40%  Precision@1: 87.04%

top1: [66.108, 66.404]
top5: [86.97200000000001, 87.044]
2024-03-15 11:06:09.927727 epoch: 3 step: 0 cls_loss= 2.62528 (77119 samples/sec)
2024-03-15 11:07:08.389746 epoch: 3 step: 500 cls_loss= 2.57255 (273 samples/sec)
2024-03-15 11:08:06.648528 epoch: 3 step: 1000 cls_loss= 2.24239 (274 samples/sec)
2024-03-15 11:09:04.670393 epoch: 3 step: 1500 cls_loss= 2.51686 (275 samples/sec)
2024-03-15 11:10:01.918926 epoch: 3 step: 2000 cls_loss= 1.71756 (279 samples/sec)
2024-03-15 11:11:00.059665 epoch: 3 step: 2500 cls_loss= 1.53075 (275 samples/sec)
2024-03-15 11:11:58.829253 epoch: 3 step: 3000 cls_loss= 1.95588 (272 samples/sec)
2024-03-15 11:15:03.473004------------------------------------------------------ Precision@1: 66.27%  Precision@1: 87.01%

top1: [66.108, 66.404, 66.268]
top5: [86.97200000000001, 87.044, 87.008]
2024-03-15 11:15:03.675439 epoch: 4 step: 0 cls_loss= 1.88929 (79486 samples/sec)
2024-03-15 11:16:01.926763 epoch: 4 step: 500 cls_loss= 2.48071 (274 samples/sec)
2024-03-15 11:16:59.927100 epoch: 4 step: 1000 cls_loss= 2.65289 (275 samples/sec)
2024-03-15 11:17:57.761940 epoch: 4 step: 1500 cls_loss= 1.84337 (276 samples/sec)
2024-03-15 11:18:55.914477 epoch: 4 step: 2000 cls_loss= 2.28534 (275 samples/sec)
2024-03-15 11:19:52.787250 epoch: 4 step: 2500 cls_loss= 1.46097 (281 samples/sec)
2024-03-15 11:20:49.830565 epoch: 4 step: 3000 cls_loss= 1.38251 (280 samples/sec)
2024-03-15 11:23:53.290515------------------------------------------------------ Precision@1: 66.05%  Precision@1: 86.84%

top1: [66.108, 66.404, 66.268, 66.046]
top5: [86.97200000000001, 87.044, 87.008, 86.84400000000001]
2024-03-15 11:23:53.502945 epoch: 5 step: 0 cls_loss= 1.84254 (75694 samples/sec)
2024-03-15 11:24:51.422911 epoch: 5 step: 500 cls_loss= 1.76176 (276 samples/sec)
2024-03-15 11:25:49.420792 epoch: 5 step: 1000 cls_loss= 1.51035 (275 samples/sec)
2024-03-15 11:26:47.492211 epoch: 5 step: 1500 cls_loss= 2.03233 (275 samples/sec)
2024-03-15 11:27:44.870103 epoch: 5 step: 2000 cls_loss= 2.22087 (278 samples/sec)
2024-03-15 11:28:42.725989 epoch: 5 step: 2500 cls_loss= 2.31218 (276 samples/sec)
2024-03-15 11:29:40.817552 epoch: 5 step: 3000 cls_loss= 1.68724 (275 samples/sec)
2024-03-15 11:32:44.077237------------------------------------------------------ Precision@1: 66.63%  Precision@1: 87.16%

top1: [66.108, 66.404, 66.268, 66.046, 66.63]
top5: [86.97200000000001, 87.044, 87.008, 86.84400000000001, 87.16]
2024-03-15 11:32:44.274350 epoch: 6 step: 0 cls_loss= 2.32505 (81676 samples/sec)
2024-03-15 11:33:41.467840 epoch: 6 step: 500 cls_loss= 2.29679 (279 samples/sec)
2024-03-15 11:34:39.636513 epoch: 6 step: 1000 cls_loss= 1.86194 (275 samples/sec)
2024-03-15 11:35:36.987847 epoch: 6 step: 1500 cls_loss= 1.80289 (279 samples/sec)
2024-03-15 11:36:35.057861 epoch: 6 step: 2000 cls_loss= 1.70282 (275 samples/sec)
2024-03-15 11:37:32.234294 epoch: 6 step: 2500 cls_loss= 2.14737 (279 samples/sec)
2024-03-15 11:38:30.409059 epoch: 6 step: 3000 cls_loss= 1.77214 (275 samples/sec)
2024-03-15 11:41:34.077971------------------------------------------------------ Precision@1: 66.70%  Precision@1: 87.32%

top1: [66.108, 66.404, 66.268, 66.046, 66.63, 66.69800000000001]
top5: [86.97200000000001, 87.044, 87.008, 86.84400000000001, 87.16, 87.32000000000001]
2024-03-15 11:41:34.278191 epoch: 7 step: 0 cls_loss= 1.62624 (80414 samples/sec)
2024-03-15 11:42:32.690581 epoch: 7 step: 500 cls_loss= 1.64956 (273 samples/sec)
2024-03-15 11:43:30.223160 epoch: 7 step: 1000 cls_loss= 1.56837 (278 samples/sec)
2024-03-15 11:44:27.237962 epoch: 7 step: 1500 cls_loss= 1.78859 (280 samples/sec)
2024-03-15 11:45:25.077106 epoch: 7 step: 2000 cls_loss= 1.93535 (276 samples/sec)
2024-03-15 11:46:22.403081 epoch: 7 step: 2500 cls_loss= 1.82545 (279 samples/sec)
2024-03-15 11:47:19.707097 epoch: 7 step: 3000 cls_loss= 1.93570 (279 samples/sec)
2024-03-15 11:50:21.143509------------------------------------------------------ Precision@1: 66.80%  Precision@1: 87.28%

top1: [66.108, 66.404, 66.268, 66.046, 66.63, 66.69800000000001, 66.802]
top5: [86.97200000000001, 87.044, 87.008, 86.84400000000001, 87.16, 87.32000000000001, 87.282]
2024-03-15 11:50:21.350994 epoch: 8 step: 0 cls_loss= 2.41169 (77562 samples/sec)
2024-03-15 11:51:18.378232 epoch: 8 step: 500 cls_loss= 2.00425 (280 samples/sec)
2024-03-15 11:52:15.922582 epoch: 8 step: 1000 cls_loss= 1.61580 (278 samples/sec)
2024-03-15 11:53:13.599064 epoch: 8 step: 1500 cls_loss= 2.06327 (277 samples/sec)
2024-03-15 11:54:10.967783 epoch: 8 step: 2000 cls_loss= 1.18564 (278 samples/sec)
2024-03-15 11:55:08.594115 epoch: 8 step: 2500 cls_loss= 1.69087 (277 samples/sec)
2024-03-15 11:56:06.268876 epoch: 8 step: 3000 cls_loss= 1.54558 (277 samples/sec)
2024-03-15 11:59:05.931935------------------------------------------------------ Precision@1: 67.04%  Precision@1: 87.52%

top1: [66.108, 66.404, 66.268, 66.046, 66.63, 66.69800000000001, 66.802, 67.036]
top5: [86.97200000000001, 87.044, 87.008, 86.84400000000001, 87.16, 87.32000000000001, 87.282, 87.524]
2024-03-15 11:59:06.130083 epoch: 9 step: 0 cls_loss= 2.01149 (81203 samples/sec)
2024-03-15 12:00:04.261406 epoch: 9 step: 500 cls_loss= 1.97341 (275 samples/sec)
2024-03-15 12:01:01.811437 epoch: 9 step: 1000 cls_loss= 2.12095 (278 samples/sec)
2024-03-15 12:01:58.759357 epoch: 9 step: 1500 cls_loss= 1.45516 (281 samples/sec)
2024-03-15 12:02:55.633453 epoch: 9 step: 2000 cls_loss= 2.32992 (281 samples/sec)
2024-03-15 12:03:52.167821 epoch: 9 step: 2500 cls_loss= 2.04317 (283 samples/sec)
2024-03-15 12:04:49.146516 epoch: 9 step: 3000 cls_loss= 1.94831 (280 samples/sec)
2024-03-15 12:07:49.708984------------------------------------------------------ Precision@1: 66.67%  Precision@1: 87.41%

top1: [66.108, 66.404, 66.268, 66.046, 66.63, 66.69800000000001, 66.802, 67.036, 66.672]
top5: [86.97200000000001, 87.044, 87.008, 86.84400000000001, 87.16, 87.32000000000001, 87.282, 87.524, 87.408]
2024-03-15 12:07:49.917448 epoch: 10 step: 0 cls_loss= 2.02171 (77152 samples/sec)
2024-03-15 12:08:47.544349 epoch: 10 step: 500 cls_loss= 2.43409 (277 samples/sec)
2024-03-15 12:09:44.236348 epoch: 10 step: 1000 cls_loss= 1.86935 (282 samples/sec)
2024-03-15 12:10:41.741902 epoch: 10 step: 1500 cls_loss= 1.66869 (278 samples/sec)
2024-03-15 12:11:39.615680 epoch: 10 step: 2000 cls_loss= 2.33266 (276 samples/sec)
2024-03-15 12:12:37.331126 epoch: 10 step: 2500 cls_loss= 1.44386 (277 samples/sec)
2024-03-15 12:13:35.758162 epoch: 10 step: 3000 cls_loss= 2.47737 (273 samples/sec)
2024-03-15 12:16:39.687711------------------------------------------------------ Precision@1: 66.59%  Precision@1: 87.19%

top1: [66.108, 66.404, 66.268, 66.046, 66.63, 66.69800000000001, 66.802, 67.036, 66.672, 66.592]
top5: [86.97200000000001, 87.044, 87.008, 86.84400000000001, 87.16, 87.32000000000001, 87.282, 87.524, 87.408, 87.186]
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ bash bash_train2.sh [K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[Kpython ./imgnet_train_eval.py --Wbits 8 --Abit 8 --lr 1e-4 --wd 1e-4  --pretrain --max_epochs 11  --if_train 1 --all_validate 1  --log_interval 500 --net mobilenet_m1 --optimizer SSGD
=> creating model mobilenet_m1 ...
 learning rate =  0.0001
SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
./imgnet_train_eval.py:177: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-15 13:31:24.772795 epoch: 1 step: 0 cls_loss= 2.14381 (19820 samples/sec)
2024-03-15 13:32:20.327408 epoch: 1 step: 500 cls_loss= 1.35833 (288 samples/sec)
2024-03-15 13:33:16.182030 epoch: 1 step: 1000 cls_loss= 2.75071 (286 samples/sec)
2024-03-15 13:34:11.817750 epoch: 1 step: 1500 cls_loss= 2.00637 (287 samples/sec)
2024-03-15 13:35:07.935179 epoch: 1 step: 2000 cls_loss= 2.36955 (285 samples/sec)
2024-03-15 13:36:03.462389 epoch: 1 step: 2500 cls_loss= 2.02629 (288 samples/sec)
2024-03-15 13:36:59.203416 epoch: 1 step: 3000 cls_loss= 1.99018 (287 samples/sec)
2024-03-15 13:39:57.068935------------------------------------------------------ Precision@1: 66.91%  Precision@1: 87.42%

top1: [66.906]
top5: [87.424]
2024-03-15 13:39:57.275969 epoch: 2 step: 0 cls_loss= 1.73343 (77729 samples/sec)
2024-03-15 13:40:53.375378 epoch: 2 step: 500 cls_loss= 1.94178 (285 samples/sec)
2024-03-15 13:41:50.335519 epoch: 2 step: 1000 cls_loss= 2.21964 (280 samples/sec)
2024-03-15 13:42:46.947996 epoch: 2 step: 1500 cls_loss= 1.74507 (282 samples/sec)
2024-03-15 13:43:44.092646 epoch: 2 step: 2000 cls_loss= 2.33002 (280 samples/sec)
2024-03-15 13:44:40.897844 epoch: 2 step: 2500 cls_loss= 1.93745 (281 samples/sec)
2024-03-15 13:45:37.379718 epoch: 2 step: 3000 cls_loss= 1.51669 (283 samples/sec)
2024-03-15 13:48:37.132314------------------------------------------------------ Precision@1: 67.24%  Precision@1: 87.67%

top1: [66.906, 67.242]
top5: [87.424, 87.674]
2024-03-15 13:48:37.329522 epoch: 3 step: 0 cls_loss= 2.15343 (81612 samples/sec)
2024-03-15 13:49:34.059842 epoch: 3 step: 500 cls_loss= 1.65392 (282 samples/sec)
2024-03-15 13:50:31.017658 epoch: 3 step: 1000 cls_loss= 2.77592 (280 samples/sec)
2024-03-15 13:51:27.629333 epoch: 3 step: 1500 cls_loss= 2.40691 (282 samples/sec)
2024-03-15 13:52:24.536348 epoch: 3 step: 2000 cls_loss= 1.95969 (281 samples/sec)
2024-03-15 13:53:21.158403 epoch: 3 step: 2500 cls_loss= 2.00190 (282 samples/sec)
2024-03-15 13:54:18.032322 epoch: 3 step: 3000 cls_loss= 1.91235 (281 samples/sec)
2024-03-15 13:57:18.084669------------------------------------------------------ Precision@1: 67.26%  Precision@1: 87.80%

top1: [66.906, 67.242, 67.264]
top5: [87.424, 87.674, 87.796]
2024-03-15 13:57:18.293743 epoch: 4 step: 0 cls_loss= 1.53141 (76962 samples/sec)
2024-03-15 13:58:16.454079 epoch: 4 step: 500 cls_loss= 1.73927 (275 samples/sec)
2024-03-15 13:59:13.908963 epoch: 4 step: 1000 cls_loss= 1.45746 (278 samples/sec)
2024-03-15 14:00:10.782573 epoch: 4 step: 1500 cls_loss= 1.62630 (281 samples/sec)
2024-03-15 14:01:07.637489 epoch: 4 step: 2000 cls_loss= 1.42250 (281 samples/sec)
2024-03-15 14:02:04.073170 epoch: 4 step: 2500 cls_loss= 1.56916 (283 samples/sec)
2024-03-15 14:02:59.902942 epoch: 4 step: 3000 cls_loss= 1.45252 (286 samples/sec)
2024-03-15 14:06:01.971621------------------------------------------------------ Precision@1: 67.41%  Precision@1: 87.77%

top1: [66.906, 67.242, 67.264, 67.412]
top5: [87.424, 87.674, 87.796, 87.772]
2024-03-15 14:06:02.170618 epoch: 5 step: 0 cls_loss= 1.65922 (80841 samples/sec)
2024-03-15 14:06:58.865004 epoch: 5 step: 500 cls_loss= 2.32573 (282 samples/sec)
2024-03-15 14:07:56.169925 epoch: 5 step: 1000 cls_loss= 1.66756 (279 samples/sec)
2024-03-15 14:08:53.647610 epoch: 5 step: 1500 cls_loss= 1.84165 (278 samples/sec)
2024-03-15 14:09:51.062176 epoch: 5 step: 2000 cls_loss= 2.07422 (278 samples/sec)
2024-03-15 14:10:48.184494 epoch: 5 step: 2500 cls_loss= 1.61708 (280 samples/sec)
2024-03-15 14:11:45.539046 epoch: 5 step: 3000 cls_loss= 2.04944 (279 samples/sec)
2024-03-15 14:14:47.072367------------------------------------------------------ Precision@1: 67.29%  Precision@1: 87.67%

top1: [66.906, 67.242, 67.264, 67.412, 67.294]
top5: [87.424, 87.674, 87.796, 87.772, 87.672]
2024-03-15 14:14:47.307354 epoch: 6 step: 0 cls_loss= 1.61119 (68424 samples/sec)
2024-03-15 14:15:43.684149 epoch: 6 step: 500 cls_loss= 2.26416 (283 samples/sec)
2024-03-15 14:16:40.199077 epoch: 6 step: 1000 cls_loss= 1.87152 (283 samples/sec)
2024-03-15 14:17:36.377986 epoch: 6 step: 1500 cls_loss= 1.80324 (284 samples/sec)
2024-03-15 14:18:32.993423 epoch: 6 step: 2000 cls_loss= 1.85275 (282 samples/sec)
2024-03-15 14:19:29.532517 epoch: 6 step: 2500 cls_loss= 1.98255 (283 samples/sec)
2024-03-15 14:20:26.079712 epoch: 6 step: 3000 cls_loss= 0.90987 (283 samples/sec)
2024-03-15 14:23:28.266981------------------------------------------------------ Precision@1: 67.13%  Precision@1: 87.59%

top1: [66.906, 67.242, 67.264, 67.412, 67.294, 67.13]
top5: [87.424, 87.674, 87.796, 87.772, 87.672, 87.59]
2024-03-15 14:23:28.467235 epoch: 7 step: 0 cls_loss= 2.04837 (80366 samples/sec)
2024-03-15 14:24:25.775964 epoch: 7 step: 500 cls_loss= 1.66137 (279 samples/sec)
2024-03-15 14:25:23.630581 epoch: 7 step: 1000 cls_loss= 1.50452 (276 samples/sec)
2024-03-15 14:26:21.631263 epoch: 7 step: 1500 cls_loss= 1.52183 (275 samples/sec)
2024-03-15 14:27:19.594248 epoch: 7 step: 2000 cls_loss= 2.00361 (276 samples/sec)
2024-03-15 14:28:17.184951 epoch: 7 step: 2500 cls_loss= 1.87612 (277 samples/sec)
2024-03-15 14:29:14.765091 epoch: 7 step: 3000 cls_loss= 1.96973 (277 samples/sec)
2024-03-15 14:32:16.266918------------------------------------------------------ Precision@1: 67.09%  Precision@1: 87.62%

top1: [66.906, 67.242, 67.264, 67.412, 67.294, 67.13, 67.09]
top5: [87.424, 87.674, 87.796, 87.772, 87.672, 87.59, 87.622]
2024-03-15 14:32:16.472605 epoch: 8 step: 0 cls_loss= 1.64599 (78131 samples/sec)
2024-03-15 14:33:13.367912 epoch: 8 step: 500 cls_loss= 1.29548 (281 samples/sec)
2024-03-15 14:34:09.767495 epoch: 8 step: 1000 cls_loss= 1.86241 (283 samples/sec)
2024-03-15 14:35:06.181636 epoch: 8 step: 1500 cls_loss= 1.39813 (283 samples/sec)
2024-03-15 14:36:02.977855 epoch: 8 step: 2000 cls_loss= 1.48080 (281 samples/sec)
2024-03-15 14:36:59.086373 epoch: 8 step: 2500 cls_loss= 1.27591 (285 samples/sec)
2024-03-15 14:37:55.458404 epoch: 8 step: 3000 cls_loss= 1.42695 (283 samples/sec)
2024-03-15 14:40:53.852782------------------------------------------------------ Precision@1: 67.11%  Precision@1: 87.68%

top1: [66.906, 67.242, 67.264, 67.412, 67.294, 67.13, 67.09, 67.112]
top5: [87.424, 87.674, 87.796, 87.772, 87.672, 87.59, 87.622, 87.676]
2024-03-15 14:40:54.059863 epoch: 9 step: 0 cls_loss= 2.06238 (77685 samples/sec)
2024-03-15 14:41:50.929164 epoch: 9 step: 500 cls_loss= 1.85525 (281 samples/sec)
2024-03-15 14:42:47.736617 epoch: 9 step: 1000 cls_loss= 1.59724 (281 samples/sec)
2024-03-15 14:43:45.165038 epoch: 9 step: 1500 cls_loss= 2.21189 (278 samples/sec)
2024-03-15 14:44:42.268029 epoch: 9 step: 2000 cls_loss= 1.41543 (280 samples/sec)
2024-03-15 14:45:40.121812 epoch: 9 step: 2500 cls_loss= 1.92819 (276 samples/sec)
2024-03-15 14:46:37.718049 epoch: 9 step: 3000 cls_loss= 1.51058 (277 samples/sec)
2024-03-15 14:49:39.179230------------------------------------------------------ Precision@1: 66.83%  Precision@1: 87.40%

top1: [66.906, 67.242, 67.264, 67.412, 67.294, 67.13, 67.09, 67.112, 66.82600000000001]
top5: [87.424, 87.674, 87.796, 87.772, 87.672, 87.59, 87.622, 87.676, 87.398]
2024-03-15 14:49:39.379348 epoch: 10 step: 0 cls_loss= 1.70010 (80386 samples/sec)
2024-03-15 14:50:36.480109 epoch: 10 step: 500 cls_loss= 2.23354 (280 samples/sec)
2024-03-15 14:51:33.794507 epoch: 10 step: 1000 cls_loss= 2.57263 (279 samples/sec)
2024-03-15 14:52:31.153901 epoch: 10 step: 1500 cls_loss= 1.39319 (279 samples/sec)
2024-03-15 14:53:27.701727 epoch: 10 step: 2000 cls_loss= 1.71386 (283 samples/sec)
2024-03-15 14:54:25.038014 epoch: 10 step: 2500 cls_loss= 1.37682 (279 samples/sec)
2024-03-15 14:55:58.606877 epoch: 10 step: 3000 cls_loss= 1.64033 (171 samples/sec)
2024-03-15 14:59:02.616650------------------------------------------------------ Precision@1: 67.20%  Precision@1: 87.68%

top1: [66.906, 67.242, 67.264, 67.412, 67.294, 67.13, 67.09, 67.112, 66.82600000000001, 67.19800000000001]
top5: [87.424, 87.674, 87.796, 87.772, 87.672, 87.59, 87.622, 87.676, 87.398, 87.682]
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ bash 
.git/                       0312-imgnet-train-1e-4.txt  autocode.py                 cifar100_train_eval.py      data/                       nets/                       training_txt/
.gitignore                  0313mobnet-abs-sqrt.txt     bash_train.sh               ckpt/                       imgnet_train_eval.py        param_saved_txt/            utils/
0311-imgnet_train-1e-5.txt  0315mobnet-abs-sqrt.txt     bash_train2.sh              core                        logs/                       parameter/                  
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ bash bash_train2.sh 
=> creating model mobilenet_m1 ...
 learning rate =  2e-05
SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
./imgnet_train_eval.py:177: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-15 15:00:43.778599 epoch: 1 step: 0 cls_loss= 2.82057 (19444 samples/sec)
2024-03-15 15:01:40.244847 epoch: 1 step: 500 cls_loss= 1.61666 (283 samples/sec)
2024-03-15 15:02:36.524861 epoch: 1 step: 1000 cls_loss= 2.84498 (284 samples/sec)
2024-03-15 15:03:34.260855 epoch: 1 step: 1500 cls_loss= 1.45409 (277 samples/sec)
2024-03-15 15:04:31.835929 epoch: 1 step: 2000 cls_loss= 2.11355 (277 samples/sec)
2024-03-15 15:05:29.300421 epoch: 1 step: 2500 cls_loss= 1.93283 (278 samples/sec)
2024-03-15 15:06:26.876275 epoch: 1 step: 3000 cls_loss= 2.43896 (277 samples/sec)
2024-03-15 15:09:27.161953------------------------------------------------------ Precision@1: 66.70%  Precision@1: 87.21%

top1: [66.7]
top5: [87.212]
2024-03-15 15:09:27.363145 epoch: 2 step: 0 cls_loss= 2.61666 (79957 samples/sec)
2024-03-15 15:10:23.689475 epoch: 2 step: 500 cls_loss= 2.15785 (284 samples/sec)
2024-03-15 15:11:20.252304 epoch: 2 step: 1000 cls_loss= 2.13938 (282 samples/sec)
2024-03-15 15:12:16.317197 epoch: 2 step: 1500 cls_loss= 1.60553 (285 samples/sec)
^CTraceback (most recent call last):
  File "./imgnet_train_eval.py", line 444, in <module>
    main()
  File "./imgnet_train_eval.py", line 436, in main
    train(epoch)
  File "./imgnet_train_eval.py", line 263, in train
    output = model(inputs.cuda())
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1480, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspaces/pytorch-dev/SLFP_CNNs/nets/mobilenetv1.py", line 83, in forward
    x = self.model(x)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1480, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1480, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1480, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspaces/pytorch-dev/SLFP_CNNs/utils/sfp_quant.py", line 102, in forward
    self.input_q = self.quantize_fn(input/self.Ka) #量化并缩放激活值
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1480, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspaces/pytorch-dev/SLFP_CNNs/utils/sfp_quant.py", line 85, in forward
    weight_q = self.uniform_q(x)  # W_sfp 
  File "/workspaces/pytorch-dev/SLFP_CNNs/utils/sfp_quant.py", line 30, in forward
    scaling_factor = pow(2, torch.floor(torch.log2(input)))
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor.py", line 40, in wrapped
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor.py", line 866, in __rpow__
    return torch.tensor(other, dtype=dtype, device=self.device) ** self
KeyboardInterrupt

]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ bash bash_train2.sh 
=> creating model mobilenet_m1 ...
 learning rate =  0.0002
SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
./imgnet_train_eval.py:177: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-15 15:12:32.121992 epoch: 1 step: 0 cls_loss= 2.42570 (20271 samples/sec)
2024-03-15 15:13:28.969218 epoch: 1 step: 500 cls_loss= 1.64466 (281 samples/sec)
2024-03-15 15:14:26.662780 epoch: 1 step: 1000 cls_loss= 2.16260 (277 samples/sec)
2024-03-15 15:15:23.663227 epoch: 1 step: 1500 cls_loss= 1.85413 (280 samples/sec)
2024-03-15 15:16:20.879865 epoch: 1 step: 2000 cls_loss= 1.88759 (279 samples/sec)
2024-03-15 15:17:18.012800 epoch: 1 step: 2500 cls_loss= 2.27590 (280 samples/sec)
2024-03-15 15:18:14.781628 epoch: 1 step: 3000 cls_loss= 2.06227 (281 samples/sec)
2024-03-15 15:21:14.778978------------------------------------------------------ Precision@1: 67.26%  Precision@1: 87.66%

top1: [67.262]
top5: [87.662]
2024-03-15 15:21:14.984147 epoch: 2 step: 0 cls_loss= 1.49261 (78476 samples/sec)
2024-03-15 15:22:12.807858 epoch: 2 step: 500 cls_loss= 1.94652 (276 samples/sec)
2024-03-15 15:23:10.311352 epoch: 2 step: 1000 cls_loss= 1.29633 (278 samples/sec)
2024-03-15 15:24:07.206108 epoch: 2 step: 1500 cls_loss= 1.77124 (281 samples/sec)
2024-03-15 15:25:04.902666 epoch: 2 step: 2000 cls_loss= 2.21803 (277 samples/sec)
2024-03-15 15:26:03.095630 epoch: 2 step: 2500 cls_loss= 2.02342 (275 samples/sec)
2024-03-15 15:27:00.796898 epoch: 2 step: 3000 cls_loss= 3.25634 (277 samples/sec)
2024-03-15 15:30:03.192766------------------------------------------------------ Precision@1: 66.86%  Precision@1: 87.37%

top1: [67.262, 66.86]
top5: [87.662, 87.374]
2024-03-15 15:30:03.389803 epoch: 3 step: 0 cls_loss= 1.26453 (81709 samples/sec)
2024-03-15 15:31:01.234446 epoch: 3 step: 500 cls_loss= 1.81531 (276 samples/sec)
2024-03-15 15:31:58.793001 epoch: 3 step: 1000 cls_loss= 2.17847 (278 samples/sec)
2024-03-15 15:32:56.454426 epoch: 3 step: 1500 cls_loss= 2.36331 (277 samples/sec)
2024-03-15 15:33:54.436161 epoch: 3 step: 2000 cls_loss= 2.37753 (276 samples/sec)
2024-03-15 15:34:51.968081 epoch: 3 step: 2500 cls_loss= 2.16708 (278 samples/sec)
2024-03-15 15:35:49.508535 epoch: 3 step: 3000 cls_loss= 2.16850 (278 samples/sec)
2024-03-15 15:38:51.801996------------------------------------------------------ Precision@1: 67.21%  Precision@1: 87.76%

top1: [67.262, 66.86, 67.214]
top5: [87.662, 87.374, 87.76]
2024-03-15 15:38:52.013528 epoch: 4 step: 0 cls_loss= 1.96976 (76059 samples/sec)
2024-03-15 15:39:48.864684 epoch: 4 step: 500 cls_loss= 1.86711 (281 samples/sec)
2024-03-15 15:40:45.954553 epoch: 4 step: 1000 cls_loss= 1.36965 (280 samples/sec)
2024-03-15 15:41:42.704418 epoch: 4 step: 1500 cls_loss= 1.78177 (282 samples/sec)
2024-03-15 15:42:41.272928 epoch: 4 step: 2000 cls_loss= 1.56652 (273 samples/sec)
2024-03-15 15:43:38.229111 epoch: 4 step: 2500 cls_loss= 1.55239 (280 samples/sec)
2024-03-15 15:44:35.170340 epoch: 4 step: 3000 cls_loss= 1.95738 (281 samples/sec)
2024-03-15 15:47:36.995935------------------------------------------------------ Precision@1: 67.40%  Precision@1: 87.83%

top1: [67.262, 66.86, 67.214, 67.396]
top5: [87.662, 87.374, 87.76, 87.828]
2024-03-15 15:47:37.202368 epoch: 5 step: 0 cls_loss= 2.37360 (77976 samples/sec)
2024-03-15 15:48:34.116159 epoch: 5 step: 500 cls_loss= 1.86756 (281 samples/sec)
2024-03-15 15:49:30.258212 epoch: 5 step: 1000 cls_loss= 1.84270 (285 samples/sec)
2024-03-15 15:50:26.951466 epoch: 5 step: 1500 cls_loss= 2.04725 (282 samples/sec)
2024-03-15 15:51:23.533308 epoch: 5 step: 2000 cls_loss= 1.73899 (282 samples/sec)
2024-03-15 15:52:20.940142 epoch: 5 step: 2500 cls_loss= 2.05476 (278 samples/sec)
2024-03-15 15:53:18.488713 epoch: 5 step: 3000 cls_loss= 2.13346 (278 samples/sec)
2024-03-15 15:56:22.953013------------------------------------------------------ Precision@1: 67.23%  Precision@1: 87.67%

top1: [67.262, 66.86, 67.214, 67.396, 67.232]
top5: [87.662, 87.374, 87.76, 87.828, 87.672]
2024-03-15 15:56:23.156338 epoch: 6 step: 0 cls_loss= 1.66313 (79135 samples/sec)
2024-03-15 15:57:20.289435 epoch: 6 step: 500 cls_loss= 1.80823 (280 samples/sec)
2024-03-15 15:58:17.585728 epoch: 6 step: 1000 cls_loss= 1.74891 (279 samples/sec)
2024-03-15 15:59:15.214267 epoch: 6 step: 1500 cls_loss= 1.84072 (277 samples/sec)
2024-03-15 16:00:12.691908 epoch: 6 step: 2000 cls_loss= 1.85749 (278 samples/sec)
2024-03-15 16:01:10.415394 epoch: 6 step: 2500 cls_loss= 1.55234 (277 samples/sec)
2024-03-15 16:02:08.206901 epoch: 6 step: 3000 cls_loss= 1.30882 (276 samples/sec)
2024-03-15 16:05:06.349948------------------------------------------------------ Precision@1: 67.32%  Precision@1: 87.64%

top1: [67.262, 66.86, 67.214, 67.396, 67.232, 67.322]
top5: [87.662, 87.374, 87.76, 87.828, 87.672, 87.642]
2024-03-15 16:05:06.551464 epoch: 7 step: 0 cls_loss= 1.41713 (79906 samples/sec)
2024-03-15 16:06:03.589806 epoch: 7 step: 500 cls_loss= 1.52517 (280 samples/sec)
2024-03-15 16:07:00.734612 epoch: 7 step: 1000 cls_loss= 1.81932 (280 samples/sec)
2024-03-15 16:07:57.488876 epoch: 7 step: 1500 cls_loss= 1.72875 (281 samples/sec)
2024-03-15 16:09:21.576484 epoch: 7 step: 2000 cls_loss= 1.55625 (190 samples/sec)
2024-03-15 16:11:47.389955 epoch: 7 step: 2500 cls_loss= 1.33715 (109 samples/sec)
2024-03-15 16:14:53.505157 epoch: 7 step: 3000 cls_loss= 1.64648 (85 samples/sec)
2024-03-15 16:20:28.370190------------------------------------------------------ Precision@1: 67.07%  Precision@1: 87.59%

top1: [67.262, 66.86, 67.214, 67.396, 67.232, 67.322, 67.068]
top5: [87.662, 87.374, 87.76, 87.828, 87.672, 87.642, 87.592]
2024-03-15 16:20:28.825175 epoch: 8 step: 0 cls_loss= 1.69392 (35262 samples/sec)
2024-03-15 16:23:48.172382 epoch: 8 step: 500 cls_loss= 2.46840 (80 samples/sec)
2024-03-15 16:27:26.779779 epoch: 8 step: 1000 cls_loss= 1.82932 (73 samples/sec)
2024-03-15 16:31:29.426400 epoch: 8 step: 1500 cls_loss= 1.91507 (65 samples/sec)
2024-03-15 16:35:43.547451 epoch: 8 step: 2000 cls_loss= 1.73649 (62 samples/sec)
2024-03-15 16:39:58.974751 epoch: 8 step: 2500 cls_loss= 1.63641 (62 samples/sec)
2024-03-15 16:44:18.654014 epoch: 8 step: 3000 cls_loss= 1.28030 (61 samples/sec)
2024-03-15 16:49:41.676180------------------------------------------------------ Precision@1: 67.23%  Precision@1: 87.56%

top1: [67.262, 66.86, 67.214, 67.396, 67.232, 67.322, 67.068, 67.23]
top5: [87.662, 87.374, 87.76, 87.828, 87.672, 87.642, 87.592, 87.562]
2024-03-15 16:49:42.108361 epoch: 9 step: 0 cls_loss= 1.69358 (37129 samples/sec)
2024-03-15 16:51:52.220310 epoch: 9 step: 500 cls_loss= 2.16819 (122 samples/sec)
2024-03-15 16:54:10.899846 epoch: 9 step: 1000 cls_loss= 2.32471 (115 samples/sec)
2024-03-15 16:56:44.665935 epoch: 9 step: 1500 cls_loss= 2.13591 (104 samples/sec)
2024-03-15 16:59:43.675505 epoch: 9 step: 2000 cls_loss= 1.66546 (89 samples/sec)
2024-03-15 17:03:31.330542 epoch: 9 step: 2500 cls_loss= 0.94796 (70 samples/sec)
2024-03-15 17:07:49.081716 epoch: 9 step: 3000 cls_loss= 2.23410 (62 samples/sec)
2024-03-15 17:12:50.212734------------------------------------------------------ Precision@1: 67.09%  Precision@1: 87.69%

top1: [67.262, 66.86, 67.214, 67.396, 67.232, 67.322, 67.068, 67.23, 67.09]
top5: [87.662, 87.374, 87.76, 87.828, 87.672, 87.642, 87.592, 87.562, 87.688]
2024-03-15 17:12:50.434077 epoch: 10 step: 0 cls_loss= 2.00226 (72687 samples/sec)
2024-03-15 17:14:15.289769 epoch: 10 step: 500 cls_loss= 1.50102 (188 samples/sec)
2024-03-15 17:15:44.009008 epoch: 10 step: 1000 cls_loss= 2.00659 (180 samples/sec)
2024-03-15 17:17:18.499039 epoch: 10 step: 1500 cls_loss= 1.54270 (169 samples/sec)
2024-03-15 17:19:02.720915 epoch: 10 step: 2000 cls_loss= 1.77649 (153 samples/sec)
2024-03-15 17:20:59.251326 epoch: 10 step: 2500 cls_loss= 1.47196 (137 samples/sec)
2024-03-15 17:23:37.959632 epoch: 10 step: 3000 cls_loss= 1.51345 (100 samples/sec)
2024-03-15 17:28:01.441030------------------------------------------------------ Precision@1: 67.19%  Precision@1: 87.62%

top1: [67.262, 66.86, 67.214, 67.396, 67.232, 67.322, 67.068, 67.23, 67.09, 67.194]
top5: [87.662, 87.374, 87.76, 87.828, 87.672, 87.642, 87.592, 87.562, 87.688, 87.616]
=> creating model mobilenet_m1 ...
 learning rate =  2e-05
SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
./imgnet_train_eval.py:177: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-15 17:28:10.756064 epoch: 1 step: 0 cls_loss= 1.69319 (15198 samples/sec)
2024-03-15 17:29:27.145522 epoch: 1 step: 500 cls_loss= 1.67907 (209 samples/sec)
2024-03-15 17:30:46.799014 epoch: 1 step: 1000 cls_loss= 1.75365 (200 samples/sec)
2024-03-15 17:32:11.285046 epoch: 1 step: 1500 cls_loss= 2.08222 (189 samples/sec)
2024-03-15 17:33:45.488126 epoch: 1 step: 2000 cls_loss= 2.19262 (169 samples/sec)
2024-03-15 17:35:34.926987 epoch: 1 step: 2500 cls_loss= 1.74321 (146 samples/sec)
2024-03-15 17:38:27.634944 epoch: 1 step: 3000 cls_loss= 1.74156 (92 samples/sec)
2024-03-15 17:42:17.299106------------------------------------------------------ Precision@1: 66.72%  Precision@1: 87.39%

top1: [66.72]
top5: [87.394]
2024-03-15 17:42:17.547433 epoch: 2 step: 0 cls_loss= 1.97238 (64713 samples/sec)
2024-03-15 17:43:23.556567 epoch: 2 step: 500 cls_loss= 1.69217 (242 samples/sec)
2024-03-15 17:44:31.188508 epoch: 2 step: 1000 cls_loss= 1.64952 (236 samples/sec)
2024-03-15 17:45:38.830653 epoch: 2 step: 1500 cls_loss= 1.78899 (236 samples/sec)
2024-03-15 17:46:50.745384 epoch: 2 step: 2000 cls_loss= 1.61858 (222 samples/sec)
2024-03-15 17:48:10.301108 epoch: 2 step: 2500 cls_loss= 1.61374 (201 samples/sec)
2024-03-15 17:49:42.584248 epoch: 2 step: 3000 cls_loss= 1.55952 (173 samples/sec)
2024-03-15 17:53:01.858562------------------------------------------------------ Precision@1: 66.71%  Precision@1: 87.46%

top1: [66.72, 66.712]
top5: [87.394, 87.456]
2024-03-15 17:53:02.095789 epoch: 3 step: 0 cls_loss= 1.03617 (67804 samples/sec)
2024-03-15 17:54:05.531634 epoch: 3 step: 500 cls_loss= 1.76958 (252 samples/sec)
2024-03-15 17:55:10.686383 epoch: 3 step: 1000 cls_loss= 2.56203 (245 samples/sec)
2024-03-15 17:56:17.098484 epoch: 3 step: 1500 cls_loss= 1.80643 (240 samples/sec)
2024-03-15 17:57:26.303009 epoch: 3 step: 2000 cls_loss= 2.05756 (231 samples/sec)
2024-03-15 17:58:49.488121 epoch: 3 step: 2500 cls_loss= 2.15998 (192 samples/sec)
2024-03-15 18:00:28.737843 epoch: 3 step: 3000 cls_loss= 2.06478 (161 samples/sec)
2024-03-15 18:04:21.698167------------------------------------------------------ Precision@1: 67.08%  Precision@1: 87.50%

top1: [66.72, 66.712, 67.078]
top5: [87.394, 87.456, 87.5]
2024-03-15 18:04:21.912919 epoch: 4 step: 0 cls_loss= 1.70482 (74906 samples/sec)
2024-03-15 18:05:23.150688 epoch: 4 step: 500 cls_loss= 1.32324 (261 samples/sec)
2024-03-15 18:06:25.680623 epoch: 4 step: 1000 cls_loss= 2.08951 (255 samples/sec)
2024-03-15 18:07:33.850274 epoch: 4 step: 1500 cls_loss= 1.75954 (234 samples/sec)
2024-03-15 18:08:45.019858 epoch: 4 step: 2000 cls_loss= 2.20356 (224 samples/sec)
2024-03-15 18:10:03.187832 epoch: 4 step: 2500 cls_loss= 1.82916 (204 samples/sec)
2024-03-15 18:11:42.005753 epoch: 4 step: 3000 cls_loss= 2.11905 (161 samples/sec)
2024-03-15 18:15:18.986083------------------------------------------------------ Precision@1: 67.11%  Precision@1: 87.72%

top1: [66.72, 66.712, 67.078, 67.108]
top5: [87.394, 87.456, 87.5, 87.71600000000001]
2024-03-15 18:15:19.211702 epoch: 5 step: 0 cls_loss= 2.08407 (71299 samples/sec)
2024-03-15 18:16:21.732764 epoch: 5 step: 500 cls_loss= 2.04234 (255 samples/sec)
2024-03-15 18:17:24.498920 epoch: 5 step: 1000 cls_loss= 1.70716 (254 samples/sec)
2024-03-15 18:18:28.548061 epoch: 5 step: 1500 cls_loss= 1.44709 (249 samples/sec)
2024-03-15 18:19:34.266510 epoch: 5 step: 2000 cls_loss= 2.31518 (243 samples/sec)
2024-03-15 18:20:48.001136 epoch: 5 step: 2500 cls_loss= 2.13064 (217 samples/sec)
2024-03-15 18:22:06.235509 epoch: 5 step: 3000 cls_loss= 2.34615 (204 samples/sec)
2024-03-15 18:25:15.337808------------------------------------------------------ Precision@1: 67.15%  Precision@1: 87.71%

top1: [66.72, 66.712, 67.078, 67.108, 67.146]
top5: [87.394, 87.456, 87.5, 87.71600000000001, 87.712]
2024-03-15 18:25:15.545555 epoch: 6 step: 0 cls_loss= 1.67536 (77445 samples/sec)
2024-03-15 18:26:13.584248 epoch: 6 step: 500 cls_loss= 1.44179 (275 samples/sec)
2024-03-15 18:27:11.833467 epoch: 6 step: 1000 cls_loss= 1.70470 (274 samples/sec)
2024-03-15 18:28:10.074537 epoch: 6 step: 1500 cls_loss= 2.04574 (274 samples/sec)
2024-03-15 18:29:08.678829 epoch: 6 step: 2000 cls_loss= 1.59503 (273 samples/sec)
2024-03-15 18:30:07.567957 epoch: 6 step: 2500 cls_loss= 1.78157 (271 samples/sec)
2024-03-15 18:31:05.552514 epoch: 6 step: 3000 cls_loss= 1.52075 (276 samples/sec)
2024-03-15 18:34:07.706163------------------------------------------------------ Precision@1: 67.04%  Precision@1: 87.68%

top1: [66.72, 66.712, 67.078, 67.108, 67.146, 67.036]
top5: [87.394, 87.456, 87.5, 87.71600000000001, 87.712, 87.678]
2024-03-15 18:34:07.905322 epoch: 7 step: 0 cls_loss= 2.83941 (80818 samples/sec)
2024-03-15 18:35:06.011798 epoch: 7 step: 500 cls_loss= 1.65387 (275 samples/sec)
2024-03-15 18:36:04.244984 epoch: 7 step: 1000 cls_loss= 1.35168 (274 samples/sec)
2024-03-15 18:37:02.103764 epoch: 7 step: 1500 cls_loss= 1.81877 (276 samples/sec)
2024-03-15 18:38:00.134661 epoch: 7 step: 2000 cls_loss= 1.78340 (275 samples/sec)
2024-03-15 18:38:58.312797 epoch: 7 step: 2500 cls_loss= 1.74685 (275 samples/sec)
2024-03-15 18:39:56.104274 epoch: 7 step: 3000 cls_loss= 1.63941 (276 samples/sec)
2024-03-15 18:43:00.343411------------------------------------------------------ Precision@1: 67.45%  Precision@1: 87.66%

top1: [66.72, 66.712, 67.078, 67.108, 67.146, 67.036, 67.446]
top5: [87.394, 87.456, 87.5, 87.71600000000001, 87.712, 87.678, 87.656]
2024-03-15 18:43:00.548345 epoch: 8 step: 0 cls_loss= 1.14342 (78505 samples/sec)
2024-03-15 18:43:59.100208 epoch: 8 step: 500 cls_loss= 2.20245 (273 samples/sec)
2024-03-15 18:44:57.225981 epoch: 8 step: 1000 cls_loss= 1.37494 (275 samples/sec)
2024-03-15 18:45:55.440173 epoch: 8 step: 1500 cls_loss= 2.22417 (274 samples/sec)
2024-03-15 18:46:53.648603 epoch: 8 step: 2000 cls_loss= 1.61580 (274 samples/sec)
2024-03-15 18:47:52.493730 epoch: 8 step: 2500 cls_loss= 1.79059 (271 samples/sec)
2024-03-15 18:48:50.907815 epoch: 8 step: 3000 cls_loss= 1.82055 (273 samples/sec)
2024-03-15 18:51:56.283773------------------------------------------------------ Precision@1: 66.72%  Precision@1: 87.45%

top1: [66.72, 66.712, 67.078, 67.108, 67.146, 67.036, 67.446, 66.72200000000001]
top5: [87.394, 87.456, 87.5, 87.71600000000001, 87.712, 87.678, 87.656, 87.44800000000001]
2024-03-15 18:51:56.496390 epoch: 9 step: 0 cls_loss= 1.16304 (75643 samples/sec)
2024-03-15 18:52:54.976723 epoch: 9 step: 500 cls_loss= 1.72280 (273 samples/sec)
2024-03-15 18:53:53.245759 epoch: 9 step: 1000 cls_loss= 1.30405 (274 samples/sec)
2024-03-15 18:54:51.376050 epoch: 9 step: 1500 cls_loss= 1.42615 (275 samples/sec)
2024-03-15 18:55:49.391255 epoch: 9 step: 2000 cls_loss= 1.86656 (275 samples/sec)
2024-03-15 18:56:47.550419 epoch: 9 step: 2500 cls_loss= 2.01576 (275 samples/sec)
2024-03-15 18:57:45.537915 epoch: 9 step: 3000 cls_loss= 1.99148 (275 samples/sec)
2024-03-15 19:00:49.257500------------------------------------------------------ Precision@1: 67.37%  Precision@1: 87.62%

top1: [66.72, 66.712, 67.078, 67.108, 67.146, 67.036, 67.446, 66.72200000000001, 67.368]
top5: [87.394, 87.456, 87.5, 87.71600000000001, 87.712, 87.678, 87.656, 87.44800000000001, 87.62]
2024-03-15 19:00:49.465658 epoch: 10 step: 0 cls_loss= 1.42203 (77343 samples/sec)
2024-03-15 19:01:50.480570 epoch: 10 step: 500 cls_loss= 1.54264 (262 samples/sec)
2024-03-15 19:02:49.731112 epoch: 10 step: 1000 cls_loss= 2.71079 (270 samples/sec)
2024-03-15 19:03:49.439679 epoch: 10 step: 1500 cls_loss= 1.64151 (268 samples/sec)
2024-03-15 19:04:49.379471 epoch: 10 step: 2000 cls_loss= 1.42483 (267 samples/sec)
2024-03-15 19:05:49.182080 epoch: 10 step: 2500 cls_loss= 1.35627 (267 samples/sec)
2024-03-15 19:06:49.567816 epoch: 10 step: 3000 cls_loss= 2.23242 (265 samples/sec)
2024-03-15 19:09:54.390420------------------------------------------------------ Precision@1: 67.27%  Precision@1: 87.59%

top1: [66.72, 66.712, 67.078, 67.108, 67.146, 67.036, 67.446, 66.72200000000001, 67.368, 67.274]
top5: [87.394, 87.456, 87.5, 87.71600000000001, 87.712, 87.678, 87.656, 87.44800000000001, 87.62, 87.59]
=> creating model mobilenet_m1 ...
 learning rate =  1e-05
SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
./imgnet_train_eval.py:177: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-15 19:10:01.557232 epoch: 1 step: 0 cls_loss= 1.89001 (14881 samples/sec)
2024-03-15 19:11:00.356570 epoch: 1 step: 500 cls_loss= 2.20571 (272 samples/sec)
2024-03-15 19:11:59.587352 epoch: 1 step: 1000 cls_loss= 1.89672 (270 samples/sec)
2024-03-15 19:12:58.722060 epoch: 1 step: 1500 cls_loss= 1.71649 (270 samples/sec)
2024-03-15 19:13:58.061780 epoch: 1 step: 2000 cls_loss= 2.12322 (269 samples/sec)
2024-03-15 19:14:57.442484 epoch: 1 step: 2500 cls_loss= 1.78187 (269 samples/sec)
2024-03-15 19:15:56.611683 epoch: 1 step: 3000 cls_loss= 2.37565 (270 samples/sec)
2024-03-15 19:19:38.869831------------------------------------------------------ Precision@1: 66.43%  Precision@1: 87.05%

top1: [66.426]
top5: [87.052]
2024-03-15 19:19:39.071215 epoch: 2 step: 0 cls_loss= 2.16719 (79952 samples/sec)
2024-03-15 19:20:37.365010 epoch: 2 step: 500 cls_loss= 2.27106 (274 samples/sec)
2024-03-15 19:21:34.925180 epoch: 2 step: 1000 cls_loss= 2.64394 (278 samples/sec)
2024-03-15 19:22:33.649073 epoch: 2 step: 1500 cls_loss= 1.70823 (272 samples/sec)
2024-03-15 19:23:33.041600 epoch: 2 step: 2000 cls_loss= 2.25632 (269 samples/sec)
2024-03-15 19:24:32.287183 epoch: 2 step: 2500 cls_loss= 2.09904 (270 samples/sec)
2024-03-15 19:25:30.528724 epoch: 2 step: 3000 cls_loss= 1.45329 (274 samples/sec)
2024-03-15 19:28:42.000976------------------------------------------------------ Precision@1: 66.72%  Precision@1: 87.20%

top1: [66.426, 66.71600000000001]
top5: [87.052, 87.196]
2024-03-15 19:28:42.213087 epoch: 3 step: 0 cls_loss= 2.32614 (75814 samples/sec)
2024-03-15 19:29:43.293673 epoch: 3 step: 500 cls_loss= 1.47082 (262 samples/sec)
2024-03-15 19:30:44.448047 epoch: 3 step: 1000 cls_loss= 2.18309 (261 samples/sec)
2024-03-15 19:31:46.057741 epoch: 3 step: 1500 cls_loss= 1.46463 (259 samples/sec)
2024-03-15 19:32:50.822297 epoch: 3 step: 2000 cls_loss= 1.93875 (247 samples/sec)
2024-03-15 19:33:57.670983 epoch: 3 step: 2500 cls_loss= 1.87625 (239 samples/sec)
2024-03-15 19:35:12.031046 epoch: 3 step: 3000 cls_loss= 1.70190 (215 samples/sec)
2024-03-15 19:38:29.099330------------------------------------------------------ Precision@1: 66.86%  Precision@1: 87.32%

top1: [66.426, 66.71600000000001, 66.86]
top5: [87.052, 87.196, 87.318]
2024-03-15 19:38:29.345269 epoch: 4 step: 0 cls_loss= 1.46498 (65375 samples/sec)
2024-03-15 19:39:32.609224 epoch: 4 step: 500 cls_loss= 1.69885 (252 samples/sec)
2024-03-15 19:40:35.364117 epoch: 4 step: 1000 cls_loss= 2.68248 (255 samples/sec)
2024-03-15 19:41:40.480489 epoch: 4 step: 1500 cls_loss= 1.64776 (245 samples/sec)
2024-03-15 19:42:48.894641 epoch: 4 step: 2000 cls_loss= 2.38283 (233 samples/sec)
2024-03-15 19:44:07.883261 epoch: 4 step: 2500 cls_loss= 1.73215 (202 samples/sec)
2024-03-15 19:45:43.739936 epoch: 4 step: 3000 cls_loss= 1.80548 (166 samples/sec)
2024-03-15 19:49:19.175471------------------------------------------------------ Precision@1: 66.98%  Precision@1: 87.57%

top1: [66.426, 66.71600000000001, 66.86, 66.982]
top5: [87.052, 87.196, 87.318, 87.566]
2024-03-15 19:49:19.545008 epoch: 5 step: 0 cls_loss= 1.89235 (43428 samples/sec)
2024-03-15 19:50:19.956108 epoch: 5 step: 500 cls_loss= 1.95039 (264 samples/sec)
2024-03-15 19:51:22.275180 epoch: 5 step: 1000 cls_loss= 2.42604 (256 samples/sec)
2024-03-15 19:52:24.930890 epoch: 5 step: 1500 cls_loss= 2.20614 (255 samples/sec)
2024-03-15 19:53:32.468080 epoch: 5 step: 2000 cls_loss= 1.31901 (236 samples/sec)
2024-03-15 19:54:48.863386 epoch: 5 step: 2500 cls_loss= 1.14016 (209 samples/sec)
2024-03-15 19:56:09.495877 epoch: 5 step: 3000 cls_loss= 2.24961 (198 samples/sec)
2024-03-15 19:59:30.669123------------------------------------------------------ Precision@1: 67.02%  Precision@1: 87.62%

top1: [66.426, 66.71600000000001, 66.86, 66.982, 67.02]
top5: [87.052, 87.196, 87.318, 87.566, 87.622]
2024-03-15 19:59:30.871664 epoch: 6 step: 0 cls_loss= 1.83649 (79385 samples/sec)
2024-03-15 20:00:28.326296 epoch: 6 step: 500 cls_loss= 3.11552 (278 samples/sec)
2024-03-15 20:01:25.282746 epoch: 6 step: 1000 cls_loss= 1.76278 (280 samples/sec)
2024-03-15 20:02:22.408430 epoch: 6 step: 1500 cls_loss= 2.34817 (280 samples/sec)
2024-03-15 20:03:21.365586 epoch: 6 step: 2000 cls_loss= 1.50643 (271 samples/sec)
2024-03-15 20:04:20.552096 epoch: 6 step: 2500 cls_loss= 2.05300 (270 samples/sec)
2024-03-15 20:05:19.801201 epoch: 6 step: 3000 cls_loss= 1.73149 (270 samples/sec)
2024-03-15 20:08:36.953769------------------------------------------------------ Precision@1: 67.16%  Precision@1: 87.68%

top1: [66.426, 66.71600000000001, 66.86, 66.982, 67.02, 67.158]
top5: [87.052, 87.196, 87.318, 87.566, 87.622, 87.684]
2024-03-15 20:08:37.153447 epoch: 7 step: 0 cls_loss= 2.23262 (80640 samples/sec)
2024-03-15 20:09:37.777681 epoch: 7 step: 500 cls_loss= 2.16602 (263 samples/sec)
2024-03-15 20:10:37.661150 epoch: 7 step: 1000 cls_loss= 1.61825 (267 samples/sec)
2024-03-15 20:11:38.348086 epoch: 7 step: 1500 cls_loss= 1.10408 (263 samples/sec)
2024-03-15 20:12:39.691813 epoch: 7 step: 2000 cls_loss= 1.49385 (260 samples/sec)
2024-03-15 20:13:39.511876 epoch: 7 step: 2500 cls_loss= 1.48885 (267 samples/sec)
2024-03-15 20:14:43.992657 epoch: 7 step: 3000 cls_loss= 1.53609 (248 samples/sec)
2024-03-15 20:17:50.562835------------------------------------------------------ Precision@1: 67.10%  Precision@1: 87.53%

top1: [66.426, 66.71600000000001, 66.86, 66.982, 67.02, 67.158, 67.102]
top5: [87.052, 87.196, 87.318, 87.566, 87.622, 87.684, 87.528]
2024-03-15 20:17:50.798603 epoch: 8 step: 0 cls_loss= 1.73368 (68192 samples/sec)
2024-03-15 20:18:51.043603 epoch: 8 step: 500 cls_loss= 1.86421 (265 samples/sec)
2024-03-15 20:19:52.214519 epoch: 8 step: 1000 cls_loss= 1.84872 (261 samples/sec)
2024-03-15 20:20:52.812253 epoch: 8 step: 1500 cls_loss= 1.79857 (264 samples/sec)
2024-03-15 20:21:54.091871 epoch: 8 step: 2000 cls_loss= 2.26856 (261 samples/sec)
2024-03-15 20:22:55.712993 epoch: 8 step: 2500 cls_loss= 1.30250 (259 samples/sec)
2024-03-15 20:23:56.627811 epoch: 8 step: 3000 cls_loss= 1.53889 (262 samples/sec)
2024-03-15 20:26:58.030691------------------------------------------------------ Precision@1: 66.99%  Precision@1: 87.65%

top1: [66.426, 66.71600000000001, 66.86, 66.982, 67.02, 67.158, 67.102, 66.994]
top5: [87.052, 87.196, 87.318, 87.566, 87.622, 87.684, 87.528, 87.652]
2024-03-15 20:26:58.241860 epoch: 9 step: 0 cls_loss= 1.93389 (76229 samples/sec)
2024-03-15 20:27:59.085767 epoch: 9 step: 500 cls_loss= 0.89055 (263 samples/sec)
2024-03-15 20:28:59.859298 epoch: 9 step: 1000 cls_loss= 2.72837 (263 samples/sec)
2024-03-15 20:29:59.981683 epoch: 9 step: 1500 cls_loss= 3.02003 (266 samples/sec)
2024-03-15 20:31:00.462386 epoch: 9 step: 2000 cls_loss= 1.53215 (264 samples/sec)
2024-03-15 20:32:00.730958 epoch: 9 step: 2500 cls_loss= 2.01159 (265 samples/sec)
2024-03-15 20:33:01.101804 epoch: 9 step: 3000 cls_loss= 1.74039 (265 samples/sec)
2024-03-15 20:36:01.064610------------------------------------------------------ Precision@1: 67.21%  Precision@1: 87.67%

top1: [66.426, 66.71600000000001, 66.86, 66.982, 67.02, 67.158, 67.102, 66.994, 67.214]
top5: [87.052, 87.196, 87.318, 87.566, 87.622, 87.684, 87.528, 87.652, 87.668]
2024-03-15 20:36:01.257682 epoch: 10 step: 0 cls_loss= 2.69512 (83345 samples/sec)
2024-03-15 20:36:59.834053 epoch: 10 step: 500 cls_loss= 2.08183 (273 samples/sec)
2024-03-15 20:37:58.503062 epoch: 10 step: 1000 cls_loss= 1.53944 (272 samples/sec)
2024-03-15 20:38:56.748131 epoch: 10 step: 1500 cls_loss= 1.45359 (274 samples/sec)
2024-03-15 20:39:54.684938 epoch: 10 step: 2000 cls_loss= 1.99902 (276 samples/sec)
2024-03-15 20:40:53.812474 epoch: 10 step: 2500 cls_loss= 1.91618 (270 samples/sec)
2024-03-15 20:41:52.930217 epoch: 10 step: 3000 cls_loss= 2.27360 (270 samples/sec)
2024-03-15 20:44:55.080893------------------------------------------------------ Precision@1: 67.02%  Precision@1: 87.49%

top1: [66.426, 66.71600000000001, 66.86, 66.982, 67.02, 67.158, 67.102, 66.994, 67.214, 67.016]
top5: [87.052, 87.196, 87.318, 87.566, 87.622, 87.684, 87.528, 87.652, 87.668, 87.494]
=> creating model mobilenet_m1 ...
 learning rate =  5e-06
SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
./imgnet_train_eval.py:177: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-15 20:45:01.799249 epoch: 1 step: 0 cls_loss= 1.60339 (10283 samples/sec)
2024-03-15 20:46:15.274955 epoch: 1 step: 500 cls_loss= 2.58708 (217 samples/sec)
2024-03-15 20:47:32.740573 epoch: 1 step: 1000 cls_loss= 1.80803 (206 samples/sec)
2024-03-15 20:48:51.772429 epoch: 1 step: 1500 cls_loss= 1.62327 (202 samples/sec)
2024-03-15 20:50:18.562297 epoch: 1 step: 2000 cls_loss= 1.81324 (184 samples/sec)
2024-03-15 20:51:58.154673 epoch: 1 step: 2500 cls_loss= 2.06108 (160 samples/sec)
2024-03-15 20:53:42.822388 epoch: 1 step: 3000 cls_loss= 2.51202 (152 samples/sec)
2024-03-15 20:57:37.033713------------------------------------------------------ Precision@1: 66.39%  Precision@1: 86.84%

top1: [66.392]
top5: [86.83800000000001]
2024-03-15 20:57:37.230315 epoch: 2 step: 0 cls_loss= 2.70326 (81871 samples/sec)
2024-03-15 20:58:40.417470 epoch: 2 step: 500 cls_loss= 1.87488 (253 samples/sec)
2024-03-15 20:59:48.042929 epoch: 2 step: 1000 cls_loss= 1.94401 (236 samples/sec)
2024-03-15 21:00:57.364571 epoch: 2 step: 1500 cls_loss= 1.58516 (230 samples/sec)
2024-03-15 21:02:13.469239 epoch: 2 step: 2000 cls_loss= 1.79915 (210 samples/sec)
2024-03-15 21:03:40.202155 epoch: 2 step: 2500 cls_loss= 1.56715 (184 samples/sec)
2024-03-15 21:05:30.791248 epoch: 2 step: 3000 cls_loss= 1.74978 (144 samples/sec)
2024-03-15 21:09:05.806139------------------------------------------------------ Precision@1: 66.44%  Precision@1: 86.99%

top1: [66.392, 66.44]
top5: [86.83800000000001, 86.994]
2024-03-15 21:09:06.011622 epoch: 3 step: 0 cls_loss= 1.58832 (78296 samples/sec)
2024-03-15 21:10:10.426078 epoch: 3 step: 500 cls_loss= 2.39724 (248 samples/sec)
2024-03-15 21:11:17.875814 epoch: 3 step: 1000 cls_loss= 1.27685 (237 samples/sec)
2024-03-15 21:12:27.999133 epoch: 3 step: 1500 cls_loss= 1.71245 (228 samples/sec)
2024-03-15 21:13:38.974086 epoch: 3 step: 2000 cls_loss= 2.07177 (225 samples/sec)
2024-03-15 21:14:58.079516 epoch: 3 step: 2500 cls_loss= 1.77502 (202 samples/sec)
2024-03-15 21:16:42.484176 epoch: 3 step: 3000 cls_loss= 1.88248 (153 samples/sec)
2024-03-15 21:20:07.669303------------------------------------------------------ Precision@1: 65.91%  Precision@1: 86.85%

top1: [66.392, 66.44, 65.912]
top5: [86.83800000000001, 86.994, 86.85000000000001]
2024-03-15 21:20:07.890751 epoch: 4 step: 0 cls_loss= 1.75392 (72615 samples/sec)
2024-03-15 21:21:10.711120 epoch: 4 step: 500 cls_loss= 1.93913 (254 samples/sec)
2024-03-15 21:22:13.234872 epoch: 4 step: 1000 cls_loss= 1.71391 (255 samples/sec)
2024-03-15 21:23:16.438721 epoch: 4 step: 1500 cls_loss= 1.85205 (253 samples/sec)
2024-03-15 21:24:20.460423 epoch: 4 step: 2000 cls_loss= 2.72764 (249 samples/sec)
2024-03-15 21:25:33.069844 epoch: 4 step: 2500 cls_loss= 1.85375 (220 samples/sec)
2024-03-15 21:27:01.704123 epoch: 4 step: 3000 cls_loss= 1.51876 (180 samples/sec)
2024-03-15 21:30:48.021093------------------------------------------------------ Precision@1: 66.92%  Precision@1: 87.32%

top1: [66.392, 66.44, 65.912, 66.924]
top5: [86.83800000000001, 86.994, 86.85000000000001, 87.318]
2024-03-15 21:30:48.235989 epoch: 5 step: 0 cls_loss= 2.70149 (74834 samples/sec)
2024-03-15 21:31:52.568701 epoch: 5 step: 500 cls_loss= 2.87490 (248 samples/sec)
2024-03-15 21:32:58.448545 epoch: 5 step: 1000 cls_loss= 1.80252 (242 samples/sec)
2024-03-15 21:34:05.708991 epoch: 5 step: 1500 cls_loss= 2.02859 (237 samples/sec)
2024-03-15 21:35:17.691217 epoch: 5 step: 2000 cls_loss= 2.35149 (222 samples/sec)
2024-03-15 21:36:58.837987 epoch: 5 step: 2500 cls_loss= 2.05747 (158 samples/sec)
2024-03-15 21:39:03.856126 epoch: 5 step: 3000 cls_loss= 1.64565 (127 samples/sec)
2024-03-15 21:42:39.215636------------------------------------------------------ Precision@1: 66.61%  Precision@1: 87.34%

top1: [66.392, 66.44, 65.912, 66.924, 66.606]
top5: [86.83800000000001, 86.994, 86.85000000000001, 87.318, 87.33800000000001]
2024-03-15 21:42:39.465723 epoch: 6 step: 0 cls_loss= 2.50505 (64291 samples/sec)
2024-03-15 21:43:47.370255 epoch: 6 step: 500 cls_loss= 1.62371 (235 samples/sec)
2024-03-15 21:44:55.973410 epoch: 6 step: 1000 cls_loss= 1.80078 (233 samples/sec)
2024-03-15 21:46:03.032914 epoch: 6 step: 1500 cls_loss= 1.94155 (238 samples/sec)
2024-03-15 21:47:11.946550 epoch: 6 step: 2000 cls_loss= 1.52866 (232 samples/sec)
2024-03-15 21:48:26.278107 epoch: 6 step: 2500 cls_loss= 1.78537 (215 samples/sec)
2024-03-15 21:50:04.569376 epoch: 6 step: 3000 cls_loss= 1.50680 (162 samples/sec)
2024-03-15 21:53:31.864363------------------------------------------------------ Precision@1: 67.01%  Precision@1: 87.27%

top1: [66.392, 66.44, 65.912, 66.924, 66.606, 67.012]
top5: [86.83800000000001, 86.994, 86.85000000000001, 87.318, 87.33800000000001, 87.27]
2024-03-15 21:53:32.063670 epoch: 7 step: 0 cls_loss= 1.61225 (80708 samples/sec)
2024-03-15 21:54:33.814272 epoch: 7 step: 500 cls_loss= 1.39922 (259 samples/sec)
2024-03-15 21:55:36.940549 epoch: 7 step: 1000 cls_loss= 2.11366 (253 samples/sec)
2024-03-15 21:56:39.858598 epoch: 7 step: 1500 cls_loss= 2.08849 (254 samples/sec)
2024-03-15 21:57:48.015307 epoch: 7 step: 2000 cls_loss= 1.97156 (234 samples/sec)
2024-03-15 21:59:03.686750 epoch: 7 step: 2500 cls_loss= 2.17828 (211 samples/sec)
2024-03-15 22:00:28.341854 epoch: 7 step: 3000 cls_loss= 1.87106 (189 samples/sec)
2024-03-15 22:04:07.109682------------------------------------------------------ Precision@1: 67.02%  Precision@1: 87.40%

top1: [66.392, 66.44, 65.912, 66.924, 66.606, 67.012, 67.018]
top5: [86.83800000000001, 86.994, 86.85000000000001, 87.318, 87.33800000000001, 87.27, 87.398]
2024-03-15 22:04:07.337626 epoch: 8 step: 0 cls_loss= 1.49537 (70526 samples/sec)
2024-03-15 22:05:15.159702 epoch: 8 step: 500 cls_loss= 2.59065 (235 samples/sec)
2024-03-15 22:06:23.480735 epoch: 8 step: 1000 cls_loss= 2.49162 (234 samples/sec)
2024-03-15 22:07:31.294396 epoch: 8 step: 1500 cls_loss= 1.97266 (235 samples/sec)
2024-03-15 22:08:38.832584 epoch: 8 step: 2000 cls_loss= 1.35271 (236 samples/sec)
2024-03-15 22:09:55.016140 epoch: 8 step: 2500 cls_loss= 2.23892 (210 samples/sec)
2024-03-15 22:11:16.172868 epoch: 8 step: 3000 cls_loss= 1.84541 (197 samples/sec)
2024-03-15 22:14:41.743303------------------------------------------------------ Precision@1: 67.13%  Precision@1: 87.50%

top1: [66.392, 66.44, 65.912, 66.924, 66.606, 67.012, 67.018, 67.126]
top5: [86.83800000000001, 86.994, 86.85000000000001, 87.318, 87.33800000000001, 87.27, 87.398, 87.504]
2024-03-15 22:14:41.979139 epoch: 9 step: 0 cls_loss= 2.08878 (68167 samples/sec)
2024-03-15 22:15:40.173732 epoch: 9 step: 500 cls_loss= 2.55311 (275 samples/sec)
2024-03-15 22:16:40.314324 epoch: 9 step: 1000 cls_loss= 1.71549 (266 samples/sec)
2024-03-15 22:17:40.870411 epoch: 9 step: 1500 cls_loss= 2.36739 (264 samples/sec)
2024-03-15 22:18:46.868187 epoch: 9 step: 2000 cls_loss= 2.16117 (242 samples/sec)
2024-03-15 22:19:53.954396 epoch: 9 step: 2500 cls_loss= 1.69947 (238 samples/sec)
2024-03-15 22:21:19.045084 epoch: 9 step: 3000 cls_loss= 2.04576 (188 samples/sec)
2024-03-15 22:25:18.462803------------------------------------------------------ Precision@1: 67.02%  Precision@1: 87.54%

top1: [66.392, 66.44, 65.912, 66.924, 66.606, 67.012, 67.018, 67.126, 67.016]
top5: [86.83800000000001, 86.994, 86.85000000000001, 87.318, 87.33800000000001, 87.27, 87.398, 87.504, 87.536]
2024-03-15 22:25:18.680106 epoch: 10 step: 0 cls_loss= 2.53756 (74016 samples/sec)
2024-03-15 22:26:21.226468 epoch: 10 step: 500 cls_loss= 1.53612 (255 samples/sec)
2024-03-15 22:27:27.748939 epoch: 10 step: 1000 cls_loss= 1.50148 (240 samples/sec)
2024-03-15 22:28:36.017069 epoch: 10 step: 1500 cls_loss= 1.79531 (234 samples/sec)
2024-03-15 22:29:47.080526 epoch: 10 step: 2000 cls_loss= 2.20655 (225 samples/sec)
2024-03-15 22:31:07.955495 epoch: 10 step: 2500 cls_loss= 2.15617 (197 samples/sec)
2024-03-15 22:33:08.563663 epoch: 10 step: 3000 cls_loss= 2.55053 (132 samples/sec)
2024-03-15 22:36:40.775289------------------------------------------------------ Precision@1: 67.10%  Precision@1: 87.63%

top1: [66.392, 66.44, 65.912, 66.924, 66.606, 67.012, 67.018, 67.126, 67.016, 67.102]
top5: [86.83800000000001, 86.994, 86.85000000000001, 87.318, 87.33800000000001, 87.27, 87.398, 87.504, 87.536, 87.628]
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ exit

Script done on 2024-03-16 08:19:24+08:00 [COMMAND_EXIT_CODE="0"]
