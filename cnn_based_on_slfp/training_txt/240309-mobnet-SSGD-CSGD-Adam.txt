Script started on 2024-03-09 22:16:16+08:00 [TERM="xterm-256color" TTY="/dev/pts/1" COLUMNS="208" LINES="12"]
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ bash bash_train.sh 
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 22:16:20.892381 epoch: 0 step: 0 cls_loss= 1.08396 (48995 samples/sec)
2024-03-09 22:16:23.871513 epoch: 0 step: 100 cls_loss= 0.97992 (10070 samples/sec)
saving....
2024-03-09 22:16:27.781802------------------------------------------------------ Precision@1: 59.82% 

[59.82]

Epoch: 1
2024-03-09 22:16:27.963581 epoch: 1 step: 0 cls_loss= 1.14128 (166041 samples/sec)
2024-03-09 22:16:30.870929 epoch: 1 step: 100 cls_loss= 1.17350 (10321 samples/sec)
saving....
2024-03-09 22:16:34.534324------------------------------------------------------ Precision@1: 59.84% 

[59.82, 59.84]

Epoch: 2
2024-03-09 22:16:34.743743 epoch: 2 step: 0 cls_loss= 1.03487 (144022 samples/sec)
2024-03-09 22:16:37.810959 epoch: 2 step: 100 cls_loss= 1.04798 (9781 samples/sec)
^CTraceback (most recent call last):
  File "./cifar100_train_eval.py", line 420, in <module>
    main()
  File "./cifar100_train_eval.py", line 405, in main
    train(epoch)
  File "./cifar100_train_eval.py", line 261, in train
    for batch_idx, (inputs, targets) in enumerate(train_loader):
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py", line 635, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py", line 1319, in _next_data
    self._shutdown_workers()
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py", line 1444, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/usr/lib/python3.8/multiprocessing/popen_fork.py", line 44, in wait
    if not wait([self.sentinel], timeout):
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/usr/lib/python3.8/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt

]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ bash bash_train.sh 
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 22:16:44.354290 epoch: 0 step: 0 cls_loss= 1.03889 (49801 samples/sec)
2024-03-09 22:16:47.317773 epoch: 0 step: 100 cls_loss= 1.03345 (10123 samples/sec)
saving....
2024-03-09 22:16:51.282671------------------------------------------------------ Precision@1: 59.57% 

[59.57]

Epoch: 1
2024-03-09 22:16:51.474347 epoch: 1 step: 0 cls_loss= 1.03233 (157387 samples/sec)
2024-03-09 22:16:54.404127 epoch: 1 step: 100 cls_loss= 1.04979 (10239 samples/sec)
saving....
2024-03-09 22:16:58.114633------------------------------------------------------ Precision@1: 59.59% 

[59.57, 59.59]

Epoch: 2
2024-03-09 22:16:58.306213 epoch: 2 step: 0 cls_loss= 0.97035 (157477 samples/sec)
2024-03-09 22:17:01.228814 epoch: 2 step: 100 cls_loss= 1.07642 (10269 samples/sec)
saving....
2024-03-09 22:17:05.001663------------------------------------------------------ Precision@1: 59.44% 

[59.57, 59.59, 59.44]

Epoch: 3
2024-03-09 22:17:05.211347 epoch: 3 step: 0 cls_loss= 1.05812 (143922 samples/sec)
2024-03-09 22:17:08.152244 epoch: 3 step: 100 cls_loss= 0.98967 (10204 samples/sec)
saving....
2024-03-09 22:17:11.925083------------------------------------------------------ Precision@1: 59.73% 

[59.57, 59.59, 59.44, 59.73]

Epoch: 4
2024-03-09 22:17:12.148729 epoch: 4 step: 0 cls_loss= 0.99439 (134687 samples/sec)
2024-03-09 22:17:15.161883 epoch: 4 step: 100 cls_loss= 1.03261 (9956 samples/sec)
saving....
2024-03-09 22:17:18.843082------------------------------------------------------ Precision@1: 59.76% 

[59.57, 59.59, 59.44, 59.73, 59.76]

Epoch: 5
2024-03-09 22:17:19.034352 epoch: 5 step: 0 cls_loss= 1.02936 (157768 samples/sec)
2024-03-09 22:17:21.944507 epoch: 5 step: 100 cls_loss= 1.04953 (10312 samples/sec)
saving....
2024-03-09 22:17:25.663902------------------------------------------------------ Precision@1: 59.75% 

[59.57, 59.59, 59.44, 59.73, 59.76, 59.75]

Epoch: 6
2024-03-09 22:17:25.867603 epoch: 6 step: 0 cls_loss= 1.19437 (147928 samples/sec)
2024-03-09 22:17:28.840021 epoch: 6 step: 100 cls_loss= 1.13564 (10093 samples/sec)
saving....
2024-03-09 22:17:32.639252------------------------------------------------------ Precision@1: 59.59% 

[59.57, 59.59, 59.44, 59.73, 59.76, 59.75, 59.59]

Epoch: 7
2024-03-09 22:17:32.862314 epoch: 7 step: 0 cls_loss= 1.03231 (135049 samples/sec)
2024-03-09 22:17:35.783876 epoch: 7 step: 100 cls_loss= 1.08836 (10271 samples/sec)
saving....
2024-03-09 22:17:39.515022------------------------------------------------------ Precision@1: 59.64% 

[59.57, 59.59, 59.44, 59.73, 59.76, 59.75, 59.59, 59.64]

Epoch: 8
2024-03-09 22:17:39.708275 epoch: 8 step: 0 cls_loss= 1.05523 (156113 samples/sec)
2024-03-09 22:17:42.623587 epoch: 8 step: 100 cls_loss= 1.10706 (10294 samples/sec)
saving....
2024-03-09 22:17:46.330637------------------------------------------------------ Precision@1: 59.70% 

[59.57, 59.59, 59.44, 59.73, 59.76, 59.75, 59.59, 59.64, 59.7]

Epoch: 9
2024-03-09 22:17:46.529165 epoch: 9 step: 0 cls_loss= 1.05328 (151790 samples/sec)
2024-03-09 22:17:49.453896 epoch: 9 step: 100 cls_loss= 1.17552 (10261 samples/sec)
saving....
2024-03-09 22:17:53.133743------------------------------------------------------ Precision@1: 59.60% 

[59.57, 59.59, 59.44, 59.73, 59.76, 59.75, 59.59, 59.64, 59.7, 59.6]

Epoch: 10
2024-03-09 22:17:53.340490 epoch: 10 step: 0 cls_loss= 1.06277 (145591 samples/sec)
2024-03-09 22:17:56.282663 epoch: 10 step: 100 cls_loss= 0.99136 (10200 samples/sec)
saving....
2024-03-09 22:18:00.129432------------------------------------------------------ Precision@1: 59.86% 

[59.57, 59.59, 59.44, 59.73, 59.76, 59.75, 59.59, 59.64, 59.7, 59.6, 59.86]

Epoch: 11
2024-03-09 22:18:00.338127 epoch: 11 step: 0 cls_loss= 1.04746 (144410 samples/sec)
2024-03-09 22:18:03.322840 epoch: 11 step: 100 cls_loss= 1.02492 (10054 samples/sec)
saving....
2024-03-09 22:18:07.003375------------------------------------------------------ Precision@1: 59.78% 

[59.57, 59.59, 59.44, 59.73, 59.76, 59.75, 59.59, 59.64, 59.7, 59.6, 59.86, 59.78]

Epoch: 12
2024-03-09 22:18:07.203795 epoch: 12 step: 0 cls_loss= 1.00578 (150468 samples/sec)
2024-03-09 22:18:10.136551 epoch: 12 step: 100 cls_loss= 1.01184 (10229 samples/sec)
saving....
2024-03-09 22:18:13.821512------------------------------------------------------ Precision@1: 59.60% 

[59.57, 59.59, 59.44, 59.73, 59.76, 59.75, 59.59, 59.64, 59.7, 59.6, 59.86, 59.78, 59.6]

Epoch: 13
2024-03-09 22:18:14.024938 epoch: 13 step: 0 cls_loss= 1.07455 (148248 samples/sec)
2024-03-09 22:18:17.178622 epoch: 13 step: 100 cls_loss= 1.06611 (9516 samples/sec)
saving....
2024-03-09 22:18:21.031461------------------------------------------------------ Precision@1: 59.74% 

[59.57, 59.59, 59.44, 59.73, 59.76, 59.75, 59.59, 59.64, 59.7, 59.6, 59.86, 59.78, 59.6, 59.74]

Epoch: 14
2024-03-09 22:18:21.227955 epoch: 14 step: 0 cls_loss= 1.04709 (153558 samples/sec)
2024-03-09 22:18:24.143759 epoch: 14 step: 100 cls_loss= 1.03910 (10290 samples/sec)
saving....
2024-03-09 22:18:28.001707------------------------------------------------------ Precision@1: 59.67% 

[59.57, 59.59, 59.44, 59.73, 59.76, 59.75, 59.59, 59.64, 59.7, 59.6, 59.86, 59.78, 59.6, 59.74, 59.67]

Epoch: 15
2024-03-09 22:18:28.200838 epoch: 15 step: 0 cls_loss= 1.00523 (151491 samples/sec)
2024-03-09 22:18:31.113428 epoch: 15 step: 100 cls_loss= 1.06282 (10300 samples/sec)
saving....
2024-03-09 22:18:34.805595------------------------------------------------------ Precision@1: 59.51% 

[59.57, 59.59, 59.44, 59.73, 59.76, 59.75, 59.59, 59.64, 59.7, 59.6, 59.86, 59.78, 59.6, 59.74, 59.67, 59.51]

Epoch: 16
2024-03-09 22:18:35.027564 epoch: 16 step: 0 cls_loss= 1.05126 (135792 samples/sec)
2024-03-09 22:18:37.958639 epoch: 16 step: 100 cls_loss= 1.05074 (10235 samples/sec)
saving....
2024-03-09 22:18:41.928838------------------------------------------------------ Precision@1: 59.46% 

[59.57, 59.59, 59.44, 59.73, 59.76, 59.75, 59.59, 59.64, 59.7, 59.6, 59.86, 59.78, 59.6, 59.74, 59.67, 59.51, 59.46]

Epoch: 17
2024-03-09 22:18:42.138549 epoch: 17 step: 0 cls_loss= 1.02938 (143813 samples/sec)
2024-03-09 22:18:45.188195 epoch: 17 step: 100 cls_loss= 1.13097 (9837 samples/sec)
saving....
2024-03-09 22:18:48.932881------------------------------------------------------ Precision@1: 59.85% 

[59.57, 59.59, 59.44, 59.73, 59.76, 59.75, 59.59, 59.64, 59.7, 59.6, 59.86, 59.78, 59.6, 59.74, 59.67, 59.51, 59.46, 59.85]

Epoch: 18
2024-03-09 22:18:49.130678 epoch: 18 step: 0 cls_loss= 1.01949 (152487 samples/sec)
2024-03-09 22:18:52.150073 epoch: 18 step: 100 cls_loss= 1.04017 (9937 samples/sec)
saving....
2024-03-09 22:18:55.927825------------------------------------------------------ Precision@1: 59.57% 

[59.57, 59.59, 59.44, 59.73, 59.76, 59.75, 59.59, 59.64, 59.7, 59.6, 59.86, 59.78, 59.6, 59.74, 59.67, 59.51, 59.46, 59.85, 59.57]

Epoch: 19
2024-03-09 22:18:56.137389 epoch: 19 step: 0 cls_loss= 0.97080 (143761 samples/sec)
2024-03-09 22:18:59.072200 epoch: 19 step: 100 cls_loss= 0.89469 (10226 samples/sec)
saving....
2024-03-09 22:19:02.805564------------------------------------------------------ Precision@1: 59.59% 

[59.57, 59.59, 59.44, 59.73, 59.76, 59.75, 59.59, 59.64, 59.7, 59.6, 59.86, 59.78, 59.6, 59.74, 59.67, 59.51, 59.46, 59.85, 59.57, 59.59]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 22:19:05.521071 epoch: 0 step: 0 cls_loss= 1.05389 (50389 samples/sec)
2024-03-09 22:19:08.507468 epoch: 0 step: 100 cls_loss= 1.02945 (10045 samples/sec)
saving....
2024-03-09 22:19:12.502888------------------------------------------------------ Precision@1: 59.70% 

[59.7]

Epoch: 1
2024-03-09 22:19:12.708910 epoch: 1 step: 0 cls_loss= 1.10837 (146479 samples/sec)
2024-03-09 22:19:15.809471 epoch: 1 step: 100 cls_loss= 1.06450 (9675 samples/sec)
saving....
2024-03-09 22:19:19.680879------------------------------------------------------ Precision@1: 59.58% 

[59.7, 59.58]

Epoch: 2
2024-03-09 22:19:19.900182 epoch: 2 step: 0 cls_loss= 1.14779 (137234 samples/sec)
2024-03-09 22:19:22.887829 epoch: 2 step: 100 cls_loss= 0.92177 (10041 samples/sec)
saving....
2024-03-09 22:19:26.612638------------------------------------------------------ Precision@1: 59.73% 

[59.7, 59.58, 59.73]

Epoch: 3
2024-03-09 22:19:26.830121 epoch: 3 step: 0 cls_loss= 1.09544 (138665 samples/sec)
2024-03-09 22:19:29.797108 epoch: 3 step: 100 cls_loss= 0.99418 (10111 samples/sec)
saving....
2024-03-09 22:19:33.613725------------------------------------------------------ Precision@1: 59.55% 

[59.7, 59.58, 59.73, 59.55]

Epoch: 4
2024-03-09 22:19:33.818654 epoch: 4 step: 0 cls_loss= 1.04641 (147073 samples/sec)
2024-03-09 22:19:36.714753 epoch: 4 step: 100 cls_loss= 1.05366 (10359 samples/sec)
saving....
2024-03-09 22:19:40.531476------------------------------------------------------ Precision@1: 59.57% 

[59.7, 59.58, 59.73, 59.55, 59.57]

Epoch: 5
2024-03-09 22:19:40.711890 epoch: 5 step: 0 cls_loss= 1.16688 (167402 samples/sec)
2024-03-09 22:19:43.612809 epoch: 5 step: 100 cls_loss= 1.04738 (10345 samples/sec)
saving....
2024-03-09 22:19:47.269899------------------------------------------------------ Precision@1: 59.72% 

[59.7, 59.58, 59.73, 59.55, 59.57, 59.72]

Epoch: 6
2024-03-09 22:19:47.468672 epoch: 6 step: 0 cls_loss= 1.11868 (151810 samples/sec)
2024-03-09 22:19:50.434882 epoch: 6 step: 100 cls_loss= 1.13123 (10117 samples/sec)
saving....
2024-03-09 22:19:54.174899------------------------------------------------------ Precision@1: 59.55% 

[59.7, 59.58, 59.73, 59.55, 59.57, 59.72, 59.55]

Epoch: 7
2024-03-09 22:19:54.373015 epoch: 7 step: 0 cls_loss= 1.06725 (152249 samples/sec)
2024-03-09 22:19:57.344898 epoch: 7 step: 100 cls_loss= 0.98335 (10095 samples/sec)
saving....
2024-03-09 22:20:01.045154------------------------------------------------------ Precision@1: 59.83% 

[59.7, 59.58, 59.73, 59.55, 59.57, 59.72, 59.55, 59.83]

Epoch: 8
2024-03-09 22:20:01.248102 epoch: 8 step: 0 cls_loss= 1.05947 (148573 samples/sec)
2024-03-09 22:20:04.181039 epoch: 8 step: 100 cls_loss= 0.96440 (10228 samples/sec)
saving....
2024-03-09 22:20:07.988640------------------------------------------------------ Precision@1: 59.81% 

[59.7, 59.58, 59.73, 59.55, 59.57, 59.72, 59.55, 59.83, 59.81]

Epoch: 9
2024-03-09 22:20:08.186797 epoch: 9 step: 0 cls_loss= 0.96865 (152272 samples/sec)
2024-03-09 22:20:11.170798 epoch: 9 step: 100 cls_loss= 1.14691 (10054 samples/sec)
saving....
2024-03-09 22:20:14.887848------------------------------------------------------ Precision@1: 59.66% 

[59.7, 59.58, 59.73, 59.55, 59.57, 59.72, 59.55, 59.83, 59.81, 59.66]

Epoch: 10
2024-03-09 22:20:15.097249 epoch: 10 step: 0 cls_loss= 1.10047 (144001 samples/sec)
2024-03-09 22:20:18.092891 epoch: 10 step: 100 cls_loss= 1.13519 (10014 samples/sec)
saving....
2024-03-09 22:20:21.803681------------------------------------------------------ Precision@1: 59.85% 

[59.7, 59.58, 59.73, 59.55, 59.57, 59.72, 59.55, 59.83, 59.81, 59.66, 59.85]

Epoch: 11
2024-03-09 22:20:22.006373 epoch: 11 step: 0 cls_loss= 1.22545 (148760 samples/sec)
2024-03-09 22:20:24.990917 epoch: 11 step: 100 cls_loss= 1.12973 (10055 samples/sec)
saving....
2024-03-09 22:20:28.773824------------------------------------------------------ Precision@1: 59.76% 

[59.7, 59.58, 59.73, 59.55, 59.57, 59.72, 59.55, 59.83, 59.81, 59.66, 59.85, 59.76]

Epoch: 12
2024-03-09 22:20:28.993647 epoch: 12 step: 0 cls_loss= 1.03347 (137220 samples/sec)
2024-03-09 22:20:31.990916 epoch: 12 step: 100 cls_loss= 0.95979 (10013 samples/sec)
saving....
2024-03-09 22:20:35.722598------------------------------------------------------ Precision@1: 59.63% 

[59.7, 59.58, 59.73, 59.55, 59.57, 59.72, 59.55, 59.83, 59.81, 59.66, 59.85, 59.76, 59.63]

Epoch: 13
2024-03-09 22:20:35.931547 epoch: 13 step: 0 cls_loss= 1.05662 (144371 samples/sec)
2024-03-09 22:20:38.913151 epoch: 13 step: 100 cls_loss= 1.10988 (10061 samples/sec)
saving....
2024-03-09 22:20:42.785450------------------------------------------------------ Precision@1: 59.73% 

[59.7, 59.58, 59.73, 59.55, 59.57, 59.72, 59.55, 59.83, 59.81, 59.66, 59.85, 59.76, 59.63, 59.73]

Epoch: 14
2024-03-09 22:20:42.982599 epoch: 14 step: 0 cls_loss= 1.02757 (152975 samples/sec)
2024-03-09 22:20:45.911062 epoch: 14 step: 100 cls_loss= 0.90778 (10248 samples/sec)
saving....
2024-03-09 22:20:49.584839------------------------------------------------------ Precision@1: 59.71% 

[59.7, 59.58, 59.73, 59.55, 59.57, 59.72, 59.55, 59.83, 59.81, 59.66, 59.85, 59.76, 59.63, 59.73, 59.71]

Epoch: 15
2024-03-09 22:20:49.784457 epoch: 15 step: 0 cls_loss= 0.92027 (151171 samples/sec)
2024-03-09 22:20:52.717706 epoch: 15 step: 100 cls_loss= 1.07658 (10229 samples/sec)
saving....
2024-03-09 22:20:56.468453------------------------------------------------------ Precision@1: 59.68% 

[59.7, 59.58, 59.73, 59.55, 59.57, 59.72, 59.55, 59.83, 59.81, 59.66, 59.85, 59.76, 59.63, 59.73, 59.71, 59.68]

Epoch: 16
2024-03-09 22:20:56.676614 epoch: 16 step: 0 cls_loss= 0.99965 (144683 samples/sec)
2024-03-09 22:20:59.576304 epoch: 16 step: 100 cls_loss= 0.91342 (10349 samples/sec)
saving....
2024-03-09 22:21:03.316806------------------------------------------------------ Precision@1: 59.82% 

[59.7, 59.58, 59.73, 59.55, 59.57, 59.72, 59.55, 59.83, 59.81, 59.66, 59.85, 59.76, 59.63, 59.73, 59.71, 59.68, 59.82]

Epoch: 17
2024-03-09 22:21:03.512397 epoch: 17 step: 0 cls_loss= 1.08416 (154340 samples/sec)
2024-03-09 22:21:06.477358 epoch: 17 step: 100 cls_loss= 0.98478 (10120 samples/sec)
saving....
2024-03-09 22:21:10.152037------------------------------------------------------ Precision@1: 59.81% 

[59.7, 59.58, 59.73, 59.55, 59.57, 59.72, 59.55, 59.83, 59.81, 59.66, 59.85, 59.76, 59.63, 59.73, 59.71, 59.68, 59.82, 59.81]

Epoch: 18
2024-03-09 22:21:10.334393 epoch: 18 step: 0 cls_loss= 1.12849 (165518 samples/sec)
2024-03-09 22:21:13.236903 epoch: 18 step: 100 cls_loss= 1.00048 (10339 samples/sec)
saving....
2024-03-09 22:21:16.967542------------------------------------------------------ Precision@1: 59.82% 

[59.7, 59.58, 59.73, 59.55, 59.57, 59.72, 59.55, 59.83, 59.81, 59.66, 59.85, 59.76, 59.63, 59.73, 59.71, 59.68, 59.82, 59.81, 59.82]

Epoch: 19
2024-03-09 22:21:17.171602 epoch: 19 step: 0 cls_loss= 0.91784 (147803 samples/sec)
2024-03-09 22:21:20.301251 epoch: 19 step: 100 cls_loss= 0.88025 (9587 samples/sec)
saving....
2024-03-09 22:21:23.998931------------------------------------------------------ Precision@1: 59.44% 

[59.7, 59.58, 59.73, 59.55, 59.57, 59.72, 59.55, 59.83, 59.81, 59.66, 59.85, 59.76, 59.63, 59.73, 59.71, 59.68, 59.82, 59.81, 59.82, 59.44]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 22:21:26.764643 epoch: 0 step: 0 cls_loss= 0.96317 (48555 samples/sec)
2024-03-09 22:21:29.701624 epoch: 0 step: 100 cls_loss= 1.08384 (10214 samples/sec)
saving....
2024-03-09 22:21:33.780245------------------------------------------------------ Precision@1: 59.62% 

[59.62]

Epoch: 1
2024-03-09 22:21:33.973015 epoch: 1 step: 0 cls_loss= 0.99378 (156561 samples/sec)
2024-03-09 22:21:36.900146 epoch: 1 step: 100 cls_loss= 0.97307 (10253 samples/sec)
saving....
2024-03-09 22:21:40.638143------------------------------------------------------ Precision@1: 59.63% 

[59.62, 59.63]

Epoch: 2
2024-03-09 22:21:40.837302 epoch: 2 step: 0 cls_loss= 1.08131 (151545 samples/sec)
2024-03-09 22:21:43.756712 epoch: 2 step: 100 cls_loss= 1.01938 (10280 samples/sec)
saving....
2024-03-09 22:21:47.508298------------------------------------------------------ Precision@1: 59.56% 

[59.62, 59.63, 59.56]

Epoch: 3
2024-03-09 22:21:47.716882 epoch: 3 step: 0 cls_loss= 1.01872 (144566 samples/sec)
2024-03-09 22:21:50.654315 epoch: 3 step: 100 cls_loss= 0.92102 (10216 samples/sec)
saving....
2024-03-09 22:21:54.373601------------------------------------------------------ Precision@1: 59.73% 

[59.62, 59.63, 59.56, 59.73]

Epoch: 4
2024-03-09 22:21:54.574034 epoch: 4 step: 0 cls_loss= 0.94624 (150502 samples/sec)
2024-03-09 22:21:57.651010 epoch: 4 step: 100 cls_loss= 1.14843 (9753 samples/sec)
saving....
2024-03-09 22:22:01.428937------------------------------------------------------ Precision@1: 59.68% 

[59.62, 59.63, 59.56, 59.73, 59.68]

Epoch: 5
2024-03-09 22:22:01.622001 epoch: 5 step: 0 cls_loss= 1.03003 (156303 samples/sec)
2024-03-09 22:22:04.697977 epoch: 5 step: 100 cls_loss= 1.00803 (9753 samples/sec)
saving....
2024-03-09 22:22:08.415526------------------------------------------------------ Precision@1: 59.72% 

[59.62, 59.63, 59.56, 59.73, 59.68, 59.72]

Epoch: 6
2024-03-09 22:22:08.619901 epoch: 6 step: 0 cls_loss= 1.06616 (147360 samples/sec)
2024-03-09 22:22:11.811350 epoch: 6 step: 100 cls_loss= 1.17604 (9400 samples/sec)
saving....
2024-03-09 22:22:15.648546------------------------------------------------------ Precision@1: 59.60% 

[59.62, 59.63, 59.56, 59.73, 59.68, 59.72, 59.6]

Epoch: 7
2024-03-09 22:22:15.860022 epoch: 7 step: 0 cls_loss= 1.23317 (142514 samples/sec)
2024-03-09 22:22:18.790479 epoch: 7 step: 100 cls_loss= 0.98318 (10240 samples/sec)
saving....
2024-03-09 22:22:22.500661------------------------------------------------------ Precision@1: 59.61% 

[59.62, 59.63, 59.56, 59.73, 59.68, 59.72, 59.6, 59.61]

Epoch: 8
2024-03-09 22:22:22.701626 epoch: 8 step: 0 cls_loss= 1.17508 (150041 samples/sec)
2024-03-09 22:22:25.814415 epoch: 8 step: 100 cls_loss= 1.10671 (9637 samples/sec)
saving....
2024-03-09 22:22:29.551876------------------------------------------------------ Precision@1: 59.48% 

[59.62, 59.63, 59.56, 59.73, 59.68, 59.72, 59.6, 59.61, 59.48]

Epoch: 9
2024-03-09 22:22:29.763826 epoch: 9 step: 0 cls_loss= 1.00324 (142286 samples/sec)
2024-03-09 22:22:32.691391 epoch: 9 step: 100 cls_loss= 1.12117 (10248 samples/sec)
saving....
2024-03-09 22:22:36.413795------------------------------------------------------ Precision@1: 59.58% 

[59.62, 59.63, 59.56, 59.73, 59.68, 59.72, 59.6, 59.61, 59.48, 59.58]

Epoch: 10
2024-03-09 22:22:36.615744 epoch: 10 step: 0 cls_loss= 1.16876 (149360 samples/sec)
2024-03-09 22:22:39.616264 epoch: 10 step: 100 cls_loss= 1.16029 (10001 samples/sec)
saving....
2024-03-09 22:22:43.452228------------------------------------------------------ Precision@1: 59.51% 

[59.62, 59.63, 59.56, 59.73, 59.68, 59.72, 59.6, 59.61, 59.48, 59.58, 59.51]

Epoch: 11
2024-03-09 22:22:43.639702 epoch: 11 step: 0 cls_loss= 0.96548 (160981 samples/sec)
2024-03-09 22:22:46.577438 epoch: 11 step: 100 cls_loss= 1.18335 (10216 samples/sec)
saving....
2024-03-09 22:22:50.303723------------------------------------------------------ Precision@1: 59.74% 

[59.62, 59.63, 59.56, 59.73, 59.68, 59.72, 59.6, 59.61, 59.48, 59.58, 59.51, 59.74]

Epoch: 12
2024-03-09 22:22:50.518963 epoch: 12 step: 0 cls_loss= 1.17105 (139881 samples/sec)
2024-03-09 22:22:53.594865 epoch: 12 step: 100 cls_loss= 1.05125 (9755 samples/sec)
saving....
2024-03-09 22:22:57.342235------------------------------------------------------ Precision@1: 59.59% 

[59.62, 59.63, 59.56, 59.73, 59.68, 59.72, 59.6, 59.61, 59.48, 59.58, 59.51, 59.74, 59.59]

Epoch: 13
2024-03-09 22:22:57.547857 epoch: 13 step: 0 cls_loss= 1.07149 (146562 samples/sec)
2024-03-09 22:23:00.509606 epoch: 13 step: 100 cls_loss= 1.18819 (10131 samples/sec)
saving....
2024-03-09 22:23:04.299910------------------------------------------------------ Precision@1: 59.58% 

[59.62, 59.63, 59.56, 59.73, 59.68, 59.72, 59.6, 59.61, 59.48, 59.58, 59.51, 59.74, 59.59, 59.58]

Epoch: 14
2024-03-09 22:23:04.531167 epoch: 14 step: 0 cls_loss= 1.10984 (130346 samples/sec)
2024-03-09 22:23:07.644120 epoch: 14 step: 100 cls_loss= 1.13979 (9637 samples/sec)
saving....
2024-03-09 22:23:11.492679------------------------------------------------------ Precision@1: 59.76% 

[59.62, 59.63, 59.56, 59.73, 59.68, 59.72, 59.6, 59.61, 59.48, 59.58, 59.51, 59.74, 59.59, 59.58, 59.76]

Epoch: 15
2024-03-09 22:23:11.696922 epoch: 15 step: 0 cls_loss= 1.06487 (147593 samples/sec)
2024-03-09 22:23:14.686510 epoch: 15 step: 100 cls_loss= 1.12839 (10037 samples/sec)
saving....
2024-03-09 22:23:18.497568------------------------------------------------------ Precision@1: 59.55% 

[59.62, 59.63, 59.56, 59.73, 59.68, 59.72, 59.6, 59.61, 59.48, 59.58, 59.51, 59.74, 59.59, 59.58, 59.76, 59.55]

Epoch: 16
2024-03-09 22:23:18.700596 epoch: 16 step: 0 cls_loss= 0.92692 (148632 samples/sec)
2024-03-09 22:23:21.672279 epoch: 16 step: 100 cls_loss= 1.06239 (10095 samples/sec)
saving....
2024-03-09 22:23:25.449266------------------------------------------------------ Precision@1: 59.66% 

[59.62, 59.63, 59.56, 59.73, 59.68, 59.72, 59.6, 59.61, 59.48, 59.58, 59.51, 59.74, 59.59, 59.58, 59.76, 59.55, 59.66]

Epoch: 17
2024-03-09 22:23:25.636507 epoch: 17 step: 0 cls_loss= 1.09191 (161175 samples/sec)
2024-03-09 22:23:28.605497 epoch: 17 step: 100 cls_loss= 1.07681 (10104 samples/sec)
saving....
2024-03-09 22:23:32.406088------------------------------------------------------ Precision@1: 59.79% 

[59.62, 59.63, 59.56, 59.73, 59.68, 59.72, 59.6, 59.61, 59.48, 59.58, 59.51, 59.74, 59.59, 59.58, 59.76, 59.55, 59.66, 59.79]

Epoch: 18
2024-03-09 22:23:32.610617 epoch: 18 step: 0 cls_loss= 0.97649 (147490 samples/sec)
2024-03-09 22:23:35.532167 epoch: 18 step: 100 cls_loss= 1.15824 (10272 samples/sec)
saving....
2024-03-09 22:23:39.236406------------------------------------------------------ Precision@1: 59.45% 

[59.62, 59.63, 59.56, 59.73, 59.68, 59.72, 59.6, 59.61, 59.48, 59.58, 59.51, 59.74, 59.59, 59.58, 59.76, 59.55, 59.66, 59.79, 59.45]

Epoch: 19
2024-03-09 22:23:39.444311 epoch: 19 step: 0 cls_loss= 1.07104 (145097 samples/sec)
2024-03-09 22:23:42.592598 epoch: 19 step: 100 cls_loss= 0.97762 (9529 samples/sec)
saving....
2024-03-09 22:23:46.395503------------------------------------------------------ Precision@1: 59.82% 

[59.62, 59.63, 59.56, 59.73, 59.68, 59.72, 59.6, 59.61, 59.48, 59.58, 59.51, 59.74, 59.59, 59.58, 59.76, 59.55, 59.66, 59.79, 59.45, 59.82]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 22:23:49.121915 epoch: 0 step: 0 cls_loss= 0.94746 (50919 samples/sec)
2024-03-09 22:23:52.035970 epoch: 0 step: 100 cls_loss= 1.14359 (10295 samples/sec)
saving....
2024-03-09 22:23:55.948351------------------------------------------------------ Precision@1: 59.53% 

[59.53]

Epoch: 1
2024-03-09 22:23:56.143958 epoch: 1 step: 0 cls_loss= 0.99933 (154151 samples/sec)
2024-03-09 22:23:59.134334 epoch: 1 step: 100 cls_loss= 1.03060 (10035 samples/sec)
saving....
2024-03-09 22:24:02.850086------------------------------------------------------ Precision@1: 59.64% 

[59.53, 59.64]

Epoch: 2
2024-03-09 22:24:03.042427 epoch: 2 step: 0 cls_loss= 1.10033 (156841 samples/sec)
2024-03-09 22:24:05.960277 epoch: 2 step: 100 cls_loss= 0.88970 (10284 samples/sec)
saving....
2024-03-09 22:24:09.695571------------------------------------------------------ Precision@1: 59.47% 

[59.53, 59.64, 59.47]

Epoch: 3
2024-03-09 22:24:09.900042 epoch: 3 step: 0 cls_loss= 1.06716 (147411 samples/sec)
2024-03-09 22:24:12.922748 epoch: 3 step: 100 cls_loss= 1.01728 (9927 samples/sec)
saving....
2024-03-09 22:24:16.775151------------------------------------------------------ Precision@1: 59.51% 

[59.53, 59.64, 59.47, 59.51]

Epoch: 4
2024-03-09 22:24:16.992669 epoch: 4 step: 0 cls_loss= 1.13840 (138654 samples/sec)
2024-03-09 22:24:19.975679 epoch: 4 step: 100 cls_loss= 1.08627 (10060 samples/sec)
saving....
2024-03-09 22:24:23.680521------------------------------------------------------ Precision@1: 59.63% 

[59.53, 59.64, 59.47, 59.51, 59.63]

Epoch: 5
2024-03-09 22:24:23.882539 epoch: 5 step: 0 cls_loss= 1.07996 (149077 samples/sec)
2024-03-09 22:24:26.786216 epoch: 5 step: 100 cls_loss= 1.00777 (10335 samples/sec)
saving....
2024-03-09 22:24:30.486363------------------------------------------------------ Precision@1: 59.68% 

[59.53, 59.64, 59.47, 59.51, 59.63, 59.68]

Epoch: 6
2024-03-09 22:24:30.704261 epoch: 6 step: 0 cls_loss= 1.08440 (138422 samples/sec)
2024-03-09 22:24:33.719801 epoch: 6 step: 100 cls_loss= 1.18387 (9948 samples/sec)
saving....
2024-03-09 22:24:37.402066------------------------------------------------------ Precision@1: 59.78% 

[59.53, 59.64, 59.47, 59.51, 59.63, 59.68, 59.78]

Epoch: 7
2024-03-09 22:24:37.609733 epoch: 7 step: 0 cls_loss= 1.17167 (145338 samples/sec)
2024-03-09 22:24:40.524871 epoch: 7 step: 100 cls_loss= 0.99154 (10294 samples/sec)
saving....
2024-03-09 22:24:44.361538------------------------------------------------------ Precision@1: 59.53% 

[59.53, 59.64, 59.47, 59.51, 59.63, 59.68, 59.78, 59.53]

Epoch: 8
2024-03-09 22:24:44.561276 epoch: 8 step: 0 cls_loss= 1.06850 (151080 samples/sec)
2024-03-09 22:24:47.536557 epoch: 8 step: 100 cls_loss= 1.05926 (10083 samples/sec)
saving....
2024-03-09 22:24:51.269185------------------------------------------------------ Precision@1: 59.45% 

[59.53, 59.64, 59.47, 59.51, 59.63, 59.68, 59.78, 59.53, 59.45]

Epoch: 9
2024-03-09 22:24:51.458206 epoch: 9 step: 0 cls_loss= 1.16183 (159785 samples/sec)
2024-03-09 22:24:54.569770 epoch: 9 step: 100 cls_loss= 0.96884 (9641 samples/sec)
saving....
2024-03-09 22:24:58.429118------------------------------------------------------ Precision@1: 59.76% 

[59.53, 59.64, 59.47, 59.51, 59.63, 59.68, 59.78, 59.53, 59.45, 59.76]

Epoch: 10
2024-03-09 22:24:58.639776 epoch: 10 step: 0 cls_loss= 1.04264 (143153 samples/sec)
2024-03-09 22:25:01.551669 epoch: 10 step: 100 cls_loss= 1.08774 (10305 samples/sec)
saving....
2024-03-09 22:25:05.389669------------------------------------------------------ Precision@1: 59.53% 

[59.53, 59.64, 59.47, 59.51, 59.63, 59.68, 59.78, 59.53, 59.45, 59.76, 59.53]

Epoch: 11
2024-03-09 22:25:05.577027 epoch: 11 step: 0 cls_loss= 1.09459 (161099 samples/sec)
2024-03-09 22:25:08.556012 epoch: 11 step: 100 cls_loss= 1.04689 (10074 samples/sec)
saving....
2024-03-09 22:25:12.300920------------------------------------------------------ Precision@1: 60.00% 

[59.53, 59.64, 59.47, 59.51, 59.63, 59.68, 59.78, 59.53, 59.45, 59.76, 59.53, 60.0]

Epoch: 12
2024-03-09 22:25:12.519563 epoch: 12 step: 0 cls_loss= 1.14753 (137936 samples/sec)
2024-03-09 22:25:15.520979 epoch: 12 step: 100 cls_loss= 1.06013 (9997 samples/sec)
saving....
2024-03-09 22:25:19.348797------------------------------------------------------ Precision@1: 59.24% 

[59.53, 59.64, 59.47, 59.51, 59.63, 59.68, 59.78, 59.53, 59.45, 59.76, 59.53, 60.0, 59.24]

Epoch: 13
2024-03-09 22:25:19.559833 epoch: 13 step: 0 cls_loss= 1.06316 (142827 samples/sec)
2024-03-09 22:25:22.546087 epoch: 13 step: 100 cls_loss= 1.04523 (10047 samples/sec)
saving....
2024-03-09 22:25:26.381637------------------------------------------------------ Precision@1: 59.69% 

[59.53, 59.64, 59.47, 59.51, 59.63, 59.68, 59.78, 59.53, 59.45, 59.76, 59.53, 60.0, 59.24, 59.69]

Epoch: 14
2024-03-09 22:25:26.579611 epoch: 14 step: 0 cls_loss= 1.03011 (152474 samples/sec)
2024-03-09 22:25:29.574009 epoch: 14 step: 100 cls_loss= 1.02798 (10018 samples/sec)
saving....
2024-03-09 22:25:33.354475------------------------------------------------------ Precision@1: 59.79% 

[59.53, 59.64, 59.47, 59.51, 59.63, 59.68, 59.78, 59.53, 59.45, 59.76, 59.53, 60.0, 59.24, 59.69, 59.79]

Epoch: 15
2024-03-09 22:25:33.554743 epoch: 15 step: 0 cls_loss= 1.11954 (150596 samples/sec)
2024-03-09 22:25:36.579403 epoch: 15 step: 100 cls_loss= 1.02645 (9920 samples/sec)
saving....
2024-03-09 22:25:40.434404------------------------------------------------------ Precision@1: 59.77% 

[59.53, 59.64, 59.47, 59.51, 59.63, 59.68, 59.78, 59.53, 59.45, 59.76, 59.53, 60.0, 59.24, 59.69, 59.79, 59.77]

Epoch: 16
2024-03-09 22:25:40.644966 epoch: 16 step: 0 cls_loss= 1.12602 (143173 samples/sec)
2024-03-09 22:25:43.606276 epoch: 16 step: 100 cls_loss= 1.14635 (10133 samples/sec)
saving....
2024-03-09 22:25:47.441349------------------------------------------------------ Precision@1: 59.82% 

[59.53, 59.64, 59.47, 59.51, 59.63, 59.68, 59.78, 59.53, 59.45, 59.76, 59.53, 60.0, 59.24, 59.69, 59.79, 59.77, 59.82]

Epoch: 17
2024-03-09 22:25:47.668135 epoch: 17 step: 0 cls_loss= 1.15012 (132974 samples/sec)
2024-03-09 22:25:50.670518 epoch: 17 step: 100 cls_loss= 1.12199 (9992 samples/sec)
saving....
2024-03-09 22:25:54.412422------------------------------------------------------ Precision@1: 59.88% 

[59.53, 59.64, 59.47, 59.51, 59.63, 59.68, 59.78, 59.53, 59.45, 59.76, 59.53, 60.0, 59.24, 59.69, 59.79, 59.77, 59.82, 59.88]

Epoch: 18
2024-03-09 22:25:54.629545 epoch: 18 step: 0 cls_loss= 1.01326 (138956 samples/sec)
2024-03-09 22:25:57.608396 epoch: 18 step: 100 cls_loss= 1.02516 (10071 samples/sec)
saving....
2024-03-09 22:26:01.295344------------------------------------------------------ Precision@1: 59.74% 

[59.53, 59.64, 59.47, 59.51, 59.63, 59.68, 59.78, 59.53, 59.45, 59.76, 59.53, 60.0, 59.24, 59.69, 59.79, 59.77, 59.82, 59.88, 59.74]

Epoch: 19
2024-03-09 22:26:01.513502 epoch: 19 step: 0 cls_loss= 1.07659 (138257 samples/sec)
2024-03-09 22:26:04.428703 epoch: 19 step: 100 cls_loss= 1.11793 (10294 samples/sec)
saving....
2024-03-09 22:26:08.110349------------------------------------------------------ Precision@1: 59.73% 

[59.53, 59.64, 59.47, 59.51, 59.63, 59.68, 59.78, 59.53, 59.45, 59.76, 59.53, 60.0, 59.24, 59.69, 59.79, 59.77, 59.82, 59.88, 59.74, 59.73]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 22:26:10.858844 epoch: 0 step: 0 cls_loss= 1.00582 (47675 samples/sec)
2024-03-09 22:26:13.746398 epoch: 0 step: 100 cls_loss= 0.95623 (10389 samples/sec)
saving....
2024-03-09 22:26:17.680230------------------------------------------------------ Precision@1: 59.85% 

[59.85]

Epoch: 1
2024-03-09 22:26:17.911024 epoch: 1 step: 0 cls_loss= 1.11636 (130462 samples/sec)
2024-03-09 22:26:20.804150 epoch: 1 step: 100 cls_loss= 1.02477 (10369 samples/sec)
saving....
2024-03-09 22:26:24.498461------------------------------------------------------ Precision@1: 59.57% 

[59.85, 59.57]

Epoch: 2
2024-03-09 22:26:24.692717 epoch: 2 step: 0 cls_loss= 1.10409 (155331 samples/sec)
2024-03-09 22:26:27.591135 epoch: 2 step: 100 cls_loss= 1.11286 (10354 samples/sec)
saving....
2024-03-09 22:26:31.260041------------------------------------------------------ Precision@1: 59.66% 

[59.85, 59.57, 59.66]

Epoch: 3
2024-03-09 22:26:31.457734 epoch: 3 step: 0 cls_loss= 1.08106 (152639 samples/sec)
2024-03-09 22:26:34.350074 epoch: 3 step: 100 cls_loss= 1.06046 (10376 samples/sec)
saving....
2024-03-09 22:26:38.022806------------------------------------------------------ Precision@1: 59.74% 

[59.85, 59.57, 59.66, 59.74]

Epoch: 4
2024-03-09 22:26:38.206153 epoch: 4 step: 0 cls_loss= 1.08246 (164620 samples/sec)
2024-03-09 22:26:41.093740 epoch: 4 step: 100 cls_loss= 0.96387 (10391 samples/sec)
saving....
2024-03-09 22:26:44.761061------------------------------------------------------ Precision@1: 59.51% 

[59.85, 59.57, 59.66, 59.74, 59.51]

Epoch: 5
2024-03-09 22:26:44.967114 epoch: 5 step: 0 cls_loss= 0.98435 (146207 samples/sec)
2024-03-09 22:26:47.931315 epoch: 5 step: 100 cls_loss= 1.22204 (10124 samples/sec)
saving....
2024-03-09 22:26:51.651091------------------------------------------------------ Precision@1: 59.54% 

[59.85, 59.57, 59.66, 59.74, 59.51, 59.54]

Epoch: 6
2024-03-09 22:26:51.873974 epoch: 6 step: 0 cls_loss= 0.99656 (135375 samples/sec)
2024-03-09 22:26:54.797091 epoch: 6 step: 100 cls_loss= 1.05365 (10263 samples/sec)
saving....
2024-03-09 22:26:58.464142------------------------------------------------------ Precision@1: 59.86% 

[59.85, 59.57, 59.66, 59.74, 59.51, 59.54, 59.86]

Epoch: 7
2024-03-09 22:26:58.671400 epoch: 7 step: 0 cls_loss= 1.03125 (145411 samples/sec)
2024-03-09 22:27:01.580901 epoch: 7 step: 100 cls_loss= 1.05239 (10315 samples/sec)
saving....
2024-03-09 22:27:05.274554------------------------------------------------------ Precision@1: 59.76% 

[59.85, 59.57, 59.66, 59.74, 59.51, 59.54, 59.86, 59.76]

Epoch: 8
2024-03-09 22:27:05.466901 epoch: 8 step: 0 cls_loss= 0.97823 (156820 samples/sec)
2024-03-09 22:27:08.475159 epoch: 8 step: 100 cls_loss= 1.13148 (9975 samples/sec)
saving....
2024-03-09 22:27:12.232536------------------------------------------------------ Precision@1: 59.88% 

[59.85, 59.57, 59.66, 59.74, 59.51, 59.54, 59.86, 59.76, 59.88]

Epoch: 9
2024-03-09 22:27:12.442367 epoch: 9 step: 0 cls_loss= 1.19589 (143770 samples/sec)
2024-03-09 22:27:15.317222 epoch: 9 step: 100 cls_loss= 1.01699 (10435 samples/sec)
saving....
2024-03-09 22:27:19.039388------------------------------------------------------ Precision@1: 59.58% 

[59.85, 59.57, 59.66, 59.74, 59.51, 59.54, 59.86, 59.76, 59.88, 59.58]

Epoch: 10
2024-03-09 22:27:19.244059 epoch: 10 step: 0 cls_loss= 1.11692 (147431 samples/sec)
2024-03-09 22:27:22.231548 epoch: 10 step: 100 cls_loss= 1.07280 (10042 samples/sec)
saving....
2024-03-09 22:27:25.898729------------------------------------------------------ Precision@1: 59.83% 

[59.85, 59.57, 59.66, 59.74, 59.51, 59.54, 59.86, 59.76, 59.88, 59.58, 59.83]

Epoch: 11
2024-03-09 22:27:26.085980 epoch: 11 step: 0 cls_loss= 1.01803 (161221 samples/sec)
2024-03-09 22:27:28.974623 epoch: 11 step: 100 cls_loss= 0.99359 (10386 samples/sec)
saving....
2024-03-09 22:27:32.652319------------------------------------------------------ Precision@1: 59.92% 

[59.85, 59.57, 59.66, 59.74, 59.51, 59.54, 59.86, 59.76, 59.88, 59.58, 59.83, 59.92]

Epoch: 12
2024-03-09 22:27:32.846557 epoch: 12 step: 0 cls_loss= 0.96262 (155307 samples/sec)
2024-03-09 22:27:35.749512 epoch: 12 step: 100 cls_loss= 1.03156 (10335 samples/sec)
saving....
2024-03-09 22:27:39.383235------------------------------------------------------ Precision@1: 59.63% 

[59.85, 59.57, 59.66, 59.74, 59.51, 59.54, 59.86, 59.76, 59.88, 59.58, 59.83, 59.92, 59.63]

Epoch: 13
2024-03-09 22:27:39.593469 epoch: 13 step: 0 cls_loss= 1.25619 (143446 samples/sec)
2024-03-09 22:27:42.486596 epoch: 13 step: 100 cls_loss= 1.02836 (10369 samples/sec)
saving....
2024-03-09 22:27:46.141310------------------------------------------------------ Precision@1: 59.24% 

[59.85, 59.57, 59.66, 59.74, 59.51, 59.54, 59.86, 59.76, 59.88, 59.58, 59.83, 59.92, 59.63, 59.24]

Epoch: 14
2024-03-09 22:27:46.349996 epoch: 14 step: 0 cls_loss= 0.89450 (144455 samples/sec)
2024-03-09 22:27:49.378204 epoch: 14 step: 100 cls_loss= 0.82758 (9909 samples/sec)
saving....
2024-03-09 22:27:53.106073------------------------------------------------------ Precision@1: 59.61% 

[59.85, 59.57, 59.66, 59.74, 59.51, 59.54, 59.86, 59.76, 59.88, 59.58, 59.83, 59.92, 59.63, 59.24, 59.61]

Epoch: 15
2024-03-09 22:27:53.300298 epoch: 15 step: 0 cls_loss= 1.10968 (155326 samples/sec)
2024-03-09 22:27:56.277110 epoch: 15 step: 100 cls_loss= 0.96045 (10082 samples/sec)
saving....
2024-03-09 22:28:00.047949------------------------------------------------------ Precision@1: 59.81% 

[59.85, 59.57, 59.66, 59.74, 59.51, 59.54, 59.86, 59.76, 59.88, 59.58, 59.83, 59.92, 59.63, 59.24, 59.61, 59.81]

Epoch: 16
2024-03-09 22:28:00.278294 epoch: 16 step: 0 cls_loss= 1.02390 (130759 samples/sec)
2024-03-09 22:28:03.288364 epoch: 16 step: 100 cls_loss= 1.14423 (9966 samples/sec)
saving....
2024-03-09 22:28:06.971880------------------------------------------------------ Precision@1: 59.63% 

[59.85, 59.57, 59.66, 59.74, 59.51, 59.54, 59.86, 59.76, 59.88, 59.58, 59.83, 59.92, 59.63, 59.24, 59.61, 59.81, 59.63]

Epoch: 17
2024-03-09 22:28:07.166330 epoch: 17 step: 0 cls_loss= 1.05231 (155122 samples/sec)
2024-03-09 22:28:10.134159 epoch: 17 step: 100 cls_loss= 1.01388 (10111 samples/sec)
saving....
2024-03-09 22:28:13.895717------------------------------------------------------ Precision@1: 59.78% 

[59.85, 59.57, 59.66, 59.74, 59.51, 59.54, 59.86, 59.76, 59.88, 59.58, 59.83, 59.92, 59.63, 59.24, 59.61, 59.81, 59.63, 59.78]

Epoch: 18
2024-03-09 22:28:14.104891 epoch: 18 step: 0 cls_loss= 1.15768 (144306 samples/sec)
2024-03-09 22:28:16.993331 epoch: 18 step: 100 cls_loss= 1.06302 (10390 samples/sec)
saving....
2024-03-09 22:28:20.664386------------------------------------------------------ Precision@1: 59.55% 

[59.85, 59.57, 59.66, 59.74, 59.51, 59.54, 59.86, 59.76, 59.88, 59.58, 59.83, 59.92, 59.63, 59.24, 59.61, 59.81, 59.63, 59.78, 59.55]

Epoch: 19
2024-03-09 22:28:20.848359 epoch: 19 step: 0 cls_loss= 1.13051 (164038 samples/sec)
2024-03-09 22:28:23.726384 epoch: 19 step: 100 cls_loss= 1.10396 (10425 samples/sec)
saving....
2024-03-09 22:28:27.406524------------------------------------------------------ Precision@1: 59.60% 

[59.85, 59.57, 59.66, 59.74, 59.51, 59.54, 59.86, 59.76, 59.88, 59.58, 59.83, 59.92, 59.63, 59.24, 59.61, 59.81, 59.63, 59.78, 59.55, 59.6]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 22:28:30.133897 epoch: 0 step: 0 cls_loss= 1.06851 (49462 samples/sec)
2024-03-09 22:28:33.108072 epoch: 0 step: 100 cls_loss= 1.02109 (10087 samples/sec)
saving....
2024-03-09 22:28:37.113493------------------------------------------------------ Precision@1: 59.61% 

[59.61]

Epoch: 1
2024-03-09 22:28:37.307096 epoch: 1 step: 0 cls_loss= 1.00899 (155834 samples/sec)
2024-03-09 22:28:40.295167 epoch: 1 step: 100 cls_loss= 0.94052 (10043 samples/sec)
saving....
2024-03-09 22:28:44.004653------------------------------------------------------ Precision@1: 59.53% 

[59.61, 59.53]

Epoch: 2
2024-03-09 22:28:44.196562 epoch: 2 step: 0 cls_loss= 1.07845 (157231 samples/sec)
2024-03-09 22:28:47.125995 epoch: 2 step: 100 cls_loss= 1.03991 (10244 samples/sec)
saving....
2024-03-09 22:28:50.806723------------------------------------------------------ Precision@1: 59.63% 

[59.61, 59.53, 59.63]

Epoch: 3
2024-03-09 22:28:50.996752 epoch: 3 step: 0 cls_loss= 1.02474 (158833 samples/sec)
2024-03-09 22:28:53.941069 epoch: 3 step: 100 cls_loss= 1.02029 (10193 samples/sec)
saving....
2024-03-09 22:28:57.638915------------------------------------------------------ Precision@1: 59.59% 

[59.61, 59.53, 59.63, 59.59]

Epoch: 4
2024-03-09 22:28:57.836004 epoch: 4 step: 0 cls_loss= 1.08319 (153109 samples/sec)
2024-03-09 22:29:00.738215 epoch: 4 step: 100 cls_loss= 0.87595 (10340 samples/sec)
saving....
2024-03-09 22:29:04.397976------------------------------------------------------ Precision@1: 59.58% 

[59.61, 59.53, 59.63, 59.59, 59.58]

Epoch: 5
2024-03-09 22:29:04.591579 epoch: 5 step: 0 cls_loss= 1.10727 (155864 samples/sec)
2024-03-09 22:29:07.497321 epoch: 5 step: 100 cls_loss= 1.02821 (10328 samples/sec)
saving....
2024-03-09 22:29:11.303196------------------------------------------------------ Precision@1: 59.69% 

[59.61, 59.53, 59.63, 59.59, 59.58, 59.69]

Epoch: 6
2024-03-09 22:29:11.548252 epoch: 6 step: 0 cls_loss= 1.16090 (122910 samples/sec)
2024-03-09 22:29:14.550825 epoch: 6 step: 100 cls_loss= 1.11146 (9991 samples/sec)
saving....
2024-03-09 22:29:18.304240------------------------------------------------------ Precision@1: 59.65% 

[59.61, 59.53, 59.63, 59.59, 59.58, 59.69, 59.65]

Epoch: 7
2024-03-09 22:29:18.495894 epoch: 7 step: 0 cls_loss= 1.12382 (157437 samples/sec)
2024-03-09 22:29:21.401974 epoch: 7 step: 100 cls_loss= 0.92245 (10323 samples/sec)
saving....
2024-03-09 22:29:25.091354------------------------------------------------------ Precision@1: 59.74% 

[59.61, 59.53, 59.63, 59.59, 59.58, 59.69, 59.65, 59.74]

Epoch: 8
2024-03-09 22:29:25.303609 epoch: 8 step: 0 cls_loss= 0.99346 (142018 samples/sec)
2024-03-09 22:29:28.192870 epoch: 8 step: 100 cls_loss= 1.08676 (10384 samples/sec)
saving....
2024-03-09 22:29:31.863362------------------------------------------------------ Precision@1: 59.80% 

[59.61, 59.53, 59.63, 59.59, 59.58, 59.69, 59.65, 59.74, 59.8]

Epoch: 9
2024-03-09 22:29:32.054359 epoch: 9 step: 0 cls_loss= 0.97356 (157960 samples/sec)
2024-03-09 22:29:35.190329 epoch: 9 step: 100 cls_loss= 1.02015 (9568 samples/sec)
saving....
2024-03-09 22:29:39.038672------------------------------------------------------ Precision@1: 59.85% 

[59.61, 59.53, 59.63, 59.59, 59.58, 59.69, 59.65, 59.74, 59.8, 59.85]

Epoch: 10
2024-03-09 22:29:39.232406 epoch: 10 step: 0 cls_loss= 1.01490 (155775 samples/sec)
2024-03-09 22:29:42.203192 epoch: 10 step: 100 cls_loss= 1.10812 (10099 samples/sec)
saving....
2024-03-09 22:29:45.971959------------------------------------------------------ Precision@1: 59.69% 

[59.61, 59.53, 59.63, 59.59, 59.58, 59.69, 59.65, 59.74, 59.8, 59.85, 59.69]

Epoch: 11
2024-03-09 22:29:46.200551 epoch: 11 step: 0 cls_loss= 0.92644 (131740 samples/sec)
2024-03-09 22:29:49.083874 epoch: 11 step: 100 cls_loss= 1.05574 (10404 samples/sec)
saving....
2024-03-09 22:29:52.783153------------------------------------------------------ Precision@1: 59.95% 

[59.61, 59.53, 59.63, 59.59, 59.58, 59.69, 59.65, 59.74, 59.8, 59.85, 59.69, 59.95]

Epoch: 12
2024-03-09 22:29:52.983111 epoch: 12 step: 0 cls_loss= 1.11690 (150717 samples/sec)
2024-03-09 22:29:55.924951 epoch: 12 step: 100 cls_loss= 1.00284 (10201 samples/sec)
saving....
2024-03-09 22:29:59.593544------------------------------------------------------ Precision@1: 59.72% 

[59.61, 59.53, 59.63, 59.59, 59.58, 59.69, 59.65, 59.74, 59.8, 59.85, 59.69, 59.95, 59.72]

Epoch: 13
2024-03-09 22:29:59.799739 epoch: 13 step: 0 cls_loss= 1.10380 (146102 samples/sec)
2024-03-09 22:30:02.774083 epoch: 13 step: 100 cls_loss= 1.01997 (10086 samples/sec)
saving....
2024-03-09 22:30:06.483559------------------------------------------------------ Precision@1: 59.73% 

[59.61, 59.53, 59.63, 59.59, 59.58, 59.69, 59.65, 59.74, 59.8, 59.85, 59.69, 59.95, 59.72, 59.73]

Epoch: 14
2024-03-09 22:30:06.683145 epoch: 14 step: 0 cls_loss= 1.07610 (151009 samples/sec)
2024-03-09 22:30:09.645612 epoch: 14 step: 100 cls_loss= 1.07324 (10128 samples/sec)
saving....
2024-03-09 22:30:13.342968------------------------------------------------------ Precision@1: 59.53% 

[59.61, 59.53, 59.63, 59.59, 59.58, 59.69, 59.65, 59.74, 59.8, 59.85, 59.69, 59.95, 59.72, 59.73, 59.53]

Epoch: 15
2024-03-09 22:30:13.541139 epoch: 15 step: 0 cls_loss= 1.03593 (152222 samples/sec)
2024-03-09 22:30:16.508676 epoch: 15 step: 100 cls_loss= 1.06319 (10111 samples/sec)
saving....
2024-03-09 22:30:20.197913------------------------------------------------------ Precision@1: 59.72% 

[59.61, 59.53, 59.63, 59.59, 59.58, 59.69, 59.65, 59.74, 59.8, 59.85, 59.69, 59.95, 59.72, 59.73, 59.53, 59.72]

Epoch: 16
2024-03-09 22:30:20.380601 epoch: 16 step: 0 cls_loss= 1.17762 (165186 samples/sec)
2024-03-09 22:30:23.333563 epoch: 16 step: 100 cls_loss= 0.96704 (10162 samples/sec)
saving....
2024-03-09 22:30:26.996183------------------------------------------------------ Precision@1: 59.69% 

[59.61, 59.53, 59.63, 59.59, 59.58, 59.69, 59.65, 59.74, 59.8, 59.85, 59.69, 59.95, 59.72, 59.73, 59.53, 59.72, 59.69]

Epoch: 17
2024-03-09 22:30:27.187641 epoch: 17 step: 0 cls_loss= 1.05247 (157586 samples/sec)
2024-03-09 22:30:30.089275 epoch: 17 step: 100 cls_loss= 1.06486 (10340 samples/sec)
saving....
2024-03-09 22:30:33.740784------------------------------------------------------ Precision@1: 59.61% 

[59.61, 59.53, 59.63, 59.59, 59.58, 59.69, 59.65, 59.74, 59.8, 59.85, 59.69, 59.95, 59.72, 59.73, 59.53, 59.72, 59.69, 59.61]

Epoch: 18
2024-03-09 22:30:33.955467 epoch: 18 step: 0 cls_loss= 1.05524 (140446 samples/sec)
2024-03-09 22:30:36.912469 epoch: 18 step: 100 cls_loss= 0.98941 (10146 samples/sec)
saving....
2024-03-09 22:30:40.628657------------------------------------------------------ Precision@1: 59.84% 

[59.61, 59.53, 59.63, 59.59, 59.58, 59.69, 59.65, 59.74, 59.8, 59.85, 59.69, 59.95, 59.72, 59.73, 59.53, 59.72, 59.69, 59.61, 59.84]

Epoch: 19
2024-03-09 22:30:40.820991 epoch: 19 step: 0 cls_loss= 0.94225 (156958 samples/sec)
2024-03-09 22:30:43.790924 epoch: 19 step: 100 cls_loss= 1.13114 (10103 samples/sec)
saving....
2024-03-09 22:30:47.496565------------------------------------------------------ Precision@1: 59.64% 

[59.61, 59.53, 59.63, 59.59, 59.58, 59.69, 59.65, 59.74, 59.8, 59.85, 59.69, 59.95, 59.72, 59.73, 59.53, 59.72, 59.69, 59.61, 59.84, 59.64]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 22:30:50.218161 epoch: 0 step: 0 cls_loss= 1.02345 (50384 samples/sec)
2024-03-09 22:30:53.259735 epoch: 0 step: 100 cls_loss= 1.07423 (9863 samples/sec)
saving....
2024-03-09 22:30:57.284093------------------------------------------------------ Precision@1: 59.64% 

[59.64]

Epoch: 1
2024-03-09 22:30:57.482321 epoch: 1 step: 0 cls_loss= 1.02507 (151996 samples/sec)
2024-03-09 22:31:00.578133 epoch: 1 step: 100 cls_loss= 1.11588 (9690 samples/sec)
saving....
2024-03-09 22:31:04.306789------------------------------------------------------ Precision@1: 59.50% 

[59.64, 59.5]

Epoch: 2
2024-03-09 22:31:04.512766 epoch: 2 step: 0 cls_loss= 0.99641 (146239 samples/sec)
2024-03-09 22:31:07.581355 epoch: 2 step: 100 cls_loss= 1.08110 (9776 samples/sec)
saving....
2024-03-09 22:31:11.388972------------------------------------------------------ Precision@1: 59.77% 

[59.64, 59.5, 59.77]

Epoch: 3
2024-03-09 22:31:11.593753 epoch: 3 step: 0 cls_loss= 0.95299 (147304 samples/sec)
2024-03-09 22:31:14.496052 epoch: 3 step: 100 cls_loss= 0.93237 (10340 samples/sec)
saving....
2024-03-09 22:31:18.192681------------------------------------------------------ Precision@1: 59.91% 

[59.64, 59.5, 59.77, 59.91]

Epoch: 4
2024-03-09 22:31:18.398669 epoch: 4 step: 0 cls_loss= 1.03993 (146440 samples/sec)
2024-03-09 22:31:21.299066 epoch: 4 step: 100 cls_loss= 1.00507 (10347 samples/sec)
saving....
2024-03-09 22:31:24.984179------------------------------------------------------ Precision@1: 59.72% 

[59.64, 59.5, 59.77, 59.91, 59.72]

Epoch: 5
2024-03-09 22:31:25.163866 epoch: 5 step: 0 cls_loss= 1.04223 (168058 samples/sec)
2024-03-09 22:31:28.129168 epoch: 5 step: 100 cls_loss= 0.94754 (10119 samples/sec)
saving....
2024-03-09 22:31:31.848821------------------------------------------------------ Precision@1: 59.78% 

[59.64, 59.5, 59.77, 59.91, 59.72, 59.78]

Epoch: 6
2024-03-09 22:31:32.040904 epoch: 6 step: 0 cls_loss= 1.02610 (157116 samples/sec)
2024-03-09 22:31:34.980514 epoch: 6 step: 100 cls_loss= 1.07565 (10209 samples/sec)
saving....
2024-03-09 22:31:38.701164------------------------------------------------------ Precision@1: 59.78% 

[59.64, 59.5, 59.77, 59.91, 59.72, 59.78, 59.78]

Epoch: 7
2024-03-09 22:31:38.901334 epoch: 7 step: 0 cls_loss= 0.93267 (150721 samples/sec)
2024-03-09 22:31:41.886080 epoch: 7 step: 100 cls_loss= 0.96662 (10055 samples/sec)
saving....
2024-03-09 22:31:45.630052------------------------------------------------------ Precision@1: 59.58% 

[59.64, 59.5, 59.77, 59.91, 59.72, 59.78, 59.78, 59.58]

Epoch: 8
2024-03-09 22:31:45.833320 epoch: 8 step: 0 cls_loss= 1.15462 (148363 samples/sec)
2024-03-09 22:31:48.813539 epoch: 8 step: 100 cls_loss= 1.25521 (10070 samples/sec)
saving....
2024-03-09 22:31:52.547334------------------------------------------------------ Precision@1: 59.45% 

[59.64, 59.5, 59.77, 59.91, 59.72, 59.78, 59.78, 59.58, 59.45]

Epoch: 9
2024-03-09 22:31:52.741240 epoch: 9 step: 0 cls_loss= 1.04844 (155393 samples/sec)
2024-03-09 22:31:55.668513 epoch: 9 step: 100 cls_loss= 1.16911 (10252 samples/sec)
saving....
2024-03-09 22:31:59.361682------------------------------------------------------ Precision@1: 59.66% 

[59.64, 59.5, 59.77, 59.91, 59.72, 59.78, 59.78, 59.58, 59.45, 59.66]

Epoch: 10
2024-03-09 22:31:59.565883 epoch: 10 step: 0 cls_loss= 0.98901 (147580 samples/sec)
2024-03-09 22:32:02.478371 epoch: 10 step: 100 cls_loss= 1.24618 (10304 samples/sec)
saving....
2024-03-09 22:32:06.177944------------------------------------------------------ Precision@1: 59.49% 

[59.64, 59.5, 59.77, 59.91, 59.72, 59.78, 59.78, 59.58, 59.45, 59.66, 59.49]

Epoch: 11
2024-03-09 22:32:06.379851 epoch: 11 step: 0 cls_loss= 1.07723 (149346 samples/sec)
2024-03-09 22:32:09.269631 epoch: 11 step: 100 cls_loss= 1.00572 (10381 samples/sec)
saving....
2024-03-09 22:32:12.930352------------------------------------------------------ Precision@1: 59.49% 

[59.64, 59.5, 59.77, 59.91, 59.72, 59.78, 59.78, 59.58, 59.45, 59.66, 59.49, 59.49]

Epoch: 12
2024-03-09 22:32:13.108683 epoch: 12 step: 0 cls_loss= 1.01231 (169417 samples/sec)
2024-03-09 22:32:16.020872 epoch: 12 step: 100 cls_loss= 1.13349 (10305 samples/sec)
saving....
2024-03-09 22:32:19.716455------------------------------------------------------ Precision@1: 59.87% 

[59.64, 59.5, 59.77, 59.91, 59.72, 59.78, 59.78, 59.58, 59.45, 59.66, 59.49, 59.49, 59.87]

Epoch: 13
2024-03-09 22:32:19.919308 epoch: 13 step: 0 cls_loss= 0.90317 (148753 samples/sec)
2024-03-09 22:32:23.067674 epoch: 13 step: 100 cls_loss= 1.00694 (9529 samples/sec)
saving....
2024-03-09 22:32:26.912455------------------------------------------------------ Precision@1: 59.67% 

[59.64, 59.5, 59.77, 59.91, 59.72, 59.78, 59.78, 59.58, 59.45, 59.66, 59.49, 59.49, 59.87, 59.67]

Epoch: 14
2024-03-09 22:32:27.101875 epoch: 14 step: 0 cls_loss= 1.11925 (159377 samples/sec)
2024-03-09 22:32:30.051644 epoch: 14 step: 100 cls_loss= 0.90263 (10172 samples/sec)
saving....
2024-03-09 22:32:33.786130------------------------------------------------------ Precision@1: 59.72% 

[59.64, 59.5, 59.77, 59.91, 59.72, 59.78, 59.78, 59.58, 59.45, 59.66, 59.49, 59.49, 59.87, 59.67, 59.72]

Epoch: 15
2024-03-09 22:32:33.977523 epoch: 15 step: 0 cls_loss= 1.00088 (157728 samples/sec)
2024-03-09 22:32:36.931106 epoch: 15 step: 100 cls_loss= 1.05187 (10160 samples/sec)
saving....
2024-03-09 22:32:40.631759------------------------------------------------------ Precision@1: 59.67% 

[59.64, 59.5, 59.77, 59.91, 59.72, 59.78, 59.78, 59.58, 59.45, 59.66, 59.49, 59.49, 59.87, 59.67, 59.72, 59.67]

Epoch: 16
2024-03-09 22:32:40.835212 epoch: 16 step: 0 cls_loss= 1.06030 (148212 samples/sec)
2024-03-09 22:32:43.746000 epoch: 16 step: 100 cls_loss= 1.11662 (10306 samples/sec)
saving....
2024-03-09 22:32:47.525680------------------------------------------------------ Precision@1: 59.78% 

[59.64, 59.5, 59.77, 59.91, 59.72, 59.78, 59.78, 59.58, 59.45, 59.66, 59.49, 59.49, 59.87, 59.67, 59.72, 59.67, 59.78]

Epoch: 17
2024-03-09 22:32:47.707835 epoch: 17 step: 0 cls_loss= 1.16420 (165698 samples/sec)
2024-03-09 22:32:50.614668 epoch: 17 step: 100 cls_loss= 0.98662 (10324 samples/sec)
saving....
2024-03-09 22:32:54.309250------------------------------------------------------ Precision@1: 59.55% 

[59.64, 59.5, 59.77, 59.91, 59.72, 59.78, 59.78, 59.58, 59.45, 59.66, 59.49, 59.49, 59.87, 59.67, 59.72, 59.67, 59.78, 59.55]

Epoch: 18
2024-03-09 22:32:54.514407 epoch: 18 step: 0 cls_loss= 1.09969 (147028 samples/sec)
2024-03-09 22:32:57.532693 epoch: 18 step: 100 cls_loss= 1.12525 (9939 samples/sec)
saving....
2024-03-09 22:33:01.217887------------------------------------------------------ Precision@1: 59.55% 

[59.64, 59.5, 59.77, 59.91, 59.72, 59.78, 59.78, 59.58, 59.45, 59.66, 59.49, 59.49, 59.87, 59.67, 59.72, 59.67, 59.78, 59.55, 59.55]

Epoch: 19
2024-03-09 22:33:01.419827 epoch: 19 step: 0 cls_loss= 1.10351 (149395 samples/sec)
2024-03-09 22:33:04.324755 epoch: 19 step: 100 cls_loss= 1.09446 (10330 samples/sec)
saving....
2024-03-09 22:33:08.032357------------------------------------------------------ Precision@1: 59.66% 

[59.64, 59.5, 59.77, 59.91, 59.72, 59.78, 59.78, 59.58, 59.45, 59.66, 59.49, 59.49, 59.87, 59.67, 59.72, 59.67, 59.78, 59.55, 59.55, 59.66]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 22:33:10.766873 epoch: 0 step: 0 cls_loss= 0.93530 (47851 samples/sec)
2024-03-09 22:33:13.817022 epoch: 0 step: 100 cls_loss= 1.04116 (9835 samples/sec)
saving....
2024-03-09 22:33:17.809605------------------------------------------------------ Precision@1: 59.53% 

[59.53]

Epoch: 1
2024-03-09 22:33:17.987409 epoch: 1 step: 0 cls_loss= 1.07976 (169767 samples/sec)
2024-03-09 22:33:20.995798 epoch: 1 step: 100 cls_loss= 1.03586 (9975 samples/sec)
saving....
2024-03-09 22:33:24.800192------------------------------------------------------ Precision@1: 59.39% 

[59.53, 59.39]

Epoch: 2
2024-03-09 22:33:24.999910 epoch: 2 step: 0 cls_loss= 1.02488 (151059 samples/sec)
2024-03-09 22:33:28.007541 epoch: 2 step: 100 cls_loss= 0.98165 (9977 samples/sec)
saving....
2024-03-09 22:33:31.796534------------------------------------------------------ Precision@1: 59.52% 

[59.53, 59.39, 59.52]

Epoch: 3
2024-03-09 22:33:32.010866 epoch: 3 step: 0 cls_loss= 0.98702 (140756 samples/sec)
2024-03-09 22:33:35.029356 epoch: 3 step: 100 cls_loss= 1.04191 (9940 samples/sec)
saving....
2024-03-09 22:33:38.843884------------------------------------------------------ Precision@1: 59.82% 

[59.53, 59.39, 59.52, 59.82]

Epoch: 4
2024-03-09 22:33:39.062420 epoch: 4 step: 0 cls_loss= 1.10081 (137987 samples/sec)
2024-03-09 22:33:41.994741 epoch: 4 step: 100 cls_loss= 0.99415 (10231 samples/sec)
saving....
2024-03-09 22:33:45.717264------------------------------------------------------ Precision@1: 59.65% 

[59.53, 59.39, 59.52, 59.82, 59.65]

Epoch: 5
2024-03-09 22:33:45.931889 epoch: 5 step: 0 cls_loss= 1.08935 (140583 samples/sec)
2024-03-09 22:33:48.965176 epoch: 5 step: 100 cls_loss= 1.10566 (9893 samples/sec)
saving....
2024-03-09 22:33:52.713621------------------------------------------------------ Precision@1: 59.54% 

[59.53, 59.39, 59.52, 59.82, 59.65, 59.54]

Epoch: 6
2024-03-09 22:33:52.910047 epoch: 6 step: 0 cls_loss= 1.06215 (153677 samples/sec)
2024-03-09 22:33:55.961492 epoch: 6 step: 100 cls_loss= 1.08058 (9831 samples/sec)
saving....
2024-03-09 22:33:59.783546------------------------------------------------------ Precision@1: 59.57% 

[59.53, 59.39, 59.52, 59.82, 59.65, 59.54, 59.57]

Epoch: 7
2024-03-09 22:33:59.981038 epoch: 7 step: 0 cls_loss= 1.08569 (152796 samples/sec)
2024-03-09 22:34:02.887718 epoch: 7 step: 100 cls_loss= 1.02567 (10322 samples/sec)
saving....
2024-03-09 22:34:06.605268------------------------------------------------------ Precision@1: 59.55% 

[59.53, 59.39, 59.52, 59.82, 59.65, 59.54, 59.57, 59.55]

Epoch: 8
2024-03-09 22:34:06.791706 epoch: 8 step: 0 cls_loss= 1.07720 (161889 samples/sec)
2024-03-09 22:34:09.722140 epoch: 8 step: 100 cls_loss= 0.98301 (10237 samples/sec)
saving....
2024-03-09 22:34:13.394276------------------------------------------------------ Precision@1: 59.78% 

[59.53, 59.39, 59.52, 59.82, 59.65, 59.54, 59.57, 59.55, 59.78]

Epoch: 9
2024-03-09 22:34:13.610244 epoch: 9 step: 0 cls_loss= 0.95637 (139626 samples/sec)
2024-03-09 22:34:16.605126 epoch: 9 step: 100 cls_loss= 1.19750 (10020 samples/sec)
saving....
2024-03-09 22:34:20.353219------------------------------------------------------ Precision@1: 59.79% 

[59.53, 59.39, 59.52, 59.82, 59.65, 59.54, 59.57, 59.55, 59.78, 59.79]

Epoch: 10
2024-03-09 22:34:20.558045 epoch: 10 step: 0 cls_loss= 1.05512 (147122 samples/sec)
2024-03-09 22:34:23.612968 epoch: 10 step: 100 cls_loss= 1.03571 (9820 samples/sec)
saving....
2024-03-09 22:34:27.437079------------------------------------------------------ Precision@1: 59.56% 

[59.53, 59.39, 59.52, 59.82, 59.65, 59.54, 59.57, 59.55, 59.78, 59.79, 59.56]

Epoch: 11
2024-03-09 22:34:27.633301 epoch: 11 step: 0 cls_loss= 0.98385 (153836 samples/sec)
2024-03-09 22:34:30.627148 epoch: 11 step: 100 cls_loss= 1.02323 (10021 samples/sec)
saving....
2024-03-09 22:34:34.394863------------------------------------------------------ Precision@1: 59.71% 

[59.53, 59.39, 59.52, 59.82, 59.65, 59.54, 59.57, 59.55, 59.78, 59.79, 59.56, 59.71]

Epoch: 12
2024-03-09 22:34:34.592200 epoch: 12 step: 0 cls_loss= 1.04416 (152897 samples/sec)
2024-03-09 22:34:37.487475 epoch: 12 step: 100 cls_loss= 1.15241 (10365 samples/sec)
saving....
2024-03-09 22:34:41.138400------------------------------------------------------ Precision@1: 59.39% 

[59.53, 59.39, 59.52, 59.82, 59.65, 59.54, 59.57, 59.55, 59.78, 59.79, 59.56, 59.71, 59.39]

Epoch: 13
2024-03-09 22:34:41.337028 epoch: 13 step: 0 cls_loss= 1.01781 (151904 samples/sec)
2024-03-09 22:34:44.344002 epoch: 13 step: 100 cls_loss= 0.97779 (9977 samples/sec)
saving....
2024-03-09 22:34:48.080035------------------------------------------------------ Precision@1: 59.66% 

[59.53, 59.39, 59.52, 59.82, 59.65, 59.54, 59.57, 59.55, 59.78, 59.79, 59.56, 59.71, 59.39, 59.66]

Epoch: 14
2024-03-09 22:34:48.290333 epoch: 14 step: 0 cls_loss= 1.05158 (143262 samples/sec)
2024-03-09 22:34:51.317664 epoch: 14 step: 100 cls_loss= 0.99121 (9909 samples/sec)
saving....
2024-03-09 22:34:55.123472------------------------------------------------------ Precision@1: 59.78% 

[59.53, 59.39, 59.52, 59.82, 59.65, 59.54, 59.57, 59.55, 59.78, 59.79, 59.56, 59.71, 59.39, 59.66, 59.78]

Epoch: 15
2024-03-09 22:34:55.336470 epoch: 15 step: 0 cls_loss= 1.01930 (141420 samples/sec)
2024-03-09 22:34:58.344109 epoch: 15 step: 100 cls_loss= 1.06231 (9978 samples/sec)
saving....
2024-03-09 22:35:02.177313------------------------------------------------------ Precision@1: 59.35% 

[59.53, 59.39, 59.52, 59.82, 59.65, 59.54, 59.57, 59.55, 59.78, 59.79, 59.56, 59.71, 59.39, 59.66, 59.78, 59.35]

Epoch: 16
2024-03-09 22:35:02.363608 epoch: 16 step: 0 cls_loss= 0.99857 (161938 samples/sec)
2024-03-09 22:35:05.376333 epoch: 16 step: 100 cls_loss= 0.96926 (9958 samples/sec)
saving....
2024-03-09 22:35:09.118301------------------------------------------------------ Precision@1: 59.58% 

[59.53, 59.39, 59.52, 59.82, 59.65, 59.54, 59.57, 59.55, 59.78, 59.79, 59.56, 59.71, 59.39, 59.66, 59.78, 59.35, 59.58]

Epoch: 17
2024-03-09 22:35:09.310015 epoch: 17 step: 0 cls_loss= 1.05222 (157178 samples/sec)
2024-03-09 22:35:12.213208 epoch: 17 step: 100 cls_loss= 1.07476 (10334 samples/sec)
saving....
2024-03-09 22:35:15.900433------------------------------------------------------ Precision@1: 59.42% 

[59.53, 59.39, 59.52, 59.82, 59.65, 59.54, 59.57, 59.55, 59.78, 59.79, 59.56, 59.71, 59.39, 59.66, 59.78, 59.35, 59.58, 59.42]

Epoch: 18
2024-03-09 22:35:16.087470 epoch: 18 step: 0 cls_loss= 1.08231 (161282 samples/sec)
2024-03-09 22:35:18.982399 epoch: 18 step: 100 cls_loss= 1.02809 (10367 samples/sec)
saving....
2024-03-09 22:35:22.715807------------------------------------------------------ Precision@1: 59.70% 

[59.53, 59.39, 59.52, 59.82, 59.65, 59.54, 59.57, 59.55, 59.78, 59.79, 59.56, 59.71, 59.39, 59.66, 59.78, 59.35, 59.58, 59.42, 59.7]

Epoch: 19
2024-03-09 22:35:22.929368 epoch: 19 step: 0 cls_loss= 1.14592 (141042 samples/sec)
2024-03-09 22:35:25.905015 epoch: 19 step: 100 cls_loss= 1.08349 (10085 samples/sec)
saving....
2024-03-09 22:35:29.630824------------------------------------------------------ Precision@1: 59.81% 

[59.53, 59.39, 59.52, 59.82, 59.65, 59.54, 59.57, 59.55, 59.78, 59.79, 59.56, 59.71, 59.39, 59.66, 59.78, 59.35, 59.58, 59.42, 59.7, 59.81]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 22:35:32.351772 epoch: 0 step: 0 cls_loss= 0.95325 (50004 samples/sec)
2024-03-09 22:35:35.347896 epoch: 0 step: 100 cls_loss= 1.05481 (10013 samples/sec)
saving....
2024-03-09 22:35:39.345892------------------------------------------------------ Precision@1: 59.43% 

[59.43]

Epoch: 1
2024-03-09 22:35:39.547313 epoch: 1 step: 0 cls_loss= 0.96267 (149773 samples/sec)
2024-03-09 22:35:42.626200 epoch: 1 step: 100 cls_loss= 0.93529 (9743 samples/sec)
saving....
2024-03-09 22:35:46.419854------------------------------------------------------ Precision@1: 59.74% 

[59.43, 59.74]

Epoch: 2
2024-03-09 22:35:46.626697 epoch: 2 step: 0 cls_loss= 1.03527 (145872 samples/sec)
2024-03-09 22:35:49.534544 epoch: 2 step: 100 cls_loss= 0.99258 (10318 samples/sec)
saving....
2024-03-09 22:35:53.209046------------------------------------------------------ Precision@1: 59.58% 

[59.43, 59.74, 59.58]

Epoch: 3
2024-03-09 22:35:53.408382 epoch: 3 step: 0 cls_loss= 1.00269 (151324 samples/sec)
2024-03-09 22:35:56.414001 epoch: 3 step: 100 cls_loss= 1.09786 (9983 samples/sec)
saving....
2024-03-09 22:36:00.164011------------------------------------------------------ Precision@1: 59.72% 

[59.43, 59.74, 59.58, 59.72]

Epoch: 4
2024-03-09 22:36:00.357239 epoch: 4 step: 0 cls_loss= 0.97254 (156124 samples/sec)
2024-03-09 22:36:03.392267 epoch: 4 step: 100 cls_loss= 1.14233 (9886 samples/sec)
saving....
2024-03-09 22:36:07.244519------------------------------------------------------ Precision@1: 59.77% 

[59.43, 59.74, 59.58, 59.72, 59.77]

Epoch: 5
2024-03-09 22:36:07.441624 epoch: 5 step: 0 cls_loss= 1.13845 (153039 samples/sec)
2024-03-09 22:36:10.415897 epoch: 5 step: 100 cls_loss= 1.03383 (10089 samples/sec)
saving....
2024-03-09 22:36:14.092930------------------------------------------------------ Precision@1: 59.84% 

[59.43, 59.74, 59.58, 59.72, 59.77, 59.84]

Epoch: 6
2024-03-09 22:36:14.319538 epoch: 6 step: 0 cls_loss= 1.02632 (133062 samples/sec)
2024-03-09 22:36:17.254711 epoch: 6 step: 100 cls_loss= 1.07814 (10221 samples/sec)
saving....
2024-03-09 22:36:20.942886------------------------------------------------------ Precision@1: 59.59% 

[59.43, 59.74, 59.58, 59.72, 59.77, 59.84, 59.59]

Epoch: 7
2024-03-09 22:36:21.152411 epoch: 7 step: 0 cls_loss= 1.05115 (143978 samples/sec)
2024-03-09 22:36:24.054189 epoch: 7 step: 100 cls_loss= 0.93404 (10342 samples/sec)
saving....
2024-03-09 22:36:27.735011------------------------------------------------------ Precision@1: 59.64% 

[59.43, 59.74, 59.58, 59.72, 59.77, 59.84, 59.59, 59.64]

Epoch: 8
2024-03-09 22:36:27.933100 epoch: 8 step: 0 cls_loss= 1.07855 (152326 samples/sec)
2024-03-09 22:36:30.876352 epoch: 8 step: 100 cls_loss= 1.01308 (10193 samples/sec)
saving....
2024-03-09 22:36:34.583288------------------------------------------------------ Precision@1: 59.71% 

[59.43, 59.74, 59.58, 59.72, 59.77, 59.84, 59.59, 59.64, 59.71]

Epoch: 9
2024-03-09 22:36:34.787345 epoch: 9 step: 0 cls_loss= 0.98126 (147723 samples/sec)
2024-03-09 22:36:37.954474 epoch: 9 step: 100 cls_loss= 1.13412 (9472 samples/sec)
saving....
2024-03-09 22:36:41.715949------------------------------------------------------ Precision@1: 59.59% 

[59.43, 59.74, 59.58, 59.72, 59.77, 59.84, 59.59, 59.64, 59.71, 59.59]

Epoch: 10
2024-03-09 22:36:41.914651 epoch: 10 step: 0 cls_loss= 0.97441 (151782 samples/sec)
2024-03-09 22:36:44.862761 epoch: 10 step: 100 cls_loss= 1.05541 (10176 samples/sec)
saving....
2024-03-09 22:36:48.566268------------------------------------------------------ Precision@1: 59.60% 

[59.43, 59.74, 59.58, 59.72, 59.77, 59.84, 59.59, 59.64, 59.71, 59.59, 59.6]

Epoch: 11
2024-03-09 22:36:48.766626 epoch: 11 step: 0 cls_loss= 1.07736 (150574 samples/sec)
2024-03-09 22:36:51.773423 epoch: 11 step: 100 cls_loss= 1.00163 (9980 samples/sec)
saving....
2024-03-09 22:36:55.521610------------------------------------------------------ Precision@1: 59.98% 

[59.43, 59.74, 59.58, 59.72, 59.77, 59.84, 59.59, 59.64, 59.71, 59.59, 59.6, 59.98]

Epoch: 12
2024-03-09 22:36:55.713322 epoch: 12 step: 0 cls_loss= 0.98370 (157375 samples/sec)
2024-03-09 22:36:58.623708 epoch: 12 step: 100 cls_loss= 1.06977 (10309 samples/sec)
saving....
2024-03-09 22:37:02.420522------------------------------------------------------ Precision@1: 59.82% 

[59.43, 59.74, 59.58, 59.72, 59.77, 59.84, 59.59, 59.64, 59.71, 59.59, 59.6, 59.98, 59.82]

Epoch: 13
2024-03-09 22:37:02.613024 epoch: 13 step: 0 cls_loss= 1.13783 (156779 samples/sec)
2024-03-09 22:37:05.551737 epoch: 13 step: 100 cls_loss= 1.04307 (10211 samples/sec)
saving....
2024-03-09 22:37:09.258138------------------------------------------------------ Precision@1: 59.81% 

[59.43, 59.74, 59.58, 59.72, 59.77, 59.84, 59.59, 59.64, 59.71, 59.59, 59.6, 59.98, 59.82, 59.81]

Epoch: 14
2024-03-09 22:37:09.446995 epoch: 14 step: 0 cls_loss= 1.21647 (159797 samples/sec)
2024-03-09 22:37:12.387000 epoch: 14 step: 100 cls_loss= 1.11146 (10208 samples/sec)
saving....
2024-03-09 22:37:16.096782------------------------------------------------------ Precision@1: 59.88% 

[59.43, 59.74, 59.58, 59.72, 59.77, 59.84, 59.59, 59.64, 59.71, 59.59, 59.6, 59.98, 59.82, 59.81, 59.88]

Epoch: 15
2024-03-09 22:37:16.307982 epoch: 15 step: 0 cls_loss= 0.94875 (142803 samples/sec)
2024-03-09 22:37:19.490619 epoch: 15 step: 100 cls_loss= 1.01520 (9426 samples/sec)
saving....
2024-03-09 22:37:23.299111------------------------------------------------------ Precision@1: 59.33% 

[59.43, 59.74, 59.58, 59.72, 59.77, 59.84, 59.59, 59.64, 59.71, 59.59, 59.6, 59.98, 59.82, 59.81, 59.88, 59.33]

Epoch: 16
2024-03-09 22:37:23.474332 epoch: 16 step: 0 cls_loss= 1.11237 (172293 samples/sec)
2024-03-09 22:37:26.409985 epoch: 16 step: 100 cls_loss= 1.02374 (10222 samples/sec)
saving....
2024-03-09 22:37:30.139192------------------------------------------------------ Precision@1: 59.63% 

[59.43, 59.74, 59.58, 59.72, 59.77, 59.84, 59.59, 59.64, 59.71, 59.59, 59.6, 59.98, 59.82, 59.81, 59.88, 59.33, 59.63]

Epoch: 17
2024-03-09 22:37:30.352751 epoch: 17 step: 0 cls_loss= 1.08665 (141178 samples/sec)
2024-03-09 22:37:33.243063 epoch: 17 step: 100 cls_loss= 1.11221 (10382 samples/sec)
saving....
2024-03-09 22:37:36.938316------------------------------------------------------ Precision@1: 59.70% 

[59.43, 59.74, 59.58, 59.72, 59.77, 59.84, 59.59, 59.64, 59.71, 59.59, 59.6, 59.98, 59.82, 59.81, 59.88, 59.33, 59.63, 59.7]

Epoch: 18
2024-03-09 22:37:37.152433 epoch: 18 step: 0 cls_loss= 1.09086 (140780 samples/sec)
2024-03-09 22:37:40.159356 epoch: 18 step: 100 cls_loss= 0.98956 (9978 samples/sec)
saving....
2024-03-09 22:37:43.879905------------------------------------------------------ Precision@1: 59.68% 

[59.43, 59.74, 59.58, 59.72, 59.77, 59.84, 59.59, 59.64, 59.71, 59.59, 59.6, 59.98, 59.82, 59.81, 59.88, 59.33, 59.63, 59.7, 59.68]

Epoch: 19
2024-03-09 22:37:44.081697 epoch: 19 step: 0 cls_loss= 0.96842 (149605 samples/sec)
2024-03-09 22:37:47.057214 epoch: 19 step: 100 cls_loss= 0.95808 (10082 samples/sec)
saving....
2024-03-09 22:37:50.786330------------------------------------------------------ Precision@1: 59.46% 

[59.43, 59.74, 59.58, 59.72, 59.77, 59.84, 59.59, 59.64, 59.71, 59.59, 59.6, 59.98, 59.82, 59.81, 59.88, 59.33, 59.63, 59.7, 59.68, 59.46]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 22:37:53.519464 epoch: 0 step: 0 cls_loss= 1.15003 (48760 samples/sec)
2024-03-09 22:37:56.460639 epoch: 0 step: 100 cls_loss= 1.01922 (10200 samples/sec)
saving....
2024-03-09 22:38:00.412333------------------------------------------------------ Precision@1: 59.67% 

[59.67]

Epoch: 1
2024-03-09 22:38:00.603336 epoch: 1 step: 0 cls_loss= 1.03596 (157975 samples/sec)
2024-03-09 22:38:03.598178 epoch: 1 step: 100 cls_loss= 1.10478 (10020 samples/sec)
saving....
2024-03-09 22:38:07.340436------------------------------------------------------ Precision@1: 59.78% 

[59.67, 59.78]

Epoch: 2
2024-03-09 22:38:07.543214 epoch: 2 step: 0 cls_loss= 0.99996 (148644 samples/sec)
2024-03-09 22:38:10.537654 epoch: 2 step: 100 cls_loss= 1.01981 (10018 samples/sec)
saving....
2024-03-09 22:38:14.197956------------------------------------------------------ Precision@1: 59.53% 

[59.67, 59.78, 59.53]

Epoch: 3
2024-03-09 22:38:14.402385 epoch: 3 step: 0 cls_loss= 1.17381 (147356 samples/sec)
2024-03-09 22:38:17.305175 epoch: 3 step: 100 cls_loss= 1.14631 (10339 samples/sec)
saving....
2024-03-09 22:38:20.963810------------------------------------------------------ Precision@1: 59.75% 

[59.67, 59.78, 59.53, 59.75]

Epoch: 4
2024-03-09 22:38:21.159805 epoch: 4 step: 0 cls_loss= 1.14067 (153919 samples/sec)
2024-03-09 22:38:24.154952 epoch: 4 step: 100 cls_loss= 0.99532 (10016 samples/sec)
saving....
2024-03-09 22:38:28.025788------------------------------------------------------ Precision@1: 59.74% 

[59.67, 59.78, 59.53, 59.75, 59.74]

Epoch: 5
2024-03-09 22:38:28.221627 epoch: 5 step: 0 cls_loss= 1.07113 (154135 samples/sec)
2024-03-09 22:38:31.119405 epoch: 5 step: 100 cls_loss= 1.13135 (10357 samples/sec)
saving....
2024-03-09 22:38:34.922695------------------------------------------------------ Precision@1: 59.79% 

[59.67, 59.78, 59.53, 59.75, 59.74, 59.79]

Epoch: 6
2024-03-09 22:38:35.112718 epoch: 6 step: 0 cls_loss= 0.95692 (158749 samples/sec)
2024-03-09 22:38:38.006364 epoch: 6 step: 100 cls_loss= 1.04687 (10371 samples/sec)
saving....
2024-03-09 22:38:41.666618------------------------------------------------------ Precision@1: 59.72% 

[59.67, 59.78, 59.53, 59.75, 59.74, 59.79, 59.72]

Epoch: 7
2024-03-09 22:38:41.896142 epoch: 7 step: 0 cls_loss= 0.96834 (131358 samples/sec)
2024-03-09 22:38:44.796826 epoch: 7 step: 100 cls_loss= 1.03677 (10345 samples/sec)
saving....
2024-03-09 22:38:48.466985------------------------------------------------------ Precision@1: 59.53% 

[59.67, 59.78, 59.53, 59.75, 59.74, 59.79, 59.72, 59.53]

Epoch: 8
2024-03-09 22:38:48.669031 epoch: 8 step: 0 cls_loss= 1.00893 (149314 samples/sec)
2024-03-09 22:38:51.572972 epoch: 8 step: 100 cls_loss= 0.91891 (10335 samples/sec)
saving....
2024-03-09 22:38:55.350503------------------------------------------------------ Precision@1: 59.60% 

[59.67, 59.78, 59.53, 59.75, 59.74, 59.79, 59.72, 59.53, 59.6]

Epoch: 9
2024-03-09 22:38:55.553083 epoch: 9 step: 0 cls_loss= 1.02010 (148908 samples/sec)
2024-03-09 22:38:58.582655 epoch: 9 step: 100 cls_loss= 1.05246 (9902 samples/sec)
saving....
2024-03-09 22:39:02.336494------------------------------------------------------ Precision@1: 59.42% 

[59.67, 59.78, 59.53, 59.75, 59.74, 59.79, 59.72, 59.53, 59.6, 59.42]

Epoch: 10
2024-03-09 22:39:02.544323 epoch: 10 step: 0 cls_loss= 1.01493 (145130 samples/sec)
2024-03-09 22:39:05.665121 epoch: 10 step: 100 cls_loss= 0.99428 (9615 samples/sec)
saving....
2024-03-09 22:39:09.397222------------------------------------------------------ Precision@1: 59.52% 

[59.67, 59.78, 59.53, 59.75, 59.74, 59.79, 59.72, 59.53, 59.6, 59.42, 59.52]

Epoch: 11
2024-03-09 22:39:09.592226 epoch: 11 step: 0 cls_loss= 1.13549 (154642 samples/sec)
2024-03-09 22:39:12.716292 epoch: 11 step: 100 cls_loss= 1.03770 (9606 samples/sec)
saving....
2024-03-09 22:39:16.709225------------------------------------------------------ Precision@1: 59.70% 

[59.67, 59.78, 59.53, 59.75, 59.74, 59.79, 59.72, 59.53, 59.6, 59.42, 59.52, 59.7]

Epoch: 12
2024-03-09 22:39:16.900132 epoch: 12 step: 0 cls_loss= 1.02636 (158057 samples/sec)
2024-03-09 22:39:19.814292 epoch: 12 step: 100 cls_loss= 1.11564 (10295 samples/sec)
saving....
2024-03-09 22:39:23.530485------------------------------------------------------ Precision@1: 59.51% 

[59.67, 59.78, 59.53, 59.75, 59.74, 59.79, 59.72, 59.53, 59.6, 59.42, 59.52, 59.7, 59.51]

Epoch: 13
2024-03-09 22:39:23.728582 epoch: 13 step: 0 cls_loss= 1.02679 (152312 samples/sec)
2024-03-09 22:39:26.735899 epoch: 13 step: 100 cls_loss= 0.91789 (9979 samples/sec)
saving....
2024-03-09 22:39:30.503106------------------------------------------------------ Precision@1: 60.07% 

[59.67, 59.78, 59.53, 59.75, 59.74, 59.79, 59.72, 59.53, 59.6, 59.42, 59.52, 59.7, 59.51, 60.07]

Epoch: 14
2024-03-09 22:39:30.704771 epoch: 14 step: 0 cls_loss= 0.97250 (149374 samples/sec)
2024-03-09 22:39:33.687066 epoch: 14 step: 100 cls_loss= 1.01979 (10061 samples/sec)
saving....
2024-03-09 22:39:37.454954------------------------------------------------------ Precision@1: 59.82% 

[59.67, 59.78, 59.53, 59.75, 59.74, 59.79, 59.72, 59.53, 59.6, 59.42, 59.52, 59.7, 59.51, 60.07, 59.82]

Epoch: 15
2024-03-09 22:39:37.662869 epoch: 15 step: 0 cls_loss= 1.07985 (145120 samples/sec)
2024-03-09 22:39:40.758352 epoch: 15 step: 100 cls_loss= 1.05760 (9694 samples/sec)
saving....
2024-03-09 22:39:44.682217------------------------------------------------------ Precision@1: 59.82% 

[59.67, 59.78, 59.53, 59.75, 59.74, 59.79, 59.72, 59.53, 59.6, 59.42, 59.52, 59.7, 59.51, 60.07, 59.82, 59.82]

Epoch: 16
2024-03-09 22:39:44.889751 epoch: 16 step: 0 cls_loss= 1.06165 (145321 samples/sec)
2024-03-09 22:39:47.962044 epoch: 16 step: 100 cls_loss= 1.07060 (9764 samples/sec)
saving....
2024-03-09 22:39:51.790819------------------------------------------------------ Precision@1: 59.72% 

[59.67, 59.78, 59.53, 59.75, 59.74, 59.79, 59.72, 59.53, 59.6, 59.42, 59.52, 59.7, 59.51, 60.07, 59.82, 59.82, 59.72]

Epoch: 17
2024-03-09 22:39:52.011224 epoch: 17 step: 0 cls_loss= 1.04563 (136770 samples/sec)
2024-03-09 22:39:55.018678 epoch: 17 step: 100 cls_loss= 1.06454 (9975 samples/sec)
saving....
2024-03-09 22:39:58.739685------------------------------------------------------ Precision@1: 59.39% 

[59.67, 59.78, 59.53, 59.75, 59.74, 59.79, 59.72, 59.53, 59.6, 59.42, 59.52, 59.7, 59.51, 60.07, 59.82, 59.82, 59.72, 59.39]

Epoch: 18
2024-03-09 22:39:58.940598 epoch: 18 step: 0 cls_loss= 1.11037 (150135 samples/sec)
2024-03-09 22:40:01.919334 epoch: 18 step: 100 cls_loss= 0.91375 (10074 samples/sec)
saving....
2024-03-09 22:40:05.754771------------------------------------------------------ Precision@1: 59.81% 

[59.67, 59.78, 59.53, 59.75, 59.74, 59.79, 59.72, 59.53, 59.6, 59.42, 59.52, 59.7, 59.51, 60.07, 59.82, 59.82, 59.72, 59.39, 59.81]

Epoch: 19
2024-03-09 22:40:05.960393 epoch: 19 step: 0 cls_loss= 1.00332 (146700 samples/sec)
2024-03-09 22:40:09.008724 epoch: 19 step: 100 cls_loss= 0.97152 (9845 samples/sec)
saving....
2024-03-09 22:40:12.781457------------------------------------------------------ Precision@1: 59.52% 

[59.67, 59.78, 59.53, 59.75, 59.74, 59.79, 59.72, 59.53, 59.6, 59.42, 59.52, 59.7, 59.51, 60.07, 59.82, 59.82, 59.72, 59.39, 59.81, 59.52]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 22:40:15.515081 epoch: 0 step: 0 cls_loss= 1.05535 (48850 samples/sec)
2024-03-09 22:40:18.536848 epoch: 0 step: 100 cls_loss= 1.06155 (9928 samples/sec)
saving....
2024-03-09 22:40:22.588050------------------------------------------------------ Precision@1: 59.62% 

[59.62]

Epoch: 1
2024-03-09 22:40:22.768251 epoch: 1 step: 0 cls_loss= 1.14264 (167242 samples/sec)
2024-03-09 22:40:25.848554 epoch: 1 step: 100 cls_loss= 0.89035 (9743 samples/sec)
saving....
2024-03-09 22:40:29.812155------------------------------------------------------ Precision@1: 59.67% 

[59.62, 59.67]

Epoch: 2
2024-03-09 22:40:30.018784 epoch: 2 step: 0 cls_loss= 0.94576 (145831 samples/sec)
2024-03-09 22:40:33.080433 epoch: 2 step: 100 cls_loss= 1.11957 (9798 samples/sec)
saving....
2024-03-09 22:40:36.889547------------------------------------------------------ Precision@1: 59.47% 

[59.62, 59.67, 59.47]

Epoch: 3
2024-03-09 22:40:37.098719 epoch: 3 step: 0 cls_loss= 1.11388 (144189 samples/sec)
2024-03-09 22:40:40.043325 epoch: 3 step: 100 cls_loss= 1.05031 (10188 samples/sec)
saving....
2024-03-09 22:40:43.837661------------------------------------------------------ Precision@1: 59.58% 

[59.62, 59.67, 59.47, 59.58]

Epoch: 4
2024-03-09 22:40:44.047154 epoch: 4 step: 0 cls_loss= 1.00162 (143944 samples/sec)
2024-03-09 22:40:46.955722 epoch: 4 step: 100 cls_loss= 0.98354 (10317 samples/sec)
saving....
2024-03-09 22:40:50.686477------------------------------------------------------ Precision@1: 59.75% 

[59.62, 59.67, 59.47, 59.58, 59.75]

Epoch: 5
2024-03-09 22:40:50.883052 epoch: 5 step: 0 cls_loss= 1.09727 (153397 samples/sec)
2024-03-09 22:40:53.970573 epoch: 5 step: 100 cls_loss= 0.98498 (9716 samples/sec)
saving....
2024-03-09 22:40:57.926323------------------------------------------------------ Precision@1: 59.77% 

[59.62, 59.67, 59.47, 59.58, 59.75, 59.77]

Epoch: 6
2024-03-09 22:40:58.135004 epoch: 6 step: 0 cls_loss= 0.96616 (144533 samples/sec)
2024-03-09 22:41:01.109181 epoch: 6 step: 100 cls_loss= 1.13640 (10088 samples/sec)
saving....
2024-03-09 22:41:04.930372------------------------------------------------------ Precision@1: 59.74% 

[59.62, 59.67, 59.47, 59.58, 59.75, 59.77, 59.74]

Epoch: 7
2024-03-09 22:41:05.135999 epoch: 7 step: 0 cls_loss= 1.02995 (146657 samples/sec)
2024-03-09 22:41:08.127920 epoch: 7 step: 100 cls_loss= 1.18683 (10027 samples/sec)
saving....
2024-03-09 22:41:11.843347------------------------------------------------------ Precision@1: 59.74% 

[59.62, 59.67, 59.47, 59.58, 59.75, 59.77, 59.74, 59.74]

Epoch: 8
2024-03-09 22:41:12.073036 epoch: 8 step: 0 cls_loss= 1.06009 (131063 samples/sec)
2024-03-09 22:41:15.056216 epoch: 8 step: 100 cls_loss= 1.15579 (10056 samples/sec)
saving....
2024-03-09 22:41:18.852309------------------------------------------------------ Precision@1: 59.66% 

[59.62, 59.67, 59.47, 59.58, 59.75, 59.77, 59.74, 59.74, 59.66]

Epoch: 9
2024-03-09 22:41:19.055316 epoch: 9 step: 0 cls_loss= 1.06087 (148583 samples/sec)
2024-03-09 22:41:22.027825 epoch: 9 step: 100 cls_loss= 1.06804 (10097 samples/sec)
saving....
2024-03-09 22:41:25.840275------------------------------------------------------ Precision@1: 59.86% 

[59.62, 59.67, 59.47, 59.58, 59.75, 59.77, 59.74, 59.74, 59.66, 59.86]

Epoch: 10
2024-03-09 22:41:26.038429 epoch: 10 step: 0 cls_loss= 1.14740 (152226 samples/sec)
2024-03-09 22:41:28.946920 epoch: 10 step: 100 cls_loss= 1.10117 (10318 samples/sec)
saving....
2024-03-09 22:41:32.650591------------------------------------------------------ Precision@1: 59.58% 

[59.62, 59.67, 59.47, 59.58, 59.75, 59.77, 59.74, 59.74, 59.66, 59.86, 59.58]

Epoch: 11
2024-03-09 22:41:32.851819 epoch: 11 step: 0 cls_loss= 0.92631 (149806 samples/sec)
2024-03-09 22:41:35.804679 epoch: 11 step: 100 cls_loss= 1.01273 (10164 samples/sec)
saving....
2024-03-09 22:41:39.566061------------------------------------------------------ Precision@1: 59.67% 

[59.62, 59.67, 59.47, 59.58, 59.75, 59.77, 59.74, 59.74, 59.66, 59.86, 59.58, 59.67]

Epoch: 12
2024-03-09 22:41:39.756106 epoch: 12 step: 0 cls_loss= 1.06221 (158761 samples/sec)
2024-03-09 22:41:42.675814 epoch: 12 step: 100 cls_loss= 1.04076 (10278 samples/sec)
saving....
2024-03-09 22:41:46.388873------------------------------------------------------ Precision@1: 59.76% 

[59.62, 59.67, 59.47, 59.58, 59.75, 59.77, 59.74, 59.74, 59.66, 59.86, 59.58, 59.67, 59.76]

Epoch: 13
2024-03-09 22:41:46.589082 epoch: 13 step: 0 cls_loss= 1.05375 (150588 samples/sec)
2024-03-09 22:41:49.507549 epoch: 13 step: 100 cls_loss= 1.05382 (10284 samples/sec)
saving....
2024-03-09 22:41:53.209739------------------------------------------------------ Precision@1: 59.64% 

[59.62, 59.67, 59.47, 59.58, 59.75, 59.77, 59.74, 59.74, 59.66, 59.86, 59.58, 59.67, 59.76, 59.64]

Epoch: 14
2024-03-09 22:41:53.396455 epoch: 14 step: 0 cls_loss= 1.09394 (161621 samples/sec)
2024-03-09 22:41:56.318302 epoch: 14 step: 100 cls_loss= 0.98241 (10272 samples/sec)
saving....
2024-03-09 22:42:00.066375------------------------------------------------------ Precision@1: 59.74% 

[59.62, 59.67, 59.47, 59.58, 59.75, 59.77, 59.74, 59.74, 59.66, 59.86, 59.58, 59.67, 59.76, 59.64, 59.74]

Epoch: 15
2024-03-09 22:42:00.263639 epoch: 15 step: 0 cls_loss= 0.92499 (152965 samples/sec)
2024-03-09 22:42:03.210388 epoch: 15 step: 100 cls_loss= 1.03562 (10185 samples/sec)
saving....
2024-03-09 22:42:07.308898------------------------------------------------------ Precision@1: 59.90% 

[59.62, 59.67, 59.47, 59.58, 59.75, 59.77, 59.74, 59.74, 59.66, 59.86, 59.58, 59.67, 59.76, 59.64, 59.74, 59.9]

Epoch: 16
2024-03-09 22:42:07.501271 epoch: 16 step: 0 cls_loss= 1.04944 (156775 samples/sec)
2024-03-09 22:42:10.632545 epoch: 16 step: 100 cls_loss= 1.06736 (9584 samples/sec)
saving....
2024-03-09 22:42:14.377201------------------------------------------------------ Precision@1: 59.58% 

[59.62, 59.67, 59.47, 59.58, 59.75, 59.77, 59.74, 59.74, 59.66, 59.86, 59.58, 59.67, 59.76, 59.64, 59.74, 59.9, 59.58]

Epoch: 17
2024-03-09 22:42:14.599270 epoch: 17 step: 0 cls_loss= 0.94100 (135669 samples/sec)
2024-03-09 22:42:17.560872 epoch: 17 step: 100 cls_loss= 0.97075 (10129 samples/sec)
saving....
2024-03-09 22:42:21.329050------------------------------------------------------ Precision@1: 59.79% 

[59.62, 59.67, 59.47, 59.58, 59.75, 59.77, 59.74, 59.74, 59.66, 59.86, 59.58, 59.67, 59.76, 59.64, 59.74, 59.9, 59.58, 59.79]

Epoch: 18
2024-03-09 22:42:21.540976 epoch: 18 step: 0 cls_loss= 0.99015 (142365 samples/sec)
2024-03-09 22:42:24.441384 epoch: 18 step: 100 cls_loss= 1.01585 (10346 samples/sec)
saving....
2024-03-09 22:42:28.150424------------------------------------------------------ Precision@1: 59.89% 

[59.62, 59.67, 59.47, 59.58, 59.75, 59.77, 59.74, 59.74, 59.66, 59.86, 59.58, 59.67, 59.76, 59.64, 59.74, 59.9, 59.58, 59.79, 59.89]

Epoch: 19
2024-03-09 22:42:28.358127 epoch: 19 step: 0 cls_loss= 0.99097 (145288 samples/sec)
2024-03-09 22:42:31.318468 epoch: 19 step: 100 cls_loss= 1.11920 (10134 samples/sec)
saving....
2024-03-09 22:42:35.130023------------------------------------------------------ Precision@1: 59.76% 

[59.62, 59.67, 59.47, 59.58, 59.75, 59.77, 59.74, 59.74, 59.66, 59.86, 59.58, 59.67, 59.76, 59.64, 59.74, 59.9, 59.58, 59.79, 59.89, 59.76]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 22:42:37.827657 epoch: 0 step: 0 cls_loss= 0.93817 (51063 samples/sec)
2024-03-09 22:42:40.758073 epoch: 0 step: 100 cls_loss= 1.13264 (10237 samples/sec)
saving....
2024-03-09 22:42:44.711869------------------------------------------------------ Precision@1: 59.51% 

[59.51]

Epoch: 1
2024-03-09 22:42:44.912785 epoch: 1 step: 0 cls_loss= 1.01924 (150183 samples/sec)
2024-03-09 22:42:47.894561 epoch: 1 step: 100 cls_loss= 0.98020 (10061 samples/sec)
saving....
2024-03-09 22:42:51.615290------------------------------------------------------ Precision@1: 59.62% 

[59.51, 59.62]

Epoch: 2
2024-03-09 22:42:51.813665 epoch: 2 step: 0 cls_loss= 0.91917 (152028 samples/sec)
2024-03-09 22:42:54.773111 epoch: 2 step: 100 cls_loss= 0.96883 (10141 samples/sec)
saving....
2024-03-09 22:42:58.495444------------------------------------------------------ Precision@1: 59.79% 

[59.51, 59.62, 59.79]

Epoch: 3
2024-03-09 22:42:58.684116 epoch: 3 step: 0 cls_loss= 0.93133 (159952 samples/sec)
2024-03-09 22:43:01.794639 epoch: 3 step: 100 cls_loss= 1.00107 (9644 samples/sec)
saving....
2024-03-09 22:43:05.592901------------------------------------------------------ Precision@1: 59.89% 

[59.51, 59.62, 59.79, 59.89]

Epoch: 4
2024-03-09 22:43:05.821415 epoch: 4 step: 0 cls_loss= 0.97332 (131985 samples/sec)
2024-03-09 22:43:08.728731 epoch: 4 step: 100 cls_loss= 1.07621 (10319 samples/sec)
saving....
2024-03-09 22:43:12.459213------------------------------------------------------ Precision@1: 59.45% 

[59.51, 59.62, 59.79, 59.89, 59.45]

Epoch: 5
2024-03-09 22:43:12.668180 epoch: 5 step: 0 cls_loss= 1.02413 (144241 samples/sec)
2024-03-09 22:43:15.554035 epoch: 5 step: 100 cls_loss= 1.04120 (10400 samples/sec)
saving....
2024-03-09 22:43:19.230473------------------------------------------------------ Precision@1: 59.34% 

[59.51, 59.62, 59.79, 59.89, 59.45, 59.34]

Epoch: 6
2024-03-09 22:43:19.433317 epoch: 6 step: 0 cls_loss= 0.89195 (148727 samples/sec)
2024-03-09 22:43:22.377570 epoch: 6 step: 100 cls_loss= 1.02672 (10194 samples/sec)
saving....
2024-03-09 22:43:26.090215------------------------------------------------------ Precision@1: 59.56% 

[59.51, 59.62, 59.79, 59.89, 59.45, 59.34, 59.56]

Epoch: 7
2024-03-09 22:43:26.293334 epoch: 7 step: 0 cls_loss= 1.08396 (148421 samples/sec)
2024-03-09 22:43:29.280098 epoch: 7 step: 100 cls_loss= 0.94951 (10044 samples/sec)
saving....
2024-03-09 22:43:33.011803------------------------------------------------------ Precision@1: 59.52% 

[59.51, 59.62, 59.79, 59.89, 59.45, 59.34, 59.56, 59.52]

Epoch: 8
2024-03-09 22:43:33.208335 epoch: 8 step: 0 cls_loss= 1.02123 (153544 samples/sec)
2024-03-09 22:43:36.154678 epoch: 8 step: 100 cls_loss= 1.03368 (10183 samples/sec)
saving....
2024-03-09 22:43:39.906429------------------------------------------------------ Precision@1: 59.69% 

[59.51, 59.62, 59.79, 59.89, 59.45, 59.34, 59.56, 59.52, 59.69]

Epoch: 9
2024-03-09 22:43:40.100564 epoch: 9 step: 0 cls_loss= 1.11201 (155501 samples/sec)
2024-03-09 22:43:43.106308 epoch: 9 step: 100 cls_loss= 0.98931 (9984 samples/sec)
saving....
2024-03-09 22:43:46.927731------------------------------------------------------ Precision@1: 59.44% 

[59.51, 59.62, 59.79, 59.89, 59.45, 59.34, 59.56, 59.52, 59.69, 59.44]

Epoch: 10
2024-03-09 22:43:47.127111 epoch: 10 step: 0 cls_loss= 1.06889 (151390 samples/sec)
2024-03-09 22:43:50.189151 epoch: 10 step: 100 cls_loss= 0.98362 (9801 samples/sec)
saving....
2024-03-09 22:43:53.932427------------------------------------------------------ Precision@1: 59.53% 

[59.51, 59.62, 59.79, 59.89, 59.45, 59.34, 59.56, 59.52, 59.69, 59.44, 59.53]

Epoch: 11
2024-03-09 22:43:54.145192 epoch: 11 step: 0 cls_loss= 1.06854 (141699 samples/sec)
2024-03-09 22:43:57.140692 epoch: 11 step: 100 cls_loss= 1.03281 (10017 samples/sec)
saving....
2024-03-09 22:44:00.858749------------------------------------------------------ Precision@1: 59.49% 

[59.51, 59.62, 59.79, 59.89, 59.45, 59.34, 59.56, 59.52, 59.69, 59.44, 59.53, 59.49]

Epoch: 12
2024-03-09 22:44:01.069042 epoch: 12 step: 0 cls_loss= 1.00219 (143494 samples/sec)
2024-03-09 22:44:04.082643 epoch: 12 step: 100 cls_loss= 1.15539 (9955 samples/sec)
saving....
2024-03-09 22:44:07.783368------------------------------------------------------ Precision@1: 59.61% 

[59.51, 59.62, 59.79, 59.89, 59.45, 59.34, 59.56, 59.52, 59.69, 59.44, 59.53, 59.49, 59.61]

Epoch: 13
2024-03-09 22:44:07.992886 epoch: 13 step: 0 cls_loss= 1.06911 (144043 samples/sec)
2024-03-09 22:44:11.051461 epoch: 13 step: 100 cls_loss= 1.01636 (9813 samples/sec)
saving....
2024-03-09 22:44:14.808856------------------------------------------------------ Precision@1: 59.66% 

[59.51, 59.62, 59.79, 59.89, 59.45, 59.34, 59.56, 59.52, 59.69, 59.44, 59.53, 59.49, 59.61, 59.66]

Epoch: 14
2024-03-09 22:44:15.008879 epoch: 14 step: 0 cls_loss= 0.92425 (150711 samples/sec)
2024-03-09 22:44:17.939544 epoch: 14 step: 100 cls_loss= 0.96854 (10237 samples/sec)
saving....
2024-03-09 22:44:21.614092------------------------------------------------------ Precision@1: 59.71% 

[59.51, 59.62, 59.79, 59.89, 59.45, 59.34, 59.56, 59.52, 59.69, 59.44, 59.53, 59.49, 59.61, 59.66, 59.71]

Epoch: 15
2024-03-09 22:44:21.831865 epoch: 15 step: 0 cls_loss= 1.06162 (138489 samples/sec)
2024-03-09 22:44:24.802138 epoch: 15 step: 100 cls_loss= 1.05131 (10100 samples/sec)
saving....
2024-03-09 22:44:28.626087------------------------------------------------------ Precision@1: 59.53% 

[59.51, 59.62, 59.79, 59.89, 59.45, 59.34, 59.56, 59.52, 59.69, 59.44, 59.53, 59.49, 59.61, 59.66, 59.71, 59.53]

Epoch: 16
2024-03-09 22:44:28.834811 epoch: 16 step: 0 cls_loss= 1.11682 (144474 samples/sec)
2024-03-09 22:44:31.778704 epoch: 16 step: 100 cls_loss= 0.94390 (10194 samples/sec)
saving....
2024-03-09 22:44:35.494842------------------------------------------------------ Precision@1: 59.79% 

[59.51, 59.62, 59.79, 59.89, 59.45, 59.34, 59.56, 59.52, 59.69, 59.44, 59.53, 59.49, 59.61, 59.66, 59.71, 59.53, 59.79]

Epoch: 17
2024-03-09 22:44:35.691062 epoch: 17 step: 0 cls_loss= 1.05685 (153741 samples/sec)
2024-03-09 22:44:38.659690 epoch: 17 step: 100 cls_loss= 0.96833 (10109 samples/sec)
saving....
2024-03-09 22:44:42.375179------------------------------------------------------ Precision@1: 59.91% 

[59.51, 59.62, 59.79, 59.89, 59.45, 59.34, 59.56, 59.52, 59.69, 59.44, 59.53, 59.49, 59.61, 59.66, 59.71, 59.53, 59.79, 59.91]

Epoch: 18
2024-03-09 22:44:42.573441 epoch: 18 step: 0 cls_loss= 1.09946 (152154 samples/sec)
2024-03-09 22:44:45.481405 epoch: 18 step: 100 cls_loss= 1.07155 (10321 samples/sec)
saving....
2024-03-09 22:44:49.175757------------------------------------------------------ Precision@1: 59.73% 

[59.51, 59.62, 59.79, 59.89, 59.45, 59.34, 59.56, 59.52, 59.69, 59.44, 59.53, 59.49, 59.61, 59.66, 59.71, 59.53, 59.79, 59.91, 59.73]

Epoch: 19
2024-03-09 22:44:49.373925 epoch: 19 step: 0 cls_loss= 0.90179 (152274 samples/sec)
2024-03-09 22:44:52.266262 epoch: 19 step: 100 cls_loss= 0.92223 (10376 samples/sec)
saving....
2024-03-09 22:44:55.955610------------------------------------------------------ Precision@1: 59.82% 

[59.51, 59.62, 59.79, 59.89, 59.45, 59.34, 59.56, 59.52, 59.69, 59.44, 59.53, 59.49, 59.61, 59.66, 59.71, 59.53, 59.79, 59.91, 59.73, 59.82]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 22:44:58.677493 epoch: 0 step: 0 cls_loss= 1.16843 (51427 samples/sec)
2024-03-09 22:45:01.649480 epoch: 0 step: 100 cls_loss= 1.16506 (10094 samples/sec)
saving....
2024-03-09 22:45:05.603464------------------------------------------------------ Precision@1: 59.69% 

[59.69]

Epoch: 1
2024-03-09 22:45:05.823641 epoch: 1 step: 0 cls_loss= 1.11494 (136831 samples/sec)
2024-03-09 22:45:08.705537 epoch: 1 step: 100 cls_loss= 0.97068 (10415 samples/sec)
saving....
2024-03-09 22:45:12.412663------------------------------------------------------ Precision@1: 59.44% 

[59.69, 59.44]

Epoch: 2
2024-03-09 22:45:12.617359 epoch: 2 step: 0 cls_loss= 1.03081 (147313 samples/sec)
2024-03-09 22:45:15.522266 epoch: 2 step: 100 cls_loss= 1.07702 (10332 samples/sec)
saving....
2024-03-09 22:45:19.188585------------------------------------------------------ Precision@1: 59.48% 

[59.69, 59.44, 59.48]

Epoch: 3
2024-03-09 22:45:19.409076 epoch: 3 step: 0 cls_loss= 0.99039 (136816 samples/sec)
2024-03-09 22:45:22.343504 epoch: 3 step: 100 cls_loss= 1.14354 (10227 samples/sec)
saving....
2024-03-09 22:45:26.130366------------------------------------------------------ Precision@1: 59.45% 

[59.69, 59.44, 59.48, 59.45]

Epoch: 4
2024-03-09 22:45:26.340881 epoch: 4 step: 0 cls_loss= 1.02204 (143265 samples/sec)
2024-03-09 22:45:29.253387 epoch: 4 step: 100 cls_loss= 1.14885 (10305 samples/sec)
saving....
2024-03-09 22:45:33.084151------------------------------------------------------ Precision@1: 59.71% 

[59.69, 59.44, 59.48, 59.45, 59.71]

Epoch: 5
2024-03-09 22:45:33.271445 epoch: 5 step: 0 cls_loss= 1.05700 (161111 samples/sec)
2024-03-09 22:45:36.149312 epoch: 5 step: 100 cls_loss= 0.95148 (10428 samples/sec)
saving....
2024-03-09 22:45:39.825856------------------------------------------------------ Precision@1: 59.55% 

[59.69, 59.44, 59.48, 59.45, 59.71, 59.55]

Epoch: 6
2024-03-09 22:45:40.021069 epoch: 6 step: 0 cls_loss= 1.19925 (154411 samples/sec)
2024-03-09 22:45:42.994347 epoch: 6 step: 100 cls_loss= 0.90300 (10090 samples/sec)
saving....
2024-03-09 22:45:46.705900------------------------------------------------------ Precision@1: 59.73% 

[59.69, 59.44, 59.48, 59.45, 59.71, 59.55, 59.73]

Epoch: 7
2024-03-09 22:45:46.911676 epoch: 7 step: 0 cls_loss= 1.02015 (146455 samples/sec)
2024-03-09 22:45:49.861760 epoch: 7 step: 100 cls_loss= 0.97541 (10169 samples/sec)
saving....
2024-03-09 22:45:53.633475------------------------------------------------------ Precision@1: 59.74% 

[59.69, 59.44, 59.48, 59.45, 59.71, 59.55, 59.73, 59.74]

Epoch: 8
2024-03-09 22:45:53.862016 epoch: 8 step: 0 cls_loss= 1.01620 (131941 samples/sec)
2024-03-09 22:45:56.804217 epoch: 8 step: 100 cls_loss= 0.96455 (10197 samples/sec)
saving....
2024-03-09 22:46:00.609235------------------------------------------------------ Precision@1: 59.56% 

[59.69, 59.44, 59.48, 59.45, 59.71, 59.55, 59.73, 59.74, 59.56]

Epoch: 9
2024-03-09 22:46:00.809762 epoch: 9 step: 0 cls_loss= 0.91974 (150586 samples/sec)
2024-03-09 22:46:03.871118 epoch: 9 step: 100 cls_loss= 0.98758 (9799 samples/sec)
saving....
2024-03-09 22:46:07.605253------------------------------------------------------ Precision@1: 59.50% 

[59.69, 59.44, 59.48, 59.45, 59.71, 59.55, 59.73, 59.74, 59.56, 59.5]

Epoch: 10
2024-03-09 22:46:07.805884 epoch: 10 step: 0 cls_loss= 0.96724 (150400 samples/sec)
2024-03-09 22:46:10.735159 epoch: 10 step: 100 cls_loss= 1.04573 (10245 samples/sec)
saving....
2024-03-09 22:46:14.568097------------------------------------------------------ Precision@1: 59.50% 

[59.69, 59.44, 59.48, 59.45, 59.71, 59.55, 59.73, 59.74, 59.56, 59.5, 59.5]

Epoch: 11
2024-03-09 22:46:14.770909 epoch: 11 step: 0 cls_loss= 1.00878 (148679 samples/sec)
2024-03-09 22:46:17.715310 epoch: 11 step: 100 cls_loss= 0.98683 (10192 samples/sec)
saving....
2024-03-09 22:46:21.431369------------------------------------------------------ Precision@1: 59.77% 

[59.69, 59.44, 59.48, 59.45, 59.71, 59.55, 59.73, 59.74, 59.56, 59.5, 59.5, 59.77]

Epoch: 12
2024-03-09 22:46:21.619996 epoch: 12 step: 0 cls_loss= 0.99591 (160028 samples/sec)
2024-03-09 22:46:24.502942 epoch: 12 step: 100 cls_loss= 0.97550 (10407 samples/sec)
saving....
2024-03-09 22:46:28.191440------------------------------------------------------ Precision@1: 59.64% 

[59.69, 59.44, 59.48, 59.45, 59.71, 59.55, 59.73, 59.74, 59.56, 59.5, 59.5, 59.77, 59.64]

Epoch: 13
2024-03-09 22:46:28.408832 epoch: 13 step: 0 cls_loss= 1.05207 (138649 samples/sec)
2024-03-09 22:46:31.530128 epoch: 13 step: 100 cls_loss= 0.94854 (9611 samples/sec)
saving....
2024-03-09 22:46:35.241723------------------------------------------------------ Precision@1: 59.64% 

[59.69, 59.44, 59.48, 59.45, 59.71, 59.55, 59.73, 59.74, 59.56, 59.5, 59.5, 59.77, 59.64, 59.64]

Epoch: 14
2024-03-09 22:46:35.436699 epoch: 14 step: 0 cls_loss= 0.90811 (154734 samples/sec)
2024-03-09 22:46:38.310382 epoch: 14 step: 100 cls_loss= 1.00555 (10444 samples/sec)
saving....
2024-03-09 22:46:41.995848------------------------------------------------------ Precision@1: 59.57% 

[59.69, 59.44, 59.48, 59.45, 59.71, 59.55, 59.73, 59.74, 59.56, 59.5, 59.5, 59.77, 59.64, 59.64, 59.57]

Epoch: 15
2024-03-09 22:46:42.204188 epoch: 15 step: 0 cls_loss= 0.99584 (144844 samples/sec)
2024-03-09 22:46:45.092293 epoch: 15 step: 100 cls_loss= 0.99879 (10392 samples/sec)
saving....
2024-03-09 22:46:48.783078------------------------------------------------------ Precision@1: 59.60% 

[59.69, 59.44, 59.48, 59.45, 59.71, 59.55, 59.73, 59.74, 59.56, 59.5, 59.5, 59.77, 59.64, 59.64, 59.57, 59.6]

Epoch: 16
2024-03-09 22:46:48.985394 epoch: 16 step: 0 cls_loss= 0.90004 (149108 samples/sec)
2024-03-09 22:46:51.942610 epoch: 16 step: 100 cls_loss= 1.06604 (10146 samples/sec)
saving....
2024-03-09 22:46:55.589611------------------------------------------------------ Precision@1: 59.52% 

[59.69, 59.44, 59.48, 59.45, 59.71, 59.55, 59.73, 59.74, 59.56, 59.5, 59.5, 59.77, 59.64, 59.64, 59.57, 59.6, 59.52]

Epoch: 17
2024-03-09 22:46:55.801370 epoch: 17 step: 0 cls_loss= 1.02849 (142438 samples/sec)
2024-03-09 22:46:58.682546 epoch: 17 step: 100 cls_loss= 0.89619 (10412 samples/sec)
saving....
2024-03-09 22:47:02.455872------------------------------------------------------ Precision@1: 59.30% 

[59.69, 59.44, 59.48, 59.45, 59.71, 59.55, 59.73, 59.74, 59.56, 59.5, 59.5, 59.77, 59.64, 59.64, 59.57, 59.6, 59.52, 59.3]

Epoch: 18
2024-03-09 22:47:02.670816 epoch: 18 step: 0 cls_loss= 1.01375 (140310 samples/sec)
2024-03-09 22:47:05.561828 epoch: 18 step: 100 cls_loss= 1.02341 (10377 samples/sec)
saving....
2024-03-09 22:47:09.236070------------------------------------------------------ Precision@1: 59.78% 

[59.69, 59.44, 59.48, 59.45, 59.71, 59.55, 59.73, 59.74, 59.56, 59.5, 59.5, 59.77, 59.64, 59.64, 59.57, 59.6, 59.52, 59.3, 59.78]

Epoch: 19
2024-03-09 22:47:09.464745 epoch: 19 step: 0 cls_loss= 1.05242 (131661 samples/sec)
2024-03-09 22:47:12.361531 epoch: 19 step: 100 cls_loss= 0.94963 (10356 samples/sec)
saving....
2024-03-09 22:47:16.077903------------------------------------------------------ Precision@1: 59.68% 

[59.69, 59.44, 59.48, 59.45, 59.71, 59.55, 59.73, 59.74, 59.56, 59.5, 59.5, 59.77, 59.64, 59.64, 59.57, 59.6, 59.52, 59.3, 59.78, 59.68]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 22:47:18.797375 epoch: 0 step: 0 cls_loss= 1.07027 (49093 samples/sec)
2024-03-09 22:47:21.842204 epoch: 0 step: 100 cls_loss= 0.99027 (9853 samples/sec)
saving....
2024-03-09 22:47:25.816529------------------------------------------------------ Precision@1: 59.57% 

[59.57]

Epoch: 1
2024-03-09 22:47:26.042639 epoch: 1 step: 0 cls_loss= 1.12507 (133232 samples/sec)
2024-03-09 22:47:28.913662 epoch: 1 step: 100 cls_loss= 1.09416 (10454 samples/sec)
saving....
2024-03-09 22:47:32.739291------------------------------------------------------ Precision@1: 59.66% 

[59.57, 59.66]

Epoch: 2
2024-03-09 22:47:32.927340 epoch: 2 step: 0 cls_loss= 0.93035 (160555 samples/sec)
2024-03-09 22:47:35.808540 epoch: 2 step: 100 cls_loss= 1.07442 (10416 samples/sec)
saving....
2024-03-09 22:47:39.465273------------------------------------------------------ Precision@1: 59.81% 

[59.57, 59.66, 59.81]

Epoch: 3
2024-03-09 22:47:39.658325 epoch: 3 step: 0 cls_loss= 0.89995 (156246 samples/sec)
2024-03-09 22:47:42.636356 epoch: 3 step: 100 cls_loss= 1.06985 (10075 samples/sec)
saving....
2024-03-09 22:47:46.382480------------------------------------------------------ Precision@1: 59.78% 

[59.57, 59.66, 59.81, 59.78]

Epoch: 4
2024-03-09 22:47:46.592478 epoch: 4 step: 0 cls_loss= 1.05262 (143684 samples/sec)
2024-03-09 22:47:49.518086 epoch: 4 step: 100 cls_loss= 0.89272 (10258 samples/sec)
saving....
2024-03-09 22:47:53.169600------------------------------------------------------ Precision@1: 59.50% 

[59.57, 59.66, 59.81, 59.78, 59.5]

Epoch: 5
2024-03-09 22:47:53.364904 epoch: 5 step: 0 cls_loss= 1.00327 (154470 samples/sec)
2024-03-09 22:47:56.393924 epoch: 5 step: 100 cls_loss= 1.08116 (9908 samples/sec)
saving....
2024-03-09 22:48:00.214494------------------------------------------------------ Precision@1: 59.41% 

[59.57, 59.66, 59.81, 59.78, 59.5, 59.41]

Epoch: 6
2024-03-09 22:48:00.438439 epoch: 6 step: 0 cls_loss= 1.00055 (134579 samples/sec)
2024-03-09 22:48:03.321750 epoch: 6 step: 100 cls_loss= 0.89586 (10405 samples/sec)
saving....
2024-03-09 22:48:07.058184------------------------------------------------------ Precision@1: 59.53% 

[59.57, 59.66, 59.81, 59.78, 59.5, 59.41, 59.53]

Epoch: 7
2024-03-09 22:48:07.253106 epoch: 7 step: 0 cls_loss= 1.21334 (154739 samples/sec)
2024-03-09 22:48:10.272431 epoch: 7 step: 100 cls_loss= 0.89367 (9938 samples/sec)
saving....
2024-03-09 22:48:13.967969------------------------------------------------------ Precision@1: 59.61% 

[59.57, 59.66, 59.81, 59.78, 59.5, 59.41, 59.53, 59.61]

Epoch: 8
2024-03-09 22:48:14.175570 epoch: 8 step: 0 cls_loss= 1.07660 (145216 samples/sec)
2024-03-09 22:48:17.043525 epoch: 8 step: 100 cls_loss= 1.06258 (10460 samples/sec)
saving....
2024-03-09 22:48:20.770552------------------------------------------------------ Precision@1: 59.75% 

[59.57, 59.66, 59.81, 59.78, 59.5, 59.41, 59.53, 59.61, 59.75]

Epoch: 9
2024-03-09 22:48:20.983275 epoch: 9 step: 0 cls_loss= 1.02291 (141669 samples/sec)
2024-03-09 22:48:23.926175 epoch: 9 step: 100 cls_loss= 1.05711 (10198 samples/sec)
saving....
2024-03-09 22:48:27.699101------------------------------------------------------ Precision@1: 59.31% 

[59.57, 59.66, 59.81, 59.78, 59.5, 59.41, 59.53, 59.61, 59.75, 59.31]

Epoch: 10
2024-03-09 22:48:27.906135 epoch: 10 step: 0 cls_loss= 0.93264 (145726 samples/sec)
2024-03-09 22:48:30.768095 epoch: 10 step: 100 cls_loss= 0.99382 (10484 samples/sec)
saving....
2024-03-09 22:48:34.442712------------------------------------------------------ Precision@1: 59.92% 

[59.57, 59.66, 59.81, 59.78, 59.5, 59.41, 59.53, 59.61, 59.75, 59.31, 59.92]

Epoch: 11
2024-03-09 22:48:34.641477 epoch: 11 step: 0 cls_loss= 0.93538 (151777 samples/sec)
2024-03-09 22:48:37.598780 epoch: 11 step: 100 cls_loss= 0.97875 (10148 samples/sec)
saving....
2024-03-09 22:48:41.385087------------------------------------------------------ Precision@1: 59.64% 

[59.57, 59.66, 59.81, 59.78, 59.5, 59.41, 59.53, 59.61, 59.75, 59.31, 59.92, 59.64]

Epoch: 12
2024-03-09 22:48:41.610066 epoch: 12 step: 0 cls_loss= 0.92368 (133949 samples/sec)
2024-03-09 22:48:44.582578 epoch: 12 step: 100 cls_loss= 1.11081 (10092 samples/sec)
saving....
2024-03-09 22:48:48.261133------------------------------------------------------ Precision@1: 59.65% 

[59.57, 59.66, 59.81, 59.78, 59.5, 59.41, 59.53, 59.61, 59.75, 59.31, 59.92, 59.64, 59.65]

Epoch: 13
2024-03-09 22:48:48.444757 epoch: 13 step: 0 cls_loss= 0.99338 (164070 samples/sec)
2024-03-09 22:48:51.385899 epoch: 13 step: 100 cls_loss= 0.98239 (10205 samples/sec)
saving....
2024-03-09 22:48:55.075944------------------------------------------------------ Precision@1: 59.70% 

[59.57, 59.66, 59.81, 59.78, 59.5, 59.41, 59.53, 59.61, 59.75, 59.31, 59.92, 59.64, 59.65, 59.7]

Epoch: 14
2024-03-09 22:48:55.289720 epoch: 14 step: 0 cls_loss= 0.91160 (140864 samples/sec)
2024-03-09 22:48:58.159630 epoch: 14 step: 100 cls_loss= 0.97359 (10458 samples/sec)
saving....
2024-03-09 22:49:01.794947------------------------------------------------------ Precision@1: 59.59% 

[59.57, 59.66, 59.81, 59.78, 59.5, 59.41, 59.53, 59.61, 59.75, 59.31, 59.92, 59.64, 59.65, 59.7, 59.59]

Epoch: 15
2024-03-09 22:49:01.987933 epoch: 15 step: 0 cls_loss= 0.99504 (156236 samples/sec)
2024-03-09 22:49:04.867715 epoch: 15 step: 100 cls_loss= 0.96290 (10420 samples/sec)
saving....
2024-03-09 22:49:08.611930------------------------------------------------------ Precision@1: 59.61% 

[59.57, 59.66, 59.81, 59.78, 59.5, 59.41, 59.53, 59.61, 59.75, 59.31, 59.92, 59.64, 59.65, 59.7, 59.59, 59.61]

Epoch: 16
2024-03-09 22:49:08.820692 epoch: 16 step: 0 cls_loss= 1.09191 (144453 samples/sec)
2024-03-09 22:49:11.683202 epoch: 16 step: 100 cls_loss= 1.04573 (10482 samples/sec)
saving....
2024-03-09 22:49:15.300223------------------------------------------------------ Precision@1: 59.63% 

[59.57, 59.66, 59.81, 59.78, 59.5, 59.41, 59.53, 59.61, 59.75, 59.31, 59.92, 59.64, 59.65, 59.7, 59.59, 59.61, 59.63]

Epoch: 17
2024-03-09 22:49:15.500687 epoch: 17 step: 0 cls_loss= 1.02689 (150433 samples/sec)
2024-03-09 22:49:18.388181 epoch: 17 step: 100 cls_loss= 0.98704 (10394 samples/sec)
saving....
2024-03-09 22:49:22.144313------------------------------------------------------ Precision@1: 59.67% 

[59.57, 59.66, 59.81, 59.78, 59.5, 59.41, 59.53, 59.61, 59.75, 59.31, 59.92, 59.64, 59.65, 59.7, 59.59, 59.61, 59.63, 59.67]

Epoch: 18
2024-03-09 22:49:22.337969 epoch: 18 step: 0 cls_loss= 1.02238 (155869 samples/sec)
2024-03-09 22:49:25.195033 epoch: 18 step: 100 cls_loss= 0.89168 (10505 samples/sec)
saving....
2024-03-09 22:49:28.843775------------------------------------------------------ Precision@1: 59.81% 

[59.57, 59.66, 59.81, 59.78, 59.5, 59.41, 59.53, 59.61, 59.75, 59.31, 59.92, 59.64, 59.65, 59.7, 59.59, 59.61, 59.63, 59.67, 59.81]

Epoch: 19
2024-03-09 22:49:29.039910 epoch: 19 step: 0 cls_loss= 0.94923 (153783 samples/sec)
2024-03-09 22:49:31.993944 epoch: 19 step: 100 cls_loss= 0.98765 (10158 samples/sec)
saving....
2024-03-09 22:49:35.758694------------------------------------------------------ Precision@1: 59.79% 

[59.57, 59.66, 59.81, 59.78, 59.5, 59.41, 59.53, 59.61, 59.75, 59.31, 59.92, 59.64, 59.65, 59.7, 59.59, 59.61, 59.63, 59.67, 59.81, 59.79]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 22:49:38.482159 epoch: 0 step: 0 cls_loss= 1.01496 (49401 samples/sec)
2024-03-09 22:49:41.392465 epoch: 0 step: 100 cls_loss= 0.99296 (10308 samples/sec)
saving....
2024-03-09 22:49:45.274728------------------------------------------------------ Precision@1: 59.92% 

[59.92]

Epoch: 1
2024-03-09 22:49:45.460234 epoch: 1 step: 0 cls_loss= 1.16283 (162739 samples/sec)
2024-03-09 22:49:48.419562 epoch: 1 step: 100 cls_loss= 1.06763 (10140 samples/sec)
saving....
2024-03-09 22:49:52.143395------------------------------------------------------ Precision@1: 59.70% 

[59.92, 59.7]

Epoch: 2
2024-03-09 22:49:52.339315 epoch: 2 step: 0 cls_loss= 1.18683 (154080 samples/sec)
2024-03-09 22:49:55.263020 epoch: 2 step: 100 cls_loss= 1.06447 (10264 samples/sec)
saving....
2024-03-09 22:49:58.935720------------------------------------------------------ Precision@1: 59.75% 

[59.92, 59.7, 59.75]

Epoch: 3
2024-03-09 22:49:59.136340 epoch: 3 step: 0 cls_loss= 1.15925 (150365 samples/sec)
2024-03-09 22:50:02.227324 epoch: 3 step: 100 cls_loss= 1.06352 (9705 samples/sec)
saving....
2024-03-09 22:50:06.037596------------------------------------------------------ Precision@1: 59.67% 

[59.92, 59.7, 59.75, 59.67]

Epoch: 4
2024-03-09 22:50:06.230346 epoch: 4 step: 0 cls_loss= 1.04616 (156373 samples/sec)
2024-03-09 22:50:09.156626 epoch: 4 step: 100 cls_loss= 0.97184 (10257 samples/sec)
saving....
2024-03-09 22:50:12.858551------------------------------------------------------ Precision@1: 59.73% 

[59.92, 59.7, 59.75, 59.67, 59.73]

Epoch: 5
2024-03-09 22:50:13.068435 epoch: 5 step: 0 cls_loss= 1.14236 (143443 samples/sec)
2024-03-09 22:50:15.993151 epoch: 5 step: 100 cls_loss= 1.06600 (10260 samples/sec)
saving....
2024-03-09 22:50:19.681012------------------------------------------------------ Precision@1: 59.79% 

[59.92, 59.7, 59.75, 59.67, 59.73, 59.79]

Epoch: 6
2024-03-09 22:50:19.867735 epoch: 6 step: 0 cls_loss= 1.15266 (161635 samples/sec)
2024-03-09 22:50:22.764231 epoch: 6 step: 100 cls_loss= 0.94514 (10357 samples/sec)
saving....
2024-03-09 22:50:26.743218------------------------------------------------------ Precision@1: 59.59% 

[59.92, 59.7, 59.75, 59.67, 59.73, 59.79, 59.59]

Epoch: 7
2024-03-09 22:50:26.945946 epoch: 7 step: 0 cls_loss= 1.04180 (148787 samples/sec)
2024-03-09 22:50:29.941454 epoch: 7 step: 100 cls_loss= 1.11427 (10017 samples/sec)
saving....
2024-03-09 22:50:33.711127------------------------------------------------------ Precision@1: 59.40% 

[59.92, 59.7, 59.75, 59.67, 59.73, 59.79, 59.59, 59.4]

Epoch: 8
2024-03-09 22:50:33.904531 epoch: 8 step: 0 cls_loss= 0.90240 (155746 samples/sec)
2024-03-09 22:50:36.849940 epoch: 8 step: 100 cls_loss= 0.90311 (10186 samples/sec)
saving....
2024-03-09 22:50:40.641888------------------------------------------------------ Precision@1: 59.83% 

[59.92, 59.7, 59.75, 59.67, 59.73, 59.79, 59.59, 59.4, 59.83]

Epoch: 9
2024-03-09 22:50:40.846298 epoch: 9 step: 0 cls_loss= 1.08402 (147553 samples/sec)
2024-03-09 22:50:43.819467 epoch: 9 step: 100 cls_loss= 1.01482 (10094 samples/sec)
saving....
2024-03-09 22:50:47.574810------------------------------------------------------ Precision@1: 59.39% 

[59.92, 59.7, 59.75, 59.67, 59.73, 59.79, 59.59, 59.4, 59.83, 59.39]

Epoch: 10
2024-03-09 22:50:47.784790 epoch: 10 step: 0 cls_loss= 0.98739 (143502 samples/sec)
2024-03-09 22:50:50.733668 epoch: 10 step: 100 cls_loss= 1.04692 (10173 samples/sec)
saving....
2024-03-09 22:50:54.452454------------------------------------------------------ Precision@1: 59.56% 

[59.92, 59.7, 59.75, 59.67, 59.73, 59.79, 59.59, 59.4, 59.83, 59.39, 59.56]

Epoch: 11
2024-03-09 22:50:54.651033 epoch: 11 step: 0 cls_loss= 1.00107 (151927 samples/sec)
2024-03-09 22:50:57.655638 epoch: 11 step: 100 cls_loss= 0.98708 (9989 samples/sec)
saving....
2024-03-09 22:51:01.365676------------------------------------------------------ Precision@1: 59.40% 

[59.92, 59.7, 59.75, 59.67, 59.73, 59.79, 59.59, 59.4, 59.83, 59.39, 59.56, 59.4]

Epoch: 12
2024-03-09 22:51:01.566144 epoch: 12 step: 0 cls_loss= 1.10077 (150454 samples/sec)
2024-03-09 22:51:04.544994 epoch: 12 step: 100 cls_loss= 1.05414 (10071 samples/sec)
saving....
2024-03-09 22:51:08.327691------------------------------------------------------ Precision@1: 59.34% 

[59.92, 59.7, 59.75, 59.67, 59.73, 59.79, 59.59, 59.4, 59.83, 59.39, 59.56, 59.4, 59.34]

Epoch: 13
2024-03-09 22:51:08.511516 epoch: 13 step: 0 cls_loss= 0.95423 (164062 samples/sec)
2024-03-09 22:51:11.545613 epoch: 13 step: 100 cls_loss= 1.08241 (9887 samples/sec)
saving....
2024-03-09 22:51:15.279368------------------------------------------------------ Precision@1: 59.81% 

[59.92, 59.7, 59.75, 59.67, 59.73, 59.79, 59.59, 59.4, 59.83, 59.39, 59.56, 59.4, 59.34, 59.81]

Epoch: 14
2024-03-09 22:51:15.488428 epoch: 14 step: 0 cls_loss= 1.07656 (144354 samples/sec)
2024-03-09 22:51:18.367071 epoch: 14 step: 100 cls_loss= 1.06964 (10426 samples/sec)
saving....
2024-03-09 22:51:22.051779------------------------------------------------------ Precision@1: 59.62% 

[59.92, 59.7, 59.75, 59.67, 59.73, 59.79, 59.59, 59.4, 59.83, 59.39, 59.56, 59.4, 59.34, 59.81, 59.62]

Epoch: 15
2024-03-09 22:51:22.243821 epoch: 15 step: 0 cls_loss= 1.02874 (157192 samples/sec)
2024-03-09 22:51:25.192934 epoch: 15 step: 100 cls_loss= 0.97684 (10174 samples/sec)
saving....
2024-03-09 22:51:28.896777------------------------------------------------------ Precision@1: 59.58% 

[59.92, 59.7, 59.75, 59.67, 59.73, 59.79, 59.59, 59.4, 59.83, 59.39, 59.56, 59.4, 59.34, 59.81, 59.62, 59.58]

Epoch: 16
2024-03-09 22:51:29.084263 epoch: 16 step: 0 cls_loss= 0.91218 (160962 samples/sec)
2024-03-09 22:51:31.996760 epoch: 16 step: 100 cls_loss= 1.03289 (10304 samples/sec)
saving....
2024-03-09 22:51:35.689253------------------------------------------------------ Precision@1: 59.48% 

[59.92, 59.7, 59.75, 59.67, 59.73, 59.79, 59.59, 59.4, 59.83, 59.39, 59.56, 59.4, 59.34, 59.81, 59.62, 59.58, 59.48]

Epoch: 17
2024-03-09 22:51:35.896327 epoch: 17 step: 0 cls_loss= 0.89408 (145642 samples/sec)
2024-03-09 22:51:38.891914 epoch: 17 step: 100 cls_loss= 1.03763 (10015 samples/sec)
saving....
2024-03-09 22:51:42.607387------------------------------------------------------ Precision@1: 59.76% 

[59.92, 59.7, 59.75, 59.67, 59.73, 59.79, 59.59, 59.4, 59.83, 59.39, 59.56, 59.4, 59.34, 59.81, 59.62, 59.58, 59.48, 59.76]

Epoch: 18
2024-03-09 22:51:42.826490 epoch: 18 step: 0 cls_loss= 1.13923 (137626 samples/sec)
2024-03-09 22:51:45.699821 epoch: 18 step: 100 cls_loss= 0.98484 (10445 samples/sec)
saving....
2024-03-09 22:51:49.362917------------------------------------------------------ Precision@1: 59.71% 

[59.92, 59.7, 59.75, 59.67, 59.73, 59.79, 59.59, 59.4, 59.83, 59.39, 59.56, 59.4, 59.34, 59.81, 59.62, 59.58, 59.48, 59.76, 59.71]

Epoch: 19
2024-03-09 22:51:49.569885 epoch: 19 step: 0 cls_loss= 1.04991 (145714 samples/sec)
2024-03-09 22:51:52.467739 epoch: 19 step: 100 cls_loss= 1.00036 (10354 samples/sec)
saving....
2024-03-09 22:51:56.146596------------------------------------------------------ Precision@1: 59.61% 

[59.92, 59.7, 59.75, 59.67, 59.73, 59.79, 59.59, 59.4, 59.83, 59.39, 59.56, 59.4, 59.34, 59.81, 59.62, 59.58, 59.48, 59.76, 59.71, 59.61]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 22:51:58.878530 epoch: 0 step: 0 cls_loss= 0.99274 (49786 samples/sec)
2024-03-09 22:52:01.783873 epoch: 0 step: 100 cls_loss= 1.03989 (10326 samples/sec)
saving....
2024-03-09 22:52:05.737326------------------------------------------------------ Precision@1: 59.52% 

[59.52]

Epoch: 1
2024-03-09 22:52:05.947480 epoch: 1 step: 0 cls_loss= 1.16482 (143320 samples/sec)
2024-03-09 22:52:08.969930 epoch: 1 step: 100 cls_loss= 0.97111 (9930 samples/sec)
saving....
2024-03-09 22:52:12.715123------------------------------------------------------ Precision@1: 59.40% 

[59.52, 59.4]

Epoch: 2
2024-03-09 22:52:12.903032 epoch: 2 step: 0 cls_loss= 0.99750 (160542 samples/sec)
2024-03-09 22:52:15.796591 epoch: 2 step: 100 cls_loss= 0.90689 (10373 samples/sec)
saving....
2024-03-09 22:52:19.475427------------------------------------------------------ Precision@1: 59.82% 

[59.52, 59.4, 59.82]

Epoch: 3
2024-03-09 22:52:19.666176 epoch: 3 step: 0 cls_loss= 1.04819 (158215 samples/sec)
2024-03-09 22:52:22.634707 epoch: 3 step: 100 cls_loss= 1.01439 (10107 samples/sec)
saving....
2024-03-09 22:52:26.358610------------------------------------------------------ Precision@1: 59.57% 

[59.52, 59.4, 59.82, 59.57]

Epoch: 4
2024-03-09 22:52:26.561190 epoch: 4 step: 0 cls_loss= 0.97586 (148661 samples/sec)
2024-03-09 22:52:29.450074 epoch: 4 step: 100 cls_loss= 1.06324 (10389 samples/sec)
saving....
2024-03-09 22:52:33.119411------------------------------------------------------ Precision@1: 59.56% 

[59.52, 59.4, 59.82, 59.57, 59.56]

Epoch: 5
2024-03-09 22:52:33.322023 epoch: 5 step: 0 cls_loss= 1.02654 (148878 samples/sec)
2024-03-09 22:52:36.341917 epoch: 5 step: 100 cls_loss= 0.97629 (9938 samples/sec)
saving....
2024-03-09 22:52:40.124880------------------------------------------------------ Precision@1: 59.56% 

[59.52, 59.4, 59.82, 59.57, 59.56, 59.56]

Epoch: 6
2024-03-09 22:52:40.320157 epoch: 6 step: 0 cls_loss= 0.98906 (154453 samples/sec)
2024-03-09 22:52:43.422932 epoch: 6 step: 100 cls_loss= 1.03067 (9671 samples/sec)
saving....
2024-03-09 22:52:47.235013------------------------------------------------------ Precision@1: 59.64% 

[59.52, 59.4, 59.82, 59.57, 59.56, 59.56, 59.64]

Epoch: 7
2024-03-09 22:52:47.436261 epoch: 7 step: 0 cls_loss= 1.03271 (149839 samples/sec)
2024-03-09 22:52:50.408318 epoch: 7 step: 100 cls_loss= 1.16015 (10094 samples/sec)
saving....
2024-03-09 22:52:54.116430------------------------------------------------------ Precision@1: 59.82% 

[59.52, 59.4, 59.82, 59.57, 59.56, 59.56, 59.64, 59.82]

Epoch: 8
2024-03-09 22:52:54.315070 epoch: 8 step: 0 cls_loss= 0.96162 (151973 samples/sec)
2024-03-09 22:52:57.229443 epoch: 8 step: 100 cls_loss= 0.92502 (10298 samples/sec)
saving....
2024-03-09 22:53:00.914222------------------------------------------------------ Precision@1: 59.75% 

[59.52, 59.4, 59.82, 59.57, 59.56, 59.56, 59.64, 59.82, 59.75]

Epoch: 9
2024-03-09 22:53:01.141768 epoch: 9 step: 0 cls_loss= 0.90510 (132330 samples/sec)
2024-03-09 22:53:04.010551 epoch: 9 step: 100 cls_loss= 0.97580 (10457 samples/sec)
saving....
2024-03-09 22:53:07.669532------------------------------------------------------ Precision@1: 59.69% 

[59.52, 59.4, 59.82, 59.57, 59.56, 59.56, 59.64, 59.82, 59.75, 59.69]

Epoch: 10
2024-03-09 22:53:07.867701 epoch: 10 step: 0 cls_loss= 0.99836 (152239 samples/sec)
2024-03-09 22:53:10.764132 epoch: 10 step: 100 cls_loss= 1.04903 (10360 samples/sec)
saving....
2024-03-09 22:53:14.502518------------------------------------------------------ Precision@1: 59.88% 

[59.52, 59.4, 59.82, 59.57, 59.56, 59.56, 59.64, 59.82, 59.75, 59.69, 59.88]

Epoch: 11
2024-03-09 22:53:14.709351 epoch: 11 step: 0 cls_loss= 0.96365 (145865 samples/sec)
2024-03-09 22:53:17.758851 epoch: 11 step: 100 cls_loss= 1.00203 (9840 samples/sec)
saving....
2024-03-09 22:53:21.500286------------------------------------------------------ Precision@1: 59.38% 

[59.52, 59.4, 59.82, 59.57, 59.56, 59.56, 59.64, 59.82, 59.75, 59.69, 59.88, 59.38]

Epoch: 12
2024-03-09 22:53:21.692708 epoch: 12 step: 0 cls_loss= 1.08558 (156716 samples/sec)
2024-03-09 22:53:24.600819 epoch: 12 step: 100 cls_loss= 0.95838 (10320 samples/sec)
saving....
2024-03-09 22:53:28.296353------------------------------------------------------ Precision@1: 60.09% 

[59.52, 59.4, 59.82, 59.57, 59.56, 59.56, 59.64, 59.82, 59.75, 59.69, 59.88, 59.38, 60.09]

Epoch: 13
2024-03-09 22:53:28.482693 epoch: 13 step: 0 cls_loss= 1.02266 (161715 samples/sec)
2024-03-09 22:53:31.453746 epoch: 13 step: 100 cls_loss= 1.10261 (10101 samples/sec)
saving....
2024-03-09 22:53:35.160099------------------------------------------------------ Precision@1: 59.70% 

[59.52, 59.4, 59.82, 59.57, 59.56, 59.56, 59.64, 59.82, 59.75, 59.69, 59.88, 59.38, 60.09, 59.7]

Epoch: 14
2024-03-09 22:53:35.358731 epoch: 14 step: 0 cls_loss= 0.90589 (151949 samples/sec)
2024-03-09 22:53:38.290226 epoch: 14 step: 100 cls_loss= 1.13808 (10238 samples/sec)
saving....
2024-03-09 22:53:41.969108------------------------------------------------------ Precision@1: 59.70% 

[59.52, 59.4, 59.82, 59.57, 59.56, 59.56, 59.64, 59.82, 59.75, 59.69, 59.88, 59.38, 60.09, 59.7, 59.7]

Epoch: 15
2024-03-09 22:53:42.178133 epoch: 15 step: 0 cls_loss= 1.07256 (144188 samples/sec)
2024-03-09 22:53:45.073818 epoch: 15 step: 100 cls_loss= 1.19217 (10365 samples/sec)
saving....
2024-03-09 22:53:48.764353------------------------------------------------------ Precision@1: 59.59% 

[59.52, 59.4, 59.82, 59.57, 59.56, 59.56, 59.64, 59.82, 59.75, 59.69, 59.88, 59.38, 60.09, 59.7, 59.7, 59.59]

Epoch: 16
2024-03-09 22:53:48.966214 epoch: 16 step: 0 cls_loss= 1.05591 (149400 samples/sec)
2024-03-09 22:53:51.855014 epoch: 16 step: 100 cls_loss= 1.09882 (10389 samples/sec)
saving....
2024-03-09 22:53:55.581798------------------------------------------------------ Precision@1: 59.77% 

[59.52, 59.4, 59.82, 59.57, 59.56, 59.56, 59.64, 59.82, 59.75, 59.69, 59.88, 59.38, 60.09, 59.7, 59.7, 59.59, 59.77]

Epoch: 17
2024-03-09 22:53:55.775890 epoch: 17 step: 0 cls_loss= 0.99862 (155437 samples/sec)
2024-03-09 22:53:58.641199 epoch: 17 step: 100 cls_loss= 0.94874 (10474 samples/sec)
saving....
2024-03-09 22:54:02.299809------------------------------------------------------ Precision@1: 59.84% 

[59.52, 59.4, 59.82, 59.57, 59.56, 59.56, 59.64, 59.82, 59.75, 59.69, 59.88, 59.38, 60.09, 59.7, 59.7, 59.59, 59.77, 59.84]

Epoch: 18
2024-03-09 22:54:02.515294 epoch: 18 step: 0 cls_loss= 1.11743 (139683 samples/sec)
2024-03-09 22:54:05.565735 epoch: 18 step: 100 cls_loss= 1.02183 (9834 samples/sec)
saving....
2024-03-09 22:54:09.397592------------------------------------------------------ Precision@1: 59.79% 

[59.52, 59.4, 59.82, 59.57, 59.56, 59.56, 59.64, 59.82, 59.75, 59.69, 59.88, 59.38, 60.09, 59.7, 59.7, 59.59, 59.77, 59.84, 59.79]

Epoch: 19
2024-03-09 22:54:09.610876 epoch: 19 step: 0 cls_loss= 1.02693 (141467 samples/sec)
2024-03-09 22:54:12.494912 epoch: 19 step: 100 cls_loss= 0.95224 (10406 samples/sec)
saving....
2024-03-09 22:54:16.196795------------------------------------------------------ Precision@1: 59.75% 

[59.52, 59.4, 59.82, 59.57, 59.56, 59.56, 59.64, 59.82, 59.75, 59.69, 59.88, 59.38, 60.09, 59.7, 59.7, 59.59, 59.77, 59.84, 59.79, 59.75]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 22:54:18.929679 epoch: 0 step: 0 cls_loss= 1.07541 (50000 samples/sec)
2024-03-09 22:54:21.931359 epoch: 0 step: 100 cls_loss= 1.03752 (9994 samples/sec)
saving....
2024-03-09 22:54:25.878013------------------------------------------------------ Precision@1: 59.82% 

[59.82]

Epoch: 1
2024-03-09 22:54:26.070948 epoch: 1 step: 0 cls_loss= 1.07550 (156342 samples/sec)
2024-03-09 22:54:28.960826 epoch: 1 step: 100 cls_loss= 1.03206 (10385 samples/sec)
saving....
2024-03-09 22:54:32.655446------------------------------------------------------ Precision@1: 59.73% 

[59.82, 59.73]

Epoch: 2
2024-03-09 22:54:32.850597 epoch: 2 step: 0 cls_loss= 1.10097 (154538 samples/sec)
2024-03-09 22:54:35.898492 epoch: 2 step: 100 cls_loss= 1.02897 (9847 samples/sec)
saving....
2024-03-09 22:54:39.771153------------------------------------------------------ Precision@1: 59.64% 

[59.82, 59.73, 59.64]

Epoch: 3
2024-03-09 22:54:39.957490 epoch: 3 step: 0 cls_loss= 0.97574 (161933 samples/sec)
2024-03-09 22:54:42.853035 epoch: 3 step: 100 cls_loss= 1.12273 (10365 samples/sec)
saving....
2024-03-09 22:54:46.600994------------------------------------------------------ Precision@1: 59.46% 

[59.82, 59.73, 59.64, 59.46]

Epoch: 4
2024-03-09 22:54:46.803365 epoch: 4 step: 0 cls_loss= 1.08112 (148795 samples/sec)
2024-03-09 22:54:49.789404 epoch: 4 step: 100 cls_loss= 1.06266 (10051 samples/sec)
saving....
2024-03-09 22:54:53.521152------------------------------------------------------ Precision@1: 59.55% 

[59.82, 59.73, 59.64, 59.46, 59.55]

Epoch: 5
2024-03-09 22:54:53.723986 epoch: 5 step: 0 cls_loss= 1.04318 (148676 samples/sec)
2024-03-09 22:54:56.713603 epoch: 5 step: 100 cls_loss= 0.92070 (10038 samples/sec)
saving....
2024-03-09 22:55:00.617662------------------------------------------------------ Precision@1: 59.74% 

[59.82, 59.73, 59.64, 59.46, 59.55, 59.74]

Epoch: 6
2024-03-09 22:55:00.817115 epoch: 6 step: 0 cls_loss= 1.08093 (151212 samples/sec)
2024-03-09 22:55:03.745718 epoch: 6 step: 100 cls_loss= 0.96118 (10248 samples/sec)
saving....
2024-03-09 22:55:07.453755------------------------------------------------------ Precision@1: 59.62% 

[59.82, 59.73, 59.64, 59.46, 59.55, 59.74, 59.62]

Epoch: 7
2024-03-09 22:55:07.651163 epoch: 7 step: 0 cls_loss= 0.91164 (152835 samples/sec)
2024-03-09 22:55:10.568034 epoch: 7 step: 100 cls_loss= 0.99045 (10287 samples/sec)
saving....
2024-03-09 22:55:14.400768------------------------------------------------------ Precision@1: 59.83% 

[59.82, 59.73, 59.64, 59.46, 59.55, 59.74, 59.62, 59.83]

Epoch: 8
2024-03-09 22:55:14.592515 epoch: 8 step: 0 cls_loss= 1.11940 (157306 samples/sec)
2024-03-09 22:55:17.601043 epoch: 8 step: 100 cls_loss= 1.00065 (9974 samples/sec)
saving....
2024-03-09 22:55:21.411217------------------------------------------------------ Precision@1: 59.46% 

[59.82, 59.73, 59.64, 59.46, 59.55, 59.74, 59.62, 59.83, 59.46]

Epoch: 9
2024-03-09 22:55:21.621175 epoch: 9 step: 0 cls_loss= 0.96287 (143538 samples/sec)
2024-03-09 22:55:24.622108 epoch: 9 step: 100 cls_loss= 0.96553 (10000 samples/sec)
saving....
2024-03-09 22:55:28.331247------------------------------------------------------ Precision@1: 59.74% 

[59.82, 59.73, 59.64, 59.46, 59.55, 59.74, 59.62, 59.83, 59.46, 59.74]

Epoch: 10
2024-03-09 22:55:28.525784 epoch: 10 step: 0 cls_loss= 0.94549 (155016 samples/sec)
2024-03-09 22:55:31.484791 epoch: 10 step: 100 cls_loss= 0.95746 (10141 samples/sec)
saving....
2024-03-09 22:55:35.202938------------------------------------------------------ Precision@1: 59.72% 

[59.82, 59.73, 59.64, 59.46, 59.55, 59.74, 59.62, 59.83, 59.46, 59.74, 59.72]

Epoch: 11
2024-03-09 22:55:35.392342 epoch: 11 step: 0 cls_loss= 1.06587 (159314 samples/sec)
2024-03-09 22:55:38.297940 epoch: 11 step: 100 cls_loss= 0.98260 (10327 samples/sec)
saving....
2024-03-09 22:55:41.970368------------------------------------------------------ Precision@1: 59.62% 

[59.82, 59.73, 59.64, 59.46, 59.55, 59.74, 59.62, 59.83, 59.46, 59.74, 59.72, 59.62]

Epoch: 12
2024-03-09 22:55:42.162590 epoch: 12 step: 0 cls_loss= 1.01548 (156992 samples/sec)
2024-03-09 22:55:45.137315 epoch: 12 step: 100 cls_loss= 1.16592 (10086 samples/sec)
saving....
2024-03-09 22:55:48.914971------------------------------------------------------ Precision@1: 59.44% 

[59.82, 59.73, 59.64, 59.46, 59.55, 59.74, 59.62, 59.83, 59.46, 59.74, 59.72, 59.62, 59.44]

Epoch: 13
2024-03-09 22:55:49.117465 epoch: 13 step: 0 cls_loss= 0.91860 (149045 samples/sec)
2024-03-09 22:55:52.269924 epoch: 13 step: 100 cls_loss= 0.96887 (9516 samples/sec)
saving....
2024-03-09 22:55:56.054367------------------------------------------------------ Precision@1: 59.60% 

[59.82, 59.73, 59.64, 59.46, 59.55, 59.74, 59.62, 59.83, 59.46, 59.74, 59.72, 59.62, 59.44, 59.6]

Epoch: 14
2024-03-09 22:55:56.262856 epoch: 14 step: 0 cls_loss= 1.09705 (144755 samples/sec)
2024-03-09 22:55:59.168277 epoch: 14 step: 100 cls_loss= 1.13479 (10330 samples/sec)
saving....
2024-03-09 22:56:02.837442------------------------------------------------------ Precision@1: 59.77% 

[59.82, 59.73, 59.64, 59.46, 59.55, 59.74, 59.62, 59.83, 59.46, 59.74, 59.72, 59.62, 59.44, 59.6, 59.77]

Epoch: 15
2024-03-09 22:56:03.036568 epoch: 15 step: 0 cls_loss= 1.05662 (151514 samples/sec)
2024-03-09 22:56:05.946869 epoch: 15 step: 100 cls_loss= 0.98807 (10312 samples/sec)
saving....
2024-03-09 22:56:09.601133------------------------------------------------------ Precision@1: 59.37% 

[59.82, 59.73, 59.64, 59.46, 59.55, 59.74, 59.62, 59.83, 59.46, 59.74, 59.72, 59.62, 59.44, 59.6, 59.77, 59.37]

Epoch: 16
2024-03-09 22:56:09.796257 epoch: 16 step: 0 cls_loss= 1.00404 (154615 samples/sec)
2024-03-09 22:56:12.783752 epoch: 16 step: 100 cls_loss= 1.07347 (10045 samples/sec)
saving....
2024-03-09 22:56:16.522005------------------------------------------------------ Precision@1: 59.77% 

[59.82, 59.73, 59.64, 59.46, 59.55, 59.74, 59.62, 59.83, 59.46, 59.74, 59.72, 59.62, 59.44, 59.6, 59.77, 59.37, 59.77]

Epoch: 17
2024-03-09 22:56:16.712846 epoch: 17 step: 0 cls_loss= 1.00893 (158081 samples/sec)
2024-03-09 22:56:19.673223 epoch: 17 step: 100 cls_loss= 1.05192 (10135 samples/sec)
saving....
2024-03-09 22:56:23.369549------------------------------------------------------ Precision@1: 59.51% 

[59.82, 59.73, 59.64, 59.46, 59.55, 59.74, 59.62, 59.83, 59.46, 59.74, 59.72, 59.62, 59.44, 59.6, 59.77, 59.37, 59.77, 59.51]

Epoch: 18
2024-03-09 22:56:23.561176 epoch: 18 step: 0 cls_loss= 0.96823 (157494 samples/sec)
2024-03-09 22:56:26.459778 epoch: 18 step: 100 cls_loss= 0.84330 (10354 samples/sec)
saving....
2024-03-09 22:56:30.148117------------------------------------------------------ Precision@1: 59.66% 

[59.82, 59.73, 59.64, 59.46, 59.55, 59.74, 59.62, 59.83, 59.46, 59.74, 59.72, 59.62, 59.44, 59.6, 59.77, 59.37, 59.77, 59.51, 59.66]

Epoch: 19
2024-03-09 22:56:30.344534 epoch: 19 step: 0 cls_loss= 1.09953 (153564 samples/sec)
2024-03-09 22:56:33.311744 epoch: 19 step: 100 cls_loss= 0.93698 (10113 samples/sec)
saving....
2024-03-09 22:56:37.060543------------------------------------------------------ Precision@1: 59.77% 

[59.82, 59.73, 59.64, 59.46, 59.55, 59.74, 59.62, 59.83, 59.46, 59.74, 59.72, 59.62, 59.44, 59.6, 59.77, 59.37, 59.77, 59.51, 59.66, 59.77]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 22:56:39.792261 epoch: 0 step: 0 cls_loss= 1.04781 (49520 samples/sec)
2024-03-09 22:56:42.668259 epoch: 0 step: 100 cls_loss= 1.08890 (10431 samples/sec)
saving....
2024-03-09 22:56:46.553303------------------------------------------------------ Precision@1: 59.58% 

[59.58]

Epoch: 1
2024-03-09 22:56:46.741193 epoch: 1 step: 0 cls_loss= 1.11496 (160739 samples/sec)
2024-03-09 22:56:49.688553 epoch: 1 step: 100 cls_loss= 1.01894 (10183 samples/sec)
saving....
2024-03-09 22:56:53.357593------------------------------------------------------ Precision@1: 59.77% 

[59.58, 59.77]

Epoch: 2
2024-03-09 22:56:53.566204 epoch: 2 step: 0 cls_loss= 0.99613 (144579 samples/sec)
2024-03-09 22:56:56.595122 epoch: 2 step: 100 cls_loss= 0.94821 (9906 samples/sec)
saving....
2024-03-09 22:57:00.435920------------------------------------------------------ Precision@1: 59.77% 

[59.58, 59.77, 59.77]

Epoch: 3
2024-03-09 22:57:00.643961 epoch: 3 step: 0 cls_loss= 1.01422 (144861 samples/sec)
2024-03-09 22:57:03.559794 epoch: 3 step: 100 cls_loss= 0.97443 (10293 samples/sec)
saving....
2024-03-09 22:57:07.241695------------------------------------------------------ Precision@1: 59.78% 

[59.58, 59.77, 59.77, 59.78]

Epoch: 4
2024-03-09 22:57:07.441482 epoch: 4 step: 0 cls_loss= 1.04831 (150949 samples/sec)
2024-03-09 22:57:10.399542 epoch: 4 step: 100 cls_loss= 1.05577 (10142 samples/sec)
saving....
2024-03-09 22:57:14.100604------------------------------------------------------ Precision@1: 59.47% 

[59.58, 59.77, 59.77, 59.78, 59.47]

Epoch: 5
2024-03-09 22:57:14.302980 epoch: 5 step: 0 cls_loss= 0.95746 (148823 samples/sec)
2024-03-09 22:57:17.192380 epoch: 5 step: 100 cls_loss= 1.03433 (10385 samples/sec)
saving....
2024-03-09 22:57:20.890452------------------------------------------------------ Precision@1: 59.50% 

[59.58, 59.77, 59.77, 59.78, 59.47, 59.5]

Epoch: 6
2024-03-09 22:57:21.101240 epoch: 6 step: 0 cls_loss= 1.03110 (142946 samples/sec)
2024-03-09 22:57:23.982924 epoch: 6 step: 100 cls_loss= 1.05545 (10416 samples/sec)
saving....
2024-03-09 22:57:27.650892------------------------------------------------------ Precision@1: 59.55% 

[59.58, 59.77, 59.77, 59.78, 59.47, 59.5, 59.55]

Epoch: 7
2024-03-09 22:57:27.842358 epoch: 7 step: 0 cls_loss= 0.96335 (157583 samples/sec)
2024-03-09 22:57:30.740245 epoch: 7 step: 100 cls_loss= 1.07234 (10357 samples/sec)
saving....
2024-03-09 22:57:34.414956------------------------------------------------------ Precision@1: 59.79% 

[59.58, 59.77, 59.77, 59.78, 59.47, 59.5, 59.55, 59.79]

Epoch: 8
2024-03-09 22:57:34.628213 epoch: 8 step: 0 cls_loss= 1.02670 (141502 samples/sec)
2024-03-09 22:57:37.514396 epoch: 8 step: 100 cls_loss= 1.10090 (10394 samples/sec)
saving....
2024-03-09 22:57:41.178491------------------------------------------------------ Precision@1: 59.68% 

[59.58, 59.77, 59.77, 59.78, 59.47, 59.5, 59.55, 59.79, 59.68]

Epoch: 9
2024-03-09 22:57:41.385706 epoch: 9 step: 0 cls_loss= 1.02701 (145549 samples/sec)
2024-03-09 22:57:44.275583 epoch: 9 step: 100 cls_loss= 0.97142 (10385 samples/sec)
saving....
2024-03-09 22:57:48.013016------------------------------------------------------ Precision@1: 59.69% 

[59.58, 59.77, 59.77, 59.78, 59.47, 59.5, 59.55, 59.79, 59.68, 59.69]

Epoch: 10
2024-03-09 22:57:48.216877 epoch: 10 step: 0 cls_loss= 1.05969 (148002 samples/sec)
2024-03-09 22:57:51.158554 epoch: 10 step: 100 cls_loss= 1.08430 (10202 samples/sec)
saving....
2024-03-09 22:57:54.923409------------------------------------------------------ Precision@1: 59.42% 

[59.58, 59.77, 59.77, 59.78, 59.47, 59.5, 59.55, 59.79, 59.68, 59.69, 59.42]

Epoch: 11
2024-03-09 22:57:55.139030 epoch: 11 step: 0 cls_loss= 1.00114 (139884 samples/sec)
2024-03-09 22:57:58.235527 epoch: 11 step: 100 cls_loss= 1.00353 (9688 samples/sec)
saving....
2024-03-09 22:58:01.925380------------------------------------------------------ Precision@1: 59.64% 

[59.58, 59.77, 59.77, 59.78, 59.47, 59.5, 59.55, 59.79, 59.68, 59.69, 59.42, 59.64]

Epoch: 12
2024-03-09 22:58:02.139787 epoch: 12 step: 0 cls_loss= 1.07749 (140774 samples/sec)
2024-03-09 22:58:05.239015 epoch: 12 step: 100 cls_loss= 1.09615 (9680 samples/sec)
saving....
2024-03-09 22:58:09.029940------------------------------------------------------ Precision@1: 59.57% 

[59.58, 59.77, 59.77, 59.78, 59.47, 59.5, 59.55, 59.79, 59.68, 59.69, 59.42, 59.64, 59.57]

Epoch: 13
2024-03-09 22:58:09.233675 epoch: 13 step: 0 cls_loss= 1.11607 (148047 samples/sec)
2024-03-09 22:58:12.133551 epoch: 13 step: 100 cls_loss= 0.99982 (10345 samples/sec)
saving....
2024-03-09 22:58:15.783544------------------------------------------------------ Precision@1: 59.61% 

[59.58, 59.77, 59.77, 59.78, 59.47, 59.5, 59.55, 59.79, 59.68, 59.69, 59.42, 59.64, 59.57, 59.61]

Epoch: 14
2024-03-09 22:58:15.981451 epoch: 14 step: 0 cls_loss= 0.92436 (152479 samples/sec)
2024-03-09 22:58:18.862980 epoch: 14 step: 100 cls_loss= 1.06769 (10414 samples/sec)
saving....
2024-03-09 22:58:22.523312------------------------------------------------------ Precision@1: 59.70% 

[59.58, 59.77, 59.77, 59.78, 59.47, 59.5, 59.55, 59.79, 59.68, 59.69, 59.42, 59.64, 59.57, 59.61, 59.7]

Epoch: 15
2024-03-09 22:58:22.723390 epoch: 15 step: 0 cls_loss= 0.99478 (150684 samples/sec)
2024-03-09 22:58:25.748342 epoch: 15 step: 100 cls_loss= 1.05356 (9921 samples/sec)
saving....
2024-03-09 22:58:29.585780------------------------------------------------------ Precision@1: 59.61% 

[59.58, 59.77, 59.77, 59.78, 59.47, 59.5, 59.55, 59.79, 59.68, 59.69, 59.42, 59.64, 59.57, 59.61, 59.7, 59.61]

Epoch: 16
2024-03-09 22:58:29.777614 epoch: 16 step: 0 cls_loss= 0.90399 (157254 samples/sec)
2024-03-09 22:58:32.642564 epoch: 16 step: 100 cls_loss= 0.93722 (10476 samples/sec)
saving....
2024-03-09 22:58:36.277404------------------------------------------------------ Precision@1: 59.80% 

[59.58, 59.77, 59.77, 59.78, 59.47, 59.5, 59.55, 59.79, 59.68, 59.69, 59.42, 59.64, 59.57, 59.61, 59.7, 59.61, 59.8]

Epoch: 17
2024-03-09 22:58:36.470210 epoch: 17 step: 0 cls_loss= 1.05923 (156506 samples/sec)
2024-03-09 22:58:39.381929 epoch: 17 step: 100 cls_loss= 1.01125 (10306 samples/sec)
saving....
2024-03-09 22:58:43.046773------------------------------------------------------ Precision@1: 59.64% 

[59.58, 59.77, 59.77, 59.78, 59.47, 59.5, 59.55, 59.79, 59.68, 59.69, 59.42, 59.64, 59.57, 59.61, 59.7, 59.61, 59.8, 59.64]

Epoch: 18
2024-03-09 22:58:43.248636 epoch: 18 step: 0 cls_loss= 0.87020 (149239 samples/sec)
2024-03-09 22:58:46.117223 epoch: 18 step: 100 cls_loss= 1.12377 (10462 samples/sec)
saving....
2024-03-09 22:58:49.771682------------------------------------------------------ Precision@1: 59.69% 

[59.58, 59.77, 59.77, 59.78, 59.47, 59.5, 59.55, 59.79, 59.68, 59.69, 59.42, 59.64, 59.57, 59.61, 59.7, 59.61, 59.8, 59.64, 59.69]

Epoch: 19
2024-03-09 22:58:49.975931 epoch: 19 step: 0 cls_loss= 1.08141 (147664 samples/sec)
2024-03-09 22:58:53.026806 epoch: 19 step: 100 cls_loss= 0.90468 (9833 samples/sec)
saving....
2024-03-09 22:58:56.811598------------------------------------------------------ Precision@1: 59.63% 

[59.58, 59.77, 59.77, 59.78, 59.47, 59.5, 59.55, 59.79, 59.68, 59.69, 59.42, 59.64, 59.57, 59.61, 59.7, 59.61, 59.8, 59.64, 59.69, 59.63]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 22:58:59.570038 epoch: 0 step: 0 cls_loss= 1.07868 (47700 samples/sec)
2024-03-09 22:59:02.446164 epoch: 0 step: 100 cls_loss= 0.99522 (10430 samples/sec)
saving....
2024-03-09 22:59:06.343866------------------------------------------------------ Precision@1: 59.47% 

[59.47]

Epoch: 1
2024-03-09 22:59:06.528321 epoch: 1 step: 0 cls_loss= 1.12302 (163526 samples/sec)
2024-03-09 22:59:09.477464 epoch: 1 step: 100 cls_loss= 1.13567 (10177 samples/sec)
saving....
2024-03-09 22:59:13.219667------------------------------------------------------ Precision@1: 59.44% 

[59.47, 59.44]

Epoch: 2
2024-03-09 22:59:13.430845 epoch: 2 step: 0 cls_loss= 1.07075 (142710 samples/sec)
2024-03-09 22:59:16.385348 epoch: 2 step: 100 cls_loss= 0.95705 (10154 samples/sec)
saving....
2024-03-09 22:59:20.139205------------------------------------------------------ Precision@1: 59.54% 

[59.47, 59.44, 59.54]

Epoch: 3
2024-03-09 22:59:20.343919 epoch: 3 step: 0 cls_loss= 1.05005 (147294 samples/sec)
2024-03-09 22:59:23.279465 epoch: 3 step: 100 cls_loss= 0.97578 (10222 samples/sec)
saving....
2024-03-09 22:59:26.944967------------------------------------------------------ Precision@1: 59.46% 

[59.47, 59.44, 59.54, 59.46]

Epoch: 4
2024-03-09 22:59:27.126131 epoch: 4 step: 0 cls_loss= 1.14037 (166602 samples/sec)
2024-03-09 22:59:30.061724 epoch: 4 step: 100 cls_loss= 0.92796 (10223 samples/sec)
saving....
2024-03-09 22:59:33.741044------------------------------------------------------ Precision@1: 59.50% 

[59.47, 59.44, 59.54, 59.46, 59.5]

Epoch: 5
2024-03-09 22:59:33.921985 epoch: 5 step: 0 cls_loss= 1.17692 (166806 samples/sec)
2024-03-09 22:59:36.860582 epoch: 5 step: 100 cls_loss= 1.04136 (10211 samples/sec)
saving....
2024-03-09 22:59:40.633142------------------------------------------------------ Precision@1: 59.67% 

[59.47, 59.44, 59.54, 59.46, 59.5, 59.67]

Epoch: 6
2024-03-09 22:59:40.838858 epoch: 6 step: 0 cls_loss= 1.04504 (146565 samples/sec)
2024-03-09 22:59:43.749805 epoch: 6 step: 100 cls_loss= 1.04629 (10311 samples/sec)
saving....
2024-03-09 22:59:47.457620------------------------------------------------------ Precision@1: 59.87% 

[59.47, 59.44, 59.54, 59.46, 59.5, 59.67, 59.87]

Epoch: 7
2024-03-09 22:59:47.668191 epoch: 7 step: 0 cls_loss= 1.10275 (143164 samples/sec)
2024-03-09 22:59:50.530504 epoch: 7 step: 100 cls_loss= 0.88326 (10485 samples/sec)
saving....
2024-03-09 22:59:54.229295------------------------------------------------------ Precision@1: 59.63% 

[59.47, 59.44, 59.54, 59.46, 59.5, 59.67, 59.87, 59.63]

Epoch: 8
2024-03-09 22:59:54.446084 epoch: 8 step: 0 cls_loss= 1.01133 (139183 samples/sec)
2024-03-09 22:59:57.563788 epoch: 8 step: 100 cls_loss= 0.99754 (9622 samples/sec)
saving....
2024-03-09 23:00:01.382328------------------------------------------------------ Precision@1: 59.51% 

[59.47, 59.44, 59.54, 59.46, 59.5, 59.67, 59.87, 59.63, 59.51]

Epoch: 9
2024-03-09 23:00:01.571493 epoch: 9 step: 0 cls_loss= 1.12610 (159506 samples/sec)
2024-03-09 23:00:04.595844 epoch: 9 step: 100 cls_loss= 1.03141 (9923 samples/sec)
saving....
2024-03-09 23:00:08.235556------------------------------------------------------ Precision@1: 59.38% 

[59.47, 59.44, 59.54, 59.46, 59.5, 59.67, 59.87, 59.63, 59.51, 59.38]

Epoch: 10
2024-03-09 23:00:08.447795 epoch: 10 step: 0 cls_loss= 1.08909 (141944 samples/sec)
2024-03-09 23:00:11.577048 epoch: 10 step: 100 cls_loss= 1.06047 (9589 samples/sec)
saving....
2024-03-09 23:00:15.361182------------------------------------------------------ Precision@1: 59.51% 

[59.47, 59.44, 59.54, 59.46, 59.5, 59.67, 59.87, 59.63, 59.51, 59.38, 59.51]

Epoch: 11
2024-03-09 23:00:15.562222 epoch: 11 step: 0 cls_loss= 0.99101 (150009 samples/sec)
2024-03-09 23:00:18.510695 epoch: 11 step: 100 cls_loss= 0.99139 (10178 samples/sec)
saving....
2024-03-09 23:00:22.238341------------------------------------------------------ Precision@1: 59.53% 

[59.47, 59.44, 59.54, 59.46, 59.5, 59.67, 59.87, 59.63, 59.51, 59.38, 59.51, 59.53]

Epoch: 12
2024-03-09 23:00:22.442578 epoch: 12 step: 0 cls_loss= 1.11836 (147769 samples/sec)
2024-03-09 23:00:25.560130 epoch: 12 step: 100 cls_loss= 0.98088 (9624 samples/sec)
saving....
2024-03-09 23:00:29.462279------------------------------------------------------ Precision@1: 59.50% 

[59.47, 59.44, 59.54, 59.46, 59.5, 59.67, 59.87, 59.63, 59.51, 59.38, 59.51, 59.53, 59.5]

Epoch: 13
2024-03-09 23:00:29.686334 epoch: 13 step: 0 cls_loss= 1.02583 (134517 samples/sec)
2024-03-09 23:00:32.553908 epoch: 13 step: 100 cls_loss= 1.01940 (10467 samples/sec)
saving....
2024-03-09 23:00:36.196494------------------------------------------------------ Precision@1: 59.57% 

[59.47, 59.44, 59.54, 59.46, 59.5, 59.67, 59.87, 59.63, 59.51, 59.38, 59.51, 59.53, 59.5, 59.57]

Epoch: 14
2024-03-09 23:00:36.393128 epoch: 14 step: 0 cls_loss= 0.96433 (153271 samples/sec)
2024-03-09 23:00:39.339777 epoch: 14 step: 100 cls_loss= 0.93880 (10185 samples/sec)
saving....
2024-03-09 23:00:43.046113------------------------------------------------------ Precision@1: 59.67% 

[59.47, 59.44, 59.54, 59.46, 59.5, 59.67, 59.87, 59.63, 59.51, 59.38, 59.51, 59.53, 59.5, 59.57, 59.67]

Epoch: 15
2024-03-09 23:00:43.242035 epoch: 15 step: 0 cls_loss= 0.89597 (153901 samples/sec)
2024-03-09 23:00:46.185244 epoch: 15 step: 100 cls_loss= 0.93903 (10194 samples/sec)
saving....
2024-03-09 23:00:50.093092------------------------------------------------------ Precision@1: 59.54% 

[59.47, 59.44, 59.54, 59.46, 59.5, 59.67, 59.87, 59.63, 59.51, 59.38, 59.51, 59.53, 59.5, 59.57, 59.67, 59.54]

Epoch: 16
2024-03-09 23:00:50.284282 epoch: 16 step: 0 cls_loss= 1.01324 (157766 samples/sec)
2024-03-09 23:00:53.216497 epoch: 16 step: 100 cls_loss= 1.08423 (10232 samples/sec)
saving....
2024-03-09 23:00:56.909501------------------------------------------------------ Precision@1: 59.87% 

[59.47, 59.44, 59.54, 59.46, 59.5, 59.67, 59.87, 59.63, 59.51, 59.38, 59.51, 59.53, 59.5, 59.57, 59.67, 59.54, 59.87]

Epoch: 17
2024-03-09 23:00:57.109542 epoch: 17 step: 0 cls_loss= 0.97535 (150883 samples/sec)
2024-03-09 23:01:00.042602 epoch: 17 step: 100 cls_loss= 0.98727 (10231 samples/sec)
saving....
2024-03-09 23:01:03.790780------------------------------------------------------ Precision@1: 59.51% 

[59.47, 59.44, 59.54, 59.46, 59.5, 59.67, 59.87, 59.63, 59.51, 59.38, 59.51, 59.53, 59.5, 59.57, 59.67, 59.54, 59.87, 59.51]

Epoch: 18
2024-03-09 23:01:03.980734 epoch: 18 step: 0 cls_loss= 0.92352 (158865 samples/sec)
2024-03-09 23:01:07.059668 epoch: 18 step: 100 cls_loss= 1.02791 (9744 samples/sec)
saving....
2024-03-09 23:01:11.015478------------------------------------------------------ Precision@1: 59.78% 

[59.47, 59.44, 59.54, 59.46, 59.5, 59.67, 59.87, 59.63, 59.51, 59.38, 59.51, 59.53, 59.5, 59.57, 59.67, 59.54, 59.87, 59.51, 59.78]

Epoch: 19
2024-03-09 23:01:11.194757 epoch: 19 step: 0 cls_loss= 0.85881 (168436 samples/sec)
2024-03-09 23:01:14.151734 epoch: 19 step: 100 cls_loss= 1.03693 (10150 samples/sec)
saving....
2024-03-09 23:01:17.973491------------------------------------------------------ Precision@1: 59.96% 

[59.47, 59.44, 59.54, 59.46, 59.5, 59.67, 59.87, 59.63, 59.51, 59.38, 59.51, 59.53, 59.5, 59.57, 59.67, 59.54, 59.87, 59.51, 59.78, 59.96]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 23:01:20.684388 epoch: 0 step: 0 cls_loss= 0.95162 (51320 samples/sec)
2024-03-09 23:01:23.544214 epoch: 0 step: 100 cls_loss= 0.98393 (10490 samples/sec)
saving....
2024-03-09 23:01:27.448067------------------------------------------------------ Precision@1: 59.72% 

[59.72]

Epoch: 1
2024-03-09 23:01:27.650618 epoch: 1 step: 0 cls_loss= 1.13234 (148970 samples/sec)
2024-03-09 23:01:30.596386 epoch: 1 step: 100 cls_loss= 0.99389 (10187 samples/sec)
saving....
2024-03-09 23:01:34.302924------------------------------------------------------ Precision@1: 59.53% 

[59.72, 59.53]

Epoch: 2
2024-03-09 23:01:34.508816 epoch: 2 step: 0 cls_loss= 1.12928 (146459 samples/sec)
2024-03-09 23:01:37.429140 epoch: 2 step: 100 cls_loss= 1.02607 (10278 samples/sec)
saving....
2024-03-09 23:01:41.176567------------------------------------------------------ Precision@1: 59.57% 

[59.72, 59.53, 59.57]

Epoch: 3
2024-03-09 23:01:41.376936 epoch: 3 step: 0 cls_loss= 1.03600 (150381 samples/sec)
2024-03-09 23:01:44.364406 epoch: 3 step: 100 cls_loss= 1.03069 (10044 samples/sec)
saving....
2024-03-09 23:01:48.110974------------------------------------------------------ Precision@1: 59.46% 

[59.72, 59.53, 59.57, 59.46]

Epoch: 4
2024-03-09 23:01:48.299239 epoch: 4 step: 0 cls_loss= 1.01362 (160322 samples/sec)
2024-03-09 23:01:51.193871 epoch: 4 step: 100 cls_loss= 1.13720 (10369 samples/sec)
saving....
2024-03-09 23:01:54.880466------------------------------------------------------ Precision@1: 59.84% 

[59.72, 59.53, 59.57, 59.46, 59.84]

Epoch: 5
2024-03-09 23:01:55.073678 epoch: 5 step: 0 cls_loss= 1.03519 (156172 samples/sec)
2024-03-09 23:01:58.048911 epoch: 5 step: 100 cls_loss= 1.08685 (10087 samples/sec)
saving....
2024-03-09 23:02:01.832269------------------------------------------------------ Precision@1: 59.41% 

[59.72, 59.53, 59.57, 59.46, 59.84, 59.41]

Epoch: 6
2024-03-09 23:02:02.035581 epoch: 6 step: 0 cls_loss= 1.11426 (148366 samples/sec)
2024-03-09 23:02:05.047841 epoch: 6 step: 100 cls_loss= 0.97614 (9959 samples/sec)
saving....
2024-03-09 23:02:08.752467------------------------------------------------------ Precision@1: 59.74% 

[59.72, 59.53, 59.57, 59.46, 59.84, 59.41, 59.74]

Epoch: 7
2024-03-09 23:02:08.953136 epoch: 7 step: 0 cls_loss= 0.98496 (150300 samples/sec)
2024-03-09 23:02:11.854275 epoch: 7 step: 100 cls_loss= 0.97610 (10345 samples/sec)
saving....
2024-03-09 23:02:15.669012------------------------------------------------------ Precision@1: 59.63% 

[59.72, 59.53, 59.57, 59.46, 59.84, 59.41, 59.74, 59.63]

Epoch: 8
2024-03-09 23:02:15.870380 epoch: 8 step: 0 cls_loss= 0.94184 (149580 samples/sec)
2024-03-09 23:02:18.742136 epoch: 8 step: 100 cls_loss= 0.85521 (10451 samples/sec)
saving....
2024-03-09 23:02:22.431747------------------------------------------------------ Precision@1: 59.42% 

[59.72, 59.53, 59.57, 59.46, 59.84, 59.41, 59.74, 59.63, 59.42]

Epoch: 9
2024-03-09 23:02:22.636199 epoch: 9 step: 0 cls_loss= 0.97762 (147381 samples/sec)
2024-03-09 23:02:25.528370 epoch: 9 step: 100 cls_loss= 1.06925 (10377 samples/sec)
saving....
2024-03-09 23:02:29.212747------------------------------------------------------ Precision@1: 59.66% 

[59.72, 59.53, 59.57, 59.46, 59.84, 59.41, 59.74, 59.63, 59.42, 59.66]

Epoch: 10
2024-03-09 23:02:29.419562 epoch: 10 step: 0 cls_loss= 1.09414 (145853 samples/sec)
2024-03-09 23:02:32.365365 epoch: 10 step: 100 cls_loss= 1.06582 (10184 samples/sec)
saving....
2024-03-09 23:02:36.049277------------------------------------------------------ Precision@1: 59.66% 

[59.72, 59.53, 59.57, 59.46, 59.84, 59.41, 59.74, 59.63, 59.42, 59.66, 59.66]

Epoch: 11
2024-03-09 23:02:36.260272 epoch: 11 step: 0 cls_loss= 1.04284 (142728 samples/sec)
2024-03-09 23:02:39.137447 epoch: 11 step: 100 cls_loss= 1.09943 (10431 samples/sec)
saving....
2024-03-09 23:02:43.028708------------------------------------------------------ Precision@1: 59.67% 

[59.72, 59.53, 59.57, 59.46, 59.84, 59.41, 59.74, 59.63, 59.42, 59.66, 59.66, 59.67]

Epoch: 12
2024-03-09 23:02:43.243922 epoch: 12 step: 0 cls_loss= 1.06394 (140057 samples/sec)
2024-03-09 23:02:46.151231 epoch: 12 step: 100 cls_loss= 0.96140 (10319 samples/sec)
saving....
2024-03-09 23:02:49.888457------------------------------------------------------ Precision@1: 59.91% 

[59.72, 59.53, 59.57, 59.46, 59.84, 59.41, 59.74, 59.63, 59.42, 59.66, 59.66, 59.67, 59.91]

Epoch: 13
2024-03-09 23:02:50.105049 epoch: 13 step: 0 cls_loss= 0.92459 (139291 samples/sec)
2024-03-09 23:02:53.042426 epoch: 13 step: 100 cls_loss= 0.98167 (10218 samples/sec)
saving....
2024-03-09 23:02:56.901639------------------------------------------------------ Precision@1: 59.94% 

[59.72, 59.53, 59.57, 59.46, 59.84, 59.41, 59.74, 59.63, 59.42, 59.66, 59.66, 59.67, 59.91, 59.94]

Epoch: 14
2024-03-09 23:02:57.114947 epoch: 14 step: 0 cls_loss= 1.02973 (141346 samples/sec)
2024-03-09 23:03:00.081312 epoch: 14 step: 100 cls_loss= 1.03642 (10115 samples/sec)
saving....
2024-03-09 23:03:03.762308------------------------------------------------------ Precision@1: 59.70% 

[59.72, 59.53, 59.57, 59.46, 59.84, 59.41, 59.74, 59.63, 59.42, 59.66, 59.66, 59.67, 59.91, 59.94, 59.7]

Epoch: 15
2024-03-09 23:03:03.952885 epoch: 15 step: 0 cls_loss= 1.05957 (158358 samples/sec)
2024-03-09 23:03:06.829417 epoch: 15 step: 100 cls_loss= 0.99384 (10434 samples/sec)
saving....
2024-03-09 23:03:10.478257------------------------------------------------------ Precision@1: 59.64% 

[59.72, 59.53, 59.57, 59.46, 59.84, 59.41, 59.74, 59.63, 59.42, 59.66, 59.66, 59.67, 59.91, 59.94, 59.7, 59.64]

Epoch: 16
2024-03-09 23:03:10.668864 epoch: 16 step: 0 cls_loss= 0.97904 (158293 samples/sec)
2024-03-09 23:03:13.545583 epoch: 16 step: 100 cls_loss= 1.13459 (10433 samples/sec)
saving....
2024-03-09 23:03:17.276119------------------------------------------------------ Precision@1: 59.40% 

[59.72, 59.53, 59.57, 59.46, 59.84, 59.41, 59.74, 59.63, 59.42, 59.66, 59.66, 59.67, 59.91, 59.94, 59.7, 59.64, 59.4]

Epoch: 17
2024-03-09 23:03:17.489933 epoch: 17 step: 0 cls_loss= 1.15248 (140933 samples/sec)
2024-03-09 23:03:20.372924 epoch: 17 step: 100 cls_loss= 1.01120 (10406 samples/sec)
saving....
2024-03-09 23:03:24.080602------------------------------------------------------ Precision@1: 59.63% 

[59.72, 59.53, 59.57, 59.46, 59.84, 59.41, 59.74, 59.63, 59.42, 59.66, 59.66, 59.67, 59.91, 59.94, 59.7, 59.64, 59.4, 59.63]

Epoch: 18
2024-03-09 23:03:24.279243 epoch: 18 step: 0 cls_loss= 1.03151 (151806 samples/sec)
2024-03-09 23:03:27.242885 epoch: 18 step: 100 cls_loss= 0.99021 (10122 samples/sec)
saving....
2024-03-09 23:03:30.953590------------------------------------------------------ Precision@1: 59.69% 

[59.72, 59.53, 59.57, 59.46, 59.84, 59.41, 59.74, 59.63, 59.42, 59.66, 59.66, 59.67, 59.91, 59.94, 59.7, 59.64, 59.4, 59.63, 59.69]

Epoch: 19
2024-03-09 23:03:31.160526 epoch: 19 step: 0 cls_loss= 1.03209 (145700 samples/sec)
2024-03-09 23:03:34.119126 epoch: 19 step: 100 cls_loss= 1.03490 (10140 samples/sec)
saving....
2024-03-09 23:03:37.873301------------------------------------------------------ Precision@1: 59.87% 

[59.72, 59.53, 59.57, 59.46, 59.84, 59.41, 59.74, 59.63, 59.42, 59.66, 59.66, 59.67, 59.91, 59.94, 59.7, 59.64, 59.4, 59.63, 59.69, 59.87]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-09 23:03:40.580035 epoch: 0 step: 0 cls_loss= 0.96290 (51039 samples/sec)
2024-03-09 23:03:43.701713 epoch: 0 step: 100 cls_loss= 1.14668 (9610 samples/sec)
saving....
2024-03-09 23:03:47.736698------------------------------------------------------ Precision@1: 59.66% 

[59.66]

Epoch: 1
2024-03-09 23:03:47.939985 epoch: 1 step: 0 cls_loss= 1.09653 (148225 samples/sec)
2024-03-09 23:03:51.091812 epoch: 1 step: 100 cls_loss= 1.02644 (9518 samples/sec)
saving....
2024-03-09 23:03:54.908192------------------------------------------------------ Precision@1: 59.40% 

[59.66, 59.4]

Epoch: 2
2024-03-09 23:03:55.117922 epoch: 2 step: 0 cls_loss= 1.02066 (143875 samples/sec)
2024-03-09 23:03:58.170423 epoch: 2 step: 100 cls_loss= 1.05523 (9828 samples/sec)
saving....
2024-03-09 23:04:01.929852------------------------------------------------------ Precision@1: 59.64% 

[59.66, 59.4, 59.64]

Epoch: 3
2024-03-09 23:04:02.136108 epoch: 3 step: 0 cls_loss= 0.90327 (146234 samples/sec)
2024-03-09 23:04:05.146277 epoch: 3 step: 100 cls_loss= 0.97903 (9966 samples/sec)
saving....
2024-03-09 23:04:08.929938------------------------------------------------------ Precision@1: 59.39% 

[59.66, 59.4, 59.64, 59.39]

Epoch: 4
2024-03-09 23:04:09.139417 epoch: 4 step: 0 cls_loss= 0.88574 (143757 samples/sec)
2024-03-09 23:04:12.254601 epoch: 4 step: 100 cls_loss= 0.88198 (9630 samples/sec)
saving....
2024-03-09 23:04:16.098352------------------------------------------------------ Precision@1: 59.58% 

[59.66, 59.4, 59.64, 59.39, 59.58]

Epoch: 5
2024-03-09 23:04:16.288835 epoch: 5 step: 0 cls_loss= 0.71055 (158414 samples/sec)
2024-03-09 23:04:19.236538 epoch: 5 step: 100 cls_loss= 0.88933 (10177 samples/sec)
saving....
2024-03-09 23:04:22.967864------------------------------------------------------ Precision@1: 59.45% 

[59.66, 59.4, 59.64, 59.39, 59.58, 59.45]

Epoch: 6
2024-03-09 23:04:23.176626 epoch: 6 step: 0 cls_loss= 0.85119 (144487 samples/sec)
2024-03-09 23:04:26.174432 epoch: 6 step: 100 cls_loss= 0.84317 (10007 samples/sec)
saving....
2024-03-09 23:04:29.912866------------------------------------------------------ Precision@1: 59.49% 

[59.66, 59.4, 59.64, 59.39, 59.58, 59.45, 59.49]

Epoch: 7
2024-03-09 23:04:30.111893 epoch: 7 step: 0 cls_loss= 0.92068 (151591 samples/sec)
2024-03-09 23:04:33.180422 epoch: 7 step: 100 cls_loss= 1.07581 (9776 samples/sec)
saving....
2024-03-09 23:04:36.989632------------------------------------------------------ Precision@1: 59.71% 

[59.66, 59.4, 59.64, 59.39, 59.58, 59.45, 59.49, 59.71]

Epoch: 8
2024-03-09 23:04:37.197613 epoch: 8 step: 0 cls_loss= 0.82430 (145073 samples/sec)
2024-03-09 23:04:40.223016 epoch: 8 step: 100 cls_loss= 0.80086 (9916 samples/sec)
saving....
2024-03-09 23:04:44.053957------------------------------------------------------ Precision@1: 59.68% 

[59.66, 59.4, 59.64, 59.39, 59.58, 59.45, 59.49, 59.71, 59.68]

Epoch: 9
2024-03-09 23:04:44.245818 epoch: 9 step: 0 cls_loss= 0.74005 (157281 samples/sec)
2024-03-09 23:04:47.310417 epoch: 9 step: 100 cls_loss= 0.89712 (9789 samples/sec)
saving....
2024-03-09 23:04:51.086200------------------------------------------------------ Precision@1: 59.45% 

[59.66, 59.4, 59.64, 59.39, 59.58, 59.45, 59.49, 59.71, 59.68, 59.45]

Epoch: 10
2024-03-09 23:04:51.287977 epoch: 10 step: 0 cls_loss= 0.84765 (149451 samples/sec)
2024-03-09 23:04:54.393594 epoch: 10 step: 100 cls_loss= 0.83460 (9660 samples/sec)
saving....
2024-03-09 23:04:58.217265------------------------------------------------------ Precision@1: 59.51% 

[59.66, 59.4, 59.64, 59.39, 59.58, 59.45, 59.49, 59.71, 59.68, 59.45, 59.51]

Epoch: 11
2024-03-09 23:04:58.425686 epoch: 11 step: 0 cls_loss= 0.74132 (144436 samples/sec)
2024-03-09 23:05:01.416364 epoch: 11 step: 100 cls_loss= 0.79709 (10031 samples/sec)
saving....
2024-03-09 23:05:05.144908------------------------------------------------------ Precision@1: 59.71% 

[59.66, 59.4, 59.64, 59.39, 59.58, 59.45, 59.49, 59.71, 59.68, 59.45, 59.51, 59.71]

Epoch: 12
2024-03-09 23:05:05.360246 epoch: 12 step: 0 cls_loss= 0.73482 (140038 samples/sec)
2024-03-09 23:05:08.300888 epoch: 12 step: 100 cls_loss= 0.91810 (10202 samples/sec)
saving....
2024-03-09 23:05:12.055608------------------------------------------------------ Precision@1: 59.00% 

[59.66, 59.4, 59.64, 59.39, 59.58, 59.45, 59.49, 59.71, 59.68, 59.45, 59.51, 59.71, 59.0]

Epoch: 13
2024-03-09 23:05:12.250897 epoch: 13 step: 0 cls_loss= 0.84217 (154444 samples/sec)
2024-03-09 23:05:15.342711 epoch: 13 step: 100 cls_loss= 0.79165 (9703 samples/sec)
saving....
2024-03-09 23:05:19.180180------------------------------------------------------ Precision@1: 59.14% 

[59.66, 59.4, 59.64, 59.39, 59.58, 59.45, 59.49, 59.71, 59.68, 59.45, 59.51, 59.71, 59.0, 59.14]

Epoch: 14
2024-03-09 23:05:19.390207 epoch: 14 step: 0 cls_loss= 0.87453 (143645 samples/sec)
2024-03-09 23:05:22.399437 epoch: 14 step: 100 cls_loss= 0.70482 (9969 samples/sec)
saving....
2024-03-09 23:05:26.160070------------------------------------------------------ Precision@1: 59.64% 

[59.66, 59.4, 59.64, 59.39, 59.58, 59.45, 59.49, 59.71, 59.68, 59.45, 59.51, 59.71, 59.0, 59.14, 59.64]

Epoch: 15
2024-03-09 23:05:26.371764 epoch: 15 step: 0 cls_loss= 0.80038 (142418 samples/sec)
2024-03-09 23:05:29.382977 epoch: 15 step: 100 cls_loss= 0.77334 (9963 samples/sec)
saving....
2024-03-09 23:05:33.123651------------------------------------------------------ Precision@1: 59.50% 

[59.66, 59.4, 59.64, 59.39, 59.58, 59.45, 59.49, 59.71, 59.68, 59.45, 59.51, 59.71, 59.0, 59.14, 59.64, 59.5]

Epoch: 16
2024-03-09 23:05:33.331980 epoch: 16 step: 0 cls_loss= 0.70637 (144838 samples/sec)
2024-03-09 23:05:36.345760 epoch: 16 step: 100 cls_loss= 0.78290 (9954 samples/sec)
saving....
2024-03-09 23:05:40.133675------------------------------------------------------ Precision@1: 59.56% 

[59.66, 59.4, 59.64, 59.39, 59.58, 59.45, 59.49, 59.71, 59.68, 59.45, 59.51, 59.71, 59.0, 59.14, 59.64, 59.5, 59.56]

Epoch: 17
2024-03-09 23:05:40.345468 epoch: 17 step: 0 cls_loss= 0.74279 (142342 samples/sec)
2024-03-09 23:05:43.510328 epoch: 17 step: 100 cls_loss= 0.73604 (9479 samples/sec)
saving....
2024-03-09 23:05:47.377759------------------------------------------------------ Precision@1: 59.34% 

[59.66, 59.4, 59.64, 59.39, 59.58, 59.45, 59.49, 59.71, 59.68, 59.45, 59.51, 59.71, 59.0, 59.14, 59.64, 59.5, 59.56, 59.34]

Epoch: 18
2024-03-09 23:05:47.588686 epoch: 18 step: 0 cls_loss= 0.62766 (142911 samples/sec)
2024-03-09 23:05:50.735282 epoch: 18 step: 100 cls_loss= 0.63345 (9534 samples/sec)
saving....
2024-03-09 23:05:54.632871------------------------------------------------------ Precision@1: 59.24% 

[59.66, 59.4, 59.64, 59.39, 59.58, 59.45, 59.49, 59.71, 59.68, 59.45, 59.51, 59.71, 59.0, 59.14, 59.64, 59.5, 59.56, 59.34, 59.24]

Epoch: 19
2024-03-09 23:05:54.839521 epoch: 19 step: 0 cls_loss= 0.79831 (145969 samples/sec)
2024-03-09 23:05:57.942314 epoch: 19 step: 100 cls_loss= 0.70195 (9669 samples/sec)
saving....
2024-03-09 23:06:01.730397------------------------------------------------------ Precision@1: 59.15% 

[59.66, 59.4, 59.64, 59.39, 59.58, 59.45, 59.49, 59.71, 59.68, 59.45, 59.51, 59.71, 59.0, 59.14, 59.64, 59.5, 59.56, 59.34, 59.24, 59.15]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-09 23:06:04.492986 epoch: 0 step: 0 cls_loss= 1.01996 (47998 samples/sec)
2024-03-09 23:06:07.618624 epoch: 0 step: 100 cls_loss= 1.05689 (9598 samples/sec)
saving....
2024-03-09 23:06:11.688259------------------------------------------------------ Precision@1: 59.41% 

[59.41]

Epoch: 1
2024-03-09 23:06:11.890229 epoch: 1 step: 0 cls_loss= 1.09727 (149111 samples/sec)
2024-03-09 23:06:14.986555 epoch: 1 step: 100 cls_loss= 0.96455 (9689 samples/sec)
saving....
2024-03-09 23:06:18.812375------------------------------------------------------ Precision@1: 59.71% 

[59.41, 59.71]

Epoch: 2
2024-03-09 23:06:19.007162 epoch: 2 step: 0 cls_loss= 1.02051 (154857 samples/sec)
2024-03-09 23:06:22.017229 epoch: 2 step: 100 cls_loss= 0.99567 (9966 samples/sec)
saving....
2024-03-09 23:06:25.766327------------------------------------------------------ Precision@1: 59.34% 

[59.41, 59.71, 59.34]

Epoch: 3
2024-03-09 23:06:25.962698 epoch: 3 step: 0 cls_loss= 0.92094 (153670 samples/sec)
2024-03-09 23:06:29.036741 epoch: 3 step: 100 cls_loss= 0.85931 (9759 samples/sec)
saving....
2024-03-09 23:06:32.793180------------------------------------------------------ Precision@1: 59.74% 

[59.41, 59.71, 59.34, 59.74]

Epoch: 4
2024-03-09 23:06:32.994348 epoch: 4 step: 0 cls_loss= 0.93425 (150028 samples/sec)
2024-03-09 23:06:36.087603 epoch: 4 step: 100 cls_loss= 0.99803 (9698 samples/sec)
saving....
2024-03-09 23:06:39.903990------------------------------------------------------ Precision@1: 59.56% 

[59.41, 59.71, 59.34, 59.74, 59.56]

Epoch: 5
2024-03-09 23:06:40.099644 epoch: 5 step: 0 cls_loss= 0.83656 (154279 samples/sec)
2024-03-09 23:06:43.200652 epoch: 5 step: 100 cls_loss= 1.01520 (9674 samples/sec)
saving....
2024-03-09 23:06:47.223127------------------------------------------------------ Precision@1: 59.53% 

[59.41, 59.71, 59.34, 59.74, 59.56, 59.53]

Epoch: 6
2024-03-09 23:06:47.415169 epoch: 6 step: 0 cls_loss= 0.85773 (157138 samples/sec)
2024-03-09 23:06:50.487515 epoch: 6 step: 100 cls_loss= 0.87813 (9764 samples/sec)
saving....
2024-03-09 23:06:54.277808------------------------------------------------------ Precision@1: 59.24% 

[59.41, 59.71, 59.34, 59.74, 59.56, 59.53, 59.24]

Epoch: 7
2024-03-09 23:06:54.475314 epoch: 7 step: 0 cls_loss= 1.06189 (152786 samples/sec)
2024-03-09 23:06:57.564766 epoch: 7 step: 100 cls_loss= 0.96252 (9710 samples/sec)
saving....
2024-03-09 23:07:01.389452------------------------------------------------------ Precision@1: 59.47% 

[59.41, 59.71, 59.34, 59.74, 59.56, 59.53, 59.24, 59.47]

Epoch: 8
2024-03-09 23:07:01.616860 epoch: 8 step: 0 cls_loss= 0.90882 (132506 samples/sec)
2024-03-09 23:07:04.654769 epoch: 8 step: 100 cls_loss= 0.88701 (9875 samples/sec)
saving....
2024-03-09 23:07:08.439629------------------------------------------------------ Precision@1: 59.67% 

[59.41, 59.71, 59.34, 59.74, 59.56, 59.53, 59.24, 59.47, 59.67]

Epoch: 9
2024-03-09 23:07:08.631128 epoch: 9 step: 0 cls_loss= 0.79362 (157591 samples/sec)
2024-03-09 23:07:11.615386 epoch: 9 step: 100 cls_loss= 0.83858 (10053 samples/sec)
saving....
2024-03-09 23:07:15.385898------------------------------------------------------ Precision@1: 59.75% 

[59.41, 59.71, 59.34, 59.74, 59.56, 59.53, 59.24, 59.47, 59.67, 59.75]

Epoch: 10
2024-03-09 23:07:15.578006 epoch: 10 step: 0 cls_loss= 0.63041 (157021 samples/sec)
2024-03-09 23:07:18.763980 epoch: 10 step: 100 cls_loss= 0.78725 (9416 samples/sec)
saving....
2024-03-09 23:07:22.606592------------------------------------------------------ Precision@1: 59.51% 

[59.41, 59.71, 59.34, 59.74, 59.56, 59.53, 59.24, 59.47, 59.67, 59.75, 59.51]

Epoch: 11
2024-03-09 23:07:22.819284 epoch: 11 step: 0 cls_loss= 0.75927 (141711 samples/sec)
2024-03-09 23:07:25.863852 epoch: 11 step: 100 cls_loss= 0.85120 (9853 samples/sec)
saving....
2024-03-09 23:07:29.660566------------------------------------------------------ Precision@1: 59.30% 

[59.41, 59.71, 59.34, 59.74, 59.56, 59.53, 59.24, 59.47, 59.67, 59.75, 59.51, 59.3]

Epoch: 12
2024-03-09 23:07:29.863074 epoch: 12 step: 0 cls_loss= 0.68514 (149037 samples/sec)
2024-03-09 23:07:33.006653 epoch: 12 step: 100 cls_loss= 0.72124 (9543 samples/sec)
saving....
2024-03-09 23:07:36.843655------------------------------------------------------ Precision@1: 59.40% 

[59.41, 59.71, 59.34, 59.74, 59.56, 59.53, 59.24, 59.47, 59.67, 59.75, 59.51, 59.3, 59.4]

Epoch: 13
2024-03-09 23:07:37.048082 epoch: 13 step: 0 cls_loss= 0.73215 (147538 samples/sec)
2024-03-09 23:07:40.183302 epoch: 13 step: 100 cls_loss= 0.74145 (9568 samples/sec)
saving....
2024-03-09 23:07:44.042589------------------------------------------------------ Precision@1: 59.62% 

[59.41, 59.71, 59.34, 59.74, 59.56, 59.53, 59.24, 59.47, 59.67, 59.75, 59.51, 59.3, 59.4, 59.62]

Epoch: 14
2024-03-09 23:07:44.241017 epoch: 14 step: 0 cls_loss= 0.79588 (152121 samples/sec)
2024-03-09 23:07:47.352109 epoch: 14 step: 100 cls_loss= 0.68640 (9643 samples/sec)
saving....
2024-03-09 23:07:51.189512------------------------------------------------------ Precision@1: 59.68% 

[59.41, 59.71, 59.34, 59.74, 59.56, 59.53, 59.24, 59.47, 59.67, 59.75, 59.51, 59.3, 59.4, 59.62, 59.68]

Epoch: 15
2024-03-09 23:07:51.398741 epoch: 15 step: 0 cls_loss= 0.64752 (144232 samples/sec)
2024-03-09 23:07:54.397452 epoch: 15 step: 100 cls_loss= 0.76888 (10004 samples/sec)
saving....
2024-03-09 23:07:58.184963------------------------------------------------------ Precision@1: 59.14% 

[59.41, 59.71, 59.34, 59.74, 59.56, 59.53, 59.24, 59.47, 59.67, 59.75, 59.51, 59.3, 59.4, 59.62, 59.68, 59.14]

Epoch: 16
2024-03-09 23:07:58.381824 epoch: 16 step: 0 cls_loss= 0.75886 (153071 samples/sec)
2024-03-09 23:08:01.533336 epoch: 16 step: 100 cls_loss= 0.80278 (9519 samples/sec)
saving....
2024-03-09 23:08:05.402570------------------------------------------------------ Precision@1: 59.16% 

[59.41, 59.71, 59.34, 59.74, 59.56, 59.53, 59.24, 59.47, 59.67, 59.75, 59.51, 59.3, 59.4, 59.62, 59.68, 59.14, 59.16]

Epoch: 17
2024-03-09 23:08:05.604729 epoch: 17 step: 0 cls_loss= 0.84612 (149121 samples/sec)
2024-03-09 23:08:08.648229 epoch: 17 step: 100 cls_loss= 0.76074 (9857 samples/sec)
saving....
2024-03-09 23:08:12.393181------------------------------------------------------ Precision@1: 59.08% 

[59.41, 59.71, 59.34, 59.74, 59.56, 59.53, 59.24, 59.47, 59.67, 59.75, 59.51, 59.3, 59.4, 59.62, 59.68, 59.14, 59.16, 59.08]

Epoch: 18
2024-03-09 23:08:12.607783 epoch: 18 step: 0 cls_loss= 0.66442 (140380 samples/sec)
2024-03-09 23:08:15.612243 epoch: 18 step: 100 cls_loss= 0.92706 (9985 samples/sec)
saving....
2024-03-09 23:08:19.380414------------------------------------------------------ Precision@1: 59.45% 

[59.41, 59.71, 59.34, 59.74, 59.56, 59.53, 59.24, 59.47, 59.67, 59.75, 59.51, 59.3, 59.4, 59.62, 59.68, 59.14, 59.16, 59.08, 59.45]

Epoch: 19
2024-03-09 23:08:19.588814 epoch: 19 step: 0 cls_loss= 0.62346 (144590 samples/sec)
2024-03-09 23:08:22.632252 epoch: 19 step: 100 cls_loss= 0.61837 (9857 samples/sec)
saving....
2024-03-09 23:08:26.554140------------------------------------------------------ Precision@1: 59.24% 

[59.41, 59.71, 59.34, 59.74, 59.56, 59.53, 59.24, 59.47, 59.67, 59.75, 59.51, 59.3, 59.4, 59.62, 59.68, 59.14, 59.16, 59.08, 59.45, 59.24]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-09 23:08:29.288822 epoch: 0 step: 0 cls_loss= 1.17145 (49218 samples/sec)
2024-03-09 23:08:32.320969 epoch: 0 step: 100 cls_loss= 1.02996 (9894 samples/sec)
saving....
2024-03-09 23:08:36.343783------------------------------------------------------ Precision@1: 59.54% 

[59.54]

Epoch: 1
2024-03-09 23:08:36.547257 epoch: 1 step: 0 cls_loss= 1.04058 (148243 samples/sec)
2024-03-09 23:08:39.546659 epoch: 1 step: 100 cls_loss= 0.87882 (10002 samples/sec)
saving....
2024-03-09 23:08:43.519175------------------------------------------------------ Precision@1: 59.41% 

[59.54, 59.41]

Epoch: 2
2024-03-09 23:08:43.712102 epoch: 2 step: 0 cls_loss= 0.99427 (156415 samples/sec)
2024-03-09 23:08:46.812008 epoch: 2 step: 100 cls_loss= 1.03635 (9677 samples/sec)
saving....
2024-03-09 23:08:50.717891------------------------------------------------------ Precision@1: 59.55% 

[59.54, 59.41, 59.55]

Epoch: 3
2024-03-09 23:08:50.914931 epoch: 3 step: 0 cls_loss= 0.94802 (153217 samples/sec)
2024-03-09 23:08:54.024688 epoch: 3 step: 100 cls_loss= 1.07634 (9647 samples/sec)
saving....
2024-03-09 23:08:57.841377------------------------------------------------------ Precision@1: 59.28% 

[59.54, 59.41, 59.55, 59.28]

Epoch: 4
2024-03-09 23:08:58.032494 epoch: 4 step: 0 cls_loss= 0.89711 (157883 samples/sec)
2024-03-09 23:09:01.113932 epoch: 4 step: 100 cls_loss= 0.92948 (9735 samples/sec)
saving....
2024-03-09 23:09:04.964869------------------------------------------------------ Precision@1: 59.97% 

[59.54, 59.41, 59.55, 59.28, 59.97]

Epoch: 5
2024-03-09 23:09:05.161855 epoch: 5 step: 0 cls_loss= 0.92025 (153133 samples/sec)
2024-03-09 23:09:08.221186 epoch: 5 step: 100 cls_loss= 0.88213 (9806 samples/sec)
saving....
2024-03-09 23:09:12.068886------------------------------------------------------ Precision@1: 59.62% 

[59.54, 59.41, 59.55, 59.28, 59.97, 59.62]

Epoch: 6
2024-03-09 23:09:12.274957 epoch: 6 step: 0 cls_loss= 0.90627 (146378 samples/sec)
2024-03-09 23:09:15.441574 epoch: 6 step: 100 cls_loss= 0.95791 (9474 samples/sec)
saving....
2024-03-09 23:09:19.341952------------------------------------------------------ Precision@1: 59.45% 

[59.54, 59.41, 59.55, 59.28, 59.97, 59.62, 59.45]

Epoch: 7
2024-03-09 23:09:19.531275 epoch: 7 step: 0 cls_loss= 0.90127 (159379 samples/sec)
2024-03-09 23:09:22.685538 epoch: 7 step: 100 cls_loss= 0.76538 (9511 samples/sec)
saving....
2024-03-09 23:09:26.553231------------------------------------------------------ Precision@1: 59.57% 

[59.54, 59.41, 59.55, 59.28, 59.97, 59.62, 59.45, 59.57]

Epoch: 8
2024-03-09 23:09:26.773098 epoch: 8 step: 0 cls_loss= 0.87942 (137115 samples/sec)
2024-03-09 23:09:29.847481 epoch: 8 step: 100 cls_loss= 0.81951 (9758 samples/sec)
saving....
2024-03-09 23:09:33.603573------------------------------------------------------ Precision@1: 59.63% 

[59.54, 59.41, 59.55, 59.28, 59.97, 59.62, 59.45, 59.57, 59.63]

Epoch: 9
2024-03-09 23:09:33.818356 epoch: 9 step: 0 cls_loss= 0.83664 (140360 samples/sec)
2024-03-09 23:09:37.098396 epoch: 9 step: 100 cls_loss= 0.74434 (9146 samples/sec)
saving....
2024-03-09 23:09:41.027366------------------------------------------------------ Precision@1: 59.77% 

[59.54, 59.41, 59.55, 59.28, 59.97, 59.62, 59.45, 59.57, 59.63, 59.77]

Epoch: 10
2024-03-09 23:09:41.234968 epoch: 10 step: 0 cls_loss= 0.81218 (145217 samples/sec)
2024-03-09 23:09:44.295438 epoch: 10 step: 100 cls_loss= 0.67276 (9802 samples/sec)
saving....
2024-03-09 23:09:48.067616------------------------------------------------------ Precision@1: 59.22% 

[59.54, 59.41, 59.55, 59.28, 59.97, 59.62, 59.45, 59.57, 59.63, 59.77, 59.22]

Epoch: 11
2024-03-09 23:09:48.268722 epoch: 11 step: 0 cls_loss= 0.74492 (150038 samples/sec)
2024-03-09 23:09:51.335062 epoch: 11 step: 100 cls_loss= 1.01680 (9783 samples/sec)
saving....
2024-03-09 23:09:55.093982------------------------------------------------------ Precision@1: 59.59% 

[59.54, 59.41, 59.55, 59.28, 59.97, 59.62, 59.45, 59.57, 59.63, 59.77, 59.22, 59.59]

Epoch: 12
2024-03-09 23:09:55.299102 epoch: 12 step: 0 cls_loss= 0.81822 (147108 samples/sec)
2024-03-09 23:09:58.421697 epoch: 12 step: 100 cls_loss= 0.85328 (9607 samples/sec)
saving....
2024-03-09 23:10:02.223428------------------------------------------------------ Precision@1: 59.23% 

[59.54, 59.41, 59.55, 59.28, 59.97, 59.62, 59.45, 59.57, 59.63, 59.77, 59.22, 59.59, 59.23]

Epoch: 13
2024-03-09 23:10:02.423893 epoch: 13 step: 0 cls_loss= 0.89295 (150589 samples/sec)
2024-03-09 23:10:05.615034 epoch: 13 step: 100 cls_loss= 0.78276 (9401 samples/sec)
saving....
2024-03-09 23:10:09.484962------------------------------------------------------ Precision@1: 59.35% 

[59.54, 59.41, 59.55, 59.28, 59.97, 59.62, 59.45, 59.57, 59.63, 59.77, 59.22, 59.59, 59.23, 59.35]

Epoch: 14
2024-03-09 23:10:09.701461 epoch: 14 step: 0 cls_loss= 0.80109 (139286 samples/sec)
2024-03-09 23:10:12.873640 epoch: 14 step: 100 cls_loss= 0.81991 (9457 samples/sec)
saving....
2024-03-09 23:10:16.660716------------------------------------------------------ Precision@1: 59.57% 

[59.54, 59.41, 59.55, 59.28, 59.97, 59.62, 59.45, 59.57, 59.63, 59.77, 59.22, 59.59, 59.23, 59.35, 59.57]

Epoch: 15
2024-03-09 23:10:16.873789 epoch: 15 step: 0 cls_loss= 0.76381 (141580 samples/sec)
2024-03-09 23:10:20.148810 epoch: 15 step: 100 cls_loss= 0.70210 (9160 samples/sec)
saving....
2024-03-09 23:10:24.143327------------------------------------------------------ Precision@1: 59.25% 

[59.54, 59.41, 59.55, 59.28, 59.97, 59.62, 59.45, 59.57, 59.63, 59.77, 59.22, 59.59, 59.23, 59.35, 59.57, 59.25]

Epoch: 16
2024-03-09 23:10:24.359617 epoch: 16 step: 0 cls_loss= 0.78419 (139586 samples/sec)
2024-03-09 23:10:27.417012 epoch: 16 step: 100 cls_loss= 0.77424 (9812 samples/sec)
saving....
2024-03-09 23:10:31.291536------------------------------------------------------ Precision@1: 59.02% 

[59.54, 59.41, 59.55, 59.28, 59.97, 59.62, 59.45, 59.57, 59.63, 59.77, 59.22, 59.59, 59.23, 59.35, 59.57, 59.25, 59.02]

Epoch: 17
2024-03-09 23:10:31.513031 epoch: 17 step: 0 cls_loss= 0.84042 (136160 samples/sec)
2024-03-09 23:10:34.564433 epoch: 17 step: 100 cls_loss= 0.82744 (9831 samples/sec)
saving....
2024-03-09 23:10:38.507211------------------------------------------------------ Precision@1: 58.77% 

[59.54, 59.41, 59.55, 59.28, 59.97, 59.62, 59.45, 59.57, 59.63, 59.77, 59.22, 59.59, 59.23, 59.35, 59.57, 59.25, 59.02, 58.77]

Epoch: 18
2024-03-09 23:10:38.708602 epoch: 18 step: 0 cls_loss= 0.73350 (149857 samples/sec)
2024-03-09 23:10:41.735439 epoch: 18 step: 100 cls_loss= 0.70038 (9911 samples/sec)
saving....
2024-03-09 23:10:45.480313------------------------------------------------------ Precision@1: 59.28% 

[59.54, 59.41, 59.55, 59.28, 59.97, 59.62, 59.45, 59.57, 59.63, 59.77, 59.22, 59.59, 59.23, 59.35, 59.57, 59.25, 59.02, 58.77, 59.28]

Epoch: 19
2024-03-09 23:10:45.701745 epoch: 19 step: 0 cls_loss= 0.75091 (136112 samples/sec)
2024-03-09 23:10:48.990438 epoch: 19 step: 100 cls_loss= 0.68637 (9122 samples/sec)
saving....
2024-03-09 23:10:52.825176------------------------------------------------------ Precision@1: 58.84% 

[59.54, 59.41, 59.55, 59.28, 59.97, 59.62, 59.45, 59.57, 59.63, 59.77, 59.22, 59.59, 59.23, 59.35, 59.57, 59.25, 59.02, 58.77, 59.28, 58.84]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-09 23:10:55.560395 epoch: 0 step: 0 cls_loss= 1.01965 (49039 samples/sec)
2024-03-09 23:10:58.690640 epoch: 0 step: 100 cls_loss= 0.88544 (9584 samples/sec)
saving....
2024-03-09 23:11:02.705598------------------------------------------------------ Precision@1: 59.57% 

[59.57]

Epoch: 1
2024-03-09 23:11:02.917276 epoch: 1 step: 0 cls_loss= 1.07925 (142491 samples/sec)
2024-03-09 23:11:06.058421 epoch: 1 step: 100 cls_loss= 1.05795 (9550 samples/sec)
saving....
2024-03-09 23:11:09.879345------------------------------------------------------ Precision@1: 59.48% 

[59.57, 59.48]

Epoch: 2
2024-03-09 23:11:10.089645 epoch: 2 step: 0 cls_loss= 0.92203 (143349 samples/sec)
2024-03-09 23:11:13.225634 epoch: 2 step: 100 cls_loss= 1.05773 (9566 samples/sec)
saving....
2024-03-09 23:11:16.977561------------------------------------------------------ Precision@1: 59.60% 

[59.57, 59.48, 59.6]

Epoch: 3
2024-03-09 23:11:17.166015 epoch: 3 step: 0 cls_loss= 1.00140 (160171 samples/sec)
2024-03-09 23:11:20.238617 epoch: 3 step: 100 cls_loss= 0.87428 (9763 samples/sec)
saving....
2024-03-09 23:11:24.028655------------------------------------------------------ Precision@1: 59.67% 

[59.57, 59.48, 59.6, 59.67]

Epoch: 4
2024-03-09 23:11:24.230148 epoch: 4 step: 0 cls_loss= 0.72999 (149407 samples/sec)
2024-03-09 23:11:27.312612 epoch: 4 step: 100 cls_loss= 1.00212 (9732 samples/sec)
saving....
2024-03-09 23:11:31.126780------------------------------------------------------ Precision@1: 59.63% 

[59.57, 59.48, 59.6, 59.67, 59.63]

Epoch: 5
2024-03-09 23:11:31.334857 epoch: 5 step: 0 cls_loss= 0.89977 (144889 samples/sec)
2024-03-09 23:11:34.506156 epoch: 5 step: 100 cls_loss= 0.93661 (9460 samples/sec)
saving....
2024-03-09 23:11:38.395834------------------------------------------------------ Precision@1: 59.64% 

[59.57, 59.48, 59.6, 59.67, 59.63, 59.64]

Epoch: 6
2024-03-09 23:11:38.587110 epoch: 6 step: 0 cls_loss= 0.74367 (157821 samples/sec)
2024-03-09 23:11:41.614475 epoch: 6 step: 100 cls_loss= 0.91615 (9909 samples/sec)
saving....
2024-03-09 23:11:45.417787------------------------------------------------------ Precision@1: 59.22% 

[59.57, 59.48, 59.6, 59.67, 59.63, 59.64, 59.22]

Epoch: 7
2024-03-09 23:11:45.608276 epoch: 7 step: 0 cls_loss= 0.82186 (158456 samples/sec)
2024-03-09 23:11:48.684051 epoch: 7 step: 100 cls_loss= 0.86194 (9753 samples/sec)
saving....
2024-03-09 23:11:52.460073------------------------------------------------------ Precision@1: 59.67% 

[59.57, 59.48, 59.6, 59.67, 59.63, 59.64, 59.22, 59.67]

Epoch: 8
2024-03-09 23:11:52.648396 epoch: 8 step: 0 cls_loss= 0.76087 (160266 samples/sec)
2024-03-09 23:11:55.825333 epoch: 8 step: 100 cls_loss= 0.72971 (9443 samples/sec)
saving....
2024-03-09 23:11:59.655949------------------------------------------------------ Precision@1: 59.59% 

[59.57, 59.48, 59.6, 59.67, 59.63, 59.64, 59.22, 59.67, 59.59]

Epoch: 9
2024-03-09 23:11:59.859422 epoch: 9 step: 0 cls_loss= 0.76022 (148199 samples/sec)
2024-03-09 23:12:02.972055 epoch: 9 step: 100 cls_loss= 0.84563 (9638 samples/sec)
saving....
2024-03-09 23:12:06.811504------------------------------------------------------ Precision@1: 59.35% 

[59.57, 59.48, 59.6, 59.67, 59.63, 59.64, 59.22, 59.67, 59.59, 59.35]

Epoch: 10
2024-03-09 23:12:07.009453 epoch: 10 step: 0 cls_loss= 0.80814 (152493 samples/sec)
2024-03-09 23:12:10.130447 epoch: 10 step: 100 cls_loss= 0.84266 (9612 samples/sec)
saving....
2024-03-09 23:12:13.920419------------------------------------------------------ Precision@1: 59.51% 

[59.57, 59.48, 59.6, 59.67, 59.63, 59.64, 59.22, 59.67, 59.59, 59.35, 59.51]

Epoch: 11
2024-03-09 23:12:14.116526 epoch: 11 step: 0 cls_loss= 0.80683 (153902 samples/sec)
2024-03-09 23:12:17.222358 epoch: 11 step: 100 cls_loss= 0.78661 (9659 samples/sec)
saving....
2024-03-09 23:12:21.029287------------------------------------------------------ Precision@1: 59.16% 

[59.57, 59.48, 59.6, 59.67, 59.63, 59.64, 59.22, 59.67, 59.59, 59.35, 59.51, 59.16]

Epoch: 12
2024-03-09 23:12:21.224069 epoch: 12 step: 0 cls_loss= 0.62828 (154855 samples/sec)
2024-03-09 23:12:24.302585 epoch: 12 step: 100 cls_loss= 0.84461 (9745 samples/sec)
saving....
2024-03-09 23:12:28.107206------------------------------------------------------ Precision@1: 59.18% 

[59.57, 59.48, 59.6, 59.67, 59.63, 59.64, 59.22, 59.67, 59.59, 59.35, 59.51, 59.16, 59.18]

Epoch: 13
2024-03-09 23:12:28.315644 epoch: 13 step: 0 cls_loss= 0.77304 (144535 samples/sec)
2024-03-09 23:12:31.339914 epoch: 13 step: 100 cls_loss= 0.79160 (9920 samples/sec)
saving....
2024-03-09 23:12:35.117458------------------------------------------------------ Precision@1: 59.31% 

[59.57, 59.48, 59.6, 59.67, 59.63, 59.64, 59.22, 59.67, 59.59, 59.35, 59.51, 59.16, 59.18, 59.31]

Epoch: 14
2024-03-09 23:12:35.327507 epoch: 14 step: 0 cls_loss= 0.82269 (143515 samples/sec)
2024-03-09 23:12:38.399714 epoch: 14 step: 100 cls_loss= 0.79388 (9765 samples/sec)
saving....
2024-03-09 23:12:42.144489------------------------------------------------------ Precision@1: 59.35% 

[59.57, 59.48, 59.6, 59.67, 59.63, 59.64, 59.22, 59.67, 59.59, 59.35, 59.51, 59.16, 59.18, 59.31, 59.35]

Epoch: 15
2024-03-09 23:12:42.361339 epoch: 15 step: 0 cls_loss= 0.93127 (138938 samples/sec)
2024-03-09 23:12:45.481866 epoch: 15 step: 100 cls_loss= 0.95118 (9613 samples/sec)
saving....
2024-03-09 23:12:49.366305------------------------------------------------------ Precision@1: 59.25% 

[59.57, 59.48, 59.6, 59.67, 59.63, 59.64, 59.22, 59.67, 59.59, 59.35, 59.51, 59.16, 59.18, 59.31, 59.35, 59.25]

Epoch: 16
2024-03-09 23:12:49.579742 epoch: 16 step: 0 cls_loss= 0.69798 (141086 samples/sec)
2024-03-09 23:12:52.605443 epoch: 16 step: 100 cls_loss= 0.78847 (9915 samples/sec)
saving....
2024-03-09 23:12:56.372242------------------------------------------------------ Precision@1: 59.11% 

[59.57, 59.48, 59.6, 59.67, 59.63, 59.64, 59.22, 59.67, 59.59, 59.35, 59.51, 59.16, 59.18, 59.31, 59.35, 59.25, 59.11]

Epoch: 17
2024-03-09 23:12:56.559221 epoch: 17 step: 0 cls_loss= 0.68775 (161396 samples/sec)
2024-03-09 23:12:59.602812 epoch: 17 step: 100 cls_loss= 0.69734 (9856 samples/sec)
saving....
2024-03-09 23:13:03.409670------------------------------------------------------ Precision@1: 59.07% 

[59.57, 59.48, 59.6, 59.67, 59.63, 59.64, 59.22, 59.67, 59.59, 59.35, 59.51, 59.16, 59.18, 59.31, 59.35, 59.25, 59.11, 59.07]

Epoch: 18
2024-03-09 23:13:03.626377 epoch: 18 step: 0 cls_loss= 0.74829 (139170 samples/sec)
2024-03-09 23:13:06.798028 epoch: 18 step: 100 cls_loss= 0.72927 (9458 samples/sec)
saving....
2024-03-09 23:13:10.618658------------------------------------------------------ Precision@1: 59.28% 

[59.57, 59.48, 59.6, 59.67, 59.63, 59.64, 59.22, 59.67, 59.59, 59.35, 59.51, 59.16, 59.18, 59.31, 59.35, 59.25, 59.11, 59.07, 59.28]

Epoch: 19
2024-03-09 23:13:10.828659 epoch: 19 step: 0 cls_loss= 0.83405 (143662 samples/sec)
2024-03-09 23:13:13.887437 epoch: 19 step: 100 cls_loss= 0.62983 (9808 samples/sec)
saving....
2024-03-09 23:13:17.747442------------------------------------------------------ Precision@1: 59.24% 

[59.57, 59.48, 59.6, 59.67, 59.63, 59.64, 59.22, 59.67, 59.59, 59.35, 59.51, 59.16, 59.18, 59.31, 59.35, 59.25, 59.11, 59.07, 59.28, 59.24]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-09 23:13:20.477957 epoch: 0 step: 0 cls_loss= 0.97433 (49053 samples/sec)
2024-03-09 23:13:23.689857 epoch: 0 step: 100 cls_loss= 0.99494 (9340 samples/sec)
saving....
2024-03-09 23:13:27.714857------------------------------------------------------ Precision@1: 59.62% 

[59.62]

Epoch: 1
2024-03-09 23:13:27.922924 epoch: 1 step: 0 cls_loss= 1.00044 (144774 samples/sec)
2024-03-09 23:13:30.978863 epoch: 1 step: 100 cls_loss= 1.09289 (9817 samples/sec)
saving....
2024-03-09 23:13:34.745913------------------------------------------------------ Precision@1: 59.52% 

[59.62, 59.52]

Epoch: 2
2024-03-09 23:13:34.941822 epoch: 2 step: 0 cls_loss= 0.96257 (153917 samples/sec)
2024-03-09 23:13:37.951898 epoch: 2 step: 100 cls_loss= 0.94737 (9966 samples/sec)
saving....
2024-03-09 23:13:41.717759------------------------------------------------------ Precision@1: 59.65% 

[59.62, 59.52, 59.65]

Epoch: 3
2024-03-09 23:13:41.912342 epoch: 3 step: 0 cls_loss= 0.84790 (155032 samples/sec)
2024-03-09 23:13:44.979813 epoch: 3 step: 100 cls_loss= 1.00069 (9780 samples/sec)
saving....
2024-03-09 23:13:48.746423------------------------------------------------------ Precision@1: 59.69% 

[59.62, 59.52, 59.65, 59.69]

Epoch: 4
2024-03-09 23:13:48.961670 epoch: 4 step: 0 cls_loss= 0.90131 (140108 samples/sec)
2024-03-09 23:13:51.931902 epoch: 4 step: 100 cls_loss= 0.97325 (10100 samples/sec)
saving....
2024-03-09 23:13:55.651579------------------------------------------------------ Precision@1: 59.45% 

[59.62, 59.52, 59.65, 59.69, 59.45]

Epoch: 5
2024-03-09 23:13:55.844075 epoch: 5 step: 0 cls_loss= 0.91890 (156698 samples/sec)
2024-03-09 23:13:58.904905 epoch: 5 step: 100 cls_loss= 0.98620 (9801 samples/sec)
saving....
2024-03-09 23:14:02.729793------------------------------------------------------ Precision@1: 59.61% 

[59.62, 59.52, 59.65, 59.69, 59.45, 59.61]

Epoch: 6
2024-03-09 23:14:02.943493 epoch: 6 step: 0 cls_loss= 0.80284 (141064 samples/sec)
2024-03-09 23:14:06.059635 epoch: 6 step: 100 cls_loss= 0.91837 (9627 samples/sec)
saving....
2024-03-09 23:14:09.907676------------------------------------------------------ Precision@1: 59.50% 

[59.62, 59.52, 59.65, 59.69, 59.45, 59.61, 59.5]

Epoch: 7
2024-03-09 23:14:10.099195 epoch: 7 step: 0 cls_loss= 0.83783 (157438 samples/sec)
2024-03-09 23:14:13.171390 epoch: 7 step: 100 cls_loss= 0.95838 (9765 samples/sec)
saving....
2024-03-09 23:14:16.984889------------------------------------------------------ Precision@1: 59.52% 

[59.62, 59.52, 59.65, 59.69, 59.45, 59.61, 59.5, 59.52]

Epoch: 8
2024-03-09 23:14:17.187096 epoch: 8 step: 0 cls_loss= 0.82659 (149149 samples/sec)
2024-03-09 23:14:20.256142 epoch: 8 step: 100 cls_loss= 0.80117 (9775 samples/sec)
saving....
2024-03-09 23:14:24.050258------------------------------------------------------ Precision@1: 59.21% 

[59.62, 59.52, 59.65, 59.69, 59.45, 59.61, 59.5, 59.52, 59.21]

Epoch: 9
2024-03-09 23:14:24.251888 epoch: 9 step: 0 cls_loss= 0.84882 (149602 samples/sec)
2024-03-09 23:14:27.281885 epoch: 9 step: 100 cls_loss= 1.02662 (9901 samples/sec)
saving....
2024-03-09 23:14:31.107641------------------------------------------------------ Precision@1: 59.12% 

[59.62, 59.52, 59.65, 59.69, 59.45, 59.61, 59.5, 59.52, 59.21, 59.12]

Epoch: 10
2024-03-09 23:14:31.309819 epoch: 10 step: 0 cls_loss= 0.81424 (149219 samples/sec)
2024-03-09 23:14:34.321131 epoch: 10 step: 100 cls_loss= 0.75862 (9962 samples/sec)
saving....
2024-03-09 23:14:38.093742------------------------------------------------------ Precision@1: 59.33% 

[59.62, 59.52, 59.65, 59.69, 59.45, 59.61, 59.5, 59.52, 59.21, 59.12, 59.33]

Epoch: 11
2024-03-09 23:14:38.290243 epoch: 11 step: 0 cls_loss= 0.73309 (153573 samples/sec)
2024-03-09 23:14:41.222226 epoch: 11 step: 100 cls_loss= 0.85941 (10232 samples/sec)
saving....
2024-03-09 23:14:44.895329------------------------------------------------------ Precision@1: 59.61% 

[59.62, 59.52, 59.65, 59.69, 59.45, 59.61, 59.5, 59.52, 59.21, 59.12, 59.33, 59.61]

Epoch: 12
2024-03-09 23:14:45.086360 epoch: 12 step: 0 cls_loss= 0.78796 (157843 samples/sec)
2024-03-09 23:14:48.095946 epoch: 12 step: 100 cls_loss= 0.88277 (9968 samples/sec)
saving....
2024-03-09 23:14:51.861600------------------------------------------------------ Precision@1: 59.40% 

[59.62, 59.52, 59.65, 59.69, 59.45, 59.61, 59.5, 59.52, 59.21, 59.12, 59.33, 59.61, 59.4]

Epoch: 13
2024-03-09 23:14:52.067050 epoch: 13 step: 0 cls_loss= 0.81085 (146819 samples/sec)
2024-03-09 23:14:55.138650 epoch: 13 step: 100 cls_loss= 0.83891 (9767 samples/sec)
saving....
2024-03-09 23:14:58.959106------------------------------------------------------ Precision@1: 59.22% 

[59.62, 59.52, 59.65, 59.69, 59.45, 59.61, 59.5, 59.52, 59.21, 59.12, 59.33, 59.61, 59.4, 59.22]

Epoch: 14
2024-03-09 23:14:59.154127 epoch: 14 step: 0 cls_loss= 0.75161 (154673 samples/sec)
2024-03-09 23:15:02.151584 epoch: 14 step: 100 cls_loss= 0.82308 (10008 samples/sec)
saving....
2024-03-09 23:15:05.956235------------------------------------------------------ Precision@1: 58.94% 

[59.62, 59.52, 59.65, 59.69, 59.45, 59.61, 59.5, 59.52, 59.21, 59.12, 59.33, 59.61, 59.4, 59.22, 58.94]

Epoch: 15
2024-03-09 23:15:06.156210 epoch: 15 step: 0 cls_loss= 0.68196 (150839 samples/sec)
2024-03-09 23:15:09.303972 epoch: 15 step: 100 cls_loss= 0.71194 (9530 samples/sec)
saving....
2024-03-09 23:15:13.099086------------------------------------------------------ Precision@1: 58.73% 

[59.62, 59.52, 59.65, 59.69, 59.45, 59.61, 59.5, 59.52, 59.21, 59.12, 59.33, 59.61, 59.4, 59.22, 58.94, 58.73]

Epoch: 16
2024-03-09 23:15:13.309505 epoch: 16 step: 0 cls_loss= 0.69922 (143143 samples/sec)
2024-03-09 23:15:16.324109 epoch: 16 step: 100 cls_loss= 0.61837 (9951 samples/sec)
saving....
2024-03-09 23:15:20.130092------------------------------------------------------ Precision@1: 58.80% 

[59.62, 59.52, 59.65, 59.69, 59.45, 59.61, 59.5, 59.52, 59.21, 59.12, 59.33, 59.61, 59.4, 59.22, 58.94, 58.73, 58.8]

Epoch: 17
2024-03-09 23:15:20.347418 epoch: 17 step: 0 cls_loss= 0.64831 (138721 samples/sec)
2024-03-09 23:15:23.349936 epoch: 17 step: 100 cls_loss= 0.79020 (9991 samples/sec)
saving....
2024-03-09 23:15:27.063479------------------------------------------------------ Precision@1: 59.27% 

[59.62, 59.52, 59.65, 59.69, 59.45, 59.61, 59.5, 59.52, 59.21, 59.12, 59.33, 59.61, 59.4, 59.22, 58.94, 58.73, 58.8, 59.27]

Epoch: 18
2024-03-09 23:15:27.280458 epoch: 18 step: 0 cls_loss= 0.71907 (138830 samples/sec)
2024-03-09 23:15:30.290920 epoch: 18 step: 100 cls_loss= 0.73048 (9965 samples/sec)
saving....
2024-03-09 23:15:34.061457------------------------------------------------------ Precision@1: 58.59% 

[59.62, 59.52, 59.65, 59.69, 59.45, 59.61, 59.5, 59.52, 59.21, 59.12, 59.33, 59.61, 59.4, 59.22, 58.94, 58.73, 58.8, 59.27, 58.59]

Epoch: 19
2024-03-09 23:15:34.270663 epoch: 19 step: 0 cls_loss= 0.69557 (144011 samples/sec)
2024-03-09 23:15:37.255191 epoch: 19 step: 100 cls_loss= 0.77632 (10052 samples/sec)
saving....
2024-03-09 23:15:40.984285------------------------------------------------------ Precision@1: 58.89% 

[59.62, 59.52, 59.65, 59.69, 59.45, 59.61, 59.5, 59.52, 59.21, 59.12, 59.33, 59.61, 59.4, 59.22, 58.94, 58.73, 58.8, 59.27, 58.59, 58.89]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-09 23:15:43.781237 epoch: 0 step: 0 cls_loss= 1.00705 (45290 samples/sec)
2024-03-09 23:15:46.904550 epoch: 0 step: 100 cls_loss= 1.04409 (9605 samples/sec)
saving....
2024-03-09 23:15:50.981856------------------------------------------------------ Precision@1: 59.67% 

[59.67]

Epoch: 1
2024-03-09 23:15:51.181880 epoch: 1 step: 0 cls_loss= 0.86496 (150804 samples/sec)
2024-03-09 23:15:54.379004 epoch: 1 step: 100 cls_loss= 1.10514 (9383 samples/sec)
saving....
2024-03-09 23:15:58.169713------------------------------------------------------ Precision@1: 59.72% 

[59.67, 59.72]

Epoch: 2
2024-03-09 23:15:58.379688 epoch: 2 step: 0 cls_loss= 0.95986 (143727 samples/sec)
2024-03-09 23:16:01.424234 epoch: 2 step: 100 cls_loss= 0.98960 (9853 samples/sec)
saving....
2024-03-09 23:16:05.203932------------------------------------------------------ Precision@1: 59.77% 

[59.67, 59.72, 59.77]

Epoch: 3
2024-03-09 23:16:05.422104 epoch: 3 step: 0 cls_loss= 1.02409 (138118 samples/sec)
2024-03-09 23:16:08.503123 epoch: 3 step: 100 cls_loss= 0.94489 (9737 samples/sec)
saving....
2024-03-09 23:16:12.396150------------------------------------------------------ Precision@1: 59.32% 

[59.67, 59.72, 59.77, 59.32]

Epoch: 4
2024-03-09 23:16:12.602643 epoch: 4 step: 0 cls_loss= 0.83647 (145976 samples/sec)
2024-03-09 23:16:15.751632 epoch: 4 step: 100 cls_loss= 1.02575 (9527 samples/sec)
saving....
2024-03-09 23:16:19.528144------------------------------------------------------ Precision@1: 59.50% 

[59.67, 59.72, 59.77, 59.32, 59.5]

Epoch: 5
2024-03-09 23:16:19.742015 epoch: 5 step: 0 cls_loss= 0.90187 (140945 samples/sec)
2024-03-09 23:16:22.899781 epoch: 5 step: 100 cls_loss= 0.92481 (9500 samples/sec)
saving....
2024-03-09 23:16:26.871249------------------------------------------------------ Precision@1: 59.30% 

[59.67, 59.72, 59.77, 59.32, 59.5, 59.3]

Epoch: 6
2024-03-09 23:16:27.081671 epoch: 6 step: 0 cls_loss= 0.86859 (143362 samples/sec)
2024-03-09 23:16:30.256745 epoch: 6 step: 100 cls_loss= 0.89370 (9448 samples/sec)
saving....
2024-03-09 23:16:34.064956------------------------------------------------------ Precision@1: 59.52% 

[59.67, 59.72, 59.77, 59.32, 59.5, 59.3, 59.52]

Epoch: 7
2024-03-09 23:16:34.273846 epoch: 7 step: 0 cls_loss= 0.91592 (144268 samples/sec)
2024-03-09 23:16:37.445300 epoch: 7 step: 100 cls_loss= 1.00074 (9459 samples/sec)
saving....
2024-03-09 23:16:41.388783------------------------------------------------------ Precision@1: 59.41% 

[59.67, 59.72, 59.77, 59.32, 59.5, 59.3, 59.52, 59.41]

Epoch: 8
2024-03-09 23:16:41.600528 epoch: 8 step: 0 cls_loss= 0.81924 (142284 samples/sec)
2024-03-09 23:16:44.613649 epoch: 8 step: 100 cls_loss= 0.83285 (9956 samples/sec)
saving....
2024-03-09 23:16:48.361764------------------------------------------------------ Precision@1: 59.31% 

[59.67, 59.72, 59.77, 59.32, 59.5, 59.3, 59.52, 59.41, 59.31]

Epoch: 9
2024-03-09 23:16:48.573392 epoch: 9 step: 0 cls_loss= 0.77531 (142549 samples/sec)
2024-03-09 23:16:51.693988 epoch: 9 step: 100 cls_loss= 0.85005 (9613 samples/sec)
saving....
2024-03-09 23:16:55.529770------------------------------------------------------ Precision@1: 59.55% 

[59.67, 59.72, 59.77, 59.32, 59.5, 59.3, 59.52, 59.41, 59.31, 59.55]

Epoch: 10
2024-03-09 23:16:55.725091 epoch: 10 step: 0 cls_loss= 0.81049 (154531 samples/sec)
2024-03-09 23:16:58.881174 epoch: 10 step: 100 cls_loss= 0.82348 (9505 samples/sec)
saving....
2024-03-09 23:17:02.723952------------------------------------------------------ Precision@1: 59.24% 

[59.67, 59.72, 59.77, 59.32, 59.5, 59.3, 59.52, 59.41, 59.31, 59.55, 59.24]

Epoch: 11
2024-03-09 23:17:02.929065 epoch: 11 step: 0 cls_loss= 0.69302 (147034 samples/sec)
2024-03-09 23:17:06.060561 epoch: 11 step: 100 cls_loss= 0.89346 (9580 samples/sec)
saving....
2024-03-09 23:17:09.902913------------------------------------------------------ Precision@1: 59.22% 

[59.67, 59.72, 59.77, 59.32, 59.5, 59.3, 59.52, 59.41, 59.31, 59.55, 59.24, 59.22]

Epoch: 12
2024-03-09 23:17:10.096118 epoch: 12 step: 0 cls_loss= 0.70071 (156055 samples/sec)
2024-03-09 23:17:13.253688 epoch: 12 step: 100 cls_loss= 0.78593 (9501 samples/sec)
saving....
2024-03-09 23:17:17.111304------------------------------------------------------ Precision@1: 59.26% 

[59.67, 59.72, 59.77, 59.32, 59.5, 59.3, 59.52, 59.41, 59.31, 59.55, 59.24, 59.22, 59.26]

Epoch: 13
2024-03-09 23:17:17.309975 epoch: 13 step: 0 cls_loss= 0.73693 (151938 samples/sec)
2024-03-09 23:17:20.419340 epoch: 13 step: 100 cls_loss= 0.80685 (9648 samples/sec)
saving....
2024-03-09 23:17:24.352920------------------------------------------------------ Precision@1: 59.21% 

[59.67, 59.72, 59.77, 59.32, 59.5, 59.3, 59.52, 59.41, 59.31, 59.55, 59.24, 59.22, 59.26, 59.21]

Epoch: 14
2024-03-09 23:17:24.547358 epoch: 14 step: 0 cls_loss= 0.58156 (154939 samples/sec)
2024-03-09 23:17:27.595728 epoch: 14 step: 100 cls_loss= 0.79471 (9841 samples/sec)
saving....
2024-03-09 23:17:31.353835------------------------------------------------------ Precision@1: 59.20% 

[59.67, 59.72, 59.77, 59.32, 59.5, 59.3, 59.52, 59.41, 59.31, 59.55, 59.24, 59.22, 59.26, 59.21, 59.2]

Epoch: 15
2024-03-09 23:17:31.568736 epoch: 15 step: 0 cls_loss= 0.70565 (140243 samples/sec)
2024-03-09 23:17:34.598230 epoch: 15 step: 100 cls_loss= 0.72981 (9902 samples/sec)
saving....
2024-03-09 23:17:38.454411------------------------------------------------------ Precision@1: 58.62% 

[59.67, 59.72, 59.77, 59.32, 59.5, 59.3, 59.52, 59.41, 59.31, 59.55, 59.24, 59.22, 59.26, 59.21, 59.2, 58.62]

Epoch: 16
2024-03-09 23:17:38.649434 epoch: 16 step: 0 cls_loss= 0.88066 (154724 samples/sec)
2024-03-09 23:17:41.782894 epoch: 16 step: 100 cls_loss= 0.78505 (9574 samples/sec)
saving....
2024-03-09 23:17:45.541100------------------------------------------------------ Precision@1: 59.34% 

[59.67, 59.72, 59.77, 59.32, 59.5, 59.3, 59.52, 59.41, 59.31, 59.55, 59.24, 59.22, 59.26, 59.21, 59.2, 58.62, 59.34]

Epoch: 17
2024-03-09 23:17:45.747289 epoch: 17 step: 0 cls_loss= 0.79772 (146289 samples/sec)
2024-03-09 23:17:48.826675 epoch: 17 step: 100 cls_loss= 0.89423 (9742 samples/sec)
saving....
2024-03-09 23:17:52.570859------------------------------------------------------ Precision@1: 59.10% 

[59.67, 59.72, 59.77, 59.32, 59.5, 59.3, 59.52, 59.41, 59.31, 59.55, 59.24, 59.22, 59.26, 59.21, 59.2, 58.62, 59.34, 59.1]

Epoch: 18
2024-03-09 23:17:52.768818 epoch: 18 step: 0 cls_loss= 0.70741 (152406 samples/sec)
2024-03-09 23:17:55.823817 epoch: 18 step: 100 cls_loss= 0.73651 (9820 samples/sec)
saving....
2024-03-09 23:17:59.609304------------------------------------------------------ Precision@1: 58.98% 

[59.67, 59.72, 59.77, 59.32, 59.5, 59.3, 59.52, 59.41, 59.31, 59.55, 59.24, 59.22, 59.26, 59.21, 59.2, 58.62, 59.34, 59.1, 58.98]

Epoch: 19
2024-03-09 23:17:59.832362 epoch: 19 step: 0 cls_loss= 0.61169 (135191 samples/sec)
2024-03-09 23:18:03.001825 epoch: 19 step: 100 cls_loss= 0.66680 (9465 samples/sec)
saving....
2024-03-09 23:18:06.978194------------------------------------------------------ Precision@1: 59.28% 

[59.67, 59.72, 59.77, 59.32, 59.5, 59.3, 59.52, 59.41, 59.31, 59.55, 59.24, 59.22, 59.26, 59.21, 59.2, 58.62, 59.34, 59.1, 58.98, 59.28]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-09 23:18:09.747537 epoch: 0 step: 0 cls_loss= 1.03628 (46660 samples/sec)
2024-03-09 23:18:12.823889 epoch: 0 step: 100 cls_loss= 0.92779 (9752 samples/sec)
saving....
2024-03-09 23:18:16.786848------------------------------------------------------ Precision@1: 59.64% 

[59.64]

Epoch: 1
2024-03-09 23:18:16.994270 epoch: 1 step: 0 cls_loss= 1.04619 (145421 samples/sec)
2024-03-09 23:18:20.199833 epoch: 1 step: 100 cls_loss= 1.15850 (9358 samples/sec)
saving....
2024-03-09 23:18:24.031221------------------------------------------------------ Precision@1: 59.84% 

[59.64, 59.84]

Epoch: 2
2024-03-09 23:18:24.244768 epoch: 2 step: 0 cls_loss= 0.90906 (141285 samples/sec)
2024-03-09 23:18:27.356773 epoch: 2 step: 100 cls_loss= 0.97953 (9640 samples/sec)
saving....
2024-03-09 23:18:31.164881------------------------------------------------------ Precision@1: 59.87% 

[59.64, 59.84, 59.87]

Epoch: 3
2024-03-09 23:18:31.367324 epoch: 3 step: 0 cls_loss= 0.79688 (148800 samples/sec)
2024-03-09 23:18:34.425835 epoch: 3 step: 100 cls_loss= 1.07066 (9808 samples/sec)
saving....
2024-03-09 23:18:38.177544------------------------------------------------------ Precision@1: 59.49% 

[59.64, 59.84, 59.87, 59.49]

Epoch: 4
2024-03-09 23:18:38.371955 epoch: 4 step: 0 cls_loss= 0.92037 (155154 samples/sec)
2024-03-09 23:18:41.433743 epoch: 4 step: 100 cls_loss= 0.88466 (9798 samples/sec)
saving....
2024-03-09 23:18:45.169371------------------------------------------------------ Precision@1: 59.68% 

[59.64, 59.84, 59.87, 59.49, 59.68]

Epoch: 5
2024-03-09 23:18:45.366544 epoch: 5 step: 0 cls_loss= 0.83422 (152796 samples/sec)
2024-03-09 23:18:48.447420 epoch: 5 step: 100 cls_loss= 0.99690 (9737 samples/sec)
saving....
2024-03-09 23:18:52.226936------------------------------------------------------ Precision@1: 59.20% 

[59.64, 59.84, 59.87, 59.49, 59.68, 59.2]

Epoch: 6
2024-03-09 23:18:52.436362 epoch: 6 step: 0 cls_loss= 0.85957 (143814 samples/sec)
2024-03-09 23:18:55.595767 epoch: 6 step: 100 cls_loss= 0.88813 (9495 samples/sec)
saving....
2024-03-09 23:18:59.397961------------------------------------------------------ Precision@1: 59.69% 

[59.64, 59.84, 59.87, 59.49, 59.68, 59.2, 59.69]

Epoch: 7
2024-03-09 23:18:59.604050 epoch: 7 step: 0 cls_loss= 0.84177 (146273 samples/sec)
2024-03-09 23:19:02.596369 epoch: 7 step: 100 cls_loss= 0.87267 (10025 samples/sec)
saving....
2024-03-09 23:19:06.305312------------------------------------------------------ Precision@1: 59.61% 

[59.64, 59.84, 59.87, 59.49, 59.68, 59.2, 59.69, 59.61]

Epoch: 8
2024-03-09 23:19:06.509973 epoch: 8 step: 0 cls_loss= 0.88124 (147377 samples/sec)
2024-03-09 23:19:09.507227 epoch: 8 step: 100 cls_loss= 0.86361 (10009 samples/sec)
saving....
2024-03-09 23:19:13.243815------------------------------------------------------ Precision@1: 59.66% 

[59.64, 59.84, 59.87, 59.49, 59.68, 59.2, 59.69, 59.61, 59.66]

Epoch: 9
2024-03-09 23:19:13.440586 epoch: 9 step: 0 cls_loss= 0.92624 (153287 samples/sec)
2024-03-09 23:19:16.538842 epoch: 9 step: 100 cls_loss= 0.88036 (9683 samples/sec)
saving....
2024-03-09 23:19:20.429475------------------------------------------------------ Precision@1: 59.43% 

[59.64, 59.84, 59.87, 59.49, 59.68, 59.2, 59.69, 59.61, 59.66, 59.43]

Epoch: 10
2024-03-09 23:19:20.644873 epoch: 10 step: 0 cls_loss= 0.78178 (140187 samples/sec)
2024-03-09 23:19:23.694751 epoch: 10 step: 100 cls_loss= 0.86814 (9836 samples/sec)
saving....
2024-03-09 23:19:27.425693------------------------------------------------------ Precision@1: 59.64% 

[59.64, 59.84, 59.87, 59.49, 59.68, 59.2, 59.69, 59.61, 59.66, 59.43, 59.64]

Epoch: 11
2024-03-09 23:19:27.625051 epoch: 11 step: 0 cls_loss= 0.83682 (151362 samples/sec)
2024-03-09 23:19:30.735029 epoch: 11 step: 100 cls_loss= 0.78405 (9646 samples/sec)
saving....
2024-03-09 23:19:34.575864------------------------------------------------------ Precision@1: 59.37% 

[59.64, 59.84, 59.87, 59.49, 59.68, 59.2, 59.69, 59.61, 59.66, 59.43, 59.64, 59.37]

Epoch: 12
2024-03-09 23:19:34.788456 epoch: 12 step: 0 cls_loss= 0.68265 (141848 samples/sec)
2024-03-09 23:19:37.831086 epoch: 12 step: 100 cls_loss= 0.84584 (9860 samples/sec)
saving....
2024-03-09 23:19:41.578919------------------------------------------------------ Precision@1: 59.74% 

[59.64, 59.84, 59.87, 59.49, 59.68, 59.2, 59.69, 59.61, 59.66, 59.43, 59.64, 59.37, 59.74]

Epoch: 13
2024-03-09 23:19:41.783335 epoch: 13 step: 0 cls_loss= 0.75463 (147503 samples/sec)
2024-03-09 23:19:44.883940 epoch: 13 step: 100 cls_loss= 0.73039 (9675 samples/sec)
saving....
2024-03-09 23:19:48.772209------------------------------------------------------ Precision@1: 59.17% 

[59.64, 59.84, 59.87, 59.49, 59.68, 59.2, 59.69, 59.61, 59.66, 59.43, 59.64, 59.37, 59.74, 59.17]

Epoch: 14
2024-03-09 23:19:48.983311 epoch: 14 step: 0 cls_loss= 0.80852 (142743 samples/sec)
2024-03-09 23:19:52.003118 epoch: 14 step: 100 cls_loss= 0.66890 (9934 samples/sec)
saving....
2024-03-09 23:19:55.741109------------------------------------------------------ Precision@1: 59.26% 

[59.64, 59.84, 59.87, 59.49, 59.68, 59.2, 59.69, 59.61, 59.66, 59.43, 59.64, 59.37, 59.74, 59.17, 59.26]

Epoch: 15
2024-03-09 23:19:55.954394 epoch: 15 step: 0 cls_loss= 0.89111 (141361 samples/sec)
2024-03-09 23:19:58.939995 epoch: 15 step: 100 cls_loss= 0.70085 (10048 samples/sec)
saving....
2024-03-09 23:20:02.659254------------------------------------------------------ Precision@1: 59.40% 

[59.64, 59.84, 59.87, 59.49, 59.68, 59.2, 59.69, 59.61, 59.66, 59.43, 59.64, 59.37, 59.74, 59.17, 59.26, 59.4]

Epoch: 16
2024-03-09 23:20:02.878329 epoch: 16 step: 0 cls_loss= 0.80672 (137639 samples/sec)
2024-03-09 23:20:05.902063 epoch: 16 step: 100 cls_loss= 0.70674 (9921 samples/sec)
saving....
2024-03-09 23:20:09.628805------------------------------------------------------ Precision@1: 59.71% 

[59.64, 59.84, 59.87, 59.49, 59.68, 59.2, 59.69, 59.61, 59.66, 59.43, 59.64, 59.37, 59.74, 59.17, 59.26, 59.4, 59.71]

Epoch: 17
2024-03-09 23:20:09.828971 epoch: 17 step: 0 cls_loss= 0.74990 (150649 samples/sec)
2024-03-09 23:20:12.854922 epoch: 17 step: 100 cls_loss= 0.94701 (9914 samples/sec)
saving....
2024-03-09 23:20:16.658113------------------------------------------------------ Precision@1: 59.21% 

[59.64, 59.84, 59.87, 59.49, 59.68, 59.2, 59.69, 59.61, 59.66, 59.43, 59.64, 59.37, 59.74, 59.17, 59.26, 59.4, 59.71, 59.21]

Epoch: 18
2024-03-09 23:20:16.844641 epoch: 18 step: 0 cls_loss= 0.63128 (161684 samples/sec)
2024-03-09 23:20:19.919633 epoch: 18 step: 100 cls_loss= 0.87081 (9756 samples/sec)
saving....
2024-03-09 23:20:23.728970------------------------------------------------------ Precision@1: 59.27% 

[59.64, 59.84, 59.87, 59.49, 59.68, 59.2, 59.69, 59.61, 59.66, 59.43, 59.64, 59.37, 59.74, 59.17, 59.26, 59.4, 59.71, 59.21, 59.27]

Epoch: 19
2024-03-09 23:20:23.933158 epoch: 19 step: 0 cls_loss= 0.67206 (147442 samples/sec)
2024-03-09 23:20:27.113212 epoch: 19 step: 100 cls_loss= 0.81395 (9434 samples/sec)
saving....
2024-03-09 23:20:30.947920------------------------------------------------------ Precision@1: 59.14% 

[59.64, 59.84, 59.87, 59.49, 59.68, 59.2, 59.69, 59.61, 59.66, 59.43, 59.64, 59.37, 59.74, 59.17, 59.26, 59.4, 59.71, 59.21, 59.27, 59.14]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-09 23:20:33.700242 epoch: 0 step: 0 cls_loss= 1.07702 (47077 samples/sec)
2024-03-09 23:20:36.793753 epoch: 0 step: 100 cls_loss= 1.16647 (9697 samples/sec)
saving....
2024-03-09 23:20:40.845873------------------------------------------------------ Precision@1: 59.31% 

[59.31]

Epoch: 1
2024-03-09 23:20:41.066102 epoch: 1 step: 0 cls_loss= 1.07236 (136867 samples/sec)
2024-03-09 23:20:44.182441 epoch: 1 step: 100 cls_loss= 1.06808 (9626 samples/sec)
saving....
2024-03-09 23:20:48.022001------------------------------------------------------ Precision@1: 59.61% 

[59.31, 59.61]

Epoch: 2
2024-03-09 23:20:48.216840 epoch: 2 step: 0 cls_loss= 0.92111 (154830 samples/sec)
2024-03-09 23:20:51.320383 epoch: 2 step: 100 cls_loss= 0.85302 (9666 samples/sec)
saving....
2024-03-09 23:20:55.159171------------------------------------------------------ Precision@1: 59.67% 

[59.31, 59.61, 59.67]

Epoch: 3
2024-03-09 23:20:55.359224 epoch: 3 step: 0 cls_loss= 0.81212 (150819 samples/sec)
2024-03-09 23:20:58.603052 epoch: 3 step: 100 cls_loss= 1.02995 (9248 samples/sec)
saving....
2024-03-09 23:21:02.524085------------------------------------------------------ Precision@1: 59.45% 

[59.31, 59.61, 59.67, 59.45]

Epoch: 4
2024-03-09 23:21:02.740354 epoch: 4 step: 0 cls_loss= 0.92227 (139464 samples/sec)
2024-03-09 23:21:05.798337 epoch: 4 step: 100 cls_loss= 0.85414 (9810 samples/sec)
saving....
2024-03-09 23:21:09.561315------------------------------------------------------ Precision@1: 59.70% 

[59.31, 59.61, 59.67, 59.45, 59.7]

Epoch: 5
2024-03-09 23:21:09.781618 epoch: 5 step: 0 cls_loss= 1.04137 (136839 samples/sec)
2024-03-09 23:21:13.024057 epoch: 5 step: 100 cls_loss= 0.95557 (9252 samples/sec)
saving....
2024-03-09 23:21:16.908442------------------------------------------------------ Precision@1: 59.50% 

[59.31, 59.61, 59.67, 59.45, 59.7, 59.5]

Epoch: 6
2024-03-09 23:21:17.112899 epoch: 6 step: 0 cls_loss= 0.81766 (147510 samples/sec)
2024-03-09 23:21:20.336928 epoch: 6 step: 100 cls_loss= 0.85336 (9305 samples/sec)
saving....
2024-03-09 23:21:24.385111------------------------------------------------------ Precision@1: 59.84% 

[59.31, 59.61, 59.67, 59.45, 59.7, 59.5, 59.84]

Epoch: 7
2024-03-09 23:21:24.588260 epoch: 7 step: 0 cls_loss= 0.78592 (148427 samples/sec)
2024-03-09 23:21:27.691059 epoch: 7 step: 100 cls_loss= 0.83753 (9668 samples/sec)
saving....
2024-03-09 23:21:31.487758------------------------------------------------------ Precision@1: 59.44% 

[59.31, 59.61, 59.67, 59.45, 59.7, 59.5, 59.84, 59.44]

Epoch: 8
2024-03-09 23:21:31.690883 epoch: 8 step: 0 cls_loss= 0.92964 (148501 samples/sec)
2024-03-09 23:21:34.825500 epoch: 8 step: 100 cls_loss= 0.88106 (9570 samples/sec)
saving....
2024-03-09 23:21:38.795114------------------------------------------------------ Precision@1: 59.48% 

[59.31, 59.61, 59.67, 59.45, 59.7, 59.5, 59.84, 59.44, 59.48]

Epoch: 9
2024-03-09 23:21:38.988963 epoch: 9 step: 0 cls_loss= 0.77307 (155719 samples/sec)
2024-03-09 23:21:42.010872 epoch: 9 step: 100 cls_loss= 0.96440 (9927 samples/sec)
saving....
2024-03-09 23:21:45.786413------------------------------------------------------ Precision@1: 59.39% 

[59.31, 59.61, 59.67, 59.45, 59.7, 59.5, 59.84, 59.44, 59.48, 59.39]

Epoch: 10
2024-03-09 23:21:46.003709 epoch: 10 step: 0 cls_loss= 0.89347 (138739 samples/sec)
2024-03-09 23:21:49.278135 epoch: 10 step: 100 cls_loss= 0.71968 (9162 samples/sec)
saving....
2024-03-09 23:21:53.150512------------------------------------------------------ Precision@1: 58.90% 

[59.31, 59.61, 59.67, 59.45, 59.7, 59.5, 59.84, 59.44, 59.48, 59.39, 58.9]

Epoch: 11
2024-03-09 23:21:53.349708 epoch: 11 step: 0 cls_loss= 0.92408 (151442 samples/sec)
2024-03-09 23:21:56.374182 epoch: 11 step: 100 cls_loss= 0.65643 (9919 samples/sec)
saving....
2024-03-09 23:22:00.296603------------------------------------------------------ Precision@1: 59.33% 

[59.31, 59.61, 59.67, 59.45, 59.7, 59.5, 59.84, 59.44, 59.48, 59.39, 58.9, 59.33]

Epoch: 12
2024-03-09 23:22:00.501029 epoch: 12 step: 0 cls_loss= 0.79969 (147487 samples/sec)
2024-03-09 23:22:03.538854 epoch: 12 step: 100 cls_loss= 0.87423 (9875 samples/sec)
saving....
2024-03-09 23:22:07.333249------------------------------------------------------ Precision@1: 59.73% 

[59.31, 59.61, 59.67, 59.45, 59.7, 59.5, 59.84, 59.44, 59.48, 59.39, 58.9, 59.33, 59.73]

Epoch: 13
2024-03-09 23:22:07.522864 epoch: 13 step: 0 cls_loss= 0.83418 (159047 samples/sec)
2024-03-09 23:22:10.637158 epoch: 13 step: 100 cls_loss= 0.81235 (9633 samples/sec)
saving....
2024-03-09 23:22:14.697408------------------------------------------------------ Precision@1: 59.36% 

[59.31, 59.61, 59.67, 59.45, 59.7, 59.5, 59.84, 59.44, 59.48, 59.39, 58.9, 59.33, 59.73, 59.36]

Epoch: 14
2024-03-09 23:22:14.932263 epoch: 14 step: 0 cls_loss= 0.82438 (128318 samples/sec)
2024-03-09 23:22:18.041476 epoch: 14 step: 100 cls_loss= 0.77603 (9649 samples/sec)
saving....
2024-03-09 23:22:21.876356------------------------------------------------------ Precision@1: 58.91% 

[59.31, 59.61, 59.67, 59.45, 59.7, 59.5, 59.84, 59.44, 59.48, 59.39, 58.9, 59.33, 59.73, 59.36, 58.91]

Epoch: 15
2024-03-09 23:22:22.078326 epoch: 15 step: 0 cls_loss= 0.76924 (149323 samples/sec)
2024-03-09 23:22:25.173724 epoch: 15 step: 100 cls_loss= 0.73654 (9692 samples/sec)
saving....
2024-03-09 23:22:29.138680------------------------------------------------------ Precision@1: 59.14% 

[59.31, 59.61, 59.67, 59.45, 59.7, 59.5, 59.84, 59.44, 59.48, 59.39, 58.9, 59.33, 59.73, 59.36, 58.91, 59.14]

Epoch: 16
2024-03-09 23:22:29.344966 epoch: 16 step: 0 cls_loss= 0.70469 (146222 samples/sec)
2024-03-09 23:22:32.637551 epoch: 16 step: 100 cls_loss= 0.71189 (9111 samples/sec)
saving....
2024-03-09 23:22:36.557689------------------------------------------------------ Precision@1: 59.28% 

[59.31, 59.61, 59.67, 59.45, 59.7, 59.5, 59.84, 59.44, 59.48, 59.39, 58.9, 59.33, 59.73, 59.36, 58.91, 59.14, 59.28]

Epoch: 17
2024-03-09 23:22:36.756836 epoch: 17 step: 0 cls_loss= 0.67782 (151456 samples/sec)
2024-03-09 23:22:39.747602 epoch: 17 step: 100 cls_loss= 0.70081 (10031 samples/sec)
saving....
2024-03-09 23:22:43.505691------------------------------------------------------ Precision@1: 58.92% 

[59.31, 59.61, 59.67, 59.45, 59.7, 59.5, 59.84, 59.44, 59.48, 59.39, 58.9, 59.33, 59.73, 59.36, 58.91, 59.14, 59.28, 58.92]

Epoch: 18
2024-03-09 23:22:43.713338 epoch: 18 step: 0 cls_loss= 0.67631 (145174 samples/sec)
2024-03-09 23:22:46.873463 epoch: 18 step: 100 cls_loss= 0.82678 (9493 samples/sec)
saving....
2024-03-09 23:22:50.674081------------------------------------------------------ Precision@1: 58.83% 

[59.31, 59.61, 59.67, 59.45, 59.7, 59.5, 59.84, 59.44, 59.48, 59.39, 58.9, 59.33, 59.73, 59.36, 58.91, 59.14, 59.28, 58.92, 58.83]

Epoch: 19
2024-03-09 23:22:50.873823 epoch: 19 step: 0 cls_loss= 0.67516 (151104 samples/sec)
2024-03-09 23:22:54.118140 epoch: 19 step: 100 cls_loss= 0.79588 (9247 samples/sec)
saving....
2024-03-09 23:22:58.075892------------------------------------------------------ Precision@1: 59.39% 

[59.31, 59.61, 59.67, 59.45, 59.7, 59.5, 59.84, 59.44, 59.48, 59.39, 58.9, 59.33, 59.73, 59.36, 58.91, 59.14, 59.28, 58.92, 58.83, 59.39]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-09 23:23:00.836149 epoch: 0 step: 0 cls_loss= 1.01633 (47996 samples/sec)
2024-03-09 23:23:04.015132 epoch: 0 step: 100 cls_loss= 1.15268 (9437 samples/sec)
saving....
2024-03-09 23:23:08.076314------------------------------------------------------ Precision@1: 59.66% 

[59.66]

Epoch: 1
2024-03-09 23:23:08.269013 epoch: 1 step: 0 cls_loss= 0.92393 (156624 samples/sec)
2024-03-09 23:23:11.246650 epoch: 1 step: 100 cls_loss= 0.88026 (10075 samples/sec)
saving....
2024-03-09 23:23:15.075278------------------------------------------------------ Precision@1: 59.63% 

[59.66, 59.63]

Epoch: 2
2024-03-09 23:23:15.271363 epoch: 2 step: 0 cls_loss= 0.87858 (153845 samples/sec)
2024-03-09 23:23:18.378119 epoch: 2 step: 100 cls_loss= 0.90432 (9656 samples/sec)
saving....
2024-03-09 23:23:22.339046------------------------------------------------------ Precision@1: 59.37% 

[59.66, 59.63, 59.37]

Epoch: 3
2024-03-09 23:23:22.551600 epoch: 3 step: 0 cls_loss= 0.98460 (141867 samples/sec)
2024-03-09 23:23:25.708178 epoch: 3 step: 100 cls_loss= 0.87515 (9504 samples/sec)
saving....
2024-03-09 23:23:29.585603------------------------------------------------------ Precision@1: 59.87% 

[59.66, 59.63, 59.37, 59.87]

Epoch: 4
2024-03-09 23:23:29.789091 epoch: 4 step: 0 cls_loss= 0.88171 (148142 samples/sec)
2024-03-09 23:23:32.872219 epoch: 4 step: 100 cls_loss= 0.99309 (9730 samples/sec)
saving....
2024-03-09 23:23:36.749222------------------------------------------------------ Precision@1: 59.59% 

[59.66, 59.63, 59.37, 59.87, 59.59]

Epoch: 5
2024-03-09 23:23:36.966080 epoch: 5 step: 0 cls_loss= 0.88142 (138880 samples/sec)
2024-03-09 23:23:39.975003 epoch: 5 step: 100 cls_loss= 0.98527 (9970 samples/sec)
saving....
2024-03-09 23:23:43.857952------------------------------------------------------ Precision@1: 59.76% 

[59.66, 59.63, 59.37, 59.87, 59.59, 59.76]

Epoch: 6
2024-03-09 23:23:44.043996 epoch: 6 step: 0 cls_loss= 0.98341 (162287 samples/sec)
2024-03-09 23:23:47.094457 epoch: 6 step: 100 cls_loss= 0.92582 (9834 samples/sec)
saving....
2024-03-09 23:23:50.929146------------------------------------------------------ Precision@1: 59.73% 

[59.66, 59.63, 59.37, 59.87, 59.59, 59.76, 59.73]

Epoch: 7
2024-03-09 23:23:51.138277 epoch: 7 step: 0 cls_loss= 1.06862 (144111 samples/sec)
2024-03-09 23:23:54.327906 epoch: 7 step: 100 cls_loss= 0.88614 (9405 samples/sec)
saving....
2024-03-09 23:23:58.164860------------------------------------------------------ Precision@1: 59.75% 

[59.66, 59.63, 59.37, 59.87, 59.59, 59.76, 59.73, 59.75]

Epoch: 8
2024-03-09 23:23:58.342410 epoch: 8 step: 0 cls_loss= 0.97205 (170029 samples/sec)
2024-03-09 23:24:01.383831 epoch: 8 step: 100 cls_loss= 0.89182 (9864 samples/sec)
saving....
2024-03-09 23:24:05.403166------------------------------------------------------ Precision@1: 59.31% 

[59.66, 59.63, 59.37, 59.87, 59.59, 59.76, 59.73, 59.75, 59.31]

Epoch: 9
2024-03-09 23:24:05.606012 epoch: 9 step: 0 cls_loss= 0.99132 (148651 samples/sec)
2024-03-09 23:24:08.753594 epoch: 9 step: 100 cls_loss= 0.78317 (9531 samples/sec)
saving....
2024-03-09 23:24:12.678654------------------------------------------------------ Precision@1: 59.47% 

[59.66, 59.63, 59.37, 59.87, 59.59, 59.76, 59.73, 59.75, 59.31, 59.47]

Epoch: 10
2024-03-09 23:24:12.888982 epoch: 10 step: 0 cls_loss= 0.85102 (143315 samples/sec)
2024-03-09 23:24:15.998663 epoch: 10 step: 100 cls_loss= 0.88964 (9647 samples/sec)
saving....
2024-03-09 23:24:19.844075------------------------------------------------------ Precision@1: 59.53% 

[59.66, 59.63, 59.37, 59.87, 59.59, 59.76, 59.73, 59.75, 59.31, 59.47, 59.53]

Epoch: 11
2024-03-09 23:24:20.054463 epoch: 11 step: 0 cls_loss= 0.82529 (143273 samples/sec)
2024-03-09 23:24:23.170606 epoch: 11 step: 100 cls_loss= 0.87786 (9627 samples/sec)
saving....
2024-03-09 23:24:27.041006------------------------------------------------------ Precision@1: 59.38% 

[59.66, 59.63, 59.37, 59.87, 59.59, 59.76, 59.73, 59.75, 59.31, 59.47, 59.53, 59.38]

Epoch: 12
2024-03-09 23:24:27.239202 epoch: 12 step: 0 cls_loss= 0.82483 (152297 samples/sec)
2024-03-09 23:24:30.300502 epoch: 12 step: 100 cls_loss= 0.81815 (9799 samples/sec)
saving....
2024-03-09 23:24:34.138000------------------------------------------------------ Precision@1: 59.18% 

[59.66, 59.63, 59.37, 59.87, 59.59, 59.76, 59.73, 59.75, 59.31, 59.47, 59.53, 59.38, 59.18]

Epoch: 13
2024-03-09 23:24:34.347782 epoch: 13 step: 0 cls_loss= 0.76992 (143712 samples/sec)
2024-03-09 23:24:37.408494 epoch: 13 step: 100 cls_loss= 0.81283 (9801 samples/sec)
saving....
2024-03-09 23:24:41.266902------------------------------------------------------ Precision@1: 59.24% 

[59.66, 59.63, 59.37, 59.87, 59.59, 59.76, 59.73, 59.75, 59.31, 59.47, 59.53, 59.38, 59.18, 59.24]

Epoch: 14
2024-03-09 23:24:41.455992 epoch: 14 step: 0 cls_loss= 0.69394 (159633 samples/sec)
2024-03-09 23:24:44.576740 epoch: 14 step: 100 cls_loss= 0.76481 (9613 samples/sec)
saving....
2024-03-09 23:24:48.435855------------------------------------------------------ Precision@1: 59.38% 

[59.66, 59.63, 59.37, 59.87, 59.59, 59.76, 59.73, 59.75, 59.31, 59.47, 59.53, 59.38, 59.18, 59.24, 59.38]

Epoch: 15
2024-03-09 23:24:48.643820 epoch: 15 step: 0 cls_loss= 0.72749 (145164 samples/sec)
2024-03-09 23:24:51.679481 epoch: 15 step: 100 cls_loss= 0.69817 (9882 samples/sec)
saving....
2024-03-09 23:24:55.473579------------------------------------------------------ Precision@1: 59.25% 

[59.66, 59.63, 59.37, 59.87, 59.59, 59.76, 59.73, 59.75, 59.31, 59.47, 59.53, 59.38, 59.18, 59.24, 59.38, 59.25]

Epoch: 16
2024-03-09 23:24:55.675892 epoch: 16 step: 0 cls_loss= 0.81861 (148974 samples/sec)
2024-03-09 23:24:58.843569 epoch: 16 step: 100 cls_loss= 0.82276 (9470 samples/sec)
saving....
2024-03-09 23:25:02.716031------------------------------------------------------ Precision@1: 59.28% 

[59.66, 59.63, 59.37, 59.87, 59.59, 59.76, 59.73, 59.75, 59.31, 59.47, 59.53, 59.38, 59.18, 59.24, 59.38, 59.25, 59.28]

Epoch: 17
2024-03-09 23:25:02.928718 epoch: 17 step: 0 cls_loss= 0.81168 (141604 samples/sec)
2024-03-09 23:25:06.034118 epoch: 17 step: 100 cls_loss= 0.68671 (9660 samples/sec)
saving....
2024-03-09 23:25:09.845338------------------------------------------------------ Precision@1: 59.02% 

[59.66, 59.63, 59.37, 59.87, 59.59, 59.76, 59.73, 59.75, 59.31, 59.47, 59.53, 59.38, 59.18, 59.24, 59.38, 59.25, 59.28, 59.02]

Epoch: 18
2024-03-09 23:25:10.030235 epoch: 18 step: 0 cls_loss= 0.68239 (163216 samples/sec)
2024-03-09 23:25:13.106754 epoch: 18 step: 100 cls_loss= 0.68424 (9751 samples/sec)
saving....
2024-03-09 23:25:17.061623------------------------------------------------------ Precision@1: 59.07% 

[59.66, 59.63, 59.37, 59.87, 59.59, 59.76, 59.73, 59.75, 59.31, 59.47, 59.53, 59.38, 59.18, 59.24, 59.38, 59.25, 59.28, 59.02, 59.07]

Epoch: 19
2024-03-09 23:25:17.260970 epoch: 19 step: 0 cls_loss= 0.65905 (151322 samples/sec)
2024-03-09 23:25:20.398146 epoch: 19 step: 100 cls_loss= 0.72819 (9562 samples/sec)
saving....
2024-03-09 23:25:24.317774------------------------------------------------------ Precision@1: 59.03% 

[59.66, 59.63, 59.37, 59.87, 59.59, 59.76, 59.73, 59.75, 59.31, 59.47, 59.53, 59.38, 59.18, 59.24, 59.38, 59.25, 59.28, 59.02, 59.07, 59.03]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-09 23:25:27.060348 epoch: 0 step: 0 cls_loss= 1.04066 (47994 samples/sec)
2024-03-09 23:25:30.049304 epoch: 0 step: 100 cls_loss= 1.10767 (10037 samples/sec)
saving....
2024-03-09 23:25:34.063352------------------------------------------------------ Precision@1: 59.66% 

[59.66]

Epoch: 1
2024-03-09 23:25:34.266530 epoch: 1 step: 0 cls_loss= 1.07818 (148406 samples/sec)
2024-03-09 23:25:37.416172 epoch: 1 step: 100 cls_loss= 1.03299 (9525 samples/sec)
saving....
2024-03-09 23:25:41.207110------------------------------------------------------ Precision@1: 59.51% 

[59.66, 59.51]

Epoch: 2
2024-03-09 23:25:41.406136 epoch: 2 step: 0 cls_loss= 0.90319 (151591 samples/sec)
2024-03-09 23:25:44.605180 epoch: 2 step: 100 cls_loss= 1.12159 (9378 samples/sec)
saving....
2024-03-09 23:25:48.498462------------------------------------------------------ Precision@1: 59.68% 

[59.66, 59.51, 59.68]

Epoch: 3
2024-03-09 23:25:48.703324 epoch: 3 step: 0 cls_loss= 0.93908 (147174 samples/sec)
2024-03-09 23:25:52.009405 epoch: 3 step: 100 cls_loss= 0.94557 (9074 samples/sec)
saving....
2024-03-09 23:25:56.151082------------------------------------------------------ Precision@1: 59.53% 

[59.66, 59.51, 59.68, 59.53]

Epoch: 4
2024-03-09 23:25:56.354732 epoch: 4 step: 0 cls_loss= 0.99318 (147997 samples/sec)
2024-03-09 23:25:59.371671 epoch: 4 step: 100 cls_loss= 1.03612 (9944 samples/sec)
saving....
2024-03-09 23:26:03.218637------------------------------------------------------ Precision@1: 59.78% 

[59.66, 59.51, 59.68, 59.53, 59.78]

Epoch: 5
2024-03-09 23:26:03.430264 epoch: 5 step: 0 cls_loss= 0.90953 (142608 samples/sec)
2024-03-09 23:26:06.500683 epoch: 5 step: 100 cls_loss= 0.90388 (9770 samples/sec)
saving....
2024-03-09 23:26:10.317690------------------------------------------------------ Precision@1: 59.36% 

[59.66, 59.51, 59.68, 59.53, 59.78, 59.36]

Epoch: 6
2024-03-09 23:26:10.518658 epoch: 6 step: 0 cls_loss= 0.84858 (150149 samples/sec)
2024-03-09 23:26:13.548687 epoch: 6 step: 100 cls_loss= 0.77171 (9901 samples/sec)
saving....
2024-03-09 23:26:17.341872------------------------------------------------------ Precision@1: 59.86% 

[59.66, 59.51, 59.68, 59.53, 59.78, 59.36, 59.86]

Epoch: 7
2024-03-09 23:26:17.551355 epoch: 7 step: 0 cls_loss= 0.94053 (143699 samples/sec)
2024-03-09 23:26:20.565927 epoch: 7 step: 100 cls_loss= 0.82471 (9951 samples/sec)
saving....
2024-03-09 23:26:24.331355------------------------------------------------------ Precision@1: 59.80% 

[59.66, 59.51, 59.68, 59.53, 59.78, 59.36, 59.86, 59.8]

Epoch: 8
2024-03-09 23:26:24.523456 epoch: 8 step: 0 cls_loss= 0.75288 (157077 samples/sec)
2024-03-09 23:26:27.575849 epoch: 8 step: 100 cls_loss= 0.92063 (9828 samples/sec)
saving....
2024-03-09 23:26:31.383755------------------------------------------------------ Precision@1: 59.48% 

[59.66, 59.51, 59.68, 59.53, 59.78, 59.36, 59.86, 59.8, 59.48]

Epoch: 9
2024-03-09 23:26:31.597010 epoch: 9 step: 0 cls_loss= 0.80290 (141353 samples/sec)
2024-03-09 23:26:34.699368 epoch: 9 step: 100 cls_loss= 0.78813 (9670 samples/sec)
saving....
2024-03-09 23:26:38.506947------------------------------------------------------ Precision@1: 59.67% 

[59.66, 59.51, 59.68, 59.53, 59.78, 59.36, 59.86, 59.8, 59.48, 59.67]

Epoch: 10
2024-03-09 23:26:38.729288 epoch: 10 step: 0 cls_loss= 0.72148 (135466 samples/sec)
2024-03-09 23:26:41.728342 epoch: 10 step: 100 cls_loss= 0.99694 (10003 samples/sec)
saving....
2024-03-09 23:26:45.502310------------------------------------------------------ Precision@1: 59.43% 

[59.66, 59.51, 59.68, 59.53, 59.78, 59.36, 59.86, 59.8, 59.48, 59.67, 59.43]

Epoch: 11
2024-03-09 23:26:45.707323 epoch: 11 step: 0 cls_loss= 0.78716 (147115 samples/sec)
2024-03-09 23:26:48.835405 epoch: 11 step: 100 cls_loss= 0.76294 (9590 samples/sec)
saving....
2024-03-09 23:26:52.662582------------------------------------------------------ Precision@1: 59.57% 

[59.66, 59.51, 59.68, 59.53, 59.78, 59.36, 59.86, 59.8, 59.48, 59.67, 59.43, 59.57]

Epoch: 12
2024-03-09 23:26:52.880736 epoch: 12 step: 0 cls_loss= 0.84848 (138245 samples/sec)
2024-03-09 23:26:56.045421 epoch: 12 step: 100 cls_loss= 0.90944 (9479 samples/sec)
saving....
2024-03-09 23:26:59.886096------------------------------------------------------ Precision@1: 59.75% 

[59.66, 59.51, 59.68, 59.53, 59.78, 59.36, 59.86, 59.8, 59.48, 59.67, 59.43, 59.57, 59.75]

Epoch: 13
2024-03-09 23:27:00.068181 epoch: 13 step: 0 cls_loss= 0.76246 (165766 samples/sec)
2024-03-09 23:27:03.213554 epoch: 13 step: 100 cls_loss= 0.71567 (9538 samples/sec)
saving....
2024-03-09 23:27:06.943149------------------------------------------------------ Precision@1: 59.46% 

[59.66, 59.51, 59.68, 59.53, 59.78, 59.36, 59.86, 59.8, 59.48, 59.67, 59.43, 59.57, 59.75, 59.46]

Epoch: 14
2024-03-09 23:27:07.149172 epoch: 14 step: 0 cls_loss= 0.70346 (146310 samples/sec)
2024-03-09 23:27:10.325534 epoch: 14 step: 100 cls_loss= 0.73264 (9444 samples/sec)
saving....
2024-03-09 23:27:14.131545------------------------------------------------------ Precision@1: 59.41% 

[59.66, 59.51, 59.68, 59.53, 59.78, 59.36, 59.86, 59.8, 59.48, 59.67, 59.43, 59.57, 59.75, 59.46, 59.41]

Epoch: 15
2024-03-09 23:27:14.337112 epoch: 15 step: 0 cls_loss= 0.79145 (146749 samples/sec)
2024-03-09 23:27:17.659786 epoch: 15 step: 100 cls_loss= 0.79832 (9029 samples/sec)
saving....
2024-03-09 23:27:21.434864------------------------------------------------------ Precision@1: 59.15% 

[59.66, 59.51, 59.68, 59.53, 59.78, 59.36, 59.86, 59.8, 59.48, 59.67, 59.43, 59.57, 59.75, 59.46, 59.41, 59.15]

Epoch: 16
2024-03-09 23:27:21.651956 epoch: 16 step: 0 cls_loss= 0.74782 (138858 samples/sec)
2024-03-09 23:27:24.625802 epoch: 16 step: 100 cls_loss= 0.73087 (10088 samples/sec)
saving....
2024-03-09 23:27:28.395285------------------------------------------------------ Precision@1: 59.39% 

[59.66, 59.51, 59.68, 59.53, 59.78, 59.36, 59.86, 59.8, 59.48, 59.67, 59.43, 59.57, 59.75, 59.46, 59.41, 59.15, 59.39]

Epoch: 17
2024-03-09 23:27:28.597544 epoch: 17 step: 0 cls_loss= 0.67954 (149271 samples/sec)
2024-03-09 23:27:31.707097 epoch: 17 step: 100 cls_loss= 0.65558 (9647 samples/sec)
saving....
2024-03-09 23:27:35.568773------------------------------------------------------ Precision@1: 59.46% 

[59.66, 59.51, 59.68, 59.53, 59.78, 59.36, 59.86, 59.8, 59.48, 59.67, 59.43, 59.57, 59.75, 59.46, 59.41, 59.15, 59.39, 59.46]

Epoch: 18
2024-03-09 23:27:35.789491 epoch: 18 step: 0 cls_loss= 0.77721 (136640 samples/sec)
2024-03-09 23:27:39.033798 epoch: 18 step: 100 cls_loss= 0.78015 (9247 samples/sec)
saving....
2024-03-09 23:27:42.996614------------------------------------------------------ Precision@1: 59.11% 

[59.66, 59.51, 59.68, 59.53, 59.78, 59.36, 59.86, 59.8, 59.48, 59.67, 59.43, 59.57, 59.75, 59.46, 59.41, 59.15, 59.39, 59.46, 59.11]

Epoch: 19
2024-03-09 23:27:43.195565 epoch: 19 step: 0 cls_loss= 0.75517 (151502 samples/sec)
2024-03-09 23:27:46.258147 epoch: 19 step: 100 cls_loss= 0.77072 (9795 samples/sec)
saving....
2024-03-09 23:27:50.038130------------------------------------------------------ Precision@1: 58.84% 

[59.66, 59.51, 59.68, 59.53, 59.78, 59.36, 59.86, 59.8, 59.48, 59.67, 59.43, 59.57, 59.75, 59.46, 59.41, 59.15, 59.39, 59.46, 59.11, 58.84]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 23:27:52.768559 epoch: 0 step: 0 cls_loss= 0.96083 (49890 samples/sec)
2024-03-09 23:27:55.806216 epoch: 0 step: 100 cls_loss= 1.17502 (9876 samples/sec)
saving....
2024-03-09 23:27:59.795449------------------------------------------------------ Precision@1: 59.76% 

[59.76]

Epoch: 1
2024-03-09 23:27:59.983950 epoch: 1 step: 0 cls_loss= 0.97477 (160162 samples/sec)
2024-03-09 23:28:02.964145 epoch: 1 step: 100 cls_loss= 1.00715 (10069 samples/sec)
saving....
2024-03-09 23:28:06.708702------------------------------------------------------ Precision@1: 59.65% 

[59.76, 59.65]

Epoch: 2
2024-03-09 23:28:06.922293 epoch: 2 step: 0 cls_loss= 1.15248 (141208 samples/sec)
2024-03-09 23:28:10.012891 epoch: 2 step: 100 cls_loss= 0.98316 (9707 samples/sec)
saving....
2024-03-09 23:28:13.807788------------------------------------------------------ Precision@1: 59.65% 

[59.76, 59.65, 59.65]

Epoch: 3
2024-03-09 23:28:14.009069 epoch: 3 step: 0 cls_loss= 1.16826 (149864 samples/sec)
2024-03-09 23:28:16.939889 epoch: 3 step: 100 cls_loss= 1.08051 (10240 samples/sec)
saving....
2024-03-09 23:28:20.625772------------------------------------------------------ Precision@1: 59.79% 

[59.76, 59.65, 59.65, 59.79]

Epoch: 4
2024-03-09 23:28:20.830406 epoch: 4 step: 0 cls_loss= 1.06284 (147327 samples/sec)
2024-03-09 23:28:23.892388 epoch: 4 step: 100 cls_loss= 1.04711 (9801 samples/sec)
saving....
2024-03-09 23:28:27.682479------------------------------------------------------ Precision@1: 59.68% 

[59.76, 59.65, 59.65, 59.79, 59.68]

Epoch: 5
2024-03-09 23:28:27.881243 epoch: 5 step: 0 cls_loss= 1.05901 (151717 samples/sec)
2024-03-09 23:28:30.783850 epoch: 5 step: 100 cls_loss= 0.96628 (10339 samples/sec)
saving....
2024-03-09 23:28:34.599605------------------------------------------------------ Precision@1: 59.80% 

[59.76, 59.65, 59.65, 59.79, 59.68, 59.8]

Epoch: 6
2024-03-09 23:28:34.801167 epoch: 6 step: 0 cls_loss= 1.05819 (149717 samples/sec)
2024-03-09 23:28:37.808351 epoch: 6 step: 100 cls_loss= 0.99907 (9978 samples/sec)
saving....
2024-03-09 23:28:41.575634------------------------------------------------------ Precision@1: 59.88% 

[59.76, 59.65, 59.65, 59.79, 59.68, 59.8, 59.88]

Epoch: 7
2024-03-09 23:28:41.770733 epoch: 7 step: 0 cls_loss= 0.92280 (154688 samples/sec)
2024-03-09 23:28:44.803707 epoch: 7 step: 100 cls_loss= 1.03026 (9891 samples/sec)
saving....
2024-03-09 23:28:48.538887------------------------------------------------------ Precision@1: 59.80% 

[59.76, 59.65, 59.65, 59.79, 59.68, 59.8, 59.88, 59.8]

Epoch: 8
2024-03-09 23:28:48.755307 epoch: 8 step: 0 cls_loss= 1.09299 (139383 samples/sec)
2024-03-09 23:28:51.832281 epoch: 8 step: 100 cls_loss= 1.02950 (9751 samples/sec)
saving....
2024-03-09 23:28:55.703714------------------------------------------------------ Precision@1: 59.25% 

[59.76, 59.65, 59.65, 59.79, 59.68, 59.8, 59.88, 59.8, 59.25]

Epoch: 9
2024-03-09 23:28:55.910926 epoch: 9 step: 0 cls_loss= 1.03178 (145560 samples/sec)
2024-03-09 23:28:58.905409 epoch: 9 step: 100 cls_loss= 0.96925 (10018 samples/sec)
saving....
2024-03-09 23:29:02.652574------------------------------------------------------ Precision@1: 59.93% 

[59.76, 59.65, 59.65, 59.79, 59.68, 59.8, 59.88, 59.8, 59.25, 59.93]

Epoch: 10
2024-03-09 23:29:02.849708 epoch: 10 step: 0 cls_loss= 1.10249 (152947 samples/sec)
2024-03-09 23:29:05.938876 epoch: 10 step: 100 cls_loss= 1.09356 (9711 samples/sec)
saving....
2024-03-09 23:29:09.804580------------------------------------------------------ Precision@1: 59.73% 

[59.76, 59.65, 59.65, 59.79, 59.68, 59.8, 59.88, 59.8, 59.25, 59.93, 59.73]

Epoch: 11
2024-03-09 23:29:10.007046 epoch: 11 step: 0 cls_loss= 1.13288 (148741 samples/sec)
2024-03-09 23:29:12.956967 epoch: 11 step: 100 cls_loss= 1.06889 (10173 samples/sec)
saving....
2024-03-09 23:29:16.834007------------------------------------------------------ Precision@1: 59.77% 

[59.76, 59.65, 59.65, 59.79, 59.68, 59.8, 59.88, 59.8, 59.25, 59.93, 59.73, 59.77]

Epoch: 12
2024-03-09 23:29:17.030547 epoch: 12 step: 0 cls_loss= 1.07145 (153566 samples/sec)
2024-03-09 23:29:19.939300 epoch: 12 step: 100 cls_loss= 1.02977 (10313 samples/sec)
saving....
2024-03-09 23:29:23.640078------------------------------------------------------ Precision@1: 59.72% 

[59.76, 59.65, 59.65, 59.79, 59.68, 59.8, 59.88, 59.8, 59.25, 59.93, 59.73, 59.77, 59.72]

Epoch: 13
2024-03-09 23:29:23.839852 epoch: 13 step: 0 cls_loss= 1.08544 (150881 samples/sec)
2024-03-09 23:29:26.751612 epoch: 13 step: 100 cls_loss= 1.14388 (10307 samples/sec)
saving....
2024-03-09 23:29:30.499497------------------------------------------------------ Precision@1: 59.68% 

[59.76, 59.65, 59.65, 59.79, 59.68, 59.8, 59.88, 59.8, 59.25, 59.93, 59.73, 59.77, 59.72, 59.68]

Epoch: 14
2024-03-09 23:29:30.729939 epoch: 14 step: 0 cls_loss= 1.02901 (130862 samples/sec)
2024-03-09 23:29:33.718920 epoch: 14 step: 100 cls_loss= 1.02392 (10037 samples/sec)
saving....
2024-03-09 23:29:37.467290------------------------------------------------------ Precision@1: 59.73% 

[59.76, 59.65, 59.65, 59.79, 59.68, 59.8, 59.88, 59.8, 59.25, 59.93, 59.73, 59.77, 59.72, 59.68, 59.73]

Epoch: 15
2024-03-09 23:29:37.659875 epoch: 15 step: 0 cls_loss= 0.98510 (156664 samples/sec)
2024-03-09 23:29:40.791821 epoch: 15 step: 100 cls_loss= 1.01237 (9578 samples/sec)
saving....
2024-03-09 23:29:44.577009------------------------------------------------------ Precision@1: 59.70% 

[59.76, 59.65, 59.65, 59.79, 59.68, 59.8, 59.88, 59.8, 59.25, 59.93, 59.73, 59.77, 59.72, 59.68, 59.73, 59.7]

Epoch: 16
2024-03-09 23:29:44.776165 epoch: 16 step: 0 cls_loss= 1.01032 (151322 samples/sec)
2024-03-09 23:29:47.748702 epoch: 16 step: 100 cls_loss= 0.94485 (10094 samples/sec)
saving....
2024-03-09 23:29:51.447451------------------------------------------------------ Precision@1: 59.44% 

[59.76, 59.65, 59.65, 59.79, 59.68, 59.8, 59.88, 59.8, 59.25, 59.93, 59.73, 59.77, 59.72, 59.68, 59.73, 59.7, 59.44]

Epoch: 17
2024-03-09 23:29:51.668379 epoch: 17 step: 0 cls_loss= 1.16807 (136413 samples/sec)
2024-03-09 23:29:54.694368 epoch: 17 step: 100 cls_loss= 1.19618 (9914 samples/sec)
saving....
2024-03-09 23:29:58.461823------------------------------------------------------ Precision@1: 59.53% 

[59.76, 59.65, 59.65, 59.79, 59.68, 59.8, 59.88, 59.8, 59.25, 59.93, 59.73, 59.77, 59.72, 59.68, 59.73, 59.7, 59.44, 59.53]

Epoch: 18
2024-03-09 23:29:58.664108 epoch: 18 step: 0 cls_loss= 1.11849 (148936 samples/sec)
2024-03-09 23:30:01.581095 epoch: 18 step: 100 cls_loss= 1.03521 (10284 samples/sec)
saving....
2024-03-09 23:30:05.287876------------------------------------------------------ Precision@1: 59.74% 

[59.76, 59.65, 59.65, 59.79, 59.68, 59.8, 59.88, 59.8, 59.25, 59.93, 59.73, 59.77, 59.72, 59.68, 59.73, 59.7, 59.44, 59.53, 59.74]

Epoch: 19
2024-03-09 23:30:05.480041 epoch: 19 step: 0 cls_loss= 1.05825 (157037 samples/sec)
2024-03-09 23:30:08.396803 epoch: 19 step: 100 cls_loss= 1.07140 (10288 samples/sec)
saving....
2024-03-09 23:30:12.207461------------------------------------------------------ Precision@1: 59.62% 

[59.76, 59.65, 59.65, 59.79, 59.68, 59.8, 59.88, 59.8, 59.25, 59.93, 59.73, 59.77, 59.72, 59.68, 59.73, 59.7, 59.44, 59.53, 59.74, 59.62]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 23:30:14.960596 epoch: 0 step: 0 cls_loss= 1.03127 (48375 samples/sec)
2024-03-09 23:30:17.928423 epoch: 0 step: 100 cls_loss= 1.07367 (10108 samples/sec)
saving....
2024-03-09 23:30:21.974567------------------------------------------------------ Precision@1: 59.69% 

[59.69]

Epoch: 1
2024-03-09 23:30:22.163594 epoch: 1 step: 0 cls_loss= 1.01590 (159598 samples/sec)
2024-03-09 23:30:25.099436 epoch: 1 step: 100 cls_loss= 0.99939 (10220 samples/sec)
saving....
2024-03-09 23:30:28.854503------------------------------------------------------ Precision@1: 59.64% 

[59.69, 59.64]

Epoch: 2
2024-03-09 23:30:29.067859 epoch: 2 step: 0 cls_loss= 1.11457 (141349 samples/sec)
2024-03-09 23:30:31.998710 epoch: 2 step: 100 cls_loss= 1.12333 (10236 samples/sec)
saving....
2024-03-09 23:30:35.709555------------------------------------------------------ Precision@1: 59.74% 

[59.69, 59.64, 59.74]

Epoch: 3
2024-03-09 23:30:35.914473 epoch: 3 step: 0 cls_loss= 1.06991 (147061 samples/sec)
2024-03-09 23:30:38.816694 epoch: 3 step: 100 cls_loss= 1.13116 (10341 samples/sec)
saving....
2024-03-09 23:30:42.539462------------------------------------------------------ Precision@1: 59.72% 

[59.69, 59.64, 59.74, 59.72]

Epoch: 4
2024-03-09 23:30:42.742687 epoch: 4 step: 0 cls_loss= 0.95939 (148413 samples/sec)
2024-03-09 23:30:45.636887 epoch: 4 step: 100 cls_loss= 0.99253 (10370 samples/sec)
saving....
2024-03-09 23:30:49.324532------------------------------------------------------ Precision@1: 59.85% 

[59.69, 59.64, 59.74, 59.72, 59.85]

Epoch: 5
2024-03-09 23:30:49.512700 epoch: 5 step: 0 cls_loss= 1.18905 (160396 samples/sec)
2024-03-09 23:30:52.664358 epoch: 5 step: 100 cls_loss= 1.17044 (9521 samples/sec)
saving....
2024-03-09 23:30:56.363530------------------------------------------------------ Precision@1: 59.71% 

[59.69, 59.64, 59.74, 59.72, 59.85, 59.71]

Epoch: 6
2024-03-09 23:30:56.556862 epoch: 6 step: 0 cls_loss= 0.99104 (156126 samples/sec)
2024-03-09 23:30:59.452510 epoch: 6 step: 100 cls_loss= 1.06572 (10364 samples/sec)
saving....
2024-03-09 23:31:03.108575------------------------------------------------------ Precision@1: 59.74% 

[59.69, 59.64, 59.74, 59.72, 59.85, 59.71, 59.74]

Epoch: 7
2024-03-09 23:31:03.321488 epoch: 7 step: 0 cls_loss= 1.09540 (141471 samples/sec)
2024-03-09 23:31:06.241713 epoch: 7 step: 100 cls_loss= 1.01143 (10273 samples/sec)
saving....
2024-03-09 23:31:09.979284------------------------------------------------------ Precision@1: 59.84% 

[59.69, 59.64, 59.74, 59.72, 59.85, 59.71, 59.74, 59.84]

Epoch: 8
2024-03-09 23:31:10.175924 epoch: 8 step: 0 cls_loss= 1.01995 (153253 samples/sec)
2024-03-09 23:31:13.068100 epoch: 8 step: 100 cls_loss= 1.08172 (10376 samples/sec)
saving....
2024-03-09 23:31:16.751590------------------------------------------------------ Precision@1: 59.47% 

[59.69, 59.64, 59.74, 59.72, 59.85, 59.71, 59.74, 59.84, 59.47]

Epoch: 9
2024-03-09 23:31:16.967399 epoch: 9 step: 0 cls_loss= 1.13935 (139644 samples/sec)
2024-03-09 23:31:19.889027 epoch: 9 step: 100 cls_loss= 1.02805 (10268 samples/sec)
saving....
2024-03-09 23:31:23.595075------------------------------------------------------ Precision@1: 59.49% 

[59.69, 59.64, 59.74, 59.72, 59.85, 59.71, 59.74, 59.84, 59.47, 59.49]

Epoch: 10
2024-03-09 23:31:23.795881 epoch: 10 step: 0 cls_loss= 1.04796 (150177 samples/sec)
2024-03-09 23:31:26.975819 epoch: 10 step: 100 cls_loss= 1.12016 (9436 samples/sec)
saving....
2024-03-09 23:31:30.782211------------------------------------------------------ Precision@1: 59.46% 

[59.69, 59.64, 59.74, 59.72, 59.85, 59.71, 59.74, 59.84, 59.47, 59.49, 59.46]

Epoch: 11
2024-03-09 23:31:30.962072 epoch: 11 step: 0 cls_loss= 1.09279 (167700 samples/sec)
2024-03-09 23:31:33.942482 epoch: 11 step: 100 cls_loss= 1.06136 (10068 samples/sec)
saving....
2024-03-09 23:31:37.747358------------------------------------------------------ Precision@1: 59.92% 

[59.69, 59.64, 59.74, 59.72, 59.85, 59.71, 59.74, 59.84, 59.47, 59.49, 59.46, 59.92]

Epoch: 12
2024-03-09 23:31:37.948183 epoch: 12 step: 0 cls_loss= 0.87536 (150103 samples/sec)
2024-03-09 23:31:41.060872 epoch: 12 step: 100 cls_loss= 0.96993 (9639 samples/sec)
saving....
2024-03-09 23:31:44.876373------------------------------------------------------ Precision@1: 59.52% 

[59.69, 59.64, 59.74, 59.72, 59.85, 59.71, 59.74, 59.84, 59.47, 59.49, 59.46, 59.92, 59.52]

Epoch: 13
2024-03-09 23:31:45.076191 epoch: 13 step: 0 cls_loss= 0.96564 (150928 samples/sec)
2024-03-09 23:31:47.986369 epoch: 13 step: 100 cls_loss= 1.02114 (10309 samples/sec)
saving....
2024-03-09 23:31:51.725969------------------------------------------------------ Precision@1: 59.67% 

[59.69, 59.64, 59.74, 59.72, 59.85, 59.71, 59.74, 59.84, 59.47, 59.49, 59.46, 59.92, 59.52, 59.67]

Epoch: 14
2024-03-09 23:31:51.919912 epoch: 14 step: 0 cls_loss= 0.97302 (155386 samples/sec)
2024-03-09 23:31:54.836438 epoch: 14 step: 100 cls_loss= 1.04568 (10288 samples/sec)
saving....
2024-03-09 23:31:58.554417------------------------------------------------------ Precision@1: 59.83% 

[59.69, 59.64, 59.74, 59.72, 59.85, 59.71, 59.74, 59.84, 59.47, 59.49, 59.46, 59.92, 59.52, 59.67, 59.83]

Epoch: 15
2024-03-09 23:31:58.751720 epoch: 15 step: 0 cls_loss= 0.94579 (152926 samples/sec)
2024-03-09 23:32:01.802766 epoch: 15 step: 100 cls_loss= 1.09257 (9835 samples/sec)
saving....
2024-03-09 23:32:05.589385------------------------------------------------------ Precision@1: 59.61% 

[59.69, 59.64, 59.74, 59.72, 59.85, 59.71, 59.74, 59.84, 59.47, 59.49, 59.46, 59.92, 59.52, 59.67, 59.83, 59.61]

Epoch: 16
2024-03-09 23:32:05.788852 epoch: 16 step: 0 cls_loss= 0.96548 (151283 samples/sec)
2024-03-09 23:32:08.731231 epoch: 16 step: 100 cls_loss= 1.08625 (10196 samples/sec)
saving....
2024-03-09 23:32:12.444934------------------------------------------------------ Precision@1: 59.68% 

[59.69, 59.64, 59.74, 59.72, 59.85, 59.71, 59.74, 59.84, 59.47, 59.49, 59.46, 59.92, 59.52, 59.67, 59.83, 59.61, 59.68]

Epoch: 17
2024-03-09 23:32:12.644763 epoch: 17 step: 0 cls_loss= 0.91354 (150971 samples/sec)
2024-03-09 23:32:15.539292 epoch: 17 step: 100 cls_loss= 1.05570 (10368 samples/sec)
saving....
2024-03-09 23:32:19.233542------------------------------------------------------ Precision@1: 59.42% 

[59.69, 59.64, 59.74, 59.72, 59.85, 59.71, 59.74, 59.84, 59.47, 59.49, 59.46, 59.92, 59.52, 59.67, 59.83, 59.61, 59.68, 59.42]

Epoch: 18
2024-03-09 23:32:19.417758 epoch: 18 step: 0 cls_loss= 1.01178 (163811 samples/sec)
2024-03-09 23:32:22.334546 epoch: 18 step: 100 cls_loss= 1.02240 (10289 samples/sec)
saving....
2024-03-09 23:32:26.030390------------------------------------------------------ Precision@1: 59.58% 

[59.69, 59.64, 59.74, 59.72, 59.85, 59.71, 59.74, 59.84, 59.47, 59.49, 59.46, 59.92, 59.52, 59.67, 59.83, 59.61, 59.68, 59.42, 59.58]

Epoch: 19
2024-03-09 23:32:26.239809 epoch: 19 step: 0 cls_loss= 1.04615 (143976 samples/sec)
2024-03-09 23:32:29.363936 epoch: 19 step: 100 cls_loss= 1.09827 (9606 samples/sec)
saving....
2024-03-09 23:32:33.080234------------------------------------------------------ Precision@1: 59.61% 

[59.69, 59.64, 59.74, 59.72, 59.85, 59.71, 59.74, 59.84, 59.47, 59.49, 59.46, 59.92, 59.52, 59.67, 59.83, 59.61, 59.68, 59.42, 59.58, 59.61]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 23:32:35.797433 epoch: 0 step: 0 cls_loss= 1.16069 (50157 samples/sec)
2024-03-09 23:32:38.695159 epoch: 0 step: 100 cls_loss= 1.07730 (10353 samples/sec)
saving....
2024-03-09 23:32:42.620078------------------------------------------------------ Precision@1: 59.51% 

[59.51]

Epoch: 1
2024-03-09 23:32:42.818698 epoch: 1 step: 0 cls_loss= 1.02926 (151874 samples/sec)
2024-03-09 23:32:45.822092 epoch: 1 step: 100 cls_loss= 1.02829 (9991 samples/sec)
saving....
2024-03-09 23:32:49.586646------------------------------------------------------ Precision@1: 59.54% 

[59.51, 59.54]

Epoch: 2
2024-03-09 23:32:49.779168 epoch: 2 step: 0 cls_loss= 1.10675 (156712 samples/sec)
2024-03-09 23:32:52.702038 epoch: 2 step: 100 cls_loss= 1.08852 (10267 samples/sec)
saving....
2024-03-09 23:32:56.404262------------------------------------------------------ Precision@1: 59.77% 

[59.51, 59.54, 59.77]

Epoch: 3
2024-03-09 23:32:56.608106 epoch: 3 step: 0 cls_loss= 1.07936 (147981 samples/sec)
2024-03-09 23:32:59.555369 epoch: 3 step: 100 cls_loss= 1.09279 (10182 samples/sec)
saving....
2024-03-09 23:33:03.224247------------------------------------------------------ Precision@1: 59.50% 

[59.51, 59.54, 59.77, 59.5]

Epoch: 4
2024-03-09 23:33:03.413457 epoch: 4 step: 0 cls_loss= 1.05521 (159410 samples/sec)
2024-03-09 23:33:06.426555 epoch: 4 step: 100 cls_loss= 0.91138 (9960 samples/sec)
saving....
2024-03-09 23:33:10.170961------------------------------------------------------ Precision@1: 59.81% 

[59.51, 59.54, 59.77, 59.5, 59.81]

Epoch: 5
2024-03-09 23:33:10.404270 epoch: 5 step: 0 cls_loss= 1.01462 (129093 samples/sec)
2024-03-09 23:33:13.299487 epoch: 5 step: 100 cls_loss= 0.97848 (10362 samples/sec)
saving....
2024-03-09 23:33:16.953252------------------------------------------------------ Precision@1: 59.59% 

[59.51, 59.54, 59.77, 59.5, 59.81, 59.59]

Epoch: 6
2024-03-09 23:33:17.148632 epoch: 6 step: 0 cls_loss= 1.11388 (154384 samples/sec)
2024-03-09 23:33:20.177257 epoch: 6 step: 100 cls_loss= 1.10851 (9908 samples/sec)
saving....
2024-03-09 23:33:23.924607------------------------------------------------------ Precision@1: 59.63% 

[59.51, 59.54, 59.77, 59.5, 59.81, 59.59, 59.63]

Epoch: 7
2024-03-09 23:33:24.128248 epoch: 7 step: 0 cls_loss= 1.15171 (148073 samples/sec)
2024-03-09 23:33:27.032624 epoch: 7 step: 100 cls_loss= 0.95435 (10332 samples/sec)
saving....
2024-03-09 23:33:30.684793------------------------------------------------------ Precision@1: 59.64% 

[59.51, 59.54, 59.77, 59.5, 59.81, 59.59, 59.63, 59.64]

Epoch: 8
2024-03-09 23:33:30.903655 epoch: 8 step: 0 cls_loss= 0.85215 (137584 samples/sec)
2024-03-09 23:33:33.808397 epoch: 8 step: 100 cls_loss= 1.01970 (10328 samples/sec)
saving....
2024-03-09 23:33:37.526608------------------------------------------------------ Precision@1: 59.66% 

[59.51, 59.54, 59.77, 59.5, 59.81, 59.59, 59.63, 59.64, 59.66]

Epoch: 9
2024-03-09 23:33:37.725414 epoch: 9 step: 0 cls_loss= 0.99395 (151814 samples/sec)
2024-03-09 23:33:40.632441 epoch: 9 step: 100 cls_loss= 1.09681 (10321 samples/sec)
saving....
2024-03-09 23:33:44.292708------------------------------------------------------ Precision@1: 59.81% 

[59.51, 59.54, 59.77, 59.5, 59.81, 59.59, 59.63, 59.64, 59.66, 59.81]

Epoch: 10
2024-03-09 23:33:44.495690 epoch: 10 step: 0 cls_loss= 0.96705 (148583 samples/sec)
2024-03-09 23:33:47.477970 epoch: 10 step: 100 cls_loss= 0.99013 (10060 samples/sec)
saving....
2024-03-09 23:33:51.195142------------------------------------------------------ Precision@1: 59.58% 

[59.51, 59.54, 59.77, 59.5, 59.81, 59.59, 59.63, 59.64, 59.66, 59.81, 59.58]

Epoch: 11
2024-03-09 23:33:51.402864 epoch: 11 step: 0 cls_loss= 1.09532 (145219 samples/sec)
2024-03-09 23:33:54.300730 epoch: 11 step: 100 cls_loss= 0.92650 (10352 samples/sec)
saving....
2024-03-09 23:33:57.972551------------------------------------------------------ Precision@1: 59.74% 

[59.51, 59.54, 59.77, 59.5, 59.81, 59.59, 59.63, 59.64, 59.66, 59.81, 59.58, 59.74]

Epoch: 12
2024-03-09 23:33:58.185497 epoch: 12 step: 0 cls_loss= 1.10616 (141678 samples/sec)
2024-03-09 23:34:01.179972 epoch: 12 step: 100 cls_loss= 1.05946 (10021 samples/sec)
saving....
2024-03-09 23:34:04.958935------------------------------------------------------ Precision@1: 59.69% 

[59.51, 59.54, 59.77, 59.5, 59.81, 59.59, 59.63, 59.64, 59.66, 59.81, 59.58, 59.74, 59.69]

Epoch: 13
2024-03-09 23:34:05.159687 epoch: 13 step: 0 cls_loss= 1.12172 (150357 samples/sec)
2024-03-09 23:34:08.126708 epoch: 13 step: 100 cls_loss= 0.97285 (10114 samples/sec)
saving....
2024-03-09 23:34:12.037076------------------------------------------------------ Precision@1: 59.76% 

[59.51, 59.54, 59.77, 59.5, 59.81, 59.59, 59.63, 59.64, 59.66, 59.81, 59.58, 59.74, 59.69, 59.76]

Epoch: 14
2024-03-09 23:34:12.228801 epoch: 14 step: 0 cls_loss= 1.02648 (157381 samples/sec)
2024-03-09 23:34:15.364972 epoch: 14 step: 100 cls_loss= 0.98131 (9567 samples/sec)
saving....
2024-03-09 23:34:19.255809------------------------------------------------------ Precision@1: 59.70% 

[59.51, 59.54, 59.77, 59.5, 59.81, 59.59, 59.63, 59.64, 59.66, 59.81, 59.58, 59.74, 59.69, 59.76, 59.7]

Epoch: 15
2024-03-09 23:34:19.479193 epoch: 15 step: 0 cls_loss= 0.91383 (134971 samples/sec)
2024-03-09 23:34:22.470905 epoch: 15 step: 100 cls_loss= 1.04154 (10030 samples/sec)
saving....
2024-03-09 23:34:26.198347------------------------------------------------------ Precision@1: 59.49% 

[59.51, 59.54, 59.77, 59.5, 59.81, 59.59, 59.63, 59.64, 59.66, 59.81, 59.58, 59.74, 59.69, 59.76, 59.7, 59.49]

Epoch: 16
2024-03-09 23:34:26.403773 epoch: 16 step: 0 cls_loss= 1.08078 (146855 samples/sec)
2024-03-09 23:34:29.298851 epoch: 16 step: 100 cls_loss= 1.06324 (10363 samples/sec)
saving....
2024-03-09 23:34:32.993496------------------------------------------------------ Precision@1: 59.81% 

[59.51, 59.54, 59.77, 59.5, 59.81, 59.59, 59.63, 59.64, 59.66, 59.81, 59.58, 59.74, 59.69, 59.76, 59.7, 59.49, 59.81]

Epoch: 17
2024-03-09 23:34:33.179108 epoch: 17 step: 0 cls_loss= 0.98248 (162652 samples/sec)
2024-03-09 23:34:36.245388 epoch: 17 step: 100 cls_loss= 1.09818 (9787 samples/sec)
saving....
2024-03-09 23:34:39.972977------------------------------------------------------ Precision@1: 59.74% 

[59.51, 59.54, 59.77, 59.5, 59.81, 59.59, 59.63, 59.64, 59.66, 59.81, 59.58, 59.74, 59.69, 59.76, 59.7, 59.49, 59.81, 59.74]

Epoch: 18
2024-03-09 23:34:40.162228 epoch: 18 step: 0 cls_loss= 0.95556 (159521 samples/sec)
2024-03-09 23:34:43.077182 epoch: 18 step: 100 cls_loss= 1.06810 (10295 samples/sec)
saving....
2024-03-09 23:34:46.740466------------------------------------------------------ Precision@1: 59.66% 

[59.51, 59.54, 59.77, 59.5, 59.81, 59.59, 59.63, 59.64, 59.66, 59.81, 59.58, 59.74, 59.69, 59.76, 59.7, 59.49, 59.81, 59.74, 59.66]

Epoch: 19
2024-03-09 23:34:46.946194 epoch: 19 step: 0 cls_loss= 1.05800 (146495 samples/sec)
2024-03-09 23:34:49.831017 epoch: 19 step: 100 cls_loss= 1.04300 (10402 samples/sec)
saving....
2024-03-09 23:34:53.515690------------------------------------------------------ Precision@1: 59.69% 

[59.51, 59.54, 59.77, 59.5, 59.81, 59.59, 59.63, 59.64, 59.66, 59.81, 59.58, 59.74, 59.69, 59.76, 59.7, 59.49, 59.81, 59.74, 59.66, 59.69]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 23:34:56.285340 epoch: 0 step: 0 cls_loss= 1.08076 (48758 samples/sec)
2024-03-09 23:34:59.192174 epoch: 0 step: 100 cls_loss= 1.17910 (10320 samples/sec)
saving....
2024-03-09 23:35:03.130676------------------------------------------------------ Precision@1: 59.68% 

[59.68]

Epoch: 1
2024-03-09 23:35:03.347861 epoch: 1 step: 0 cls_loss= 1.13605 (138619 samples/sec)
2024-03-09 23:35:06.311032 epoch: 1 step: 100 cls_loss= 1.06953 (10128 samples/sec)
saving....
2024-03-09 23:35:10.062634------------------------------------------------------ Precision@1: 59.77% 

[59.68, 59.77]

Epoch: 2
2024-03-09 23:35:10.264624 epoch: 2 step: 0 cls_loss= 1.01207 (149340 samples/sec)
2024-03-09 23:35:13.176108 epoch: 2 step: 100 cls_loss= 0.95785 (10308 samples/sec)
saving....
2024-03-09 23:35:16.829828------------------------------------------------------ Precision@1: 59.66% 

[59.68, 59.77, 59.66]

Epoch: 3
2024-03-09 23:35:17.017248 epoch: 3 step: 0 cls_loss= 1.03454 (161069 samples/sec)
2024-03-09 23:35:20.000150 epoch: 3 step: 100 cls_loss= 1.11095 (10057 samples/sec)
saving....
2024-03-09 23:35:23.766471------------------------------------------------------ Precision@1: 59.80% 

[59.68, 59.77, 59.66, 59.8]

Epoch: 4
2024-03-09 23:35:23.975878 epoch: 4 step: 0 cls_loss= 1.04142 (144156 samples/sec)
2024-03-09 23:35:27.061219 epoch: 4 step: 100 cls_loss= 1.05762 (9723 samples/sec)
saving....
2024-03-09 23:35:30.823883------------------------------------------------------ Precision@1: 59.47% 

[59.68, 59.77, 59.66, 59.8, 59.47]

Epoch: 5
2024-03-09 23:35:31.027003 epoch: 5 step: 0 cls_loss= 0.93075 (148500 samples/sec)
2024-03-09 23:35:33.926876 epoch: 5 step: 100 cls_loss= 0.91461 (10348 samples/sec)
saving....
2024-03-09 23:35:37.560758------------------------------------------------------ Precision@1: 59.80% 

[59.68, 59.77, 59.66, 59.8, 59.47, 59.8]

Epoch: 6
2024-03-09 23:35:37.761953 epoch: 6 step: 0 cls_loss= 1.05773 (149933 samples/sec)
2024-03-09 23:35:40.708051 epoch: 6 step: 100 cls_loss= 1.07733 (10187 samples/sec)
saving....
2024-03-09 23:35:44.523221------------------------------------------------------ Precision@1: 59.78% 

[59.68, 59.77, 59.66, 59.8, 59.47, 59.8, 59.78]

Epoch: 7
2024-03-09 23:35:44.735352 epoch: 7 step: 0 cls_loss= 0.95794 (142025 samples/sec)
2024-03-09 23:35:47.627699 epoch: 7 step: 100 cls_loss= 1.12081 (10376 samples/sec)
saving....
2024-03-09 23:35:51.522604------------------------------------------------------ Precision@1: 59.55% 

[59.68, 59.77, 59.66, 59.8, 59.47, 59.8, 59.78, 59.55]

Epoch: 8
2024-03-09 23:35:51.719140 epoch: 8 step: 0 cls_loss= 1.03722 (153439 samples/sec)
2024-03-09 23:35:54.625493 epoch: 8 step: 100 cls_loss= 1.04243 (10325 samples/sec)
saving....
2024-03-09 23:35:58.324713------------------------------------------------------ Precision@1: 59.59% 

[59.68, 59.77, 59.66, 59.8, 59.47, 59.8, 59.78, 59.55, 59.59]

Epoch: 9
2024-03-09 23:35:58.520969 epoch: 9 step: 0 cls_loss= 0.98403 (153683 samples/sec)
2024-03-09 23:36:01.509577 epoch: 9 step: 100 cls_loss= 0.92534 (10040 samples/sec)
saving....
2024-03-09 23:36:05.418522------------------------------------------------------ Precision@1: 59.70% 

[59.68, 59.77, 59.66, 59.8, 59.47, 59.8, 59.78, 59.55, 59.59, 59.7]

Epoch: 10
2024-03-09 23:36:05.615584 epoch: 10 step: 0 cls_loss= 0.92843 (153020 samples/sec)
2024-03-09 23:36:08.653909 epoch: 10 step: 100 cls_loss= 1.02658 (9876 samples/sec)
saving....
2024-03-09 23:36:12.511369------------------------------------------------------ Precision@1: 59.65% 

[59.68, 59.77, 59.66, 59.8, 59.47, 59.8, 59.78, 59.55, 59.59, 59.7, 59.65]

Epoch: 11
2024-03-09 23:36:12.707924 epoch: 11 step: 0 cls_loss= 1.11905 (153416 samples/sec)
2024-03-09 23:36:15.583989 epoch: 11 step: 100 cls_loss= 0.97100 (10434 samples/sec)
saving....
2024-03-09 23:36:19.315443------------------------------------------------------ Precision@1: 59.71% 

[59.68, 59.77, 59.66, 59.8, 59.47, 59.8, 59.78, 59.55, 59.59, 59.7, 59.65, 59.71]

Epoch: 12
2024-03-09 23:36:19.533334 epoch: 12 step: 0 cls_loss= 1.02042 (138387 samples/sec)
2024-03-09 23:36:22.667793 epoch: 12 step: 100 cls_loss= 1.06652 (9571 samples/sec)
saving....
2024-03-09 23:36:26.411209------------------------------------------------------ Precision@1: 59.54% 

[59.68, 59.77, 59.66, 59.8, 59.47, 59.8, 59.78, 59.55, 59.59, 59.7, 59.65, 59.71, 59.54]

Epoch: 13
2024-03-09 23:36:26.621933 epoch: 13 step: 0 cls_loss= 1.12853 (143034 samples/sec)
2024-03-09 23:36:29.589024 epoch: 13 step: 100 cls_loss= 1.00309 (10111 samples/sec)
saving....
2024-03-09 23:36:33.284950------------------------------------------------------ Precision@1: 59.65% 

[59.68, 59.77, 59.66, 59.8, 59.47, 59.8, 59.78, 59.55, 59.59, 59.7, 59.65, 59.71, 59.54, 59.65]

Epoch: 14
2024-03-09 23:36:33.486650 epoch: 14 step: 0 cls_loss= 1.13613 (149572 samples/sec)
2024-03-09 23:36:36.447116 epoch: 14 step: 100 cls_loss= 1.16827 (10137 samples/sec)
saving....
2024-03-09 23:36:40.211110------------------------------------------------------ Precision@1: 59.74% 

[59.68, 59.77, 59.66, 59.8, 59.47, 59.8, 59.78, 59.55, 59.59, 59.7, 59.65, 59.71, 59.54, 59.65, 59.74]

Epoch: 15
2024-03-09 23:36:40.422143 epoch: 15 step: 0 cls_loss= 1.04234 (142649 samples/sec)
2024-03-09 23:36:43.553785 epoch: 15 step: 100 cls_loss= 1.03438 (9579 samples/sec)
saving....
2024-03-09 23:36:47.353006------------------------------------------------------ Precision@1: 59.63% 

[59.68, 59.77, 59.66, 59.8, 59.47, 59.8, 59.78, 59.55, 59.59, 59.7, 59.65, 59.71, 59.54, 59.65, 59.74, 59.63]

Epoch: 16
2024-03-09 23:36:47.546922 epoch: 16 step: 0 cls_loss= 1.08282 (155650 samples/sec)
2024-03-09 23:36:50.527156 epoch: 16 step: 100 cls_loss= 1.05894 (10067 samples/sec)
saving....
2024-03-09 23:36:54.222878------------------------------------------------------ Precision@1: 59.54% 

[59.68, 59.77, 59.66, 59.8, 59.47, 59.8, 59.78, 59.55, 59.59, 59.7, 59.65, 59.71, 59.54, 59.65, 59.74, 59.63, 59.54]

Epoch: 17
2024-03-09 23:36:54.422288 epoch: 17 step: 0 cls_loss= 1.00800 (151283 samples/sec)
2024-03-09 23:36:57.410893 epoch: 17 step: 100 cls_loss= 1.18396 (10038 samples/sec)
saving....
2024-03-09 23:37:01.216862------------------------------------------------------ Precision@1: 59.50% 

[59.68, 59.77, 59.66, 59.8, 59.47, 59.8, 59.78, 59.55, 59.59, 59.7, 59.65, 59.71, 59.54, 59.65, 59.74, 59.63, 59.54, 59.5]

Epoch: 18
2024-03-09 23:37:01.417679 epoch: 18 step: 0 cls_loss= 1.04508 (150168 samples/sec)
2024-03-09 23:37:04.325345 epoch: 18 step: 100 cls_loss= 0.98909 (10321 samples/sec)
saving....
2024-03-09 23:37:08.151107------------------------------------------------------ Precision@1: 59.49% 

[59.68, 59.77, 59.66, 59.8, 59.47, 59.8, 59.78, 59.55, 59.59, 59.7, 59.65, 59.71, 59.54, 59.65, 59.74, 59.63, 59.54, 59.5, 59.49]

Epoch: 19
2024-03-09 23:37:08.335100 epoch: 19 step: 0 cls_loss= 1.06181 (164042 samples/sec)
2024-03-09 23:37:11.239321 epoch: 19 step: 100 cls_loss= 1.04634 (10333 samples/sec)
saving....
2024-03-09 23:37:14.936855------------------------------------------------------ Precision@1: 59.63% 

[59.68, 59.77, 59.66, 59.8, 59.47, 59.8, 59.78, 59.55, 59.59, 59.7, 59.65, 59.71, 59.54, 59.65, 59.74, 59.63, 59.54, 59.5, 59.49, 59.63]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 23:37:17.642824 epoch: 0 step: 0 cls_loss= 0.96974 (50206 samples/sec)
2024-03-09 23:37:20.557384 epoch: 0 step: 100 cls_loss= 1.05296 (10293 samples/sec)
saving....
2024-03-09 23:37:24.474879------------------------------------------------------ Precision@1: 59.56% 

[59.56]

Epoch: 1
2024-03-09 23:37:24.685760 epoch: 1 step: 0 cls_loss= 1.05089 (143049 samples/sec)
2024-03-09 23:37:27.677076 epoch: 1 step: 100 cls_loss= 1.08858 (10031 samples/sec)
saving....
2024-03-09 23:37:31.428090------------------------------------------------------ Precision@1: 59.76% 

[59.56, 59.76]

Epoch: 2
2024-03-09 23:37:31.634951 epoch: 2 step: 0 cls_loss= 1.10312 (145653 samples/sec)
2024-03-09 23:37:34.524780 epoch: 2 step: 100 cls_loss= 1.04446 (10385 samples/sec)
saving....
2024-03-09 23:37:38.207831------------------------------------------------------ Precision@1: 59.58% 

[59.56, 59.76, 59.58]

Epoch: 3
2024-03-09 23:37:38.416768 epoch: 3 step: 0 cls_loss= 0.94984 (144223 samples/sec)
2024-03-09 23:37:41.546301 epoch: 3 step: 100 cls_loss= 1.01429 (9588 samples/sec)
saving....
2024-03-09 23:37:45.368904------------------------------------------------------ Precision@1: 59.47% 

[59.56, 59.76, 59.58, 59.47]

Epoch: 4
2024-03-09 23:37:45.581012 epoch: 4 step: 0 cls_loss= 0.99764 (142109 samples/sec)
2024-03-09 23:37:48.743810 epoch: 4 step: 100 cls_loss= 1.10672 (9485 samples/sec)
saving....
2024-03-09 23:37:52.649549------------------------------------------------------ Precision@1: 59.50% 

[59.56, 59.76, 59.58, 59.47, 59.5]

Epoch: 5
2024-03-09 23:37:52.860156 epoch: 5 step: 0 cls_loss= 1.19805 (143272 samples/sec)
2024-03-09 23:37:56.016036 epoch: 5 step: 100 cls_loss= 0.92129 (9506 samples/sec)
saving....
2024-03-09 23:37:59.904402------------------------------------------------------ Precision@1: 59.68% 

[59.56, 59.76, 59.58, 59.47, 59.5, 59.68]

Epoch: 6
2024-03-09 23:38:00.099117 epoch: 6 step: 0 cls_loss= 0.90752 (154979 samples/sec)
2024-03-09 23:38:03.019030 epoch: 6 step: 100 cls_loss= 1.05122 (10274 samples/sec)
saving....
2024-03-09 23:38:06.720554------------------------------------------------------ Precision@1: 59.52% 

[59.56, 59.76, 59.58, 59.47, 59.5, 59.68, 59.52]

Epoch: 7
2024-03-09 23:38:06.928308 epoch: 7 step: 0 cls_loss= 1.07368 (145143 samples/sec)
2024-03-09 23:38:09.854320 epoch: 7 step: 100 cls_loss= 1.24558 (10257 samples/sec)
saving....
2024-03-09 23:38:13.570114------------------------------------------------------ Precision@1: 59.64% 

[59.56, 59.76, 59.58, 59.47, 59.5, 59.68, 59.52, 59.64]

Epoch: 8
2024-03-09 23:38:13.778866 epoch: 8 step: 0 cls_loss= 1.10693 (144426 samples/sec)
2024-03-09 23:38:16.725627 epoch: 8 step: 100 cls_loss= 1.06174 (10180 samples/sec)
saving....
2024-03-09 23:38:20.480862------------------------------------------------------ Precision@1: 59.54% 

[59.56, 59.76, 59.58, 59.47, 59.5, 59.68, 59.52, 59.64, 59.54]

Epoch: 9
2024-03-09 23:38:20.676946 epoch: 9 step: 0 cls_loss= 1.01773 (153872 samples/sec)
2024-03-09 23:38:23.673582 epoch: 9 step: 100 cls_loss= 0.99172 (10013 samples/sec)
saving....
2024-03-09 23:38:27.415370------------------------------------------------------ Precision@1: 59.68% 

[59.56, 59.76, 59.58, 59.47, 59.5, 59.68, 59.52, 59.64, 59.54, 59.68]

Epoch: 10
2024-03-09 23:38:27.637537 epoch: 10 step: 0 cls_loss= 1.11894 (135727 samples/sec)
2024-03-09 23:38:30.536924 epoch: 10 step: 100 cls_loss= 0.88307 (10347 samples/sec)
saving....
2024-03-09 23:38:34.208666------------------------------------------------------ Precision@1: 59.71% 

[59.56, 59.76, 59.58, 59.47, 59.5, 59.68, 59.52, 59.64, 59.54, 59.68, 59.71]

Epoch: 11
2024-03-09 23:38:34.390114 epoch: 11 step: 0 cls_loss= 1.17879 (166340 samples/sec)
2024-03-09 23:38:37.310827 epoch: 11 step: 100 cls_loss= 1.18898 (10273 samples/sec)
saving....
2024-03-09 23:38:41.023587------------------------------------------------------ Precision@1: 59.70% 

[59.56, 59.76, 59.58, 59.47, 59.5, 59.68, 59.52, 59.64, 59.54, 59.68, 59.71, 59.7]

Epoch: 12
2024-03-09 23:38:41.220798 epoch: 12 step: 0 cls_loss= 0.98955 (152809 samples/sec)
2024-03-09 23:38:44.231969 epoch: 12 step: 100 cls_loss= 0.98199 (9966 samples/sec)
saving....
2024-03-09 23:38:47.943063------------------------------------------------------ Precision@1: 59.69% 

[59.56, 59.76, 59.58, 59.47, 59.5, 59.68, 59.52, 59.64, 59.54, 59.68, 59.71, 59.7, 59.69]

Epoch: 13
2024-03-09 23:38:48.143076 epoch: 13 step: 0 cls_loss= 1.00436 (150742 samples/sec)
2024-03-09 23:38:51.140604 epoch: 13 step: 100 cls_loss= 1.04619 (10010 samples/sec)
saving....
2024-03-09 23:38:54.935102------------------------------------------------------ Precision@1: 59.43% 

[59.56, 59.76, 59.58, 59.47, 59.5, 59.68, 59.52, 59.64, 59.54, 59.68, 59.71, 59.7, 59.69, 59.43]

Epoch: 14
2024-03-09 23:38:55.126643 epoch: 14 step: 0 cls_loss= 1.16225 (157549 samples/sec)
2024-03-09 23:38:58.155829 epoch: 14 step: 100 cls_loss= 1.16794 (9903 samples/sec)
saving....
2024-03-09 23:39:01.939355------------------------------------------------------ Precision@1: 59.86% 

[59.56, 59.76, 59.58, 59.47, 59.5, 59.68, 59.52, 59.64, 59.54, 59.68, 59.71, 59.7, 59.69, 59.43, 59.86]

Epoch: 15
2024-03-09 23:39:02.122255 epoch: 15 step: 0 cls_loss= 0.95470 (165064 samples/sec)
2024-03-09 23:39:05.110750 epoch: 15 step: 100 cls_loss= 1.05108 (10042 samples/sec)
saving....
2024-03-09 23:39:08.869230------------------------------------------------------ Precision@1: 59.47% 

[59.56, 59.76, 59.58, 59.47, 59.5, 59.68, 59.52, 59.64, 59.54, 59.68, 59.71, 59.7, 59.69, 59.43, 59.86, 59.47]

Epoch: 16
2024-03-09 23:39:09.069545 epoch: 16 step: 0 cls_loss= 1.09141 (150514 samples/sec)
2024-03-09 23:39:12.068642 epoch: 16 step: 100 cls_loss= 1.03924 (10005 samples/sec)
saving....
2024-03-09 23:39:15.766955------------------------------------------------------ Precision@1: 59.68% 

[59.56, 59.76, 59.58, 59.47, 59.5, 59.68, 59.52, 59.64, 59.54, 59.68, 59.71, 59.7, 59.69, 59.43, 59.86, 59.47, 59.68]

Epoch: 17
2024-03-09 23:39:15.967475 epoch: 17 step: 0 cls_loss= 1.06337 (150419 samples/sec)
2024-03-09 23:39:19.150867 epoch: 17 step: 100 cls_loss= 1.19101 (9424 samples/sec)
saving....
2024-03-09 23:39:22.924543------------------------------------------------------ Precision@1: 59.77% 

[59.56, 59.76, 59.58, 59.47, 59.5, 59.68, 59.52, 59.64, 59.54, 59.68, 59.71, 59.7, 59.69, 59.43, 59.86, 59.47, 59.68, 59.77]

Epoch: 18
2024-03-09 23:39:23.121941 epoch: 18 step: 0 cls_loss= 1.04783 (152790 samples/sec)
2024-03-09 23:39:26.070331 epoch: 18 step: 100 cls_loss= 1.21584 (10175 samples/sec)
saving....
2024-03-09 23:39:29.827560------------------------------------------------------ Precision@1: 59.73% 

[59.56, 59.76, 59.58, 59.47, 59.5, 59.68, 59.52, 59.64, 59.54, 59.68, 59.71, 59.7, 59.69, 59.43, 59.86, 59.47, 59.68, 59.77, 59.73]

Epoch: 19
2024-03-09 23:39:30.040250 epoch: 19 step: 0 cls_loss= 1.05572 (141871 samples/sec)
2024-03-09 23:39:32.992828 epoch: 19 step: 100 cls_loss= 0.97195 (10163 samples/sec)
saving....
2024-03-09 23:39:36.821495------------------------------------------------------ Precision@1: 59.73% 

[59.56, 59.76, 59.58, 59.47, 59.5, 59.68, 59.52, 59.64, 59.54, 59.68, 59.71, 59.7, 59.69, 59.43, 59.86, 59.47, 59.68, 59.77, 59.73, 59.73]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 23:39:39.567208 epoch: 0 step: 0 cls_loss= 1.09483 (48142 samples/sec)
2024-03-09 23:39:42.516565 epoch: 0 step: 100 cls_loss= 0.92633 (10171 samples/sec)
saving....
2024-03-09 23:39:46.511786------------------------------------------------------ Precision@1: 59.54% 

[59.54]

Epoch: 1
2024-03-09 23:39:46.722866 epoch: 1 step: 0 cls_loss= 1.05194 (142872 samples/sec)
2024-03-09 23:39:49.778514 epoch: 1 step: 100 cls_loss= 1.02074 (9821 samples/sec)
saving....
2024-03-09 23:39:53.597356------------------------------------------------------ Precision@1: 59.75% 

[59.54, 59.75]

Epoch: 2
2024-03-09 23:39:53.790731 epoch: 2 step: 0 cls_loss= 1.00232 (156097 samples/sec)
2024-03-09 23:39:56.841550 epoch: 2 step: 100 cls_loss= 0.99390 (9837 samples/sec)
saving....
2024-03-09 23:40:00.644667------------------------------------------------------ Precision@1: 59.61% 

[59.54, 59.75, 59.61]

Epoch: 3
2024-03-09 23:40:00.860684 epoch: 3 step: 0 cls_loss= 1.03749 (139542 samples/sec)
2024-03-09 23:40:03.808235 epoch: 3 step: 100 cls_loss= 1.13038 (10179 samples/sec)
saving....
2024-03-09 23:40:07.502069------------------------------------------------------ Precision@1: 59.57% 

[59.54, 59.75, 59.61, 59.57]

Epoch: 4
2024-03-09 23:40:07.715409 epoch: 4 step: 0 cls_loss= 1.08670 (141342 samples/sec)
2024-03-09 23:40:10.622114 epoch: 4 step: 100 cls_loss= 1.07060 (10325 samples/sec)
saving....
2024-03-09 23:40:14.312598------------------------------------------------------ Precision@1: 59.51% 

[59.54, 59.75, 59.61, 59.57, 59.51]

Epoch: 5
2024-03-09 23:40:14.519713 epoch: 5 step: 0 cls_loss= 1.10186 (145796 samples/sec)
2024-03-09 23:40:17.415716 epoch: 5 step: 100 cls_loss= 1.01106 (10363 samples/sec)
saving....
2024-03-09 23:40:21.119716------------------------------------------------------ Precision@1: 59.62% 

[59.54, 59.75, 59.61, 59.57, 59.51, 59.62]

Epoch: 6
2024-03-09 23:40:21.308671 epoch: 6 step: 0 cls_loss= 1.02986 (159450 samples/sec)
2024-03-09 23:40:24.394795 epoch: 6 step: 100 cls_loss= 1.11973 (9721 samples/sec)
saving....
2024-03-09 23:40:28.272783------------------------------------------------------ Precision@1: 59.79% 

[59.54, 59.75, 59.61, 59.57, 59.51, 59.62, 59.79]

Epoch: 7
2024-03-09 23:40:28.472565 epoch: 7 step: 0 cls_loss= 1.01647 (150991 samples/sec)
2024-03-09 23:40:31.551044 epoch: 7 step: 100 cls_loss= 1.11374 (9747 samples/sec)
saving....
2024-03-09 23:40:35.245295------------------------------------------------------ Precision@1: 59.76% 

[59.54, 59.75, 59.61, 59.57, 59.51, 59.62, 59.79, 59.76]

Epoch: 8
2024-03-09 23:40:35.430977 epoch: 8 step: 0 cls_loss= 0.91840 (162471 samples/sec)
2024-03-09 23:40:38.325713 epoch: 8 step: 100 cls_loss= 1.08842 (10367 samples/sec)
saving....
2024-03-09 23:40:41.996118------------------------------------------------------ Precision@1: 59.72% 

[59.54, 59.75, 59.61, 59.57, 59.51, 59.62, 59.79, 59.76, 59.72]

Epoch: 9
2024-03-09 23:40:42.207513 epoch: 9 step: 0 cls_loss= 1.00539 (142637 samples/sec)
2024-03-09 23:40:45.349367 epoch: 9 step: 100 cls_loss= 1.08439 (9548 samples/sec)
saving....
2024-03-09 23:40:49.192248------------------------------------------------------ Precision@1: 59.74% 

[59.54, 59.75, 59.61, 59.57, 59.51, 59.62, 59.79, 59.76, 59.72, 59.74]

Epoch: 10
2024-03-09 23:40:49.391348 epoch: 10 step: 0 cls_loss= 1.07627 (151509 samples/sec)
2024-03-09 23:40:52.405931 epoch: 10 step: 100 cls_loss= 1.13378 (9951 samples/sec)
saving....
2024-03-09 23:40:56.140554------------------------------------------------------ Precision@1: 59.74% 

[59.54, 59.75, 59.61, 59.57, 59.51, 59.62, 59.79, 59.76, 59.72, 59.74, 59.74]

Epoch: 11
2024-03-09 23:40:56.357720 epoch: 11 step: 0 cls_loss= 1.06601 (138841 samples/sec)
2024-03-09 23:40:59.323896 epoch: 11 step: 100 cls_loss= 1.17795 (10115 samples/sec)
saving....
2024-03-09 23:41:03.055514------------------------------------------------------ Precision@1: 59.67% 

[59.54, 59.75, 59.61, 59.57, 59.51, 59.62, 59.79, 59.76, 59.72, 59.74, 59.74, 59.67]

Epoch: 12
2024-03-09 23:41:03.267197 epoch: 12 step: 0 cls_loss= 1.08703 (142331 samples/sec)
2024-03-09 23:41:06.208775 epoch: 12 step: 100 cls_loss= 1.10414 (10202 samples/sec)
saving....
2024-03-09 23:41:09.936383------------------------------------------------------ Precision@1: 59.72% 

[59.54, 59.75, 59.61, 59.57, 59.51, 59.62, 59.79, 59.76, 59.72, 59.74, 59.74, 59.67, 59.72]

Epoch: 13
2024-03-09 23:41:10.160356 epoch: 13 step: 0 cls_loss= 0.97987 (134549 samples/sec)
2024-03-09 23:41:13.103158 epoch: 13 step: 100 cls_loss= 1.01027 (10198 samples/sec)
saving....
2024-03-09 23:41:16.823141------------------------------------------------------ Precision@1: 59.73% 

[59.54, 59.75, 59.61, 59.57, 59.51, 59.62, 59.79, 59.76, 59.72, 59.74, 59.74, 59.67, 59.72, 59.73]

Epoch: 14
2024-03-09 23:41:17.008527 epoch: 14 step: 0 cls_loss= 0.95832 (162933 samples/sec)
2024-03-09 23:41:20.007992 epoch: 14 step: 100 cls_loss= 0.96694 (10002 samples/sec)
saving....
2024-03-09 23:41:23.782379------------------------------------------------------ Precision@1: 59.74% 

[59.54, 59.75, 59.61, 59.57, 59.51, 59.62, 59.79, 59.76, 59.72, 59.74, 59.74, 59.67, 59.72, 59.73, 59.74]

Epoch: 15
2024-03-09 23:41:23.975193 epoch: 15 step: 0 cls_loss= 1.11433 (156626 samples/sec)
2024-03-09 23:41:27.091690 epoch: 15 step: 100 cls_loss= 1.07564 (9628 samples/sec)
saving....
2024-03-09 23:41:30.893832------------------------------------------------------ Precision@1: 59.57% 

[59.54, 59.75, 59.61, 59.57, 59.51, 59.62, 59.79, 59.76, 59.72, 59.74, 59.74, 59.67, 59.72, 59.73, 59.74, 59.57]

Epoch: 16
2024-03-09 23:41:31.104470 epoch: 16 step: 0 cls_loss= 1.03841 (143247 samples/sec)
2024-03-09 23:41:34.009424 epoch: 16 step: 100 cls_loss= 0.98222 (10327 samples/sec)
saving....
2024-03-09 23:41:37.721230------------------------------------------------------ Precision@1: 59.60% 

[59.54, 59.75, 59.61, 59.57, 59.51, 59.62, 59.79, 59.76, 59.72, 59.74, 59.74, 59.67, 59.72, 59.73, 59.74, 59.57, 59.6]

Epoch: 17
2024-03-09 23:41:37.954257 epoch: 17 step: 0 cls_loss= 1.16827 (129273 samples/sec)
2024-03-09 23:41:40.929555 epoch: 17 step: 100 cls_loss= 0.90034 (10083 samples/sec)
saving....
2024-03-09 23:41:44.646729------------------------------------------------------ Precision@1: 59.72% 

[59.54, 59.75, 59.61, 59.57, 59.51, 59.62, 59.79, 59.76, 59.72, 59.74, 59.74, 59.67, 59.72, 59.73, 59.74, 59.57, 59.6, 59.72]

Epoch: 18
2024-03-09 23:41:44.858956 epoch: 18 step: 0 cls_loss= 1.14521 (142033 samples/sec)
2024-03-09 23:41:47.901929 epoch: 18 step: 100 cls_loss= 1.07736 (9859 samples/sec)
saving....
2024-03-09 23:41:51.595174------------------------------------------------------ Precision@1: 59.69% 

[59.54, 59.75, 59.61, 59.57, 59.51, 59.62, 59.79, 59.76, 59.72, 59.74, 59.74, 59.67, 59.72, 59.73, 59.74, 59.57, 59.6, 59.72, 59.69]

Epoch: 19
2024-03-09 23:41:51.793660 epoch: 19 step: 0 cls_loss= 1.08257 (151970 samples/sec)
2024-03-09 23:41:54.679639 epoch: 19 step: 100 cls_loss= 1.15015 (10399 samples/sec)
saving....
2024-03-09 23:41:58.359727------------------------------------------------------ Precision@1: 59.68% 

[59.54, 59.75, 59.61, 59.57, 59.51, 59.62, 59.79, 59.76, 59.72, 59.74, 59.74, 59.67, 59.72, 59.73, 59.74, 59.57, 59.6, 59.72, 59.69, 59.68]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 23:42:01.074887 epoch: 0 step: 0 cls_loss= 1.02468 (48537 samples/sec)
2024-03-09 23:42:04.012430 epoch: 0 step: 100 cls_loss= 1.18115 (10212 samples/sec)
saving....
2024-03-09 23:42:07.922171------------------------------------------------------ Precision@1: 59.82% 

[59.82]

Epoch: 1
2024-03-09 23:42:08.118918 epoch: 1 step: 0 cls_loss= 0.99549 (153314 samples/sec)
2024-03-09 23:42:11.029592 epoch: 1 step: 100 cls_loss= 1.00982 (10308 samples/sec)
saving....
2024-03-09 23:42:14.724700------------------------------------------------------ Precision@1: 59.50% 

[59.82, 59.5]

Epoch: 2
2024-03-09 23:42:14.917242 epoch: 2 step: 0 cls_loss= 1.05715 (156770 samples/sec)
2024-03-09 23:42:17.834601 epoch: 2 step: 100 cls_loss= 0.93040 (10287 samples/sec)
saving....
2024-03-09 23:42:21.524824------------------------------------------------------ Precision@1: 59.72% 

[59.82, 59.5, 59.72]

Epoch: 3
2024-03-09 23:42:21.724548 epoch: 3 step: 0 cls_loss= 1.11761 (151136 samples/sec)
2024-03-09 23:42:24.655972 epoch: 3 step: 100 cls_loss= 1.14667 (10234 samples/sec)
saving....
2024-03-09 23:42:28.374989------------------------------------------------------ Precision@1: 59.71% 

[59.82, 59.5, 59.72, 59.71]

Epoch: 4
2024-03-09 23:42:28.572188 epoch: 4 step: 0 cls_loss= 0.97613 (153020 samples/sec)
2024-03-09 23:42:31.494097 epoch: 4 step: 100 cls_loss= 1.02043 (10267 samples/sec)
saving....
2024-03-09 23:42:35.239416------------------------------------------------------ Precision@1: 59.42% 

[59.82, 59.5, 59.72, 59.71, 59.42]

Epoch: 5
2024-03-09 23:42:35.445180 epoch: 5 step: 0 cls_loss= 0.98115 (146571 samples/sec)
2024-03-09 23:42:38.393246 epoch: 5 step: 100 cls_loss= 1.05731 (10179 samples/sec)
saving....
2024-03-09 23:42:42.195133------------------------------------------------------ Precision@1: 59.41% 

[59.82, 59.5, 59.72, 59.71, 59.42, 59.41]

Epoch: 6
2024-03-09 23:42:42.383299 epoch: 6 step: 0 cls_loss= 0.99688 (160437 samples/sec)
2024-03-09 23:42:45.342878 epoch: 6 step: 100 cls_loss= 0.93777 (10137 samples/sec)
saving....
2024-03-09 23:42:49.122556------------------------------------------------------ Precision@1: 59.70% 

[59.82, 59.5, 59.72, 59.71, 59.42, 59.41, 59.7]

Epoch: 7
2024-03-09 23:42:49.339858 epoch: 7 step: 0 cls_loss= 1.07533 (138660 samples/sec)
2024-03-09 23:42:52.335124 epoch: 7 step: 100 cls_loss= 1.01202 (10016 samples/sec)
saving....
2024-03-09 23:42:56.154263------------------------------------------------------ Precision@1: 59.69% 

[59.82, 59.5, 59.72, 59.71, 59.42, 59.41, 59.7, 59.69]

Epoch: 8
2024-03-09 23:42:56.373434 epoch: 8 step: 0 cls_loss= 1.01243 (137620 samples/sec)
2024-03-09 23:42:59.467553 epoch: 8 step: 100 cls_loss= 1.03053 (9696 samples/sec)
saving....
2024-03-09 23:43:03.269141------------------------------------------------------ Precision@1: 59.69% 

[59.82, 59.5, 59.72, 59.71, 59.42, 59.41, 59.7, 59.69, 59.69]

Epoch: 9
2024-03-09 23:43:03.467427 epoch: 9 step: 0 cls_loss= 1.04529 (152227 samples/sec)
2024-03-09 23:43:06.393321 epoch: 9 step: 100 cls_loss= 1.09140 (10256 samples/sec)
saving....
2024-03-09 23:43:10.197818------------------------------------------------------ Precision@1: 59.67% 

[59.82, 59.5, 59.72, 59.71, 59.42, 59.41, 59.7, 59.69, 59.69, 59.67]

Epoch: 10
2024-03-09 23:43:10.410200 epoch: 10 step: 0 cls_loss= 1.07056 (142006 samples/sec)
2024-03-09 23:43:13.354546 epoch: 10 step: 100 cls_loss= 1.04863 (10192 samples/sec)
saving....
2024-03-09 23:43:17.227206------------------------------------------------------ Precision@1: 59.72% 

[59.82, 59.5, 59.72, 59.71, 59.42, 59.41, 59.7, 59.69, 59.69, 59.67, 59.72]

Epoch: 11
2024-03-09 23:43:17.430391 epoch: 11 step: 0 cls_loss= 1.08978 (148453 samples/sec)
2024-03-09 23:43:20.333856 epoch: 11 step: 100 cls_loss= 0.89511 (10335 samples/sec)
saving....
2024-03-09 23:43:24.077271------------------------------------------------------ Precision@1: 59.47% 

[59.82, 59.5, 59.72, 59.71, 59.42, 59.41, 59.7, 59.69, 59.69, 59.67, 59.72, 59.47]

Epoch: 12
2024-03-09 23:43:24.293249 epoch: 12 step: 0 cls_loss= 1.05010 (139640 samples/sec)
2024-03-09 23:43:27.344645 epoch: 12 step: 100 cls_loss= 1.03572 (9831 samples/sec)
saving....
2024-03-09 23:43:31.107665------------------------------------------------------ Precision@1: 59.62% 

[59.82, 59.5, 59.72, 59.71, 59.42, 59.41, 59.7, 59.69, 59.69, 59.67, 59.72, 59.47, 59.62]

Epoch: 13
2024-03-09 23:43:31.314138 epoch: 13 step: 0 cls_loss= 1.00095 (146050 samples/sec)
2024-03-09 23:43:34.364089 epoch: 13 step: 100 cls_loss= 1.24805 (9836 samples/sec)
saving....
2024-03-09 23:43:38.193134------------------------------------------------------ Precision@1: 59.74% 

[59.82, 59.5, 59.72, 59.71, 59.42, 59.41, 59.7, 59.69, 59.69, 59.67, 59.72, 59.47, 59.62, 59.74]

Epoch: 14
2024-03-09 23:43:38.401581 epoch: 14 step: 0 cls_loss= 0.96186 (144615 samples/sec)
2024-03-09 23:43:41.319886 epoch: 14 step: 100 cls_loss= 1.05240 (10280 samples/sec)
saving....
2024-03-09 23:43:45.095402------------------------------------------------------ Precision@1: 59.72% 

[59.82, 59.5, 59.72, 59.71, 59.42, 59.41, 59.7, 59.69, 59.69, 59.67, 59.72, 59.47, 59.62, 59.74, 59.72]

Epoch: 15
2024-03-09 23:43:45.289950 epoch: 15 step: 0 cls_loss= 1.03540 (155057 samples/sec)
2024-03-09 23:43:48.204025 epoch: 15 step: 100 cls_loss= 1.02553 (10295 samples/sec)
saving....
2024-03-09 23:43:51.937643------------------------------------------------------ Precision@1: 59.62% 

[59.82, 59.5, 59.72, 59.71, 59.42, 59.41, 59.7, 59.69, 59.69, 59.67, 59.72, 59.47, 59.62, 59.74, 59.72, 59.62]

Epoch: 16
2024-03-09 23:43:52.140146 epoch: 16 step: 0 cls_loss= 1.22770 (148831 samples/sec)
2024-03-09 23:43:55.075956 epoch: 16 step: 100 cls_loss= 1.09329 (10222 samples/sec)
saving....
2024-03-09 23:43:58.814278------------------------------------------------------ Precision@1: 59.63% 

[59.82, 59.5, 59.72, 59.71, 59.42, 59.41, 59.7, 59.69, 59.69, 59.67, 59.72, 59.47, 59.62, 59.74, 59.72, 59.62, 59.63]

Epoch: 17
2024-03-09 23:43:59.035965 epoch: 17 step: 0 cls_loss= 1.04844 (136035 samples/sec)
2024-03-09 23:44:01.973808 epoch: 17 step: 100 cls_loss= 1.04611 (10211 samples/sec)
saving....
2024-03-09 23:44:05.847502------------------------------------------------------ Precision@1: 59.47% 

[59.82, 59.5, 59.72, 59.71, 59.42, 59.41, 59.7, 59.69, 59.69, 59.67, 59.72, 59.47, 59.62, 59.74, 59.72, 59.62, 59.63, 59.47]

Epoch: 18
2024-03-09 23:44:06.046595 epoch: 18 step: 0 cls_loss= 1.01128 (151632 samples/sec)
2024-03-09 23:44:08.946678 epoch: 18 step: 100 cls_loss= 0.99257 (10347 samples/sec)
saving....
2024-03-09 23:44:12.635068------------------------------------------------------ Precision@1: 59.41% 

[59.82, 59.5, 59.72, 59.71, 59.42, 59.41, 59.7, 59.69, 59.69, 59.67, 59.72, 59.47, 59.62, 59.74, 59.72, 59.62, 59.63, 59.47, 59.41]

Epoch: 19
2024-03-09 23:44:12.831297 epoch: 19 step: 0 cls_loss= 1.12857 (153717 samples/sec)
2024-03-09 23:44:15.782149 epoch: 19 step: 100 cls_loss= 1.01489 (10170 samples/sec)
saving....
2024-03-09 23:44:19.501444------------------------------------------------------ Precision@1: 59.59% 

[59.82, 59.5, 59.72, 59.71, 59.42, 59.41, 59.7, 59.69, 59.69, 59.67, 59.72, 59.47, 59.62, 59.74, 59.72, 59.62, 59.63, 59.47, 59.41, 59.59]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 23:44:22.207271 epoch: 0 step: 0 cls_loss= 0.96215 (50747 samples/sec)
2024-03-09 23:44:25.184316 epoch: 0 step: 100 cls_loss= 0.95443 (10077 samples/sec)
saving....
2024-03-09 23:44:29.152199------------------------------------------------------ Precision@1: 59.71% 

[59.71]

Epoch: 1
2024-03-09 23:44:29.359105 epoch: 1 step: 0 cls_loss= 1.10853 (145958 samples/sec)
2024-03-09 23:44:32.273550 epoch: 1 step: 100 cls_loss= 1.04847 (10297 samples/sec)
saving....
2024-03-09 23:44:35.967679------------------------------------------------------ Precision@1: 59.65% 

[59.71, 59.65]

Epoch: 2
2024-03-09 23:44:36.171499 epoch: 2 step: 0 cls_loss= 1.03028 (147811 samples/sec)
2024-03-09 23:44:39.092781 epoch: 2 step: 100 cls_loss= 0.98506 (10273 samples/sec)
saving....
2024-03-09 23:44:42.820845------------------------------------------------------ Precision@1: 59.48% 

[59.71, 59.65, 59.48]

Epoch: 3
2024-03-09 23:44:43.029412 epoch: 3 step: 0 cls_loss= 1.00587 (144390 samples/sec)
2024-03-09 23:44:46.027444 epoch: 3 step: 100 cls_loss= 0.95539 (10006 samples/sec)
saving....
2024-03-09 23:44:49.803643------------------------------------------------------ Precision@1: 59.81% 

[59.71, 59.65, 59.48, 59.81]

Epoch: 4
2024-03-09 23:44:50.017568 epoch: 4 step: 0 cls_loss= 1.18169 (140768 samples/sec)
2024-03-09 23:44:52.918465 epoch: 4 step: 100 cls_loss= 0.95979 (10342 samples/sec)
saving....
2024-03-09 23:44:56.706979------------------------------------------------------ Precision@1: 59.56% 

[59.71, 59.65, 59.48, 59.81, 59.56]

Epoch: 5
2024-03-09 23:44:56.902914 epoch: 5 step: 0 cls_loss= 1.08353 (154082 samples/sec)
2024-03-09 23:44:59.814829 epoch: 5 step: 100 cls_loss= 1.01966 (10306 samples/sec)
saving....
2024-03-09 23:45:03.690611------------------------------------------------------ Precision@1: 59.42% 

[59.71, 59.65, 59.48, 59.81, 59.56, 59.42]

Epoch: 6
2024-03-09 23:45:03.880970 epoch: 6 step: 0 cls_loss= 1.04287 (158542 samples/sec)
2024-03-09 23:45:06.964585 epoch: 6 step: 100 cls_loss= 1.09584 (9732 samples/sec)
saving....
2024-03-09 23:45:10.926666------------------------------------------------------ Precision@1: 59.57% 

[59.71, 59.65, 59.48, 59.81, 59.56, 59.42, 59.57]

Epoch: 7
2024-03-09 23:45:11.128164 epoch: 7 step: 0 cls_loss= 1.04244 (149667 samples/sec)
2024-03-09 23:45:14.142087 epoch: 7 step: 100 cls_loss= 1.11199 (9957 samples/sec)
saving....
2024-03-09 23:45:18.019884------------------------------------------------------ Precision@1: 59.76% 

[59.71, 59.65, 59.48, 59.81, 59.56, 59.42, 59.57, 59.76]

Epoch: 8
2024-03-09 23:45:18.200837 epoch: 8 step: 0 cls_loss= 1.06173 (166879 samples/sec)
2024-03-09 23:45:21.099484 epoch: 8 step: 100 cls_loss= 1.07939 (10353 samples/sec)
saving....
2024-03-09 23:45:24.801438------------------------------------------------------ Precision@1: 59.62% 

[59.71, 59.65, 59.48, 59.81, 59.56, 59.42, 59.57, 59.76, 59.62]

Epoch: 9
2024-03-09 23:45:25.008368 epoch: 9 step: 0 cls_loss= 1.09322 (145783 samples/sec)
2024-03-09 23:45:27.989396 epoch: 9 step: 100 cls_loss= 0.95994 (10063 samples/sec)
saving....
2024-03-09 23:45:31.684324------------------------------------------------------ Precision@1: 59.83% 

[59.71, 59.65, 59.48, 59.81, 59.56, 59.42, 59.57, 59.76, 59.62, 59.83]

Epoch: 10
2024-03-09 23:45:31.923731 epoch: 10 step: 0 cls_loss= 1.04898 (125830 samples/sec)
2024-03-09 23:45:34.940972 epoch: 10 step: 100 cls_loss= 0.89713 (9943 samples/sec)
saving....
2024-03-09 23:45:38.691816------------------------------------------------------ Precision@1: 59.46% 

[59.71, 59.65, 59.48, 59.81, 59.56, 59.42, 59.57, 59.76, 59.62, 59.83, 59.46]

Epoch: 11
2024-03-09 23:45:38.903460 epoch: 11 step: 0 cls_loss= 1.10640 (142428 samples/sec)
2024-03-09 23:45:41.800376 epoch: 11 step: 100 cls_loss= 1.01670 (10359 samples/sec)
saving....
2024-03-09 23:45:45.472693------------------------------------------------------ Precision@1: 59.57% 

[59.71, 59.65, 59.48, 59.81, 59.56, 59.42, 59.57, 59.76, 59.62, 59.83, 59.46, 59.57]

Epoch: 12
2024-03-09 23:45:45.671454 epoch: 12 step: 0 cls_loss= 1.04641 (151805 samples/sec)
2024-03-09 23:45:48.610297 epoch: 12 step: 100 cls_loss= 0.85302 (10209 samples/sec)
saving....
2024-03-09 23:45:52.298228------------------------------------------------------ Precision@1: 59.83% 

[59.71, 59.65, 59.48, 59.81, 59.56, 59.42, 59.57, 59.76, 59.62, 59.83, 59.46, 59.57, 59.83]

Epoch: 13
2024-03-09 23:45:52.509857 epoch: 13 step: 0 cls_loss= 0.99765 (142501 samples/sec)
2024-03-09 23:45:55.499267 epoch: 13 step: 100 cls_loss= 0.91716 (10035 samples/sec)
saving....
2024-03-09 23:45:59.202781------------------------------------------------------ Precision@1: 59.32% 

[59.71, 59.65, 59.48, 59.81, 59.56, 59.42, 59.57, 59.76, 59.62, 59.83, 59.46, 59.57, 59.83, 59.32]

Epoch: 14
2024-03-09 23:45:59.399333 epoch: 14 step: 0 cls_loss= 1.05145 (153467 samples/sec)
2024-03-09 23:46:02.420210 epoch: 14 step: 100 cls_loss= 0.98258 (9931 samples/sec)
saving....
2024-03-09 23:46:06.230202------------------------------------------------------ Precision@1: 59.60% 

[59.71, 59.65, 59.48, 59.81, 59.56, 59.42, 59.57, 59.76, 59.62, 59.83, 59.46, 59.57, 59.83, 59.32, 59.6]

Epoch: 15
2024-03-09 23:46:06.435915 epoch: 15 step: 0 cls_loss= 1.02355 (146609 samples/sec)
2024-03-09 23:46:09.429023 epoch: 15 step: 100 cls_loss= 0.97961 (10025 samples/sec)
saving....
2024-03-09 23:46:13.153500------------------------------------------------------ Precision@1: 59.58% 

[59.71, 59.65, 59.48, 59.81, 59.56, 59.42, 59.57, 59.76, 59.62, 59.83, 59.46, 59.57, 59.83, 59.32, 59.6, 59.58]

Epoch: 16
2024-03-09 23:46:13.360200 epoch: 16 step: 0 cls_loss= 1.02534 (145921 samples/sec)
2024-03-09 23:46:16.336078 epoch: 16 step: 100 cls_loss= 1.12873 (10081 samples/sec)
saving....
2024-03-09 23:46:20.082216------------------------------------------------------ Precision@1: 59.75% 

[59.71, 59.65, 59.48, 59.81, 59.56, 59.42, 59.57, 59.76, 59.62, 59.83, 59.46, 59.57, 59.83, 59.32, 59.6, 59.58, 59.75]

Epoch: 17
2024-03-09 23:46:20.283940 epoch: 17 step: 0 cls_loss= 1.05804 (149543 samples/sec)
2024-03-09 23:46:23.244637 epoch: 17 step: 100 cls_loss= 1.23591 (10136 samples/sec)
saving....
2024-03-09 23:46:26.973658------------------------------------------------------ Precision@1: 59.67% 

[59.71, 59.65, 59.48, 59.81, 59.56, 59.42, 59.57, 59.76, 59.62, 59.83, 59.46, 59.57, 59.83, 59.32, 59.6, 59.58, 59.75, 59.67]

Epoch: 18
2024-03-09 23:46:27.182986 epoch: 18 step: 0 cls_loss= 1.04707 (143922 samples/sec)
2024-03-09 23:46:30.182265 epoch: 18 step: 100 cls_loss= 1.16071 (10002 samples/sec)
saving....
2024-03-09 23:46:33.957059------------------------------------------------------ Precision@1: 59.53% 

[59.71, 59.65, 59.48, 59.81, 59.56, 59.42, 59.57, 59.76, 59.62, 59.83, 59.46, 59.57, 59.83, 59.32, 59.6, 59.58, 59.75, 59.67, 59.53]

Epoch: 19
2024-03-09 23:46:34.158671 epoch: 19 step: 0 cls_loss= 1.07141 (149623 samples/sec)
2024-03-09 23:46:37.175910 epoch: 19 step: 100 cls_loss= 0.98107 (9943 samples/sec)
saving....
2024-03-09 23:46:40.953112------------------------------------------------------ Precision@1: 59.66% 

[59.71, 59.65, 59.48, 59.81, 59.56, 59.42, 59.57, 59.76, 59.62, 59.83, 59.46, 59.57, 59.83, 59.32, 59.6, 59.58, 59.75, 59.67, 59.53, 59.66]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 23:46:43.679332 epoch: 0 step: 0 cls_loss= 1.15357 (48720 samples/sec)
2024-03-09 23:46:46.609849 epoch: 0 step: 100 cls_loss= 1.00414 (10237 samples/sec)
saving....
2024-03-09 23:46:50.517832------------------------------------------------------ Precision@1: 59.52% 

[59.52]

Epoch: 1
2024-03-09 23:46:50.712166 epoch: 1 step: 0 cls_loss= 0.89284 (155243 samples/sec)
2024-03-09 23:46:53.811030 epoch: 1 step: 100 cls_loss= 0.97593 (9684 samples/sec)
saving....
2024-03-09 23:46:57.601808------------------------------------------------------ Precision@1: 59.81% 

[59.52, 59.81]

Epoch: 2
2024-03-09 23:46:57.825305 epoch: 2 step: 0 cls_loss= 1.07535 (134846 samples/sec)
2024-03-09 23:47:00.806273 epoch: 2 step: 100 cls_loss= 1.08363 (10064 samples/sec)
saving....
2024-03-09 23:47:04.567443------------------------------------------------------ Precision@1: 59.79% 

[59.52, 59.81, 59.79]

Epoch: 3
2024-03-09 23:47:04.772949 epoch: 3 step: 0 cls_loss= 1.06243 (146865 samples/sec)
2024-03-09 23:47:07.679932 epoch: 3 step: 100 cls_loss= 1.10203 (10320 samples/sec)
saving....
2024-03-09 23:47:11.464648------------------------------------------------------ Precision@1: 59.59% 

[59.52, 59.81, 59.79, 59.59]

Epoch: 4
2024-03-09 23:47:11.660106 epoch: 4 step: 0 cls_loss= 1.11933 (154302 samples/sec)
2024-03-09 23:47:14.727659 epoch: 4 step: 100 cls_loss= 0.99271 (9783 samples/sec)
saving....
2024-03-09 23:47:18.582252------------------------------------------------------ Precision@1: 59.51% 

[59.52, 59.81, 59.79, 59.59, 59.51]

Epoch: 5
2024-03-09 23:47:18.774479 epoch: 5 step: 0 cls_loss= 1.04438 (156889 samples/sec)
2024-03-09 23:47:21.667264 epoch: 5 step: 100 cls_loss= 1.06184 (10374 samples/sec)
saving....
2024-03-09 23:47:25.380700------------------------------------------------------ Precision@1: 59.36% 

[59.52, 59.81, 59.79, 59.59, 59.51, 59.36]

Epoch: 6
2024-03-09 23:47:25.591232 epoch: 6 step: 0 cls_loss= 1.03747 (143199 samples/sec)
2024-03-09 23:47:28.650194 epoch: 6 step: 100 cls_loss= 1.14893 (9810 samples/sec)
saving....
2024-03-09 23:47:32.539717------------------------------------------------------ Precision@1: 59.80% 

[59.52, 59.81, 59.79, 59.59, 59.51, 59.36, 59.8]

Epoch: 7
2024-03-09 23:47:32.753222 epoch: 7 step: 0 cls_loss= 1.09127 (141325 samples/sec)
2024-03-09 23:47:35.758139 epoch: 7 step: 100 cls_loss= 1.09521 (9983 samples/sec)
saving....
2024-03-09 23:47:39.625695------------------------------------------------------ Precision@1: 59.75% 

[59.52, 59.81, 59.79, 59.59, 59.51, 59.36, 59.8, 59.75]

Epoch: 8
2024-03-09 23:47:39.831256 epoch: 8 step: 0 cls_loss= 1.13255 (146736 samples/sec)
2024-03-09 23:47:42.781108 epoch: 8 step: 100 cls_loss= 0.96700 (10171 samples/sec)
saving....
2024-03-09 23:47:46.509831------------------------------------------------------ Precision@1: 59.74% 

[59.52, 59.81, 59.79, 59.59, 59.51, 59.36, 59.8, 59.75, 59.74]

Epoch: 9
2024-03-09 23:47:46.701787 epoch: 9 step: 0 cls_loss= 1.09055 (157163 samples/sec)
2024-03-09 23:47:49.726048 epoch: 9 step: 100 cls_loss= 1.02410 (9920 samples/sec)
saving....
2024-03-09 23:47:53.391370------------------------------------------------------ Precision@1: 59.70% 

[59.52, 59.81, 59.79, 59.59, 59.51, 59.36, 59.8, 59.75, 59.74, 59.7]

Epoch: 10
2024-03-09 23:47:53.576361 epoch: 10 step: 0 cls_loss= 1.04115 (162953 samples/sec)
2024-03-09 23:47:56.589812 epoch: 10 step: 100 cls_loss= 0.97716 (9958 samples/sec)
saving....
2024-03-09 23:48:00.292006------------------------------------------------------ Precision@1: 59.63% 

[59.52, 59.81, 59.79, 59.59, 59.51, 59.36, 59.8, 59.75, 59.74, 59.7, 59.63]

Epoch: 11
2024-03-09 23:48:00.491066 epoch: 11 step: 0 cls_loss= 1.03100 (151437 samples/sec)
2024-03-09 23:48:03.401212 epoch: 11 step: 100 cls_loss= 1.14826 (10309 samples/sec)
saving....
2024-03-09 23:48:07.105242------------------------------------------------------ Precision@1: 59.52% 

[59.52, 59.81, 59.79, 59.59, 59.51, 59.36, 59.8, 59.75, 59.74, 59.7, 59.63, 59.52]

Epoch: 12
2024-03-09 23:48:07.295215 epoch: 12 step: 0 cls_loss= 1.06828 (158893 samples/sec)
2024-03-09 23:48:10.255438 epoch: 12 step: 100 cls_loss= 1.03093 (10137 samples/sec)
saving....
2024-03-09 23:48:13.972322------------------------------------------------------ Precision@1: 59.39% 

[59.52, 59.81, 59.79, 59.59, 59.51, 59.36, 59.8, 59.75, 59.74, 59.7, 59.63, 59.52, 59.39]

Epoch: 13
2024-03-09 23:48:14.174879 epoch: 13 step: 0 cls_loss= 1.10583 (148880 samples/sec)
2024-03-09 23:48:17.222999 epoch: 13 step: 100 cls_loss= 0.94400 (9842 samples/sec)
saving....
2024-03-09 23:48:21.108066------------------------------------------------------ Precision@1: 59.61% 

[59.52, 59.81, 59.79, 59.59, 59.51, 59.36, 59.8, 59.75, 59.74, 59.7, 59.63, 59.52, 59.39, 59.61]

Epoch: 14
2024-03-09 23:48:21.296643 epoch: 14 step: 0 cls_loss= 1.08313 (159998 samples/sec)
2024-03-09 23:48:24.223379 epoch: 14 step: 100 cls_loss= 1.01289 (10252 samples/sec)
saving....
2024-03-09 23:48:27.893273------------------------------------------------------ Precision@1: 59.92% 

[59.52, 59.81, 59.79, 59.59, 59.51, 59.36, 59.8, 59.75, 59.74, 59.7, 59.63, 59.52, 59.39, 59.61, 59.92]

Epoch: 15
2024-03-09 23:48:28.101999 epoch: 15 step: 0 cls_loss= 1.00199 (144511 samples/sec)
2024-03-09 23:48:31.167852 epoch: 15 step: 100 cls_loss= 1.10447 (9785 samples/sec)
saving....
2024-03-09 23:48:34.845741------------------------------------------------------ Precision@1: 59.67% 

[59.52, 59.81, 59.79, 59.59, 59.51, 59.36, 59.8, 59.75, 59.74, 59.7, 59.63, 59.52, 59.39, 59.61, 59.92, 59.67]

Epoch: 16
2024-03-09 23:48:35.030168 epoch: 16 step: 0 cls_loss= 0.96703 (163639 samples/sec)
2024-03-09 23:48:38.042158 epoch: 16 step: 100 cls_loss= 0.97102 (9964 samples/sec)
saving....
2024-03-09 23:48:41.807981------------------------------------------------------ Precision@1: 59.69% 

[59.52, 59.81, 59.79, 59.59, 59.51, 59.36, 59.8, 59.75, 59.74, 59.7, 59.63, 59.52, 59.39, 59.61, 59.92, 59.67, 59.69]

Epoch: 17
2024-03-09 23:48:42.008964 epoch: 17 step: 0 cls_loss= 1.04644 (150021 samples/sec)
2024-03-09 23:48:44.986754 epoch: 17 step: 100 cls_loss= 1.02351 (10078 samples/sec)
saving....
2024-03-09 23:48:48.705566------------------------------------------------------ Precision@1: 59.64% 

[59.52, 59.81, 59.79, 59.59, 59.51, 59.36, 59.8, 59.75, 59.74, 59.7, 59.63, 59.52, 59.39, 59.61, 59.92, 59.67, 59.69, 59.64]

Epoch: 18
2024-03-09 23:48:48.912080 epoch: 18 step: 0 cls_loss= 1.06273 (146024 samples/sec)
2024-03-09 23:48:51.975346 epoch: 18 step: 100 cls_loss= 1.06022 (9793 samples/sec)
saving....
2024-03-09 23:48:55.728984------------------------------------------------------ Precision@1: 59.72% 

[59.52, 59.81, 59.79, 59.59, 59.51, 59.36, 59.8, 59.75, 59.74, 59.7, 59.63, 59.52, 59.39, 59.61, 59.92, 59.67, 59.69, 59.64, 59.72]

Epoch: 19
2024-03-09 23:48:55.923262 epoch: 19 step: 0 cls_loss= 1.09331 (155271 samples/sec)
2024-03-09 23:48:58.873535 epoch: 19 step: 100 cls_loss= 1.03191 (10172 samples/sec)
saving....
2024-03-09 23:49:02.588986------------------------------------------------------ Precision@1: 59.80% 

[59.52, 59.81, 59.79, 59.59, 59.51, 59.36, 59.8, 59.75, 59.74, 59.7, 59.63, 59.52, 59.39, 59.61, 59.92, 59.67, 59.69, 59.64, 59.72, 59.8]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 23:49:05.312243 epoch: 0 step: 0 cls_loss= 1.04145 (49220 samples/sec)
2024-03-09 23:49:08.228466 epoch: 0 step: 100 cls_loss= 0.91344 (10287 samples/sec)
saving....
2024-03-09 23:49:12.165375------------------------------------------------------ Precision@1: 59.84% 

[59.84]

Epoch: 1
2024-03-09 23:49:12.369499 epoch: 1 step: 0 cls_loss= 1.06905 (147710 samples/sec)
2024-03-09 23:49:15.269374 epoch: 1 step: 100 cls_loss= 1.05068 (10345 samples/sec)
saving....
2024-03-09 23:49:18.971835------------------------------------------------------ Precision@1: 59.53% 

[59.84, 59.53]

Epoch: 2
2024-03-09 23:49:19.181862 epoch: 2 step: 0 cls_loss= 0.95742 (143443 samples/sec)
2024-03-09 23:49:22.093684 epoch: 2 step: 100 cls_loss= 1.02390 (10303 samples/sec)
saving....
2024-03-09 23:49:25.816834------------------------------------------------------ Precision@1: 59.77% 

[59.84, 59.53, 59.77]

Epoch: 3
2024-03-09 23:49:26.029539 epoch: 3 step: 0 cls_loss= 0.94958 (141697 samples/sec)
2024-03-09 23:49:28.943174 epoch: 3 step: 100 cls_loss= 0.98243 (10300 samples/sec)
saving....
2024-03-09 23:49:32.690852------------------------------------------------------ Precision@1: 59.73% 

[59.84, 59.53, 59.77, 59.73]

Epoch: 4
2024-03-09 23:49:32.902884 epoch: 4 step: 0 cls_loss= 1.09785 (142313 samples/sec)
2024-03-09 23:49:35.907252 epoch: 4 step: 100 cls_loss= 0.99599 (9987 samples/sec)
saving....
2024-03-09 23:49:39.680576------------------------------------------------------ Precision@1: 59.58% 

[59.84, 59.53, 59.77, 59.73, 59.58]

Epoch: 5
2024-03-09 23:49:39.907472 epoch: 5 step: 0 cls_loss= 0.92319 (132812 samples/sec)
2024-03-09 23:49:42.863930 epoch: 5 step: 100 cls_loss= 0.92917 (10147 samples/sec)
saving....
2024-03-09 23:49:46.621593------------------------------------------------------ Precision@1: 59.73% 

[59.84, 59.53, 59.77, 59.73, 59.58, 59.73]

Epoch: 6
2024-03-09 23:49:46.824247 epoch: 6 step: 0 cls_loss= 1.06120 (148773 samples/sec)
2024-03-09 23:49:49.777931 epoch: 6 step: 100 cls_loss= 0.94869 (10157 samples/sec)
saving....
2024-03-09 23:49:53.462469------------------------------------------------------ Precision@1: 59.80% 

[59.84, 59.53, 59.77, 59.73, 59.58, 59.73, 59.8]

Epoch: 7
2024-03-09 23:49:53.685235 epoch: 7 step: 0 cls_loss= 1.06787 (135295 samples/sec)
2024-03-09 23:49:56.645618 epoch: 7 step: 100 cls_loss= 1.09447 (10134 samples/sec)
saving....
2024-03-09 23:50:00.438993------------------------------------------------------ Precision@1: 59.49% 

[59.84, 59.53, 59.77, 59.73, 59.58, 59.73, 59.8, 59.49]

Epoch: 8
2024-03-09 23:50:00.653266 epoch: 8 step: 0 cls_loss= 0.98427 (140567 samples/sec)
2024-03-09 23:50:03.653894 epoch: 8 step: 100 cls_loss= 1.00082 (9998 samples/sec)
saving....
2024-03-09 23:50:07.363325------------------------------------------------------ Precision@1: 59.71% 

[59.84, 59.53, 59.77, 59.73, 59.58, 59.73, 59.8, 59.49, 59.71]

Epoch: 9
2024-03-09 23:50:07.559884 epoch: 9 step: 0 cls_loss= 1.16939 (153448 samples/sec)
2024-03-09 23:50:10.461071 epoch: 9 step: 100 cls_loss= 0.98664 (10344 samples/sec)
saving....
2024-03-09 23:50:14.156664------------------------------------------------------ Precision@1: 59.63% 

[59.84, 59.53, 59.77, 59.73, 59.58, 59.73, 59.8, 59.49, 59.71, 59.63]

Epoch: 10
2024-03-09 23:50:14.359641 epoch: 10 step: 0 cls_loss= 1.04216 (148542 samples/sec)
2024-03-09 23:50:17.344585 epoch: 10 step: 100 cls_loss= 1.14781 (10054 samples/sec)
saving....
2024-03-09 23:50:21.066348------------------------------------------------------ Precision@1: 59.65% 

[59.84, 59.53, 59.77, 59.73, 59.58, 59.73, 59.8, 59.49, 59.71, 59.63, 59.65]

Epoch: 11
2024-03-09 23:50:21.273443 epoch: 11 step: 0 cls_loss= 0.98324 (145454 samples/sec)
2024-03-09 23:50:24.280764 epoch: 11 step: 100 cls_loss= 0.95075 (9975 samples/sec)
saving....
2024-03-09 23:50:28.091619------------------------------------------------------ Precision@1: 59.62% 

[59.84, 59.53, 59.77, 59.73, 59.58, 59.73, 59.8, 59.49, 59.71, 59.63, 59.65, 59.62]

Epoch: 12
2024-03-09 23:50:28.300042 epoch: 12 step: 0 cls_loss= 0.97010 (144743 samples/sec)
2024-03-09 23:50:31.215835 epoch: 12 step: 100 cls_loss= 0.96701 (10292 samples/sec)
saving....
2024-03-09 23:50:34.886815------------------------------------------------------ Precision@1: 59.79% 

[59.84, 59.53, 59.77, 59.73, 59.58, 59.73, 59.8, 59.49, 59.71, 59.63, 59.65, 59.62, 59.79]

Epoch: 13
2024-03-09 23:50:35.092154 epoch: 13 step: 0 cls_loss= 1.09413 (146893 samples/sec)
2024-03-09 23:50:37.993974 epoch: 13 step: 100 cls_loss= 1.12763 (10342 samples/sec)
saving....
2024-03-09 23:50:41.749027------------------------------------------------------ Precision@1: 59.51% 

[59.84, 59.53, 59.77, 59.73, 59.58, 59.73, 59.8, 59.49, 59.71, 59.63, 59.65, 59.62, 59.79, 59.51]

Epoch: 14
2024-03-09 23:50:41.952693 epoch: 14 step: 0 cls_loss= 1.09704 (148078 samples/sec)
2024-03-09 23:50:44.932280 epoch: 14 step: 100 cls_loss= 1.03235 (10071 samples/sec)
saving....
2024-03-09 23:50:48.745801------------------------------------------------------ Precision@1: 59.66% 

[59.84, 59.53, 59.77, 59.73, 59.58, 59.73, 59.8, 59.49, 59.71, 59.63, 59.65, 59.62, 59.79, 59.51, 59.66]

Epoch: 15
2024-03-09 23:50:48.947726 epoch: 15 step: 0 cls_loss= 1.04267 (149319 samples/sec)
2024-03-09 23:50:51.924286 epoch: 15 step: 100 cls_loss= 0.99165 (10079 samples/sec)
saving....
2024-03-09 23:50:55.631232------------------------------------------------------ Precision@1: 59.77% 

[59.84, 59.53, 59.77, 59.73, 59.58, 59.73, 59.8, 59.49, 59.71, 59.63, 59.65, 59.62, 59.79, 59.51, 59.66, 59.77]

Epoch: 16
2024-03-09 23:50:55.839633 epoch: 16 step: 0 cls_loss= 1.06202 (144666 samples/sec)
2024-03-09 23:50:58.803315 epoch: 16 step: 100 cls_loss= 1.01429 (10126 samples/sec)
saving....
2024-03-09 23:51:02.509544------------------------------------------------------ Precision@1: 59.83% 

[59.84, 59.53, 59.77, 59.73, 59.58, 59.73, 59.8, 59.49, 59.71, 59.63, 59.65, 59.62, 59.79, 59.51, 59.66, 59.77, 59.83]

Epoch: 17
2024-03-09 23:51:02.715890 epoch: 17 step: 0 cls_loss= 1.13330 (146116 samples/sec)
2024-03-09 23:51:05.630424 epoch: 17 step: 100 cls_loss= 1.03388 (10294 samples/sec)
saving....
2024-03-09 23:51:09.364211------------------------------------------------------ Precision@1: 59.91% 

[59.84, 59.53, 59.77, 59.73, 59.58, 59.73, 59.8, 59.49, 59.71, 59.63, 59.65, 59.62, 59.79, 59.51, 59.66, 59.77, 59.83, 59.91]

Epoch: 18
2024-03-09 23:51:09.569495 epoch: 18 step: 0 cls_loss= 1.08234 (146906 samples/sec)
2024-03-09 23:51:12.514317 epoch: 18 step: 100 cls_loss= 1.02587 (10187 samples/sec)
saving....
2024-03-09 23:51:16.234664------------------------------------------------------ Precision@1: 59.86% 

[59.84, 59.53, 59.77, 59.73, 59.58, 59.73, 59.8, 59.49, 59.71, 59.63, 59.65, 59.62, 59.79, 59.51, 59.66, 59.77, 59.83, 59.91, 59.86]

Epoch: 19
2024-03-09 23:51:16.430857 epoch: 19 step: 0 cls_loss= 1.01162 (153726 samples/sec)
2024-03-09 23:51:19.532242 epoch: 19 step: 100 cls_loss= 1.01574 (9674 samples/sec)
saving....
2024-03-09 23:51:23.398054------------------------------------------------------ Precision@1: 59.66% 

[59.84, 59.53, 59.77, 59.73, 59.58, 59.73, 59.8, 59.49, 59.71, 59.63, 59.65, 59.62, 59.79, 59.51, 59.66, 59.77, 59.83, 59.91, 59.86, 59.66]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 23:51:26.115059 epoch: 0 step: 0 cls_loss= 1.01663 (50095 samples/sec)
2024-03-09 23:51:29.102652 epoch: 0 step: 100 cls_loss= 0.97006 (10041 samples/sec)
saving....
2024-03-09 23:51:33.073472------------------------------------------------------ Precision@1: 59.27% 

[59.27]

Epoch: 1
2024-03-09 23:51:33.264161 epoch: 1 step: 0 cls_loss= 1.17587 (158028 samples/sec)
2024-03-09 23:51:36.203646 epoch: 1 step: 100 cls_loss= 0.97103 (10210 samples/sec)
saving....
2024-03-09 23:51:39.976786------------------------------------------------------ Precision@1: 59.58% 

[59.27, 59.58]

Epoch: 2
2024-03-09 23:51:40.173324 epoch: 2 step: 0 cls_loss= 0.94645 (153430 samples/sec)
2024-03-09 23:51:43.100437 epoch: 2 step: 100 cls_loss= 1.00424 (10249 samples/sec)
saving....
2024-03-09 23:51:46.875560------------------------------------------------------ Precision@1: 59.65% 

[59.27, 59.58, 59.65]

Epoch: 3
2024-03-09 23:51:47.093728 epoch: 3 step: 0 cls_loss= 0.99233 (138073 samples/sec)
2024-03-09 23:51:50.170280 epoch: 3 step: 100 cls_loss= 1.03879 (9751 samples/sec)
saving....
2024-03-09 23:51:53.901404------------------------------------------------------ Precision@1: 59.72% 

[59.27, 59.58, 59.65, 59.72]

Epoch: 4
2024-03-09 23:51:54.086721 epoch: 4 step: 0 cls_loss= 1.07374 (162821 samples/sec)
2024-03-09 23:51:56.999227 epoch: 4 step: 100 cls_loss= 1.11076 (10305 samples/sec)
saving....
2024-03-09 23:52:00.709387------------------------------------------------------ Precision@1: 59.65% 

[59.27, 59.58, 59.65, 59.72, 59.65]

Epoch: 5
2024-03-09 23:52:00.911532 epoch: 5 step: 0 cls_loss= 0.99959 (149230 samples/sec)
2024-03-09 23:52:03.875784 epoch: 5 step: 100 cls_loss= 1.04429 (10120 samples/sec)
saving....
2024-03-09 23:52:07.597498------------------------------------------------------ Precision@1: 59.74% 

[59.27, 59.58, 59.65, 59.72, 59.65, 59.74]

Epoch: 6
2024-03-09 23:52:07.802879 epoch: 6 step: 0 cls_loss= 0.99990 (146803 samples/sec)
2024-03-09 23:52:10.904768 epoch: 6 step: 100 cls_loss= 1.16627 (9672 samples/sec)
saving....
2024-03-09 23:52:14.637357------------------------------------------------------ Precision@1: 59.70% 

[59.27, 59.58, 59.65, 59.72, 59.65, 59.74, 59.7]

Epoch: 7
2024-03-09 23:52:14.835070 epoch: 7 step: 0 cls_loss= 1.04488 (152693 samples/sec)
2024-03-09 23:52:17.723371 epoch: 7 step: 100 cls_loss= 1.12258 (10391 samples/sec)
saving....
2024-03-09 23:52:21.371473------------------------------------------------------ Precision@1: 59.81% 

[59.27, 59.58, 59.65, 59.72, 59.65, 59.74, 59.7, 59.81]

Epoch: 8
2024-03-09 23:52:21.593073 epoch: 8 step: 0 cls_loss= 1.04388 (136012 samples/sec)
2024-03-09 23:52:24.534567 epoch: 8 step: 100 cls_loss= 1.03736 (10199 samples/sec)
saving....
2024-03-09 23:52:28.218748------------------------------------------------------ Precision@1: 59.82% 

[59.27, 59.58, 59.65, 59.72, 59.65, 59.74, 59.7, 59.81, 59.82]

Epoch: 9
2024-03-09 23:52:28.421895 epoch: 9 step: 0 cls_loss= 0.96458 (148369 samples/sec)
2024-03-09 23:52:31.327892 epoch: 9 step: 100 cls_loss= 1.02921 (10326 samples/sec)
saving....
2024-03-09 23:52:35.030510------------------------------------------------------ Precision@1: 59.74% 

[59.27, 59.58, 59.65, 59.72, 59.65, 59.74, 59.7, 59.81, 59.82, 59.74]

Epoch: 10
2024-03-09 23:52:35.227981 epoch: 10 step: 0 cls_loss= 1.02053 (152804 samples/sec)
2024-03-09 23:52:38.155876 epoch: 10 step: 100 cls_loss= 1.02612 (10249 samples/sec)
saving....
2024-03-09 23:52:41.905625------------------------------------------------------ Precision@1: 59.62% 

[59.27, 59.58, 59.65, 59.72, 59.65, 59.74, 59.7, 59.81, 59.82, 59.74, 59.62]

Epoch: 11
2024-03-09 23:52:42.113382 epoch: 11 step: 0 cls_loss= 0.98091 (145204 samples/sec)
2024-03-09 23:52:45.048572 epoch: 11 step: 100 cls_loss= 1.12780 (10221 samples/sec)
saving....
2024-03-09 23:52:48.798315------------------------------------------------------ Precision@1: 60.04% 

[59.27, 59.58, 59.65, 59.72, 59.65, 59.74, 59.7, 59.81, 59.82, 59.74, 59.62, 60.04]

Epoch: 12
2024-03-09 23:52:48.990887 epoch: 12 step: 0 cls_loss= 1.05300 (156684 samples/sec)
2024-03-09 23:52:51.854280 epoch: 12 step: 100 cls_loss= 1.04937 (10481 samples/sec)
saving....
2024-03-09 23:52:55.535100------------------------------------------------------ Precision@1: 59.72% 

[59.27, 59.58, 59.65, 59.72, 59.65, 59.74, 59.7, 59.81, 59.82, 59.74, 59.62, 60.04, 59.72]

Epoch: 13
2024-03-09 23:52:55.737292 epoch: 13 step: 0 cls_loss= 1.00224 (149022 samples/sec)
2024-03-09 23:52:58.626905 epoch: 13 step: 100 cls_loss= 1.03474 (10383 samples/sec)
saving....
2024-03-09 23:53:02.368929------------------------------------------------------ Precision@1: 59.48% 

[59.27, 59.58, 59.65, 59.72, 59.65, 59.74, 59.7, 59.81, 59.82, 59.74, 59.62, 60.04, 59.72, 59.48]

Epoch: 14
2024-03-09 23:53:02.554285 epoch: 14 step: 0 cls_loss= 0.98644 (162826 samples/sec)
2024-03-09 23:53:05.478244 epoch: 14 step: 100 cls_loss= 0.98803 (10263 samples/sec)
saving....
2024-03-09 23:53:09.193718------------------------------------------------------ Precision@1: 59.79% 

[59.27, 59.58, 59.65, 59.72, 59.65, 59.74, 59.7, 59.81, 59.82, 59.74, 59.62, 60.04, 59.72, 59.48, 59.79]

Epoch: 15
2024-03-09 23:53:09.399454 epoch: 15 step: 0 cls_loss= 1.06827 (146673 samples/sec)
2024-03-09 23:53:12.294167 epoch: 15 step: 100 cls_loss= 1.12667 (10368 samples/sec)
saving....
2024-03-09 23:53:15.976272------------------------------------------------------ Precision@1: 59.76% 

[59.27, 59.58, 59.65, 59.72, 59.65, 59.74, 59.7, 59.81, 59.82, 59.74, 59.62, 60.04, 59.72, 59.48, 59.79, 59.76]

Epoch: 16
2024-03-09 23:53:16.167689 epoch: 16 step: 0 cls_loss= 0.99466 (157594 samples/sec)
2024-03-09 23:53:19.097986 epoch: 16 step: 100 cls_loss= 1.11034 (10238 samples/sec)
saving....
2024-03-09 23:53:22.818957------------------------------------------------------ Precision@1: 59.59% 

[59.27, 59.58, 59.65, 59.72, 59.65, 59.74, 59.7, 59.81, 59.82, 59.74, 59.62, 60.04, 59.72, 59.48, 59.79, 59.76, 59.59]

Epoch: 17
2024-03-09 23:53:23.027386 epoch: 17 step: 0 cls_loss= 1.02879 (144627 samples/sec)
2024-03-09 23:53:25.907559 epoch: 17 step: 100 cls_loss= 1.05836 (10421 samples/sec)
saving....
2024-03-09 23:53:29.595402------------------------------------------------------ Precision@1: 59.61% 

[59.27, 59.58, 59.65, 59.72, 59.65, 59.74, 59.7, 59.81, 59.82, 59.74, 59.62, 60.04, 59.72, 59.48, 59.79, 59.76, 59.59, 59.61]

Epoch: 18
2024-03-09 23:53:29.803920 epoch: 18 step: 0 cls_loss= 1.02484 (144595 samples/sec)
2024-03-09 23:53:32.717498 epoch: 18 step: 100 cls_loss= 1.06748 (10300 samples/sec)
saving....
2024-03-09 23:53:36.567850------------------------------------------------------ Precision@1: 59.61% 

[59.27, 59.58, 59.65, 59.72, 59.65, 59.74, 59.7, 59.81, 59.82, 59.74, 59.62, 60.04, 59.72, 59.48, 59.79, 59.76, 59.59, 59.61, 59.61]

Epoch: 19
2024-03-09 23:53:36.778785 epoch: 19 step: 0 cls_loss= 1.01898 (143021 samples/sec)
2024-03-09 23:53:39.669704 epoch: 19 step: 100 cls_loss= 1.13139 (10381 samples/sec)
saving....
2024-03-09 23:53:43.342198------------------------------------------------------ Precision@1: 59.70% 

[59.27, 59.58, 59.65, 59.72, 59.65, 59.74, 59.7, 59.81, 59.82, 59.74, 59.62, 60.04, 59.72, 59.48, 59.79, 59.76, 59.59, 59.61, 59.61, 59.7]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 23:53:46.076788 epoch: 0 step: 0 cls_loss= 1.10878 (49690 samples/sec)
2024-03-09 23:53:48.960886 epoch: 0 step: 100 cls_loss= 1.10819 (10402 samples/sec)
saving....
2024-03-09 23:53:52.949111------------------------------------------------------ Precision@1: 59.83% 

[59.83]

Epoch: 1
2024-03-09 23:53:53.152184 epoch: 1 step: 0 cls_loss= 1.16204 (148552 samples/sec)
2024-03-09 23:53:56.179129 epoch: 1 step: 100 cls_loss= 1.03358 (9911 samples/sec)
saving....
2024-03-09 23:53:59.908448------------------------------------------------------ Precision@1: 59.60% 

[59.83, 59.6]

Epoch: 2
2024-03-09 23:54:00.102290 epoch: 2 step: 0 cls_loss= 1.04070 (155510 samples/sec)
2024-03-09 23:54:03.002824 epoch: 2 step: 100 cls_loss= 1.08540 (10348 samples/sec)
saving....
2024-03-09 23:54:06.719464------------------------------------------------------ Precision@1: 59.59% 

[59.83, 59.6, 59.59]

Epoch: 3
2024-03-09 23:54:06.903297 epoch: 3 step: 0 cls_loss= 1.13255 (164212 samples/sec)
2024-03-09 23:54:09.865827 epoch: 3 step: 100 cls_loss= 1.05865 (10128 samples/sec)
saving....
2024-03-09 23:54:13.594433------------------------------------------------------ Precision@1: 59.82% 

[59.83, 59.6, 59.59, 59.82]

Epoch: 4
2024-03-09 23:54:13.795889 epoch: 4 step: 0 cls_loss= 1.05236 (149741 samples/sec)
2024-03-09 23:54:16.693324 epoch: 4 step: 100 cls_loss= 1.16254 (10358 samples/sec)
saving....
2024-03-09 23:54:20.407978------------------------------------------------------ Precision@1: 59.64% 

[59.83, 59.6, 59.59, 59.82, 59.64]

Epoch: 5
2024-03-09 23:54:20.606844 epoch: 5 step: 0 cls_loss= 1.16253 (151682 samples/sec)
2024-03-09 23:54:23.548813 epoch: 5 step: 100 cls_loss= 1.12822 (10200 samples/sec)
saving....
2024-03-09 23:54:27.335473------------------------------------------------------ Precision@1: 59.65% 

[59.83, 59.6, 59.59, 59.82, 59.64, 59.65]

Epoch: 6
2024-03-09 23:54:27.518252 epoch: 6 step: 0 cls_loss= 1.00771 (164955 samples/sec)
2024-03-09 23:54:30.442296 epoch: 6 step: 100 cls_loss= 1.17539 (10259 samples/sec)
saving....
2024-03-09 23:54:34.333896------------------------------------------------------ Precision@1: 59.67% 

[59.83, 59.6, 59.59, 59.82, 59.64, 59.65, 59.67]

Epoch: 7
2024-03-09 23:54:34.547097 epoch: 7 step: 0 cls_loss= 1.12545 (141448 samples/sec)
2024-03-09 23:54:37.491073 epoch: 7 step: 100 cls_loss= 1.00933 (10193 samples/sec)
saving....
2024-03-09 23:54:41.292717------------------------------------------------------ Precision@1: 59.89% 

[59.83, 59.6, 59.59, 59.82, 59.64, 59.65, 59.67, 59.89]

Epoch: 8
2024-03-09 23:54:41.486214 epoch: 8 step: 0 cls_loss= 1.04360 (155935 samples/sec)
2024-03-09 23:54:44.407223 epoch: 8 step: 100 cls_loss= 1.08307 (10274 samples/sec)
saving....
2024-03-09 23:54:48.121222------------------------------------------------------ Precision@1: 59.53% 

[59.83, 59.6, 59.59, 59.82, 59.64, 59.65, 59.67, 59.89, 59.53]

Epoch: 9
2024-03-09 23:54:48.330789 epoch: 9 step: 0 cls_loss= 1.01628 (143894 samples/sec)
2024-03-09 23:54:51.231658 epoch: 9 step: 100 cls_loss= 1.09312 (10345 samples/sec)
saving....
2024-03-09 23:54:55.113529------------------------------------------------------ Precision@1: 59.74% 

[59.83, 59.6, 59.59, 59.82, 59.64, 59.65, 59.67, 59.89, 59.53, 59.74]

Epoch: 10
2024-03-09 23:54:55.309624 epoch: 10 step: 0 cls_loss= 1.04193 (153919 samples/sec)
2024-03-09 23:54:58.303870 epoch: 10 step: 100 cls_loss= 0.98764 (10023 samples/sec)
saving....
2024-03-09 23:55:02.055358------------------------------------------------------ Precision@1: 59.53% 

[59.83, 59.6, 59.59, 59.82, 59.64, 59.65, 59.67, 59.89, 59.53, 59.74, 59.53]

Epoch: 11
2024-03-09 23:55:02.242891 epoch: 11 step: 0 cls_loss= 1.10948 (160902 samples/sec)
2024-03-09 23:55:05.298203 epoch: 11 step: 100 cls_loss= 1.02366 (9823 samples/sec)
saving....
2024-03-09 23:55:09.043244------------------------------------------------------ Precision@1: 59.47% 

[59.83, 59.6, 59.59, 59.82, 59.64, 59.65, 59.67, 59.89, 59.53, 59.74, 59.53, 59.47]

Epoch: 12
2024-03-09 23:55:09.243800 epoch: 12 step: 0 cls_loss= 1.08103 (150410 samples/sec)
2024-03-09 23:55:12.133447 epoch: 12 step: 100 cls_loss= 1.12734 (10382 samples/sec)
saving....
2024-03-09 23:55:15.913012------------------------------------------------------ Precision@1: 59.61% 

[59.83, 59.6, 59.59, 59.82, 59.64, 59.65, 59.67, 59.89, 59.53, 59.74, 59.53, 59.47, 59.61]

Epoch: 13
2024-03-09 23:55:16.124067 epoch: 13 step: 0 cls_loss= 1.02335 (142810 samples/sec)
2024-03-09 23:55:19.003868 epoch: 13 step: 100 cls_loss= 1.11562 (10422 samples/sec)
saving....
2024-03-09 23:55:22.782656------------------------------------------------------ Precision@1: 59.68% 

[59.83, 59.6, 59.59, 59.82, 59.64, 59.65, 59.67, 59.89, 59.53, 59.74, 59.53, 59.47, 59.61, 59.68]

Epoch: 14
2024-03-09 23:55:22.988839 epoch: 14 step: 0 cls_loss= 1.02020 (146314 samples/sec)
2024-03-09 23:55:25.963510 epoch: 14 step: 100 cls_loss= 1.03142 (10086 samples/sec)
saving....
2024-03-09 23:55:29.809374------------------------------------------------------ Precision@1: 59.68% 

[59.83, 59.6, 59.59, 59.82, 59.64, 59.65, 59.67, 59.89, 59.53, 59.74, 59.53, 59.47, 59.61, 59.68, 59.68]

Epoch: 15
2024-03-09 23:55:30.005170 epoch: 15 step: 0 cls_loss= 1.13098 (154152 samples/sec)
2024-03-09 23:55:32.975888 epoch: 15 step: 100 cls_loss= 1.18503 (10101 samples/sec)
saving....
2024-03-09 23:55:36.699600------------------------------------------------------ Precision@1: 59.48% 

[59.83, 59.6, 59.59, 59.82, 59.64, 59.65, 59.67, 59.89, 59.53, 59.74, 59.53, 59.47, 59.61, 59.68, 59.68, 59.48]

Epoch: 16
2024-03-09 23:55:36.890286 epoch: 16 step: 0 cls_loss= 1.16786 (157905 samples/sec)
2024-03-09 23:55:39.782960 epoch: 16 step: 100 cls_loss= 1.05708 (10372 samples/sec)
saving....
2024-03-09 23:55:43.456287------------------------------------------------------ Precision@1: 59.71% 

[59.83, 59.6, 59.59, 59.82, 59.64, 59.65, 59.67, 59.89, 59.53, 59.74, 59.53, 59.47, 59.61, 59.68, 59.68, 59.48, 59.71]

Epoch: 17
2024-03-09 23:55:43.641169 epoch: 17 step: 0 cls_loss= 0.98044 (163247 samples/sec)
2024-03-09 23:55:46.609726 epoch: 17 step: 100 cls_loss= 0.97371 (10108 samples/sec)
saving....
2024-03-09 23:55:50.410907------------------------------------------------------ Precision@1: 59.74% 

[59.83, 59.6, 59.59, 59.82, 59.64, 59.65, 59.67, 59.89, 59.53, 59.74, 59.53, 59.47, 59.61, 59.68, 59.68, 59.48, 59.71, 59.74]

Epoch: 18
2024-03-09 23:55:50.618168 epoch: 18 step: 0 cls_loss= 0.99518 (145554 samples/sec)
2024-03-09 23:55:53.497818 epoch: 18 step: 100 cls_loss= 1.10829 (10422 samples/sec)
saving....
2024-03-09 23:55:57.131633------------------------------------------------------ Precision@1: 59.56% 

[59.83, 59.6, 59.59, 59.82, 59.64, 59.65, 59.67, 59.89, 59.53, 59.74, 59.53, 59.47, 59.61, 59.68, 59.68, 59.48, 59.71, 59.74, 59.56]

Epoch: 19
2024-03-09 23:55:57.347180 epoch: 19 step: 0 cls_loss= 0.97905 (139895 samples/sec)
2024-03-09 23:56:00.316683 epoch: 19 step: 100 cls_loss= 0.93638 (10105 samples/sec)
saving....
2024-03-09 23:56:04.106567------------------------------------------------------ Precision@1: 59.73% 

[59.83, 59.6, 59.59, 59.82, 59.64, 59.65, 59.67, 59.89, 59.53, 59.74, 59.53, 59.47, 59.61, 59.68, 59.68, 59.48, 59.71, 59.74, 59.56, 59.73]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 23:56:06.864249 epoch: 0 step: 0 cls_loss= 1.02044 (48546 samples/sec)
2024-03-09 23:56:09.818490 epoch: 0 step: 100 cls_loss= 1.05167 (10155 samples/sec)
saving....
2024-03-09 23:56:13.729334------------------------------------------------------ Precision@1: 59.48% 

[59.48]

Epoch: 1
2024-03-09 23:56:13.948382 epoch: 1 step: 0 cls_loss= 1.06212 (137474 samples/sec)
2024-03-09 23:56:16.806549 epoch: 1 step: 100 cls_loss= 1.00247 (10501 samples/sec)
saving....
2024-03-09 23:56:20.464502------------------------------------------------------ Precision@1: 59.22% 

[59.48, 59.22]

Epoch: 2
2024-03-09 23:56:20.667058 epoch: 2 step: 0 cls_loss= 1.07359 (148681 samples/sec)
2024-03-09 23:56:23.537587 epoch: 2 step: 100 cls_loss= 1.01055 (10451 samples/sec)
saving....
2024-03-09 23:56:27.155111------------------------------------------------------ Precision@1: 59.60% 

[59.48, 59.22, 59.6]

Epoch: 3
2024-03-09 23:56:27.360367 epoch: 3 step: 0 cls_loss= 1.18870 (146795 samples/sec)
2024-03-09 23:56:30.285230 epoch: 3 step: 100 cls_loss= 0.98798 (10261 samples/sec)
saving....
2024-03-09 23:56:33.972207------------------------------------------------------ Precision@1: 59.76% 

[59.48, 59.22, 59.6, 59.76]

Epoch: 4
2024-03-09 23:56:34.171015 epoch: 4 step: 0 cls_loss= 1.02440 (151522 samples/sec)
2024-03-09 23:56:37.173948 epoch: 4 step: 100 cls_loss= 0.97275 (9994 samples/sec)
saving....
2024-03-09 23:56:40.932402------------------------------------------------------ Precision@1: 59.78% 

[59.48, 59.22, 59.6, 59.76, 59.78]

Epoch: 5
2024-03-09 23:56:41.129090 epoch: 5 step: 0 cls_loss= 1.12361 (153191 samples/sec)
2024-03-09 23:56:44.097616 epoch: 5 step: 100 cls_loss= 1.07933 (10108 samples/sec)
saving....
2024-03-09 23:56:47.772004------------------------------------------------------ Precision@1: 59.60% 

[59.48, 59.22, 59.6, 59.76, 59.78, 59.6]

Epoch: 6
2024-03-09 23:56:47.980149 epoch: 6 step: 0 cls_loss= 0.97649 (144669 samples/sec)
2024-03-09 23:56:51.053567 epoch: 6 step: 100 cls_loss= 0.94980 (9764 samples/sec)
saving....
2024-03-09 23:56:54.887816------------------------------------------------------ Precision@1: 59.44% 

[59.48, 59.22, 59.6, 59.76, 59.78, 59.6, 59.44]

Epoch: 7
2024-03-09 23:56:55.105076 epoch: 7 step: 0 cls_loss= 1.08813 (138741 samples/sec)
2024-03-09 23:56:58.048191 epoch: 7 step: 100 cls_loss= 1.15222 (10196 samples/sec)
saving....
2024-03-09 23:57:01.906159------------------------------------------------------ Precision@1: 59.39% 

[59.48, 59.22, 59.6, 59.76, 59.78, 59.6, 59.44, 59.39]

Epoch: 8
2024-03-09 23:57:02.107732 epoch: 8 step: 0 cls_loss= 0.95869 (149688 samples/sec)
2024-03-09 23:57:05.152816 epoch: 8 step: 100 cls_loss= 0.98783 (9852 samples/sec)
saving....
2024-03-09 23:57:08.861209------------------------------------------------------ Precision@1: 59.69% 

[59.48, 59.22, 59.6, 59.76, 59.78, 59.6, 59.44, 59.39, 59.69]

Epoch: 9
2024-03-09 23:57:09.054407 epoch: 9 step: 0 cls_loss= 1.10032 (155960 samples/sec)
2024-03-09 23:57:12.033079 epoch: 9 step: 100 cls_loss= 0.93788 (10071 samples/sec)
saving....
2024-03-09 23:57:15.697547------------------------------------------------------ Precision@1: 59.50% 

[59.48, 59.22, 59.6, 59.76, 59.78, 59.6, 59.44, 59.39, 59.69, 59.5]

Epoch: 10
2024-03-09 23:57:15.901070 epoch: 10 step: 0 cls_loss= 1.06873 (148214 samples/sec)
2024-03-09 23:57:18.816887 epoch: 10 step: 100 cls_loss= 0.96664 (10293 samples/sec)
saving....
2024-03-09 23:57:22.492722------------------------------------------------------ Precision@1: 59.76% 

[59.48, 59.22, 59.6, 59.76, 59.78, 59.6, 59.44, 59.39, 59.69, 59.5, 59.76]

Epoch: 11
2024-03-09 23:57:22.680902 epoch: 11 step: 0 cls_loss= 0.92226 (160208 samples/sec)
2024-03-09 23:57:25.625715 epoch: 11 step: 100 cls_loss= 1.10631 (10191 samples/sec)
saving....
2024-03-09 23:57:29.302414------------------------------------------------------ Precision@1: 59.60% 

[59.48, 59.22, 59.6, 59.76, 59.78, 59.6, 59.44, 59.39, 59.69, 59.5, 59.76, 59.6]

Epoch: 12
2024-03-09 23:57:29.504014 epoch: 12 step: 0 cls_loss= 0.99733 (149683 samples/sec)
2024-03-09 23:57:32.390378 epoch: 12 step: 100 cls_loss= 1.03285 (10398 samples/sec)
saving....
2024-03-09 23:57:36.075747------------------------------------------------------ Precision@1: 59.89% 

[59.48, 59.22, 59.6, 59.76, 59.78, 59.6, 59.44, 59.39, 59.69, 59.5, 59.76, 59.6, 59.89]

Epoch: 13
2024-03-09 23:57:36.285828 epoch: 13 step: 0 cls_loss= 1.09562 (143559 samples/sec)
2024-03-09 23:57:39.256329 epoch: 13 step: 100 cls_loss= 0.97436 (10103 samples/sec)
saving....
2024-03-09 23:57:42.951956------------------------------------------------------ Precision@1: 59.84% 

[59.48, 59.22, 59.6, 59.76, 59.78, 59.6, 59.44, 59.39, 59.69, 59.5, 59.76, 59.6, 59.89, 59.84]

Epoch: 14
2024-03-09 23:57:43.167766 epoch: 14 step: 0 cls_loss= 1.08842 (139623 samples/sec)
2024-03-09 23:57:46.111111 epoch: 14 step: 100 cls_loss= 0.97765 (10192 samples/sec)
saving....
2024-03-09 23:57:49.821763------------------------------------------------------ Precision@1: 59.58% 

[59.48, 59.22, 59.6, 59.76, 59.78, 59.6, 59.44, 59.39, 59.69, 59.5, 59.76, 59.6, 59.89, 59.84, 59.58]

Epoch: 15
2024-03-09 23:57:50.019721 epoch: 15 step: 0 cls_loss= 1.13331 (152423 samples/sec)
2024-03-09 23:57:52.952163 epoch: 15 step: 100 cls_loss= 1.12056 (10234 samples/sec)
saving....
2024-03-09 23:57:56.662465------------------------------------------------------ Precision@1: 59.75% 

[59.48, 59.22, 59.6, 59.76, 59.78, 59.6, 59.44, 59.39, 59.69, 59.5, 59.76, 59.6, 59.89, 59.84, 59.58, 59.75]

Epoch: 16
2024-03-09 23:57:56.849948 epoch: 16 step: 0 cls_loss= 1.09471 (161058 samples/sec)
2024-03-09 23:57:59.743101 epoch: 16 step: 100 cls_loss= 0.99932 (10370 samples/sec)
saving....
2024-03-09 23:58:03.427941------------------------------------------------------ Precision@1: 59.96% 

[59.48, 59.22, 59.6, 59.76, 59.78, 59.6, 59.44, 59.39, 59.69, 59.5, 59.76, 59.6, 59.89, 59.84, 59.58, 59.75, 59.96]

Epoch: 17
2024-03-09 23:58:03.621194 epoch: 17 step: 0 cls_loss= 1.05870 (156180 samples/sec)
2024-03-09 23:58:06.517434 epoch: 17 step: 100 cls_loss= 0.97412 (10362 samples/sec)
saving....
2024-03-09 23:58:10.280757------------------------------------------------------ Precision@1: 59.92% 

[59.48, 59.22, 59.6, 59.76, 59.78, 59.6, 59.44, 59.39, 59.69, 59.5, 59.76, 59.6, 59.89, 59.84, 59.58, 59.75, 59.96, 59.92]

Epoch: 18
2024-03-09 23:58:10.473045 epoch: 18 step: 0 cls_loss= 0.87540 (156935 samples/sec)
2024-03-09 23:58:13.416267 epoch: 18 step: 100 cls_loss= 1.07694 (10197 samples/sec)
saving....
2024-03-09 23:58:17.247968------------------------------------------------------ Precision@1: 59.73% 

[59.48, 59.22, 59.6, 59.76, 59.78, 59.6, 59.44, 59.39, 59.69, 59.5, 59.76, 59.6, 59.89, 59.84, 59.58, 59.75, 59.96, 59.92, 59.73]

Epoch: 19
2024-03-09 23:58:17.445361 epoch: 19 step: 0 cls_loss= 1.15620 (152786 samples/sec)
2024-03-09 23:58:20.376565 epoch: 19 step: 100 cls_loss= 0.94930 (10234 samples/sec)
saving....
2024-03-09 23:58:24.166829------------------------------------------------------ Precision@1: 59.67% 

[59.48, 59.22, 59.6, 59.76, 59.78, 59.6, 59.44, 59.39, 59.69, 59.5, 59.76, 59.6, 59.89, 59.84, 59.58, 59.75, 59.96, 59.92, 59.73, 59.67]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-09 23:58:26.892719 epoch: 0 step: 0 cls_loss= 0.97269 (49201 samples/sec)
2024-03-09 23:58:30.013573 epoch: 0 step: 100 cls_loss= 1.01239 (9612 samples/sec)
saving....
2024-03-09 23:58:34.068770------------------------------------------------------ Precision@1: 59.63% 

[59.63]

Epoch: 1
2024-03-09 23:58:34.287788 epoch: 1 step: 0 cls_loss= 0.97729 (137633 samples/sec)
2024-03-09 23:58:37.363412 epoch: 1 step: 100 cls_loss= 0.96166 (9756 samples/sec)
saving....
2024-03-09 23:58:41.017655------------------------------------------------------ Precision@1: 59.78% 

[59.63, 59.78]

Epoch: 2
2024-03-09 23:58:41.216190 epoch: 2 step: 0 cls_loss= 0.89199 (151851 samples/sec)
2024-03-09 23:58:44.102138 epoch: 2 step: 100 cls_loss= 1.06688 (10400 samples/sec)
saving....
2024-03-09 23:58:47.742245------------------------------------------------------ Precision@1: 59.74% 

[59.63, 59.78, 59.74]

Epoch: 3
2024-03-09 23:58:47.936999 epoch: 3 step: 0 cls_loss= 1.12000 (154884 samples/sec)
2024-03-09 23:58:50.869976 epoch: 3 step: 100 cls_loss= 1.03705 (10229 samples/sec)
saving....
2024-03-09 23:58:54.552304------------------------------------------------------ Precision@1: 59.74% 

[59.63, 59.78, 59.74, 59.74]

Epoch: 4
2024-03-09 23:58:54.745138 epoch: 4 step: 0 cls_loss= 0.95611 (156450 samples/sec)
2024-03-09 23:58:57.679652 epoch: 4 step: 100 cls_loss= 1.10492 (10224 samples/sec)
saving....
2024-03-09 23:59:01.383720------------------------------------------------------ Precision@1: 59.62% 

[59.63, 59.78, 59.74, 59.74, 59.62]

Epoch: 5
2024-03-09 23:59:01.580430 epoch: 5 step: 0 cls_loss= 1.08474 (153284 samples/sec)
2024-03-09 23:59:04.529308 epoch: 5 step: 100 cls_loss= 1.09128 (10174 samples/sec)
saving....
2024-03-09 23:59:08.274831------------------------------------------------------ Precision@1: 59.53% 

[59.63, 59.78, 59.74, 59.74, 59.62, 59.53]

Epoch: 6
2024-03-09 23:59:08.493307 epoch: 6 step: 0 cls_loss= 0.99074 (137997 samples/sec)
2024-03-09 23:59:11.399553 epoch: 6 step: 100 cls_loss= 1.07160 (10326 samples/sec)
saving....
2024-03-09 23:59:15.069393------------------------------------------------------ Precision@1: 59.79% 

[59.63, 59.78, 59.74, 59.74, 59.62, 59.53, 59.79]

Epoch: 7
2024-03-09 23:59:15.300290 epoch: 7 step: 0 cls_loss= 0.93143 (130547 samples/sec)
2024-03-09 23:59:18.407468 epoch: 7 step: 100 cls_loss= 1.16074 (9656 samples/sec)
saving....
2024-03-09 23:59:22.382503------------------------------------------------------ Precision@1: 59.70% 

[59.63, 59.78, 59.74, 59.74, 59.62, 59.53, 59.79, 59.7]

Epoch: 8
2024-03-09 23:59:22.573737 epoch: 8 step: 0 cls_loss= 1.02278 (157873 samples/sec)
2024-03-09 23:59:25.616965 epoch: 8 step: 100 cls_loss= 1.04135 (9861 samples/sec)
saving....
2024-03-09 23:59:29.412032------------------------------------------------------ Precision@1: 59.67% 

[59.63, 59.78, 59.74, 59.74, 59.62, 59.53, 59.79, 59.7, 59.67]

Epoch: 9
2024-03-09 23:59:29.626645 epoch: 9 step: 0 cls_loss= 1.15175 (140454 samples/sec)
2024-03-09 23:59:32.491602 epoch: 9 step: 100 cls_loss= 1.06680 (10476 samples/sec)
saving....
2024-03-09 23:59:36.129169------------------------------------------------------ Precision@1: 59.62% 

[59.63, 59.78, 59.74, 59.74, 59.62, 59.53, 59.79, 59.7, 59.67, 59.62]

Epoch: 10
2024-03-09 23:59:36.322475 epoch: 10 step: 0 cls_loss= 0.97093 (155984 samples/sec)
2024-03-09 23:59:39.237225 epoch: 10 step: 100 cls_loss= 1.08095 (10294 samples/sec)
saving....
2024-03-09 23:59:42.937367------------------------------------------------------ Precision@1: 59.60% 

[59.63, 59.78, 59.74, 59.74, 59.62, 59.53, 59.79, 59.7, 59.67, 59.62, 59.6]

Epoch: 11
2024-03-09 23:59:43.157406 epoch: 11 step: 0 cls_loss= 0.99173 (137060 samples/sec)
2024-03-09 23:59:46.020093 epoch: 11 step: 100 cls_loss= 1.11278 (10485 samples/sec)
saving....
2024-03-09 23:59:49.655671------------------------------------------------------ Precision@1: 59.71% 

[59.63, 59.78, 59.74, 59.74, 59.62, 59.53, 59.79, 59.7, 59.67, 59.62, 59.6, 59.71]

Epoch: 12
2024-03-09 23:59:49.871602 epoch: 12 step: 0 cls_loss= 0.99520 (139538 samples/sec)
2024-03-09 23:59:52.810504 epoch: 12 step: 100 cls_loss= 1.10349 (10212 samples/sec)
saving....
2024-03-09 23:59:56.470856------------------------------------------------------ Precision@1: 59.51% 

[59.63, 59.78, 59.74, 59.74, 59.62, 59.53, 59.79, 59.7, 59.67, 59.62, 59.6, 59.71, 59.51]

Epoch: 13
2024-03-09 23:59:56.685879 epoch: 13 step: 0 cls_loss= 1.13426 (140151 samples/sec)
2024-03-09 23:59:59.654569 epoch: 13 step: 100 cls_loss= 0.99620 (10108 samples/sec)
saving....
2024-03-10 00:00:03.392937------------------------------------------------------ Precision@1: 59.66% 

[59.63, 59.78, 59.74, 59.74, 59.62, 59.53, 59.79, 59.7, 59.67, 59.62, 59.6, 59.71, 59.51, 59.66]

Epoch: 14
2024-03-10 00:00:03.579668 epoch: 14 step: 0 cls_loss= 0.89160 (161615 samples/sec)
2024-03-10 00:00:06.629722 epoch: 14 step: 100 cls_loss= 1.06476 (9840 samples/sec)
saving....
2024-03-10 00:00:10.312765------------------------------------------------------ Precision@1: 59.75% 

[59.63, 59.78, 59.74, 59.74, 59.62, 59.53, 59.79, 59.7, 59.67, 59.62, 59.6, 59.71, 59.51, 59.66, 59.75]

Epoch: 15
2024-03-10 00:00:10.501475 epoch: 15 step: 0 cls_loss= 1.15245 (159898 samples/sec)
2024-03-10 00:00:13.450911 epoch: 15 step: 100 cls_loss= 0.95385 (10175 samples/sec)
saving....
2024-03-10 00:00:17.153077------------------------------------------------------ Precision@1: 59.54% 

[59.63, 59.78, 59.74, 59.74, 59.62, 59.53, 59.79, 59.7, 59.67, 59.62, 59.6, 59.71, 59.51, 59.66, 59.75, 59.54]

Epoch: 16
2024-03-10 00:00:17.340815 epoch: 16 step: 0 cls_loss= 1.03330 (160703 samples/sec)
2024-03-10 00:00:20.330861 epoch: 16 step: 100 cls_loss= 1.10400 (10036 samples/sec)
saving....
2024-03-10 00:00:24.046240------------------------------------------------------ Precision@1: 59.81% 

[59.63, 59.78, 59.74, 59.74, 59.62, 59.53, 59.79, 59.7, 59.67, 59.62, 59.6, 59.71, 59.51, 59.66, 59.75, 59.54, 59.81]

Epoch: 17
2024-03-10 00:00:24.244466 epoch: 17 step: 0 cls_loss= 1.11811 (152017 samples/sec)
2024-03-10 00:00:27.173904 epoch: 17 step: 100 cls_loss= 0.99237 (10241 samples/sec)
saving....
2024-03-10 00:00:30.828810------------------------------------------------------ Precision@1: 59.42% 

[59.63, 59.78, 59.74, 59.74, 59.62, 59.53, 59.79, 59.7, 59.67, 59.62, 59.6, 59.71, 59.51, 59.66, 59.75, 59.54, 59.81, 59.42]

Epoch: 18
2024-03-10 00:00:31.010255 epoch: 18 step: 0 cls_loss= 1.08902 (166340 samples/sec)
2024-03-10 00:00:33.927152 epoch: 18 step: 100 cls_loss= 1.08275 (10289 samples/sec)
saving....
2024-03-10 00:00:37.633341------------------------------------------------------ Precision@1: 59.65% 

[59.63, 59.78, 59.74, 59.74, 59.62, 59.53, 59.79, 59.7, 59.67, 59.62, 59.6, 59.71, 59.51, 59.66, 59.75, 59.54, 59.81, 59.42, 59.65]

Epoch: 19
2024-03-10 00:00:37.824934 epoch: 19 step: 0 cls_loss= 1.05448 (157560 samples/sec)
2024-03-10 00:00:40.744134 epoch: 19 step: 100 cls_loss= 1.12930 (10282 samples/sec)
saving....
2024-03-10 00:00:44.511807------------------------------------------------------ Precision@1: 59.95% 

[59.63, 59.78, 59.74, 59.74, 59.62, 59.53, 59.79, 59.7, 59.67, 59.62, 59.6, 59.71, 59.51, 59.66, 59.75, 59.54, 59.81, 59.42, 59.65, 59.95]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 00:00:47.227397 epoch: 0 step: 0 cls_loss= 1.09083 (49156 samples/sec)
2024-03-10 00:00:50.100256 epoch: 0 step: 100 cls_loss= 1.03184 (10442 samples/sec)
saving....
2024-03-10 00:00:53.983112------------------------------------------------------ Precision@1: 59.66% 

[59.66]

Epoch: 1
2024-03-10 00:00:54.173459 epoch: 1 step: 0 cls_loss= 1.08585 (158445 samples/sec)
2024-03-10 00:00:57.098328 epoch: 1 step: 100 cls_loss= 1.04559 (10261 samples/sec)
saving....
2024-03-10 00:01:00.780066------------------------------------------------------ Precision@1: 59.80% 

[59.66, 59.8]

Epoch: 2
2024-03-10 00:01:00.985717 epoch: 2 step: 0 cls_loss= 1.11917 (146608 samples/sec)
2024-03-10 00:01:03.949462 epoch: 2 step: 100 cls_loss= 1.04416 (10126 samples/sec)
saving....
2024-03-10 00:01:07.633959------------------------------------------------------ Precision@1: 59.65% 

[59.66, 59.8, 59.65]

Epoch: 3
2024-03-10 00:01:07.823050 epoch: 3 step: 0 cls_loss= 0.96435 (159686 samples/sec)
2024-03-10 00:01:10.723418 epoch: 3 step: 100 cls_loss= 1.13016 (10348 samples/sec)
saving....
2024-03-10 00:01:14.433468------------------------------------------------------ Precision@1: 59.65% 

[59.66, 59.8, 59.65, 59.65]

Epoch: 4
2024-03-10 00:01:14.630542 epoch: 4 step: 0 cls_loss= 1.11441 (152915 samples/sec)
2024-03-10 00:01:17.586229 epoch: 4 step: 100 cls_loss= 0.94886 (10150 samples/sec)
saving....
2024-03-10 00:01:21.309065------------------------------------------------------ Precision@1: 59.63% 

[59.66, 59.8, 59.65, 59.65, 59.63]

Epoch: 5
2024-03-10 00:01:21.517042 epoch: 5 step: 0 cls_loss= 0.93024 (144992 samples/sec)
2024-03-10 00:01:24.463506 epoch: 5 step: 100 cls_loss= 1.13032 (10181 samples/sec)
saving....
2024-03-10 00:01:28.169149------------------------------------------------------ Precision@1: 59.78% 

[59.66, 59.8, 59.65, 59.65, 59.63, 59.78]

Epoch: 6
2024-03-10 00:01:28.386674 epoch: 6 step: 0 cls_loss= 0.91004 (138591 samples/sec)
2024-03-10 00:01:31.343629 epoch: 6 step: 100 cls_loss= 1.11525 (10148 samples/sec)
saving....
2024-03-10 00:01:35.039059------------------------------------------------------ Precision@1: 59.72% 

[59.66, 59.8, 59.65, 59.65, 59.63, 59.78, 59.72]

Epoch: 7
2024-03-10 00:01:35.232867 epoch: 7 step: 0 cls_loss= 1.05685 (155620 samples/sec)
2024-03-10 00:01:38.254131 epoch: 7 step: 100 cls_loss= 0.94601 (9929 samples/sec)
saving....
2024-03-10 00:01:41.972002------------------------------------------------------ Precision@1: 59.55% 

[59.66, 59.8, 59.65, 59.65, 59.63, 59.78, 59.72, 59.55]

Epoch: 8
2024-03-10 00:01:42.183130 epoch: 8 step: 0 cls_loss= 1.13774 (142835 samples/sec)
2024-03-10 00:01:45.050486 epoch: 8 step: 100 cls_loss= 0.96266 (10468 samples/sec)
saving....
2024-03-10 00:01:48.707083------------------------------------------------------ Precision@1: 59.72% 

[59.66, 59.8, 59.65, 59.65, 59.63, 59.78, 59.72, 59.55, 59.72]

Epoch: 9
2024-03-10 00:01:48.903946 epoch: 9 step: 0 cls_loss= 1.07577 (153168 samples/sec)
2024-03-10 00:01:51.778751 epoch: 9 step: 100 cls_loss= 1.08590 (10440 samples/sec)
saving....
2024-03-10 00:01:55.469159------------------------------------------------------ Precision@1: 59.68% 

[59.66, 59.8, 59.65, 59.65, 59.63, 59.78, 59.72, 59.55, 59.72, 59.68]

Epoch: 10
2024-03-10 00:01:55.675894 epoch: 10 step: 0 cls_loss= 1.08270 (145819 samples/sec)
2024-03-10 00:01:58.572280 epoch: 10 step: 100 cls_loss= 0.95692 (10362 samples/sec)
saving....
2024-03-10 00:02:02.255558------------------------------------------------------ Precision@1: 59.65% 

[59.66, 59.8, 59.65, 59.65, 59.63, 59.78, 59.72, 59.55, 59.72, 59.68, 59.65]

Epoch: 11
2024-03-10 00:02:02.463631 epoch: 11 step: 0 cls_loss= 1.17035 (144880 samples/sec)
2024-03-10 00:02:05.320659 epoch: 11 step: 100 cls_loss= 1.11629 (10505 samples/sec)
saving....
2024-03-10 00:02:08.981220------------------------------------------------------ Precision@1: 59.76% 

[59.66, 59.8, 59.65, 59.65, 59.63, 59.78, 59.72, 59.55, 59.72, 59.68, 59.65, 59.76]

Epoch: 12
2024-03-10 00:02:09.193940 epoch: 12 step: 0 cls_loss= 1.13524 (141807 samples/sec)
2024-03-10 00:02:12.057644 epoch: 12 step: 100 cls_loss= 0.99862 (10480 samples/sec)
saving....
2024-03-10 00:02:15.730360------------------------------------------------------ Precision@1: 59.69% 

[59.66, 59.8, 59.65, 59.65, 59.63, 59.78, 59.72, 59.55, 59.72, 59.68, 59.65, 59.76, 59.69]

Epoch: 13
2024-03-10 00:02:15.926379 epoch: 13 step: 0 cls_loss= 0.95244 (153889 samples/sec)
2024-03-10 00:02:18.793857 epoch: 13 step: 100 cls_loss= 1.00314 (10466 samples/sec)
saving....
2024-03-10 00:02:22.495443------------------------------------------------------ Precision@1: 59.66% 

[59.66, 59.8, 59.65, 59.65, 59.63, 59.78, 59.72, 59.55, 59.72, 59.68, 59.65, 59.76, 59.69, 59.66]

Epoch: 14
2024-03-10 00:02:22.710794 epoch: 14 step: 0 cls_loss= 1.10140 (139942 samples/sec)
2024-03-10 00:02:25.621726 epoch: 14 step: 100 cls_loss= 1.17591 (10306 samples/sec)
saving....
2024-03-10 00:02:29.334298------------------------------------------------------ Precision@1: 59.56% 

[59.66, 59.8, 59.65, 59.65, 59.63, 59.78, 59.72, 59.55, 59.72, 59.68, 59.65, 59.76, 59.69, 59.66, 59.56]

Epoch: 15
2024-03-10 00:02:29.539160 epoch: 15 step: 0 cls_loss= 0.96289 (147249 samples/sec)
2024-03-10 00:02:32.652250 epoch: 15 step: 100 cls_loss= 1.12590 (9640 samples/sec)
saving....
2024-03-10 00:02:36.362449------------------------------------------------------ Precision@1: 59.63% 

[59.66, 59.8, 59.65, 59.65, 59.63, 59.78, 59.72, 59.55, 59.72, 59.68, 59.65, 59.76, 59.69, 59.66, 59.56, 59.63]

Epoch: 16
2024-03-10 00:02:36.576326 epoch: 16 step: 0 cls_loss= 0.95600 (141050 samples/sec)
2024-03-10 00:02:39.474079 epoch: 16 step: 100 cls_loss= 1.03298 (10356 samples/sec)
saving....
2024-03-10 00:02:43.147490------------------------------------------------------ Precision@1: 59.68% 

[59.66, 59.8, 59.65, 59.65, 59.63, 59.78, 59.72, 59.55, 59.72, 59.68, 59.65, 59.76, 59.69, 59.66, 59.56, 59.63, 59.68]

Epoch: 17
2024-03-10 00:02:43.366229 epoch: 17 step: 0 cls_loss= 1.04501 (137800 samples/sec)
2024-03-10 00:02:46.264603 epoch: 17 step: 100 cls_loss= 1.05137 (10355 samples/sec)
saving....
2024-03-10 00:02:49.929611------------------------------------------------------ Precision@1: 59.72% 

[59.66, 59.8, 59.65, 59.65, 59.63, 59.78, 59.72, 59.55, 59.72, 59.68, 59.65, 59.76, 59.69, 59.66, 59.56, 59.63, 59.68, 59.72]

Epoch: 18
2024-03-10 00:02:50.124648 epoch: 18 step: 0 cls_loss= 0.98770 (154632 samples/sec)
2024-03-10 00:02:53.002354 epoch: 18 step: 100 cls_loss= 1.10047 (10427 samples/sec)
saving....
2024-03-10 00:02:56.691128------------------------------------------------------ Precision@1: 59.44% 

[59.66, 59.8, 59.65, 59.65, 59.63, 59.78, 59.72, 59.55, 59.72, 59.68, 59.65, 59.76, 59.69, 59.66, 59.56, 59.63, 59.68, 59.72, 59.44]

Epoch: 19
2024-03-10 00:02:56.907910 epoch: 19 step: 0 cls_loss= 1.06704 (139179 samples/sec)
2024-03-10 00:02:59.964565 epoch: 19 step: 100 cls_loss= 1.08324 (9814 samples/sec)
saving....
2024-03-10 00:03:03.687443------------------------------------------------------ Precision@1: 59.48% 

[59.66, 59.8, 59.65, 59.65, 59.63, 59.78, 59.72, 59.55, 59.72, 59.68, 59.65, 59.76, 59.69, 59.66, 59.56, 59.63, 59.68, 59.72, 59.44, 59.48]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 00:03:06.429723 epoch: 0 step: 0 cls_loss= 1.08825 (50318 samples/sec)
2024-03-10 00:03:09.317002 epoch: 0 step: 100 cls_loss= 0.96547 (10390 samples/sec)
saving....
2024-03-10 00:03:13.287988------------------------------------------------------ Precision@1: 59.57% 

[59.57]

Epoch: 1
2024-03-10 00:03:13.479623 epoch: 1 step: 0 cls_loss= 0.97022 (157369 samples/sec)
2024-03-10 00:03:16.534969 epoch: 1 step: 100 cls_loss= 0.97889 (9819 samples/sec)
saving....
2024-03-10 00:03:20.319670------------------------------------------------------ Precision@1: 59.56% 

[59.57, 59.56]

Epoch: 2
2024-03-10 00:03:20.527853 epoch: 2 step: 0 cls_loss= 0.96518 (144824 samples/sec)
2024-03-10 00:03:23.409057 epoch: 2 step: 100 cls_loss= 1.07105 (10417 samples/sec)
saving....
2024-03-10 00:03:27.045390------------------------------------------------------ Precision@1: 59.74% 

[59.57, 59.56, 59.74]

Epoch: 3
2024-03-10 00:03:27.256725 epoch: 3 step: 0 cls_loss= 1.12459 (142622 samples/sec)
2024-03-10 00:03:30.173553 epoch: 3 step: 100 cls_loss= 1.06899 (10288 samples/sec)
saving....
2024-03-10 00:03:34.000462------------------------------------------------------ Precision@1: 59.62% 

[59.57, 59.56, 59.74, 59.62]

Epoch: 4
2024-03-10 00:03:34.207858 epoch: 4 step: 0 cls_loss= 1.05722 (145330 samples/sec)
2024-03-10 00:03:37.096080 epoch: 4 step: 100 cls_loss= 0.91261 (10392 samples/sec)
saving....
2024-03-10 00:03:40.851699------------------------------------------------------ Precision@1: 59.74% 

[59.57, 59.56, 59.74, 59.62, 59.74]

Epoch: 5
2024-03-10 00:03:41.044931 epoch: 5 step: 0 cls_loss= 1.05270 (156047 samples/sec)
2024-03-10 00:03:43.974883 epoch: 5 step: 100 cls_loss= 0.99973 (10244 samples/sec)
saving....
2024-03-10 00:03:47.669368------------------------------------------------------ Precision@1: 59.56% 

[59.57, 59.56, 59.74, 59.62, 59.74, 59.56]

Epoch: 6
2024-03-10 00:03:47.869276 epoch: 6 step: 0 cls_loss= 1.07766 (150818 samples/sec)
2024-03-10 00:03:50.759906 epoch: 6 step: 100 cls_loss= 0.94195 (10384 samples/sec)
saving....
2024-03-10 00:03:54.499845------------------------------------------------------ Precision@1: 59.68% 

[59.57, 59.56, 59.74, 59.62, 59.74, 59.56, 59.68]

Epoch: 7
2024-03-10 00:03:54.706587 epoch: 7 step: 0 cls_loss= 1.06608 (145852 samples/sec)
2024-03-10 00:03:57.653816 epoch: 7 step: 100 cls_loss= 1.07723 (10179 samples/sec)
saving....
2024-03-10 00:04:01.331118------------------------------------------------------ Precision@1: 59.49% 

[59.57, 59.56, 59.74, 59.62, 59.74, 59.56, 59.68, 59.49]

Epoch: 8
2024-03-10 00:04:01.534713 epoch: 8 step: 0 cls_loss= 1.06628 (148039 samples/sec)
2024-03-10 00:04:04.478005 epoch: 8 step: 100 cls_loss= 1.00579 (10199 samples/sec)
saving....
2024-03-10 00:04:08.249009------------------------------------------------------ Precision@1: 59.68% 

[59.57, 59.56, 59.74, 59.62, 59.74, 59.56, 59.68, 59.49, 59.68]

Epoch: 9
2024-03-10 00:04:08.451039 epoch: 9 step: 0 cls_loss= 1.05768 (149238 samples/sec)
2024-03-10 00:04:11.396782 epoch: 9 step: 100 cls_loss= 0.93904 (10188 samples/sec)
saving....
2024-03-10 00:04:15.172276------------------------------------------------------ Precision@1: 59.60% 

[59.57, 59.56, 59.74, 59.62, 59.74, 59.56, 59.68, 59.49, 59.68, 59.6]

Epoch: 10
2024-03-10 00:04:15.368168 epoch: 10 step: 0 cls_loss= 0.98951 (153963 samples/sec)
2024-03-10 00:04:18.396294 epoch: 10 step: 100 cls_loss= 1.06592 (9907 samples/sec)
saving....
2024-03-10 00:04:22.113125------------------------------------------------------ Precision@1: 59.61% 

[59.57, 59.56, 59.74, 59.62, 59.74, 59.56, 59.68, 59.49, 59.68, 59.6, 59.61]

Epoch: 11
2024-03-10 00:04:22.326577 epoch: 11 step: 0 cls_loss= 1.09110 (141267 samples/sec)
2024-03-10 00:04:25.317279 epoch: 11 step: 100 cls_loss= 1.10733 (10036 samples/sec)
saving....
2024-03-10 00:04:29.055406------------------------------------------------------ Precision@1: 59.95% 

[59.57, 59.56, 59.74, 59.62, 59.74, 59.56, 59.68, 59.49, 59.68, 59.6, 59.61, 59.95]

Epoch: 12
2024-03-10 00:04:29.257605 epoch: 12 step: 0 cls_loss= 1.13315 (149225 samples/sec)
2024-03-10 00:04:32.254844 epoch: 12 step: 100 cls_loss= 1.03384 (10009 samples/sec)
saving....
2024-03-10 00:04:35.984142------------------------------------------------------ Precision@1: 59.59% 

[59.57, 59.56, 59.74, 59.62, 59.74, 59.56, 59.68, 59.49, 59.68, 59.6, 59.61, 59.95, 59.59]

Epoch: 13
2024-03-10 00:04:36.200441 epoch: 13 step: 0 cls_loss= 0.96953 (139447 samples/sec)
2024-03-10 00:04:39.093813 epoch: 13 step: 100 cls_loss= 1.08622 (10373 samples/sec)
saving....
2024-03-10 00:04:42.864588------------------------------------------------------ Precision@1: 59.76% 

[59.57, 59.56, 59.74, 59.62, 59.74, 59.56, 59.68, 59.49, 59.68, 59.6, 59.61, 59.95, 59.59, 59.76]

Epoch: 14
2024-03-10 00:04:43.067620 epoch: 14 step: 0 cls_loss= 1.02447 (148514 samples/sec)
2024-03-10 00:04:46.015285 epoch: 14 step: 100 cls_loss= 1.03128 (10177 samples/sec)
saving....
2024-03-10 00:04:49.776046------------------------------------------------------ Precision@1: 59.76% 

[59.57, 59.56, 59.74, 59.62, 59.74, 59.56, 59.68, 59.49, 59.68, 59.6, 59.61, 59.95, 59.59, 59.76, 59.76]

Epoch: 15
2024-03-10 00:04:49.987604 epoch: 15 step: 0 cls_loss= 1.10533 (142551 samples/sec)
2024-03-10 00:04:53.126979 epoch: 15 step: 100 cls_loss= 1.07134 (9556 samples/sec)
saving....
2024-03-10 00:04:56.980143------------------------------------------------------ Precision@1: 59.51% 

[59.57, 59.56, 59.74, 59.62, 59.74, 59.56, 59.68, 59.49, 59.68, 59.6, 59.61, 59.95, 59.59, 59.76, 59.76, 59.51]

Epoch: 16
2024-03-10 00:04:57.190695 epoch: 16 step: 0 cls_loss= 1.01592 (143179 samples/sec)
2024-03-10 00:05:00.108710 epoch: 16 step: 100 cls_loss= 1.00735 (10285 samples/sec)
saving....
2024-03-10 00:05:03.815231------------------------------------------------------ Precision@1: 59.59% 

[59.57, 59.56, 59.74, 59.62, 59.74, 59.56, 59.68, 59.49, 59.68, 59.6, 59.61, 59.95, 59.59, 59.76, 59.76, 59.51, 59.59]

Epoch: 17
2024-03-10 00:05:04.020229 epoch: 17 step: 0 cls_loss= 1.14059 (147031 samples/sec)
2024-03-10 00:05:06.956872 epoch: 17 step: 100 cls_loss= 1.01288 (10220 samples/sec)
saving....
2024-03-10 00:05:10.660712------------------------------------------------------ Precision@1: 59.93% 

[59.57, 59.56, 59.74, 59.62, 59.74, 59.56, 59.68, 59.49, 59.68, 59.6, 59.61, 59.95, 59.59, 59.76, 59.76, 59.51, 59.59, 59.93]

Epoch: 18
2024-03-10 00:05:10.886519 epoch: 18 step: 0 cls_loss= 1.06010 (133461 samples/sec)
2024-03-10 00:05:13.823298 epoch: 18 step: 100 cls_loss= 1.07637 (10215 samples/sec)
saving....
2024-03-10 00:05:17.456005------------------------------------------------------ Precision@1: 59.89% 

[59.57, 59.56, 59.74, 59.62, 59.74, 59.56, 59.68, 59.49, 59.68, 59.6, 59.61, 59.95, 59.59, 59.76, 59.76, 59.51, 59.59, 59.93, 59.89]

Epoch: 19
2024-03-10 00:05:17.645115 epoch: 19 step: 0 cls_loss= 1.14554 (159582 samples/sec)
2024-03-10 00:05:20.767337 epoch: 19 step: 100 cls_loss= 1.16198 (9611 samples/sec)
saving....
2024-03-10 00:05:24.602240------------------------------------------------------ Precision@1: 59.69% 

[59.57, 59.56, 59.74, 59.62, 59.74, 59.56, 59.68, 59.49, 59.68, 59.6, 59.61, 59.95, 59.59, 59.76, 59.76, 59.51, 59.59, 59.93, 59.89, 59.69]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 00:05:27.293875 epoch: 0 step: 0 cls_loss= 1.06234 (51404 samples/sec)
2024-03-10 00:05:30.263077 epoch: 0 step: 100 cls_loss= 0.98593 (10104 samples/sec)
saving....
2024-03-10 00:05:34.198343------------------------------------------------------ Precision@1: 59.61% 

[59.61]

Epoch: 1
2024-03-10 00:05:34.413124 epoch: 1 step: 0 cls_loss= 1.09292 (140241 samples/sec)
2024-03-10 00:05:37.313476 epoch: 1 step: 100 cls_loss= 1.09779 (10349 samples/sec)
saving....
2024-03-10 00:05:41.040392------------------------------------------------------ Precision@1: 59.83% 

[59.61, 59.83]

Epoch: 2
2024-03-10 00:05:41.236611 epoch: 2 step: 0 cls_loss= 1.15773 (153709 samples/sec)
2024-03-10 00:05:44.098091 epoch: 2 step: 100 cls_loss= 0.96774 (10484 samples/sec)
saving....
2024-03-10 00:05:47.761894------------------------------------------------------ Precision@1: 59.89% 

[59.61, 59.83, 59.89]

Epoch: 3
2024-03-10 00:05:47.965399 epoch: 3 step: 0 cls_loss= 0.98964 (148127 samples/sec)
2024-03-10 00:05:50.847209 epoch: 3 step: 100 cls_loss= 1.06740 (10413 samples/sec)
saving....
2024-03-10 00:05:54.570509------------------------------------------------------ Precision@1: 59.74% 

[59.61, 59.83, 59.89, 59.74]

Epoch: 4
2024-03-10 00:05:54.777417 epoch: 4 step: 0 cls_loss= 0.96907 (145713 samples/sec)
2024-03-10 00:05:57.716807 epoch: 4 step: 100 cls_loss= 1.12613 (10207 samples/sec)
saving....
2024-03-10 00:06:01.463713------------------------------------------------------ Precision@1: 59.63% 

[59.61, 59.83, 59.89, 59.74, 59.63]

Epoch: 5
2024-03-10 00:06:01.671964 epoch: 5 step: 0 cls_loss= 1.04300 (144766 samples/sec)
2024-03-10 00:06:04.634053 epoch: 5 step: 100 cls_loss= 1.04997 (10131 samples/sec)
saving....
2024-03-10 00:06:08.363296------------------------------------------------------ Precision@1: 59.69% 

[59.61, 59.83, 59.89, 59.74, 59.63, 59.69]

Epoch: 6
2024-03-10 00:06:08.566591 epoch: 6 step: 0 cls_loss= 1.06399 (148297 samples/sec)
2024-03-10 00:06:11.504673 epoch: 6 step: 100 cls_loss= 1.09744 (10214 samples/sec)
saving....
2024-03-10 00:06:15.200626------------------------------------------------------ Precision@1: 59.85% 

[59.61, 59.83, 59.89, 59.74, 59.63, 59.69, 59.85]

Epoch: 7
2024-03-10 00:06:15.401540 epoch: 7 step: 0 cls_loss= 1.05575 (150161 samples/sec)
2024-03-10 00:06:18.355345 epoch: 7 step: 100 cls_loss= 0.97821 (10159 samples/sec)
saving....
2024-03-10 00:06:22.093656------------------------------------------------------ Precision@1: 59.49% 

[59.61, 59.83, 59.89, 59.74, 59.63, 59.69, 59.85, 59.49]

Epoch: 8
2024-03-10 00:06:22.288957 epoch: 8 step: 0 cls_loss= 1.10463 (154414 samples/sec)
2024-03-10 00:06:25.215211 epoch: 8 step: 100 cls_loss= 1.11440 (10256 samples/sec)
saving....
2024-03-10 00:06:28.918113------------------------------------------------------ Precision@1: 59.70% 

[59.61, 59.83, 59.89, 59.74, 59.63, 59.69, 59.85, 59.49, 59.7]

Epoch: 9
2024-03-10 00:06:29.116063 epoch: 9 step: 0 cls_loss= 0.98925 (152349 samples/sec)
2024-03-10 00:06:32.237262 epoch: 9 step: 100 cls_loss= 0.92960 (9613 samples/sec)
saving....
2024-03-10 00:06:36.019865------------------------------------------------------ Precision@1: 59.77% 

[59.61, 59.83, 59.89, 59.74, 59.63, 59.69, 59.85, 59.49, 59.7, 59.77]

Epoch: 10
2024-03-10 00:06:36.214275 epoch: 10 step: 0 cls_loss= 1.04383 (155079 samples/sec)
2024-03-10 00:06:39.141517 epoch: 10 step: 100 cls_loss= 1.09752 (10249 samples/sec)
saving....
2024-03-10 00:06:42.891017------------------------------------------------------ Precision@1: 59.68% 

[59.61, 59.83, 59.89, 59.74, 59.63, 59.69, 59.85, 59.49, 59.7, 59.77, 59.68]

Epoch: 11
2024-03-10 00:06:43.092433 epoch: 11 step: 0 cls_loss= 1.07666 (149823 samples/sec)
2024-03-10 00:06:46.138458 epoch: 11 step: 100 cls_loss= 1.15083 (9852 samples/sec)
saving....
2024-03-10 00:06:49.938210------------------------------------------------------ Precision@1: 59.67% 

[59.61, 59.83, 59.89, 59.74, 59.63, 59.69, 59.85, 59.49, 59.7, 59.77, 59.68, 59.67]

Epoch: 12
2024-03-10 00:06:50.166041 epoch: 12 step: 0 cls_loss= 0.95466 (132195 samples/sec)
2024-03-10 00:06:53.103922 epoch: 12 step: 100 cls_loss= 1.14553 (10211 samples/sec)
saving....
2024-03-10 00:06:56.875396------------------------------------------------------ Precision@1: 59.60% 

[59.61, 59.83, 59.89, 59.74, 59.63, 59.69, 59.85, 59.49, 59.7, 59.77, 59.68, 59.67, 59.6]

Epoch: 13
2024-03-10 00:06:57.083640 epoch: 13 step: 0 cls_loss= 1.04652 (144839 samples/sec)
2024-03-10 00:07:00.034850 epoch: 13 step: 100 cls_loss= 1.03673 (10169 samples/sec)
saving....
2024-03-10 00:07:03.822811------------------------------------------------------ Precision@1: 59.63% 

[59.61, 59.83, 59.89, 59.74, 59.63, 59.69, 59.85, 59.49, 59.7, 59.77, 59.68, 59.67, 59.6, 59.63]

Epoch: 14
2024-03-10 00:07:04.037549 epoch: 14 step: 0 cls_loss= 1.08796 (140230 samples/sec)
2024-03-10 00:07:06.998320 epoch: 14 step: 100 cls_loss= 1.04861 (10133 samples/sec)
saving....
2024-03-10 00:07:10.734675------------------------------------------------------ Precision@1: 59.95% 

[59.61, 59.83, 59.89, 59.74, 59.63, 59.69, 59.85, 59.49, 59.7, 59.77, 59.68, 59.67, 59.6, 59.63, 59.95]

Epoch: 15
2024-03-10 00:07:10.956760 epoch: 15 step: 0 cls_loss= 1.10299 (135720 samples/sec)
2024-03-10 00:07:13.866080 epoch: 15 step: 100 cls_loss= 0.95500 (10312 samples/sec)
saving....
2024-03-10 00:07:17.573224------------------------------------------------------ Precision@1: 59.69% 

[59.61, 59.83, 59.89, 59.74, 59.63, 59.69, 59.85, 59.49, 59.7, 59.77, 59.68, 59.67, 59.6, 59.63, 59.95, 59.69]

Epoch: 16
2024-03-10 00:07:17.792478 epoch: 16 step: 0 cls_loss= 1.00342 (137699 samples/sec)
2024-03-10 00:07:20.713498 epoch: 16 step: 100 cls_loss= 1.06830 (10275 samples/sec)
saving....
2024-03-10 00:07:24.452423------------------------------------------------------ Precision@1: 59.48% 

[59.61, 59.83, 59.89, 59.74, 59.63, 59.69, 59.85, 59.49, 59.7, 59.77, 59.68, 59.67, 59.6, 59.63, 59.95, 59.69, 59.48]

Epoch: 17
2024-03-10 00:07:24.665691 epoch: 17 step: 0 cls_loss= 1.02097 (141447 samples/sec)
2024-03-10 00:07:27.619119 epoch: 17 step: 100 cls_loss= 0.92937 (10158 samples/sec)
saving....
2024-03-10 00:07:31.352306------------------------------------------------------ Precision@1: 59.60% 

[59.61, 59.83, 59.89, 59.74, 59.63, 59.69, 59.85, 59.49, 59.7, 59.77, 59.68, 59.67, 59.6, 59.63, 59.95, 59.69, 59.48, 59.6]

Epoch: 18
2024-03-10 00:07:31.550460 epoch: 18 step: 0 cls_loss= 0.94363 (151921 samples/sec)
2024-03-10 00:07:34.512302 epoch: 18 step: 100 cls_loss= 1.02231 (10129 samples/sec)
saving....
2024-03-10 00:07:38.306952------------------------------------------------------ Precision@1: 59.56% 

[59.61, 59.83, 59.89, 59.74, 59.63, 59.69, 59.85, 59.49, 59.7, 59.77, 59.68, 59.67, 59.6, 59.63, 59.95, 59.69, 59.48, 59.6, 59.56]

Epoch: 19
2024-03-10 00:07:38.488482 epoch: 19 step: 0 cls_loss= 1.08913 (166191 samples/sec)
2024-03-10 00:07:41.393721 epoch: 19 step: 100 cls_loss= 1.10580 (10330 samples/sec)
saving....
2024-03-10 00:07:45.202281------------------------------------------------------ Precision@1: 59.56% 

[59.61, 59.83, 59.89, 59.74, 59.63, 59.69, 59.85, 59.49, 59.7, 59.77, 59.68, 59.67, 59.6, 59.63, 59.95, 59.69, 59.48, 59.6, 59.56, 59.56]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 00:07:47.940292 epoch: 0 step: 0 cls_loss= 1.04363 (48739 samples/sec)
2024-03-10 00:07:50.822557 epoch: 0 step: 100 cls_loss= 1.11203 (10408 samples/sec)
saving....
2024-03-10 00:07:54.714349------------------------------------------------------ Precision@1: 59.52% 

[59.52]

Epoch: 1
2024-03-10 00:07:54.922483 epoch: 1 step: 0 cls_loss= 1.00503 (144974 samples/sec)
2024-03-10 00:07:57.805775 epoch: 1 step: 100 cls_loss= 1.05831 (10410 samples/sec)
saving....
2024-03-10 00:08:01.497316------------------------------------------------------ Precision@1: 59.80% 

[59.52, 59.8]

Epoch: 2
2024-03-10 00:08:01.690866 epoch: 2 step: 0 cls_loss= 1.07899 (155704 samples/sec)
2024-03-10 00:08:04.552033 epoch: 2 step: 100 cls_loss= 1.07283 (10490 samples/sec)
saving....
2024-03-10 00:08:08.202668------------------------------------------------------ Precision@1: 59.52% 

[59.52, 59.8, 59.52]

Epoch: 3
2024-03-10 00:08:08.404633 epoch: 3 step: 0 cls_loss= 0.97172 (149377 samples/sec)
2024-03-10 00:08:11.287476 epoch: 3 step: 100 cls_loss= 1.04517 (10411 samples/sec)
saving....
2024-03-10 00:08:15.033581------------------------------------------------------ Precision@1: 59.86% 

[59.52, 59.8, 59.52, 59.86]

Epoch: 4
2024-03-10 00:08:15.225800 epoch: 4 step: 0 cls_loss= 1.06110 (157028 samples/sec)
2024-03-10 00:08:18.359016 epoch: 4 step: 100 cls_loss= 0.98191 (9577 samples/sec)
saving....
2024-03-10 00:08:22.115896------------------------------------------------------ Precision@1: 59.75% 

[59.52, 59.8, 59.52, 59.86, 59.75]

Epoch: 5
2024-03-10 00:08:22.311180 epoch: 5 step: 0 cls_loss= 1.13889 (154554 samples/sec)
2024-03-10 00:08:25.218093 epoch: 5 step: 100 cls_loss= 1.04181 (10322 samples/sec)
saving....
2024-03-10 00:08:28.917380------------------------------------------------------ Precision@1: 59.80% 

[59.52, 59.8, 59.52, 59.86, 59.75, 59.8]

Epoch: 6
2024-03-10 00:08:29.147457 epoch: 6 step: 0 cls_loss= 1.03922 (130989 samples/sec)
2024-03-10 00:08:32.084146 epoch: 6 step: 100 cls_loss= 1.04417 (10216 samples/sec)
saving....
2024-03-10 00:08:35.789588------------------------------------------------------ Precision@1: 59.79% 

[59.52, 59.8, 59.52, 59.86, 59.75, 59.8, 59.79]

Epoch: 7
2024-03-10 00:08:35.995571 epoch: 7 step: 0 cls_loss= 0.94206 (146411 samples/sec)
2024-03-10 00:08:38.871804 epoch: 7 step: 100 cls_loss= 1.14501 (10435 samples/sec)
saving....
2024-03-10 00:08:42.517556------------------------------------------------------ Precision@1: 59.29% 

[59.52, 59.8, 59.52, 59.86, 59.75, 59.8, 59.79, 59.29]

Epoch: 8
2024-03-10 00:08:42.719398 epoch: 8 step: 0 cls_loss= 1.08405 (149470 samples/sec)
2024-03-10 00:08:45.638294 epoch: 8 step: 100 cls_loss= 1.05363 (10278 samples/sec)
saving....
2024-03-10 00:08:49.362179------------------------------------------------------ Precision@1: 59.71% 

[59.52, 59.8, 59.52, 59.86, 59.75, 59.8, 59.79, 59.29, 59.71]

Epoch: 9
2024-03-10 00:08:49.561570 epoch: 9 step: 0 cls_loss= 1.09798 (151178 samples/sec)
2024-03-10 00:08:52.421349 epoch: 9 step: 100 cls_loss= 1.10722 (10493 samples/sec)
saving....
2024-03-10 00:08:56.125897------------------------------------------------------ Precision@1: 59.62% 

[59.52, 59.8, 59.52, 59.86, 59.75, 59.8, 59.79, 59.29, 59.71, 59.62]

Epoch: 10
2024-03-10 00:08:56.336199 epoch: 10 step: 0 cls_loss= 1.03205 (143188 samples/sec)
2024-03-10 00:08:59.198291 epoch: 10 step: 100 cls_loss= 1.07221 (10486 samples/sec)
saving....
2024-03-10 00:09:02.865956------------------------------------------------------ Precision@1: 59.60% 

[59.52, 59.8, 59.52, 59.86, 59.75, 59.8, 59.79, 59.29, 59.71, 59.62, 59.6]

Epoch: 11
2024-03-10 00:09:03.070148 epoch: 11 step: 0 cls_loss= 1.02819 (147743 samples/sec)
2024-03-10 00:09:05.959619 epoch: 11 step: 100 cls_loss= 1.09497 (10382 samples/sec)
saving....
2024-03-10 00:09:09.624448------------------------------------------------------ Precision@1: 59.52% 

[59.52, 59.8, 59.52, 59.86, 59.75, 59.8, 59.79, 59.29, 59.71, 59.62, 59.6, 59.52]

Epoch: 12
2024-03-10 00:09:09.843482 epoch: 12 step: 0 cls_loss= 1.07508 (137506 samples/sec)
2024-03-10 00:09:12.776218 epoch: 12 step: 100 cls_loss= 1.08058 (10229 samples/sec)
saving....
2024-03-10 00:09:16.557132------------------------------------------------------ Precision@1: 59.66% 

[59.52, 59.8, 59.52, 59.86, 59.75, 59.8, 59.79, 59.29, 59.71, 59.62, 59.6, 59.52, 59.66]

Epoch: 13
2024-03-10 00:09:16.754744 epoch: 13 step: 0 cls_loss= 1.04811 (152604 samples/sec)
2024-03-10 00:09:19.621369 epoch: 13 step: 100 cls_loss= 1.07804 (10467 samples/sec)
saving....
2024-03-10 00:09:23.264134------------------------------------------------------ Precision@1: 59.65% 

[59.52, 59.8, 59.52, 59.86, 59.75, 59.8, 59.79, 59.29, 59.71, 59.62, 59.6, 59.52, 59.66, 59.65]

Epoch: 14
2024-03-10 00:09:23.457324 epoch: 14 step: 0 cls_loss= 1.03745 (156158 samples/sec)
2024-03-10 00:09:26.462886 epoch: 14 step: 100 cls_loss= 1.03039 (9981 samples/sec)
saving....
2024-03-10 00:09:30.186092------------------------------------------------------ Precision@1: 59.56% 

[59.52, 59.8, 59.52, 59.86, 59.75, 59.8, 59.79, 59.29, 59.71, 59.62, 59.6, 59.52, 59.66, 59.65, 59.56]

Epoch: 15
2024-03-10 00:09:30.376844 epoch: 15 step: 0 cls_loss= 1.08027 (158177 samples/sec)
2024-03-10 00:09:33.282686 epoch: 15 step: 100 cls_loss= 1.10740 (10329 samples/sec)
saving....
2024-03-10 00:09:37.086531------------------------------------------------------ Precision@1: 59.71% 

[59.52, 59.8, 59.52, 59.86, 59.75, 59.8, 59.79, 59.29, 59.71, 59.62, 59.6, 59.52, 59.66, 59.65, 59.56, 59.71]

Epoch: 16
2024-03-10 00:09:37.291609 epoch: 16 step: 0 cls_loss= 1.07072 (146921 samples/sec)
2024-03-10 00:09:40.332873 epoch: 16 step: 100 cls_loss= 1.09978 (9864 samples/sec)
saving....
2024-03-10 00:09:44.104159------------------------------------------------------ Precision@1: 59.67% 

[59.52, 59.8, 59.52, 59.86, 59.75, 59.8, 59.79, 59.29, 59.71, 59.62, 59.6, 59.52, 59.66, 59.65, 59.56, 59.71, 59.67]

Epoch: 17
2024-03-10 00:09:44.312557 epoch: 17 step: 0 cls_loss= 1.00282 (144752 samples/sec)
2024-03-10 00:09:47.249045 epoch: 17 step: 100 cls_loss= 0.98563 (10221 samples/sec)
saving....
2024-03-10 00:09:50.972443------------------------------------------------------ Precision@1: 59.47% 

[59.52, 59.8, 59.52, 59.86, 59.75, 59.8, 59.79, 59.29, 59.71, 59.62, 59.6, 59.52, 59.66, 59.65, 59.56, 59.71, 59.67, 59.47]

Epoch: 18
2024-03-10 00:09:51.167935 epoch: 18 step: 0 cls_loss= 1.01412 (154322 samples/sec)
2024-03-10 00:09:54.052022 epoch: 18 step: 100 cls_loss= 1.12177 (10407 samples/sec)
saving....
2024-03-10 00:09:57.737427------------------------------------------------------ Precision@1: 59.58% 

[59.52, 59.8, 59.52, 59.86, 59.75, 59.8, 59.79, 59.29, 59.71, 59.62, 59.6, 59.52, 59.66, 59.65, 59.56, 59.71, 59.67, 59.47, 59.58]

Epoch: 19
2024-03-10 00:09:57.929101 epoch: 19 step: 0 cls_loss= 1.25789 (157430 samples/sec)
2024-03-10 00:10:00.900209 epoch: 19 step: 100 cls_loss= 1.02071 (10099 samples/sec)
saving....
2024-03-10 00:10:04.701292------------------------------------------------------ Precision@1: 59.31% 

[59.52, 59.8, 59.52, 59.86, 59.75, 59.8, 59.79, 59.29, 59.71, 59.62, 59.6, 59.52, 59.66, 59.65, 59.56, 59.71, 59.67, 59.47, 59.58, 59.31]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 00:10:07.424967 epoch: 0 step: 0 cls_loss= 0.98149 (49898 samples/sec)
2024-03-10 00:10:10.388884 epoch: 0 step: 100 cls_loss= 1.02803 (10121 samples/sec)
saving....
2024-03-10 00:10:14.387111------------------------------------------------------ Precision@1: 59.61% 

[59.61]

Epoch: 1
2024-03-10 00:10:14.594315 epoch: 1 step: 0 cls_loss= 1.13805 (145605 samples/sec)
2024-03-10 00:10:17.527158 epoch: 1 step: 100 cls_loss= 1.07721 (10232 samples/sec)
saving....
2024-03-10 00:10:21.240617------------------------------------------------------ Precision@1: 59.60% 

[59.61, 59.6]

Epoch: 2
2024-03-10 00:10:21.439840 epoch: 2 step: 0 cls_loss= 1.04937 (151305 samples/sec)
2024-03-10 00:10:24.579169 epoch: 2 step: 100 cls_loss= 0.93531 (9556 samples/sec)
saving....
2024-03-10 00:10:28.359549------------------------------------------------------ Precision@1: 59.88% 

[59.61, 59.6, 59.88]

Epoch: 3
2024-03-10 00:10:28.568456 epoch: 3 step: 0 cls_loss= 0.98336 (144275 samples/sec)
2024-03-10 00:10:31.453124 epoch: 3 step: 100 cls_loss= 1.03508 (10404 samples/sec)
saving....
2024-03-10 00:10:35.119675------------------------------------------------------ Precision@1: 59.51% 

[59.61, 59.6, 59.88, 59.51]

Epoch: 4
2024-03-10 00:10:35.332772 epoch: 4 step: 0 cls_loss= 1.02000 (141505 samples/sec)
2024-03-10 00:10:38.287420 epoch: 4 step: 100 cls_loss= 1.13216 (10154 samples/sec)
saving....
2024-03-10 00:10:42.002325------------------------------------------------------ Precision@1: 59.76% 

[59.61, 59.6, 59.88, 59.51, 59.76]

Epoch: 5
2024-03-10 00:10:42.197588 epoch: 5 step: 0 cls_loss= 1.07445 (154563 samples/sec)
2024-03-10 00:10:45.133597 epoch: 5 step: 100 cls_loss= 0.89889 (10220 samples/sec)
saving....
2024-03-10 00:10:48.862179------------------------------------------------------ Precision@1: 59.54% 

[59.61, 59.6, 59.88, 59.51, 59.76, 59.54]

Epoch: 6
2024-03-10 00:10:49.059850 epoch: 6 step: 0 cls_loss= 0.98673 (152463 samples/sec)
2024-03-10 00:10:52.052734 epoch: 6 step: 100 cls_loss= 1.02140 (10028 samples/sec)
saving....
2024-03-10 00:10:55.984142------------------------------------------------------ Precision@1: 59.84% 

[59.61, 59.6, 59.88, 59.51, 59.76, 59.54, 59.84]

Epoch: 7
2024-03-10 00:10:56.188054 epoch: 7 step: 0 cls_loss= 1.08747 (147880 samples/sec)
2024-03-10 00:10:59.130501 epoch: 7 step: 100 cls_loss= 1.05198 (10199 samples/sec)
saving....
2024-03-10 00:11:02.832634------------------------------------------------------ Precision@1: 59.61% 

[59.61, 59.6, 59.88, 59.51, 59.76, 59.54, 59.84, 59.61]

Epoch: 8
2024-03-10 00:11:03.036731 epoch: 8 step: 0 cls_loss= 1.10662 (147652 samples/sec)
2024-03-10 00:11:06.023793 epoch: 8 step: 100 cls_loss= 1.03903 (10048 samples/sec)
saving....
2024-03-10 00:11:09.918231------------------------------------------------------ Precision@1: 59.71% 

[59.61, 59.6, 59.88, 59.51, 59.76, 59.54, 59.84, 59.61, 59.71]

Epoch: 9
2024-03-10 00:11:10.121638 epoch: 9 step: 0 cls_loss= 1.15721 (148353 samples/sec)
2024-03-10 00:11:13.037373 epoch: 9 step: 100 cls_loss= 0.93895 (10289 samples/sec)
saving....
2024-03-10 00:11:16.773243------------------------------------------------------ Precision@1: 59.80% 

[59.61, 59.6, 59.88, 59.51, 59.76, 59.54, 59.84, 59.61, 59.71, 59.8]

Epoch: 10
2024-03-10 00:11:16.968804 epoch: 10 step: 0 cls_loss= 1.04595 (154136 samples/sec)
2024-03-10 00:11:19.936467 epoch: 10 step: 100 cls_loss= 0.95206 (10112 samples/sec)
saving....
2024-03-10 00:11:23.664333------------------------------------------------------ Precision@1: 59.75% 

[59.61, 59.6, 59.88, 59.51, 59.76, 59.54, 59.84, 59.61, 59.71, 59.8, 59.75]

Epoch: 11
2024-03-10 00:11:23.857877 epoch: 11 step: 0 cls_loss= 1.06757 (155898 samples/sec)
2024-03-10 00:11:26.835504 epoch: 11 step: 100 cls_loss= 1.06950 (10078 samples/sec)
saving....
2024-03-10 00:11:30.688093------------------------------------------------------ Precision@1: 59.66% 

[59.61, 59.6, 59.88, 59.51, 59.76, 59.54, 59.84, 59.61, 59.71, 59.8, 59.75, 59.66]

Epoch: 12
2024-03-10 00:11:30.898826 epoch: 12 step: 0 cls_loss= 1.05643 (143200 samples/sec)
2024-03-10 00:11:33.791596 epoch: 12 step: 100 cls_loss= 1.05853 (10371 samples/sec)
saving....
2024-03-10 00:11:37.523706------------------------------------------------------ Precision@1: 59.63% 

[59.61, 59.6, 59.88, 59.51, 59.76, 59.54, 59.84, 59.61, 59.71, 59.8, 59.75, 59.66, 59.63]

Epoch: 13
2024-03-10 00:11:37.733579 epoch: 13 step: 0 cls_loss= 1.15324 (143738 samples/sec)
2024-03-10 00:11:40.686632 epoch: 13 step: 100 cls_loss= 1.03186 (10161 samples/sec)
saving....
2024-03-10 00:11:44.421012------------------------------------------------------ Precision@1: 59.58% 

[59.61, 59.6, 59.88, 59.51, 59.76, 59.54, 59.84, 59.61, 59.71, 59.8, 59.75, 59.66, 59.63, 59.58]

Epoch: 14
2024-03-10 00:11:44.629280 epoch: 14 step: 0 cls_loss= 1.13349 (144658 samples/sec)
2024-03-10 00:11:47.567979 epoch: 14 step: 100 cls_loss= 1.00600 (10213 samples/sec)
saving....
2024-03-10 00:11:51.349186------------------------------------------------------ Precision@1: 59.49% 

[59.61, 59.6, 59.88, 59.51, 59.76, 59.54, 59.84, 59.61, 59.71, 59.8, 59.75, 59.66, 59.63, 59.58, 59.49]

Epoch: 15
2024-03-10 00:11:51.550881 epoch: 15 step: 0 cls_loss= 1.09957 (149506 samples/sec)
2024-03-10 00:11:54.459079 epoch: 15 step: 100 cls_loss= 1.03582 (10320 samples/sec)
saving....
2024-03-10 00:11:58.131666------------------------------------------------------ Precision@1: 59.66% 

[59.61, 59.6, 59.88, 59.51, 59.76, 59.54, 59.84, 59.61, 59.71, 59.8, 59.75, 59.66, 59.63, 59.58, 59.49, 59.66]

Epoch: 16
2024-03-10 00:11:58.345648 epoch: 16 step: 0 cls_loss= 0.81787 (140926 samples/sec)
2024-03-10 00:12:01.295878 epoch: 16 step: 100 cls_loss= 1.12921 (10170 samples/sec)
saving....
2024-03-10 00:12:05.097620------------------------------------------------------ Precision@1: 59.57% 

[59.61, 59.6, 59.88, 59.51, 59.76, 59.54, 59.84, 59.61, 59.71, 59.8, 59.75, 59.66, 59.63, 59.58, 59.49, 59.66, 59.57]

Epoch: 17
2024-03-10 00:12:05.278654 epoch: 17 step: 0 cls_loss= 1.10693 (166712 samples/sec)
2024-03-10 00:12:08.334190 epoch: 17 step: 100 cls_loss= 1.14065 (9818 samples/sec)
saving....
2024-03-10 00:12:12.061079------------------------------------------------------ Precision@1: 59.96% 

[59.61, 59.6, 59.88, 59.51, 59.76, 59.54, 59.84, 59.61, 59.71, 59.8, 59.75, 59.66, 59.63, 59.58, 59.49, 59.66, 59.57, 59.96]

Epoch: 18
2024-03-10 00:12:12.262373 epoch: 18 step: 0 cls_loss= 1.13663 (149778 samples/sec)
2024-03-10 00:12:15.187183 epoch: 18 step: 100 cls_loss= 1.01830 (10261 samples/sec)
saving....
2024-03-10 00:12:18.859196------------------------------------------------------ Precision@1: 59.53% 

[59.61, 59.6, 59.88, 59.51, 59.76, 59.54, 59.84, 59.61, 59.71, 59.8, 59.75, 59.66, 59.63, 59.58, 59.49, 59.66, 59.57, 59.96, 59.53]

Epoch: 19
2024-03-10 00:12:19.075649 epoch: 19 step: 0 cls_loss= 1.15333 (139262 samples/sec)
2024-03-10 00:12:22.094895 epoch: 19 step: 100 cls_loss= 1.20003 (9936 samples/sec)
saving....
2024-03-10 00:12:25.814719------------------------------------------------------ Precision@1: 59.67% 

[59.61, 59.6, 59.88, 59.51, 59.76, 59.54, 59.84, 59.61, 59.71, 59.8, 59.75, 59.66, 59.63, 59.58, 59.49, 59.66, 59.57, 59.96, 59.53, 59.67]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 00:12:28.542896 epoch: 0 step: 0 cls_loss= 0.89582 (49701 samples/sec)
2024-03-10 00:12:31.419667 epoch: 0 step: 100 cls_loss= 0.99200 (10428 samples/sec)
saving....
2024-03-10 00:12:35.367228------------------------------------------------------ Precision@1: 59.49% 

[59.49]

Epoch: 1
2024-03-10 00:12:35.574289 epoch: 1 step: 0 cls_loss= 1.14929 (145472 samples/sec)
2024-03-10 00:12:38.561390 epoch: 1 step: 100 cls_loss= 1.00734 (10046 samples/sec)
saving....
2024-03-10 00:12:42.340651------------------------------------------------------ Precision@1: 59.47% 

[59.49, 59.47]

Epoch: 2
2024-03-10 00:12:42.536740 epoch: 2 step: 0 cls_loss= 1.09221 (153936 samples/sec)
2024-03-10 00:12:45.429859 epoch: 2 step: 100 cls_loss= 1.06302 (10374 samples/sec)
saving....
2024-03-10 00:12:49.162979------------------------------------------------------ Precision@1: 59.59% 

[59.49, 59.47, 59.59]

Epoch: 3
2024-03-10 00:12:49.374702 epoch: 3 step: 0 cls_loss= 1.08799 (142594 samples/sec)
2024-03-10 00:12:52.393880 epoch: 3 step: 100 cls_loss= 1.14713 (9940 samples/sec)
saving....
2024-03-10 00:12:56.128332------------------------------------------------------ Precision@1: 59.72% 

[59.49, 59.47, 59.59, 59.72]

Epoch: 4
2024-03-10 00:12:56.350137 epoch: 4 step: 0 cls_loss= 1.03008 (135685 samples/sec)
2024-03-10 00:12:59.279658 epoch: 4 step: 100 cls_loss= 1.17268 (10243 samples/sec)
saving....
2024-03-10 00:13:02.959661------------------------------------------------------ Precision@1: 59.61% 

[59.49, 59.47, 59.59, 59.72, 59.61]

Epoch: 5
2024-03-10 00:13:03.163447 epoch: 5 step: 0 cls_loss= 1.05808 (147763 samples/sec)
2024-03-10 00:13:06.158911 epoch: 5 step: 100 cls_loss= 1.05420 (10017 samples/sec)
saving....
2024-03-10 00:13:09.887250------------------------------------------------------ Precision@1: 59.63% 

[59.49, 59.47, 59.59, 59.72, 59.61, 59.63]

Epoch: 6
2024-03-10 00:13:10.095122 epoch: 6 step: 0 cls_loss= 1.09502 (145060 samples/sec)
2024-03-10 00:13:13.089798 epoch: 6 step: 100 cls_loss= 1.03334 (10018 samples/sec)
saving....
2024-03-10 00:13:16.772569------------------------------------------------------ Precision@1: 59.58% 

[59.49, 59.47, 59.59, 59.72, 59.61, 59.63, 59.58]

Epoch: 7
2024-03-10 00:13:16.975733 epoch: 7 step: 0 cls_loss= 1.06617 (148308 samples/sec)
2024-03-10 00:13:19.926439 epoch: 7 step: 100 cls_loss= 1.05059 (10170 samples/sec)
saving....
2024-03-10 00:13:23.611743------------------------------------------------------ Precision@1: 59.59% 

[59.49, 59.47, 59.59, 59.72, 59.61, 59.63, 59.58, 59.59]

Epoch: 8
2024-03-10 00:13:23.816035 epoch: 8 step: 0 cls_loss= 1.12011 (147635 samples/sec)
2024-03-10 00:13:26.778499 epoch: 8 step: 100 cls_loss= 1.06783 (10129 samples/sec)
saving....
2024-03-10 00:13:30.535536------------------------------------------------------ Precision@1: 59.60% 

[59.49, 59.47, 59.59, 59.72, 59.61, 59.63, 59.58, 59.59, 59.6]

Epoch: 9
2024-03-10 00:13:30.748646 epoch: 9 step: 0 cls_loss= 0.87756 (141597 samples/sec)
2024-03-10 00:13:33.632269 epoch: 9 step: 100 cls_loss= 1.11249 (10408 samples/sec)
saving....
2024-03-10 00:13:37.306129------------------------------------------------------ Precision@1: 59.62% 

[59.49, 59.47, 59.59, 59.72, 59.61, 59.63, 59.58, 59.59, 59.6, 59.62]

Epoch: 10
2024-03-10 00:13:37.504563 epoch: 10 step: 0 cls_loss= 1.14705 (152009 samples/sec)
2024-03-10 00:13:40.394977 epoch: 10 step: 100 cls_loss= 0.99955 (10384 samples/sec)
saving....
2024-03-10 00:13:44.065087------------------------------------------------------ Precision@1: 59.62% 

[59.49, 59.47, 59.59, 59.72, 59.61, 59.63, 59.58, 59.59, 59.6, 59.62, 59.62]

Epoch: 11
2024-03-10 00:13:44.273727 epoch: 11 step: 0 cls_loss= 1.08946 (144569 samples/sec)
2024-03-10 00:13:47.146734 epoch: 11 step: 100 cls_loss= 0.90356 (10447 samples/sec)
saving....
2024-03-10 00:13:50.801870------------------------------------------------------ Precision@1: 59.85% 

[59.49, 59.47, 59.59, 59.72, 59.61, 59.63, 59.58, 59.59, 59.6, 59.62, 59.62, 59.85]

Epoch: 12
2024-03-10 00:13:51.015409 epoch: 12 step: 0 cls_loss= 1.09792 (141232 samples/sec)
2024-03-10 00:13:53.895704 epoch: 12 step: 100 cls_loss= 0.91775 (10420 samples/sec)
saving....
2024-03-10 00:13:57.567285------------------------------------------------------ Precision@1: 59.90% 

[59.49, 59.47, 59.59, 59.72, 59.61, 59.63, 59.58, 59.59, 59.6, 59.62, 59.62, 59.85, 59.9]

Epoch: 13
2024-03-10 00:13:57.775051 epoch: 13 step: 0 cls_loss= 1.01383 (144986 samples/sec)
2024-03-10 00:14:00.775401 epoch: 13 step: 100 cls_loss= 1.04810 (9999 samples/sec)
saving....
2024-03-10 00:14:04.525437------------------------------------------------------ Precision@1: 59.42% 

[59.49, 59.47, 59.59, 59.72, 59.61, 59.63, 59.58, 59.59, 59.6, 59.62, 59.62, 59.85, 59.9, 59.42]

Epoch: 14
2024-03-10 00:14:04.708896 epoch: 14 step: 0 cls_loss= 1.04296 (164508 samples/sec)
2024-03-10 00:14:07.592330 epoch: 14 step: 100 cls_loss= 0.97036 (10408 samples/sec)
saving....
2024-03-10 00:14:11.242006------------------------------------------------------ Precision@1: 59.53% 

[59.49, 59.47, 59.59, 59.72, 59.61, 59.63, 59.58, 59.59, 59.6, 59.62, 59.62, 59.85, 59.9, 59.42, 59.53]

Epoch: 15
2024-03-10 00:14:11.448854 epoch: 15 step: 0 cls_loss= 1.02798 (145807 samples/sec)
2024-03-10 00:14:14.386552 epoch: 15 step: 100 cls_loss= 0.93000 (10212 samples/sec)
saving....
2024-03-10 00:14:18.148558------------------------------------------------------ Precision@1: 59.65% 

[59.49, 59.47, 59.59, 59.72, 59.61, 59.63, 59.58, 59.59, 59.6, 59.62, 59.62, 59.85, 59.9, 59.42, 59.53, 59.65]

Epoch: 16
2024-03-10 00:14:18.354273 epoch: 16 step: 0 cls_loss= 1.09440 (146343 samples/sec)
2024-03-10 00:14:21.293804 epoch: 16 step: 100 cls_loss= 0.97067 (10209 samples/sec)
saving....
2024-03-10 00:14:25.008031------------------------------------------------------ Precision@1: 59.50% 

[59.49, 59.47, 59.59, 59.72, 59.61, 59.63, 59.58, 59.59, 59.6, 59.62, 59.62, 59.85, 59.9, 59.42, 59.53, 59.65, 59.5]

Epoch: 17
2024-03-10 00:14:25.209133 epoch: 17 step: 0 cls_loss= 1.03495 (150086 samples/sec)
2024-03-10 00:14:28.164720 epoch: 17 step: 100 cls_loss= 0.95948 (10154 samples/sec)
saving....
2024-03-10 00:14:31.899840------------------------------------------------------ Precision@1: 59.69% 

[59.49, 59.47, 59.59, 59.72, 59.61, 59.63, 59.58, 59.59, 59.6, 59.62, 59.62, 59.85, 59.9, 59.42, 59.53, 59.65, 59.5, 59.69]

Epoch: 18
2024-03-10 00:14:32.099932 epoch: 18 step: 0 cls_loss= 1.05124 (150503 samples/sec)
2024-03-10 00:14:35.060833 epoch: 18 step: 100 cls_loss= 1.07803 (10133 samples/sec)
saving....
2024-03-10 00:14:38.803962------------------------------------------------------ Precision@1: 59.82% 

[59.49, 59.47, 59.59, 59.72, 59.61, 59.63, 59.58, 59.59, 59.6, 59.62, 59.62, 59.85, 59.9, 59.42, 59.53, 59.65, 59.5, 59.69, 59.82]

Epoch: 19
2024-03-10 00:14:38.997782 epoch: 19 step: 0 cls_loss= 1.02032 (155674 samples/sec)
2024-03-10 00:14:41.944212 epoch: 19 step: 100 cls_loss= 1.01551 (10185 samples/sec)
saving....
2024-03-10 00:14:45.816546------------------------------------------------------ Precision@1: 59.66% 

[59.49, 59.47, 59.59, 59.72, 59.61, 59.63, 59.58, 59.59, 59.6, 59.62, 59.62, 59.85, 59.9, 59.42, 59.53, 59.65, 59.5, 59.69, 59.82, 59.66]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 00:14:48.558462 epoch: 0 step: 0 cls_loss= 0.98931 (51401 samples/sec)
2024-03-10 00:14:51.554020 epoch: 0 step: 100 cls_loss= 0.96100 (10015 samples/sec)
saving....
2024-03-10 00:14:55.521164------------------------------------------------------ Precision@1: 59.45% 

[59.45]

Epoch: 1
2024-03-10 00:14:55.726485 epoch: 1 step: 0 cls_loss= 0.97606 (146852 samples/sec)
2024-03-10 00:14:58.737036 epoch: 1 step: 100 cls_loss= 0.97900 (9965 samples/sec)
saving....
2024-03-10 00:15:02.531199------------------------------------------------------ Precision@1: 60.02% 

[59.45, 60.02]

Epoch: 2
2024-03-10 00:15:02.724841 epoch: 2 step: 0 cls_loss= 0.99325 (155769 samples/sec)
2024-03-10 00:15:05.798159 epoch: 2 step: 100 cls_loss= 1.03832 (9761 samples/sec)
saving....
2024-03-10 00:15:09.621540------------------------------------------------------ Precision@1: 59.89% 

[59.45, 60.02, 59.89]

Epoch: 3
2024-03-10 00:15:09.817114 epoch: 3 step: 0 cls_loss= 1.10692 (154234 samples/sec)
2024-03-10 00:15:12.913793 epoch: 3 step: 100 cls_loss= 1.10486 (9687 samples/sec)
saving....
2024-03-10 00:15:16.723081------------------------------------------------------ Precision@1: 59.70% 

[59.45, 60.02, 59.89, 59.7]

Epoch: 4
2024-03-10 00:15:16.926447 epoch: 4 step: 0 cls_loss= 0.89428 (148378 samples/sec)
2024-03-10 00:15:20.006269 epoch: 4 step: 100 cls_loss= 1.05877 (9741 samples/sec)
saving....
2024-03-10 00:15:23.832438------------------------------------------------------ Precision@1: 59.63% 

[59.45, 60.02, 59.89, 59.7, 59.63]

Epoch: 5
2024-03-10 00:15:24.011648 epoch: 5 step: 0 cls_loss= 1.03459 (168403 samples/sec)
2024-03-10 00:15:27.074279 epoch: 5 step: 100 cls_loss= 0.90169 (9795 samples/sec)
saving....
2024-03-10 00:15:30.853372------------------------------------------------------ Precision@1: 59.73% 

[59.45, 60.02, 59.89, 59.7, 59.63, 59.73]

Epoch: 6
2024-03-10 00:15:31.062018 epoch: 6 step: 0 cls_loss= 0.96822 (144596 samples/sec)
2024-03-10 00:15:34.205564 epoch: 6 step: 100 cls_loss= 0.96816 (9543 samples/sec)
saving....
2024-03-10 00:15:38.016831------------------------------------------------------ Precision@1: 59.95% 

[59.45, 60.02, 59.89, 59.7, 59.63, 59.73, 59.95]

Epoch: 7
2024-03-10 00:15:38.229029 epoch: 7 step: 0 cls_loss= 0.90681 (142159 samples/sec)
2024-03-10 00:15:41.352205 epoch: 7 step: 100 cls_loss= 0.92097 (9605 samples/sec)
saving....
2024-03-10 00:15:45.152817------------------------------------------------------ Precision@1: 59.87% 

[59.45, 60.02, 59.89, 59.7, 59.63, 59.73, 59.95, 59.87]

Epoch: 8
2024-03-10 00:15:45.360280 epoch: 8 step: 0 cls_loss= 1.12862 (145253 samples/sec)
2024-03-10 00:15:48.516391 epoch: 8 step: 100 cls_loss= 0.99211 (9505 samples/sec)
saving....
2024-03-10 00:15:52.343909------------------------------------------------------ Precision@1: 59.62% 

[59.45, 60.02, 59.89, 59.7, 59.63, 59.73, 59.95, 59.87, 59.62]

Epoch: 9
2024-03-10 00:15:52.560927 epoch: 9 step: 0 cls_loss= 1.00128 (138953 samples/sec)
2024-03-10 00:15:55.640317 epoch: 9 step: 100 cls_loss= 1.04152 (9742 samples/sec)
saving....
2024-03-10 00:15:59.461393------------------------------------------------------ Precision@1: 60.07% 

[59.45, 60.02, 59.89, 59.7, 59.63, 59.73, 59.95, 59.87, 59.62, 60.07]

Epoch: 10
2024-03-10 00:15:59.657875 epoch: 10 step: 0 cls_loss= 1.00089 (153448 samples/sec)
2024-03-10 00:16:02.723217 epoch: 10 step: 100 cls_loss= 1.00951 (9787 samples/sec)
saving....
2024-03-10 00:16:06.451268------------------------------------------------------ Precision@1: 59.78% 

[59.45, 60.02, 59.89, 59.7, 59.63, 59.73, 59.95, 59.87, 59.62, 60.07, 59.78]

Epoch: 11
2024-03-10 00:16:06.661166 epoch: 11 step: 0 cls_loss= 0.97034 (143549 samples/sec)
2024-03-10 00:16:09.790797 epoch: 11 step: 100 cls_loss= 0.96736 (9586 samples/sec)
saving....
2024-03-10 00:16:13.646460------------------------------------------------------ Precision@1: 59.68% 

[59.45, 60.02, 59.89, 59.7, 59.63, 59.73, 59.95, 59.87, 59.62, 60.07, 59.78, 59.68]

Epoch: 12
2024-03-10 00:16:13.847745 epoch: 12 step: 0 cls_loss= 1.13055 (149925 samples/sec)
2024-03-10 00:16:16.844312 epoch: 12 step: 100 cls_loss= 0.93265 (10011 samples/sec)
saving....
2024-03-10 00:16:20.607126------------------------------------------------------ Precision@1: 59.82% 

[59.45, 60.02, 59.89, 59.7, 59.63, 59.73, 59.95, 59.87, 59.62, 60.07, 59.78, 59.68, 59.82]

Epoch: 13
2024-03-10 00:16:20.813075 epoch: 13 step: 0 cls_loss= 0.84727 (146470 samples/sec)
2024-03-10 00:16:23.837946 epoch: 13 step: 100 cls_loss= 0.90545 (9917 samples/sec)
saving....
2024-03-10 00:16:27.701972------------------------------------------------------ Precision@1: 59.76% 

[59.45, 60.02, 59.89, 59.7, 59.63, 59.73, 59.95, 59.87, 59.62, 60.07, 59.78, 59.68, 59.82, 59.76]

Epoch: 14
2024-03-10 00:16:27.915404 epoch: 14 step: 0 cls_loss= 1.10094 (141285 samples/sec)
2024-03-10 00:16:30.939136 epoch: 14 step: 100 cls_loss= 1.01715 (9921 samples/sec)
saving....
2024-03-10 00:16:34.694227------------------------------------------------------ Precision@1: 59.76% 

[59.45, 60.02, 59.89, 59.7, 59.63, 59.73, 59.95, 59.87, 59.62, 60.07, 59.78, 59.68, 59.82, 59.76, 59.76]

Epoch: 15
2024-03-10 00:16:34.885335 epoch: 15 step: 0 cls_loss= 0.96909 (157940 samples/sec)
2024-03-10 00:16:37.880423 epoch: 15 step: 100 cls_loss= 0.98156 (10016 samples/sec)
saving....
2024-03-10 00:16:41.626812------------------------------------------------------ Precision@1: 59.74% 

[59.45, 60.02, 59.89, 59.7, 59.63, 59.73, 59.95, 59.87, 59.62, 60.07, 59.78, 59.68, 59.82, 59.76, 59.76, 59.74]

Epoch: 16
2024-03-10 00:16:41.830732 epoch: 16 step: 0 cls_loss= 0.89167 (147909 samples/sec)
2024-03-10 00:16:44.987361 epoch: 16 step: 100 cls_loss= 0.90108 (9504 samples/sec)
saving....
2024-03-10 00:16:48.867746------------------------------------------------------ Precision@1: 59.80% 

[59.45, 60.02, 59.89, 59.7, 59.63, 59.73, 59.95, 59.87, 59.62, 60.07, 59.78, 59.68, 59.82, 59.76, 59.76, 59.74, 59.8]

Epoch: 17
2024-03-10 00:16:49.073564 epoch: 17 step: 0 cls_loss= 1.00226 (146499 samples/sec)
2024-03-10 00:16:52.198733 epoch: 17 step: 100 cls_loss= 1.03112 (9599 samples/sec)
saving....
2024-03-10 00:16:55.995771------------------------------------------------------ Precision@1: 59.75% 

[59.45, 60.02, 59.89, 59.7, 59.63, 59.73, 59.95, 59.87, 59.62, 60.07, 59.78, 59.68, 59.82, 59.76, 59.76, 59.74, 59.8, 59.75]

Epoch: 18
2024-03-10 00:16:56.203962 epoch: 18 step: 0 cls_loss= 0.97353 (144689 samples/sec)
2024-03-10 00:16:59.222975 epoch: 18 step: 100 cls_loss= 1.00203 (9937 samples/sec)
saving....
2024-03-10 00:17:03.004018------------------------------------------------------ Precision@1: 59.74% 

[59.45, 60.02, 59.89, 59.7, 59.63, 59.73, 59.95, 59.87, 59.62, 60.07, 59.78, 59.68, 59.82, 59.76, 59.76, 59.74, 59.8, 59.75, 59.74]

Epoch: 19
2024-03-10 00:17:03.216501 epoch: 19 step: 0 cls_loss= 0.83554 (141937 samples/sec)
2024-03-10 00:17:06.340062 epoch: 19 step: 100 cls_loss= 1.09233 (9604 samples/sec)
saving....
2024-03-10 00:17:10.210881------------------------------------------------------ Precision@1: 59.57% 

[59.45, 60.02, 59.89, 59.7, 59.63, 59.73, 59.95, 59.87, 59.62, 60.07, 59.78, 59.68, 59.82, 59.76, 59.76, 59.74, 59.8, 59.75, 59.74, 59.57]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 00:17:12.940539 epoch: 0 step: 0 cls_loss= 1.09175 (48724 samples/sec)
2024-03-10 00:17:15.938251 epoch: 0 step: 100 cls_loss= 1.05252 (10007 samples/sec)
saving....
2024-03-10 00:17:19.971206------------------------------------------------------ Precision@1: 59.53% 

[59.53]

Epoch: 1
2024-03-10 00:17:20.178997 epoch: 1 step: 0 cls_loss= 1.12390 (145082 samples/sec)
2024-03-10 00:17:23.182938 epoch: 1 step: 100 cls_loss= 0.93004 (9987 samples/sec)
saving....
2024-03-10 00:17:26.971686------------------------------------------------------ Precision@1: 59.70% 

[59.53, 59.7]

Epoch: 2
2024-03-10 00:17:27.171451 epoch: 2 step: 0 cls_loss= 0.92474 (150985 samples/sec)
2024-03-10 00:17:30.167749 epoch: 2 step: 100 cls_loss= 0.97437 (10012 samples/sec)
saving....
2024-03-10 00:17:33.918664------------------------------------------------------ Precision@1: 59.68% 

[59.53, 59.7, 59.68]

Epoch: 3
2024-03-10 00:17:34.127293 epoch: 3 step: 0 cls_loss= 1.01100 (144438 samples/sec)
2024-03-10 00:17:37.114062 epoch: 3 step: 100 cls_loss= 0.87667 (10044 samples/sec)
saving....
2024-03-10 00:17:40.845576------------------------------------------------------ Precision@1: 59.63% 

[59.53, 59.7, 59.68, 59.63]

Epoch: 4
2024-03-10 00:17:41.036128 epoch: 4 step: 0 cls_loss= 1.15745 (158082 samples/sec)
2024-03-10 00:17:44.150292 epoch: 4 step: 100 cls_loss= 1.04129 (9633 samples/sec)
saving....
2024-03-10 00:17:48.003388------------------------------------------------------ Precision@1: 59.71% 

[59.53, 59.7, 59.68, 59.63, 59.71]

Epoch: 5
2024-03-10 00:17:48.197416 epoch: 5 step: 0 cls_loss= 1.10844 (155444 samples/sec)
2024-03-10 00:17:51.227674 epoch: 5 step: 100 cls_loss= 1.14490 (9900 samples/sec)
saving....
2024-03-10 00:17:54.968770------------------------------------------------------ Precision@1: 59.93% 

[59.53, 59.7, 59.68, 59.63, 59.71, 59.93]

Epoch: 6
2024-03-10 00:17:55.183373 epoch: 6 step: 0 cls_loss= 1.01066 (140513 samples/sec)
2024-03-10 00:17:58.288274 epoch: 6 step: 100 cls_loss= 0.96107 (9662 samples/sec)
saving....
2024-03-10 00:18:02.125586------------------------------------------------------ Precision@1: 59.86% 

[59.53, 59.7, 59.68, 59.63, 59.71, 59.93, 59.86]

Epoch: 7
2024-03-10 00:18:02.337584 epoch: 7 step: 0 cls_loss= 1.05398 (142111 samples/sec)
2024-03-10 00:18:05.416658 epoch: 7 step: 100 cls_loss= 1.02672 (9743 samples/sec)
saving....
2024-03-10 00:18:09.222588------------------------------------------------------ Precision@1: 59.64% 

[59.53, 59.7, 59.68, 59.63, 59.71, 59.93, 59.86, 59.64]

Epoch: 8
2024-03-10 00:18:09.443668 epoch: 8 step: 0 cls_loss= 0.97211 (136258 samples/sec)
2024-03-10 00:18:12.419029 epoch: 8 step: 100 cls_loss= 0.99535 (10083 samples/sec)
saving....
2024-03-10 00:18:16.190428------------------------------------------------------ Precision@1: 59.88% 

[59.53, 59.7, 59.68, 59.63, 59.71, 59.93, 59.86, 59.64, 59.88]

Epoch: 9
2024-03-10 00:18:16.396822 epoch: 9 step: 0 cls_loss= 0.90147 (145993 samples/sec)
2024-03-10 00:18:19.382020 epoch: 9 step: 100 cls_loss= 1.09516 (10049 samples/sec)
saving....
2024-03-10 00:18:23.082518------------------------------------------------------ Precision@1: 59.89% 

[59.53, 59.7, 59.68, 59.63, 59.71, 59.93, 59.86, 59.64, 59.88, 59.89]

Epoch: 10
2024-03-10 00:18:23.291861 epoch: 10 step: 0 cls_loss= 0.99208 (144013 samples/sec)
2024-03-10 00:18:26.457826 epoch: 10 step: 100 cls_loss= 0.97910 (9475 samples/sec)
saving....
2024-03-10 00:18:30.274814------------------------------------------------------ Precision@1: 59.79% 

[59.53, 59.7, 59.68, 59.63, 59.71, 59.93, 59.86, 59.64, 59.88, 59.89, 59.79]

Epoch: 11
2024-03-10 00:18:30.474526 epoch: 11 step: 0 cls_loss= 1.03369 (150931 samples/sec)
2024-03-10 00:18:33.633461 epoch: 11 step: 100 cls_loss= 0.99636 (9497 samples/sec)
saving....
2024-03-10 00:18:37.435715------------------------------------------------------ Precision@1: 59.92% 

[59.53, 59.7, 59.68, 59.63, 59.71, 59.93, 59.86, 59.64, 59.88, 59.89, 59.79, 59.92]

Epoch: 12
2024-03-10 00:18:37.632705 epoch: 12 step: 0 cls_loss= 1.05613 (153222 samples/sec)
2024-03-10 00:18:40.685312 epoch: 12 step: 100 cls_loss= 0.99621 (9827 samples/sec)
saving....
2024-03-10 00:18:44.517898------------------------------------------------------ Precision@1: 60.12% 

[59.53, 59.7, 59.68, 59.63, 59.71, 59.93, 59.86, 59.64, 59.88, 59.89, 59.79, 59.92, 60.12]

Epoch: 13
2024-03-10 00:18:44.725357 epoch: 13 step: 0 cls_loss= 1.02310 (145185 samples/sec)
2024-03-10 00:18:47.792198 epoch: 13 step: 100 cls_loss= 0.92758 (9782 samples/sec)
saving....
2024-03-10 00:18:51.604641------------------------------------------------------ Precision@1: 59.90% 

[59.53, 59.7, 59.68, 59.63, 59.71, 59.93, 59.86, 59.64, 59.88, 59.89, 59.79, 59.92, 60.12, 59.9]

Epoch: 14
2024-03-10 00:18:51.805931 epoch: 14 step: 0 cls_loss= 0.84441 (149774 samples/sec)
2024-03-10 00:18:54.823419 epoch: 14 step: 100 cls_loss= 0.94817 (9942 samples/sec)
saving....
2024-03-10 00:18:58.591438------------------------------------------------------ Precision@1: 59.76% 

[59.53, 59.7, 59.68, 59.63, 59.71, 59.93, 59.86, 59.64, 59.88, 59.89, 59.79, 59.92, 60.12, 59.9, 59.76]

Epoch: 15
2024-03-10 00:18:58.788948 epoch: 15 step: 0 cls_loss= 0.89344 (152830 samples/sec)
2024-03-10 00:19:01.888118 epoch: 15 step: 100 cls_loss= 0.95789 (9680 samples/sec)
saving....
2024-03-10 00:19:05.631512------------------------------------------------------ Precision@1: 60.10% 

[59.53, 59.7, 59.68, 59.63, 59.71, 59.93, 59.86, 59.64, 59.88, 59.89, 59.79, 59.92, 60.12, 59.9, 59.76, 60.1]

Epoch: 16
2024-03-10 00:19:05.832221 epoch: 16 step: 0 cls_loss= 1.02069 (150223 samples/sec)
2024-03-10 00:19:08.946369 epoch: 16 step: 100 cls_loss= 1.13537 (9633 samples/sec)
saving....
2024-03-10 00:19:12.759596------------------------------------------------------ Precision@1: 59.82% 

[59.53, 59.7, 59.68, 59.63, 59.71, 59.93, 59.86, 59.64, 59.88, 59.89, 59.79, 59.92, 60.12, 59.9, 59.76, 60.1, 59.82]

Epoch: 17
2024-03-10 00:19:12.963372 epoch: 17 step: 0 cls_loss= 0.94825 (147788 samples/sec)
2024-03-10 00:19:16.030607 epoch: 17 step: 100 cls_loss= 0.94821 (9781 samples/sec)
saving....
2024-03-10 00:19:19.798689------------------------------------------------------ Precision@1: 59.78% 

[59.53, 59.7, 59.68, 59.63, 59.71, 59.93, 59.86, 59.64, 59.88, 59.89, 59.79, 59.92, 60.12, 59.9, 59.76, 60.1, 59.82, 59.78]

Epoch: 18
2024-03-10 00:19:20.012609 epoch: 18 step: 0 cls_loss= 0.97357 (140780 samples/sec)
2024-03-10 00:19:23.068819 epoch: 18 step: 100 cls_loss= 1.00127 (9816 samples/sec)
saving....
2024-03-10 00:19:26.811973------------------------------------------------------ Precision@1: 59.84% 

[59.53, 59.7, 59.68, 59.63, 59.71, 59.93, 59.86, 59.64, 59.88, 59.89, 59.79, 59.92, 60.12, 59.9, 59.76, 60.1, 59.82, 59.78, 59.84]

Epoch: 19
2024-03-10 00:19:27.028418 epoch: 19 step: 0 cls_loss= 0.95314 (139340 samples/sec)
2024-03-10 00:19:30.030766 epoch: 19 step: 100 cls_loss= 0.95916 (9992 samples/sec)
saving....
2024-03-10 00:19:33.816649------------------------------------------------------ Precision@1: 60.04% 

[59.53, 59.7, 59.68, 59.63, 59.71, 59.93, 59.86, 59.64, 59.88, 59.89, 59.79, 59.92, 60.12, 59.9, 59.76, 60.1, 59.82, 59.78, 59.84, 60.04]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 00:19:36.549635 epoch: 0 step: 0 cls_loss= 1.02016 (48979 samples/sec)
2024-03-10 00:19:39.595879 epoch: 0 step: 100 cls_loss= 0.97281 (9848 samples/sec)
saving....
2024-03-10 00:19:43.609008------------------------------------------------------ Precision@1: 59.79% 

[59.79]

Epoch: 1
2024-03-10 00:19:43.840643 epoch: 1 step: 0 cls_loss= 1.01928 (130128 samples/sec)
2024-03-10 00:19:46.836077 epoch: 1 step: 100 cls_loss= 1.03567 (10015 samples/sec)
saving....
2024-03-10 00:19:50.630784------------------------------------------------------ Precision@1: 59.62% 

[59.79, 59.62]

Epoch: 2
2024-03-10 00:19:50.818352 epoch: 2 step: 0 cls_loss= 0.93760 (160943 samples/sec)
2024-03-10 00:19:53.837894 epoch: 2 step: 100 cls_loss= 1.02653 (9935 samples/sec)
saving....
2024-03-10 00:19:57.673649------------------------------------------------------ Precision@1: 59.80% 

[59.79, 59.62, 59.8]

Epoch: 3
2024-03-10 00:19:57.875475 epoch: 3 step: 0 cls_loss= 1.01510 (149475 samples/sec)
2024-03-10 00:20:01.046130 epoch: 3 step: 100 cls_loss= 1.06653 (9461 samples/sec)
saving....
2024-03-10 00:20:04.982143------------------------------------------------------ Precision@1: 59.76% 

[59.79, 59.62, 59.8, 59.76]

Epoch: 4
2024-03-10 00:20:05.205159 epoch: 4 step: 0 cls_loss= 1.02981 (135041 samples/sec)
2024-03-10 00:20:08.313901 epoch: 4 step: 100 cls_loss= 1.17297 (9650 samples/sec)
saving....
2024-03-10 00:20:12.169095------------------------------------------------------ Precision@1: 59.76% 

[59.79, 59.62, 59.8, 59.76, 59.76]

Epoch: 5
2024-03-10 00:20:12.371581 epoch: 5 step: 0 cls_loss= 1.06679 (148984 samples/sec)
2024-03-10 00:20:15.469875 epoch: 5 step: 100 cls_loss= 1.03445 (9682 samples/sec)
saving....
2024-03-10 00:20:19.284261------------------------------------------------------ Precision@1: 59.82% 

[59.79, 59.62, 59.8, 59.76, 59.76, 59.82]

Epoch: 6
2024-03-10 00:20:19.473664 epoch: 6 step: 0 cls_loss= 0.89023 (159211 samples/sec)
2024-03-10 00:20:22.621418 epoch: 6 step: 100 cls_loss= 0.98991 (9530 samples/sec)
saving....
2024-03-10 00:20:26.439339------------------------------------------------------ Precision@1: 59.95% 

[59.79, 59.62, 59.8, 59.76, 59.76, 59.82, 59.95]

Epoch: 7
2024-03-10 00:20:26.623285 epoch: 7 step: 0 cls_loss= 1.05980 (164071 samples/sec)
2024-03-10 00:20:29.731577 epoch: 7 step: 100 cls_loss= 0.94902 (9651 samples/sec)
saving....
2024-03-10 00:20:33.554111------------------------------------------------------ Precision@1: 59.75% 

[59.79, 59.62, 59.8, 59.76, 59.76, 59.82, 59.95, 59.75]

Epoch: 8
2024-03-10 00:20:33.735039 epoch: 8 step: 0 cls_loss= 0.99016 (166833 samples/sec)
2024-03-10 00:20:36.761053 epoch: 8 step: 100 cls_loss= 1.04950 (9914 samples/sec)
saving....
2024-03-10 00:20:40.523351------------------------------------------------------ Precision@1: 59.78% 

[59.79, 59.62, 59.8, 59.76, 59.76, 59.82, 59.95, 59.75, 59.78]

Epoch: 9
2024-03-10 00:20:40.721363 epoch: 9 step: 0 cls_loss= 1.04830 (152296 samples/sec)
2024-03-10 00:20:43.759801 epoch: 9 step: 100 cls_loss= 0.93225 (9873 samples/sec)
saving....
2024-03-10 00:20:47.585619------------------------------------------------------ Precision@1: 59.59% 

[59.79, 59.62, 59.8, 59.76, 59.76, 59.82, 59.95, 59.75, 59.78, 59.59]

Epoch: 10
2024-03-10 00:20:47.797969 epoch: 10 step: 0 cls_loss= 0.83788 (142104 samples/sec)
2024-03-10 00:20:50.804511 epoch: 10 step: 100 cls_loss= 0.78674 (9978 samples/sec)
saving....
2024-03-10 00:20:54.742394------------------------------------------------------ Precision@1: 59.75% 

[59.79, 59.62, 59.8, 59.76, 59.76, 59.82, 59.95, 59.75, 59.78, 59.59, 59.75]

Epoch: 11
2024-03-10 00:20:54.935160 epoch: 11 step: 0 cls_loss= 0.94954 (156444 samples/sec)
2024-03-10 00:20:58.026634 epoch: 11 step: 100 cls_loss= 0.95787 (9704 samples/sec)
saving....
2024-03-10 00:21:01.840279------------------------------------------------------ Precision@1: 59.80% 

[59.79, 59.62, 59.8, 59.76, 59.76, 59.82, 59.95, 59.75, 59.78, 59.59, 59.75, 59.8]

Epoch: 12
2024-03-10 00:21:02.053971 epoch: 12 step: 0 cls_loss= 0.86036 (141148 samples/sec)
2024-03-10 00:21:05.301101 epoch: 12 step: 100 cls_loss= 0.98577 (9239 samples/sec)
saving....
2024-03-10 00:21:09.245905------------------------------------------------------ Precision@1: 59.72% 

[59.79, 59.62, 59.8, 59.76, 59.76, 59.82, 59.95, 59.75, 59.78, 59.59, 59.75, 59.8, 59.72]

Epoch: 13
2024-03-10 00:21:09.467065 epoch: 13 step: 0 cls_loss= 0.77302 (136300 samples/sec)
2024-03-10 00:21:12.644227 epoch: 13 step: 100 cls_loss= 1.08832 (9442 samples/sec)
saving....
2024-03-10 00:21:16.537849------------------------------------------------------ Precision@1: 59.49% 

[59.79, 59.62, 59.8, 59.76, 59.76, 59.82, 59.95, 59.75, 59.78, 59.59, 59.75, 59.8, 59.72, 59.49]

Epoch: 14
2024-03-10 00:21:16.752443 epoch: 14 step: 0 cls_loss= 0.96281 (140429 samples/sec)
2024-03-10 00:21:19.792267 epoch: 14 step: 100 cls_loss= 1.08303 (9869 samples/sec)
saving....
2024-03-10 00:21:23.556620------------------------------------------------------ Precision@1: 59.85% 

[59.79, 59.62, 59.8, 59.76, 59.76, 59.82, 59.95, 59.75, 59.78, 59.59, 59.75, 59.8, 59.72, 59.49, 59.85]

Epoch: 15
2024-03-10 00:21:23.751527 epoch: 15 step: 0 cls_loss= 0.98706 (154791 samples/sec)
2024-03-10 00:21:26.787815 epoch: 15 step: 100 cls_loss= 0.85294 (9880 samples/sec)
saving....
2024-03-10 00:21:30.596037------------------------------------------------------ Precision@1: 59.63% 

[59.79, 59.62, 59.8, 59.76, 59.76, 59.82, 59.95, 59.75, 59.78, 59.59, 59.75, 59.8, 59.72, 59.49, 59.85, 59.63]

Epoch: 16
2024-03-10 00:21:30.788190 epoch: 16 step: 0 cls_loss= 0.85486 (157106 samples/sec)
2024-03-10 00:21:33.822868 epoch: 16 step: 100 cls_loss= 0.85959 (9885 samples/sec)
saving....
2024-03-10 00:21:37.587417------------------------------------------------------ Precision@1: 59.82% 

[59.79, 59.62, 59.8, 59.76, 59.76, 59.82, 59.95, 59.75, 59.78, 59.59, 59.75, 59.8, 59.72, 59.49, 59.85, 59.63, 59.82]

Epoch: 17
2024-03-10 00:21:37.788077 epoch: 17 step: 0 cls_loss= 0.93932 (150528 samples/sec)
2024-03-10 00:21:40.788943 epoch: 17 step: 100 cls_loss= 1.01255 (9997 samples/sec)
saving....
2024-03-10 00:21:44.569723------------------------------------------------------ Precision@1: 59.84% 

[59.79, 59.62, 59.8, 59.76, 59.76, 59.82, 59.95, 59.75, 59.78, 59.59, 59.75, 59.8, 59.72, 59.49, 59.85, 59.63, 59.82, 59.84]

Epoch: 18
2024-03-10 00:21:44.766759 epoch: 18 step: 0 cls_loss= 0.79742 (153076 samples/sec)
2024-03-10 00:21:47.881926 epoch: 18 step: 100 cls_loss= 0.97598 (9630 samples/sec)
saving....
2024-03-10 00:21:51.642322------------------------------------------------------ Precision@1: 59.66% 

[59.79, 59.62, 59.8, 59.76, 59.76, 59.82, 59.95, 59.75, 59.78, 59.59, 59.75, 59.8, 59.72, 59.49, 59.85, 59.63, 59.82, 59.84, 59.66]

Epoch: 19
2024-03-10 00:21:51.843610 epoch: 19 step: 0 cls_loss= 1.02198 (149905 samples/sec)
2024-03-10 00:21:54.904886 epoch: 19 step: 100 cls_loss= 0.89891 (9800 samples/sec)
saving....
2024-03-10 00:21:58.804768------------------------------------------------------ Precision@1: 59.97% 

[59.79, 59.62, 59.8, 59.76, 59.76, 59.82, 59.95, 59.75, 59.78, 59.59, 59.75, 59.8, 59.72, 59.49, 59.85, 59.63, 59.82, 59.84, 59.66, 59.97]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 00:22:01.519616 epoch: 0 step: 0 cls_loss= 0.99796 (49480 samples/sec)
2024-03-10 00:22:04.690785 epoch: 0 step: 100 cls_loss= 1.07162 (9460 samples/sec)
saving....
2024-03-10 00:22:08.779956------------------------------------------------------ Precision@1: 59.72% 

[59.72]

Epoch: 1
2024-03-10 00:22:08.984083 epoch: 1 step: 0 cls_loss= 0.95151 (147744 samples/sec)
2024-03-10 00:22:12.040022 epoch: 1 step: 100 cls_loss= 1.07068 (9817 samples/sec)
saving....
2024-03-10 00:22:15.857100------------------------------------------------------ Precision@1: 59.62% 

[59.72, 59.62]

Epoch: 2
2024-03-10 00:22:16.069167 epoch: 2 step: 0 cls_loss= 0.96403 (142193 samples/sec)
2024-03-10 00:22:19.077476 epoch: 2 step: 100 cls_loss= 1.12929 (9972 samples/sec)
saving....
2024-03-10 00:22:22.863574------------------------------------------------------ Precision@1: 59.65% 

[59.72, 59.62, 59.65]

Epoch: 3
2024-03-10 00:22:23.079313 epoch: 3 step: 0 cls_loss= 1.04928 (139759 samples/sec)
2024-03-10 00:22:26.116545 epoch: 3 step: 100 cls_loss= 1.02385 (9877 samples/sec)
saving....
2024-03-10 00:22:29.891440------------------------------------------------------ Precision@1: 59.59% 

[59.72, 59.62, 59.65, 59.59]

Epoch: 4
2024-03-10 00:22:30.094831 epoch: 4 step: 0 cls_loss= 0.85830 (148218 samples/sec)
2024-03-10 00:22:33.256478 epoch: 4 step: 100 cls_loss= 0.97322 (9488 samples/sec)
saving....
2024-03-10 00:22:37.117733------------------------------------------------------ Precision@1: 59.71% 

[59.72, 59.62, 59.65, 59.59, 59.71]

Epoch: 5
2024-03-10 00:22:37.324924 epoch: 5 step: 0 cls_loss= 1.12534 (145472 samples/sec)
2024-03-10 00:22:40.463057 epoch: 5 step: 100 cls_loss= 1.15023 (9560 samples/sec)
saving....
2024-03-10 00:22:44.331537------------------------------------------------------ Precision@1: 59.92% 

[59.72, 59.62, 59.65, 59.59, 59.71, 59.92]

Epoch: 6
2024-03-10 00:22:44.531586 epoch: 6 step: 0 cls_loss= 0.98447 (150640 samples/sec)
2024-03-10 00:22:47.765632 epoch: 6 step: 100 cls_loss= 1.18395 (9277 samples/sec)
saving....
2024-03-10 00:22:51.611731------------------------------------------------------ Precision@1: 59.77% 

[59.72, 59.62, 59.65, 59.59, 59.71, 59.92, 59.77]

Epoch: 7
2024-03-10 00:22:51.799771 epoch: 7 step: 0 cls_loss= 1.06926 (160453 samples/sec)
2024-03-10 00:22:54.949327 epoch: 7 step: 100 cls_loss= 0.93511 (9525 samples/sec)
saving....
2024-03-10 00:22:58.922090------------------------------------------------------ Precision@1: 59.61% 

[59.72, 59.62, 59.65, 59.59, 59.71, 59.92, 59.77, 59.61]

Epoch: 8
2024-03-10 00:22:59.111164 epoch: 8 step: 0 cls_loss= 1.05491 (159580 samples/sec)
2024-03-10 00:23:02.177576 epoch: 8 step: 100 cls_loss= 1.08759 (9783 samples/sec)
saving....
2024-03-10 00:23:05.995613------------------------------------------------------ Precision@1: 59.66% 

[59.72, 59.62, 59.65, 59.59, 59.71, 59.92, 59.77, 59.61, 59.66]

Epoch: 9
2024-03-10 00:23:06.194453 epoch: 9 step: 0 cls_loss= 0.93221 (151623 samples/sec)
2024-03-10 00:23:09.372768 epoch: 9 step: 100 cls_loss= 1.01517 (9439 samples/sec)
saving....
2024-03-10 00:23:13.257965------------------------------------------------------ Precision@1: 59.63% 

[59.72, 59.62, 59.65, 59.59, 59.71, 59.92, 59.77, 59.61, 59.66, 59.63]

Epoch: 10
2024-03-10 00:23:13.461776 epoch: 10 step: 0 cls_loss= 1.08700 (147935 samples/sec)
2024-03-10 00:23:16.578958 epoch: 10 step: 100 cls_loss= 0.99834 (9624 samples/sec)
saving....
2024-03-10 00:23:20.434363------------------------------------------------------ Precision@1: 59.43% 

[59.72, 59.62, 59.65, 59.59, 59.71, 59.92, 59.77, 59.61, 59.66, 59.63, 59.43]

Epoch: 11
2024-03-10 00:23:20.637470 epoch: 11 step: 0 cls_loss= 0.86665 (148587 samples/sec)
2024-03-10 00:23:23.731912 epoch: 11 step: 100 cls_loss= 1.02862 (9695 samples/sec)
saving....
2024-03-10 00:23:27.587523------------------------------------------------------ Precision@1: 59.81% 

[59.72, 59.62, 59.65, 59.59, 59.71, 59.92, 59.77, 59.61, 59.66, 59.63, 59.43, 59.81]

Epoch: 12
2024-03-10 00:23:27.786340 epoch: 12 step: 0 cls_loss= 1.04956 (151816 samples/sec)
2024-03-10 00:23:30.855473 epoch: 12 step: 100 cls_loss= 1.05471 (9774 samples/sec)
saving....
2024-03-10 00:23:34.673889------------------------------------------------------ Precision@1: 59.99% 

[59.72, 59.62, 59.65, 59.59, 59.71, 59.92, 59.77, 59.61, 59.66, 59.63, 59.43, 59.81, 59.99]

Epoch: 13
2024-03-10 00:23:34.896332 epoch: 13 step: 0 cls_loss= 0.99574 (135567 samples/sec)
2024-03-10 00:23:37.970184 epoch: 13 step: 100 cls_loss= 0.92714 (9759 samples/sec)
saving....
2024-03-10 00:23:41.824956------------------------------------------------------ Precision@1: 59.93% 

[59.72, 59.62, 59.65, 59.59, 59.71, 59.92, 59.77, 59.61, 59.66, 59.63, 59.43, 59.81, 59.99, 59.93]

Epoch: 14
2024-03-10 00:23:42.038648 epoch: 14 step: 0 cls_loss= 1.05841 (141067 samples/sec)
2024-03-10 00:23:45.068644 epoch: 14 step: 100 cls_loss= 1.07448 (9901 samples/sec)
saving....
2024-03-10 00:23:48.850675------------------------------------------------------ Precision@1: 59.59% 

[59.72, 59.62, 59.65, 59.59, 59.71, 59.92, 59.77, 59.61, 59.66, 59.63, 59.43, 59.81, 59.99, 59.93, 59.59]

Epoch: 15
2024-03-10 00:23:49.052023 epoch: 15 step: 0 cls_loss= 0.93782 (149818 samples/sec)
2024-03-10 00:23:52.172641 epoch: 15 step: 100 cls_loss= 0.99380 (9613 samples/sec)
saving....
2024-03-10 00:23:56.113593------------------------------------------------------ Precision@1: 59.62% 

[59.72, 59.62, 59.65, 59.59, 59.71, 59.92, 59.77, 59.61, 59.66, 59.63, 59.43, 59.81, 59.99, 59.93, 59.59, 59.62]

Epoch: 16
2024-03-10 00:23:56.317183 epoch: 16 step: 0 cls_loss= 0.94174 (148244 samples/sec)
2024-03-10 00:23:59.390573 epoch: 16 step: 100 cls_loss= 0.99689 (9761 samples/sec)
saving....
2024-03-10 00:24:03.284100------------------------------------------------------ Precision@1: 59.61% 

[59.72, 59.62, 59.65, 59.59, 59.71, 59.92, 59.77, 59.61, 59.66, 59.63, 59.43, 59.81, 59.99, 59.93, 59.59, 59.62, 59.61]

Epoch: 17
2024-03-10 00:24:03.498955 epoch: 17 step: 0 cls_loss= 0.96956 (140214 samples/sec)
2024-03-10 00:24:06.659910 epoch: 17 step: 100 cls_loss= 0.89966 (9491 samples/sec)
saving....
2024-03-10 00:24:10.544880------------------------------------------------------ Precision@1: 60.01% 

[59.72, 59.62, 59.65, 59.59, 59.71, 59.92, 59.77, 59.61, 59.66, 59.63, 59.43, 59.81, 59.99, 59.93, 59.59, 59.62, 59.61, 60.01]

Epoch: 18
2024-03-10 00:24:10.750864 epoch: 18 step: 0 cls_loss= 0.78692 (146501 samples/sec)
2024-03-10 00:24:13.934142 epoch: 18 step: 100 cls_loss= 0.93191 (9424 samples/sec)
saving....
2024-03-10 00:24:17.820004------------------------------------------------------ Precision@1: 59.83% 

[59.72, 59.62, 59.65, 59.59, 59.71, 59.92, 59.77, 59.61, 59.66, 59.63, 59.43, 59.81, 59.99, 59.93, 59.59, 59.62, 59.61, 60.01, 59.83]

Epoch: 19
2024-03-10 00:24:18.015854 epoch: 19 step: 0 cls_loss= 0.97364 (154058 samples/sec)
2024-03-10 00:24:21.136025 epoch: 19 step: 100 cls_loss= 1.00422 (9615 samples/sec)
saving....
2024-03-10 00:24:25.042435------------------------------------------------------ Precision@1: 59.97% 

[59.72, 59.62, 59.65, 59.59, 59.71, 59.92, 59.77, 59.61, 59.66, 59.63, 59.43, 59.81, 59.99, 59.93, 59.59, 59.62, 59.61, 60.01, 59.83, 59.97]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 00:24:27.776823 epoch: 0 step: 0 cls_loss= 1.21574 (48591 samples/sec)
2024-03-10 00:24:30.936706 epoch: 0 step: 100 cls_loss= 1.08546 (9494 samples/sec)
saving....
2024-03-10 00:24:35.040273------------------------------------------------------ Precision@1: 59.65% 

[59.65]

Epoch: 1
2024-03-10 00:24:35.255202 epoch: 1 step: 0 cls_loss= 1.17379 (140273 samples/sec)
2024-03-10 00:24:38.374090 epoch: 1 step: 100 cls_loss= 1.06934 (9619 samples/sec)
saving....
2024-03-10 00:24:42.126607------------------------------------------------------ Precision@1: 59.73% 

[59.65, 59.73]

Epoch: 2
2024-03-10 00:24:42.337031 epoch: 2 step: 0 cls_loss= 1.02048 (143314 samples/sec)
2024-03-10 00:24:45.488732 epoch: 2 step: 100 cls_loss= 1.10824 (9518 samples/sec)
saving....
2024-03-10 00:24:49.278408------------------------------------------------------ Precision@1: 59.76% 

[59.65, 59.73, 59.76]

Epoch: 3
2024-03-10 00:24:49.471446 epoch: 3 step: 0 cls_loss= 1.08299 (156308 samples/sec)
2024-03-10 00:24:52.495435 epoch: 3 step: 100 cls_loss= 1.12967 (9920 samples/sec)
saving....
2024-03-10 00:24:56.244703------------------------------------------------------ Precision@1: 59.66% 

[59.65, 59.73, 59.76, 59.66]

Epoch: 4
2024-03-10 00:24:56.442653 epoch: 4 step: 0 cls_loss= 1.02378 (152307 samples/sec)
2024-03-10 00:24:59.528544 epoch: 4 step: 100 cls_loss= 1.00263 (9721 samples/sec)
saving....
2024-03-10 00:25:03.291393------------------------------------------------------ Precision@1: 59.83% 

[59.65, 59.73, 59.76, 59.66, 59.83]

Epoch: 5
2024-03-10 00:25:03.502524 epoch: 5 step: 0 cls_loss= 0.98326 (142849 samples/sec)
2024-03-10 00:25:06.550440 epoch: 5 step: 100 cls_loss= 1.09641 (9842 samples/sec)
saving....
2024-03-10 00:25:10.291492------------------------------------------------------ Precision@1: 59.78% 

[59.65, 59.73, 59.76, 59.66, 59.83, 59.78]

Epoch: 6
2024-03-10 00:25:10.510649 epoch: 6 step: 0 cls_loss= 1.04975 (137534 samples/sec)
2024-03-10 00:25:13.498983 epoch: 6 step: 100 cls_loss= 1.09442 (10039 samples/sec)
saving....
2024-03-10 00:25:17.302947------------------------------------------------------ Precision@1: 59.87% 

[59.65, 59.73, 59.76, 59.66, 59.83, 59.78, 59.87]

Epoch: 7
2024-03-10 00:25:17.489936 epoch: 7 step: 0 cls_loss= 1.09513 (161424 samples/sec)
2024-03-10 00:25:20.574621 epoch: 7 step: 100 cls_loss= 0.85514 (9725 samples/sec)
saving....
2024-03-10 00:25:24.344785------------------------------------------------------ Precision@1: 59.72% 

[59.65, 59.73, 59.76, 59.66, 59.83, 59.78, 59.87, 59.72]

Epoch: 8
2024-03-10 00:25:24.558377 epoch: 8 step: 0 cls_loss= 0.98957 (141075 samples/sec)
2024-03-10 00:25:27.671365 epoch: 8 step: 100 cls_loss= 1.01480 (9637 samples/sec)
saving....
2024-03-10 00:25:31.438614------------------------------------------------------ Precision@1: 59.84% 

[59.65, 59.73, 59.76, 59.66, 59.83, 59.78, 59.87, 59.72, 59.84]

Epoch: 9
2024-03-10 00:25:31.645023 epoch: 9 step: 0 cls_loss= 1.00303 (146079 samples/sec)
2024-03-10 00:25:34.817337 epoch: 9 step: 100 cls_loss= 1.15116 (9457 samples/sec)
saving....
2024-03-10 00:25:38.602707------------------------------------------------------ Precision@1: 59.97% 

[59.65, 59.73, 59.76, 59.66, 59.83, 59.78, 59.87, 59.72, 59.84, 59.97]

Epoch: 10
2024-03-10 00:25:38.804242 epoch: 10 step: 0 cls_loss= 0.87242 (149621 samples/sec)
2024-03-10 00:25:41.980171 epoch: 10 step: 100 cls_loss= 1.01420 (9446 samples/sec)
saving....
2024-03-10 00:25:45.785620------------------------------------------------------ Precision@1: 60.00% 

[59.65, 59.73, 59.76, 59.66, 59.83, 59.78, 59.87, 59.72, 59.84, 59.97, 60.0]

Epoch: 11
2024-03-10 00:25:45.981046 epoch: 11 step: 0 cls_loss= 0.99640 (154430 samples/sec)
2024-03-10 00:25:49.026042 epoch: 11 step: 100 cls_loss= 0.86616 (9852 samples/sec)
saving....
2024-03-10 00:25:52.834665------------------------------------------------------ Precision@1: 59.59% 

[59.65, 59.73, 59.76, 59.66, 59.83, 59.78, 59.87, 59.72, 59.84, 59.97, 60.0, 59.59]

Epoch: 12
2024-03-10 00:25:53.040896 epoch: 12 step: 0 cls_loss= 1.08051 (146333 samples/sec)
2024-03-10 00:25:56.164079 epoch: 12 step: 100 cls_loss= 0.96856 (9605 samples/sec)
saving....
2024-03-10 00:26:00.019881------------------------------------------------------ Precision@1: 59.85% 

[59.65, 59.73, 59.76, 59.66, 59.83, 59.78, 59.87, 59.72, 59.84, 59.97, 60.0, 59.59, 59.85]

Epoch: 13
2024-03-10 00:26:00.220159 epoch: 13 step: 0 cls_loss= 0.98179 (150627 samples/sec)
2024-03-10 00:26:03.381036 epoch: 13 step: 100 cls_loss= 0.98159 (9491 samples/sec)
saving....
2024-03-10 00:26:07.231070------------------------------------------------------ Precision@1: 59.63% 

[59.65, 59.73, 59.76, 59.66, 59.83, 59.78, 59.87, 59.72, 59.84, 59.97, 60.0, 59.59, 59.85, 59.63]

Epoch: 14
2024-03-10 00:26:07.437614 epoch: 14 step: 0 cls_loss= 0.98735 (145724 samples/sec)
2024-03-10 00:26:10.478618 epoch: 14 step: 100 cls_loss= 1.02695 (9865 samples/sec)
saving....
2024-03-10 00:26:14.231676------------------------------------------------------ Precision@1: 59.85% 

[59.65, 59.73, 59.76, 59.66, 59.83, 59.78, 59.87, 59.72, 59.84, 59.97, 60.0, 59.59, 59.85, 59.63, 59.85]

Epoch: 15
2024-03-10 00:26:14.439458 epoch: 15 step: 0 cls_loss= 0.87759 (145131 samples/sec)
2024-03-10 00:26:17.585213 epoch: 15 step: 100 cls_loss= 1.02661 (9536 samples/sec)
saving....
2024-03-10 00:26:21.346598------------------------------------------------------ Precision@1: 59.82% 

[59.65, 59.73, 59.76, 59.66, 59.83, 59.78, 59.87, 59.72, 59.84, 59.97, 60.0, 59.59, 59.85, 59.63, 59.85, 59.82]

Epoch: 16
2024-03-10 00:26:21.546809 epoch: 16 step: 0 cls_loss= 1.04647 (150592 samples/sec)
2024-03-10 00:26:24.656985 epoch: 16 step: 100 cls_loss= 1.01743 (9645 samples/sec)
saving....
2024-03-10 00:26:28.506198------------------------------------------------------ Precision@1: 60.00% 

[59.65, 59.73, 59.76, 59.66, 59.83, 59.78, 59.87, 59.72, 59.84, 59.97, 60.0, 59.59, 59.85, 59.63, 59.85, 59.82, 60.0]

Epoch: 17
2024-03-10 00:26:28.684696 epoch: 17 step: 0 cls_loss= 0.96198 (169055 samples/sec)
2024-03-10 00:26:31.803235 epoch: 17 step: 100 cls_loss= 0.92381 (9620 samples/sec)
saving....
2024-03-10 00:26:35.603320------------------------------------------------------ Precision@1: 59.70% 

[59.65, 59.73, 59.76, 59.66, 59.83, 59.78, 59.87, 59.72, 59.84, 59.97, 60.0, 59.59, 59.85, 59.63, 59.85, 59.82, 60.0, 59.7]

Epoch: 18
2024-03-10 00:26:35.825557 epoch: 18 step: 0 cls_loss= 0.88370 (135537 samples/sec)
2024-03-10 00:26:38.825891 epoch: 18 step: 100 cls_loss= 0.88392 (9999 samples/sec)
saving....
2024-03-10 00:26:42.543040------------------------------------------------------ Precision@1: 59.78% 

[59.65, 59.73, 59.76, 59.66, 59.83, 59.78, 59.87, 59.72, 59.84, 59.97, 60.0, 59.59, 59.85, 59.63, 59.85, 59.82, 60.0, 59.7, 59.78]

Epoch: 19
2024-03-10 00:26:42.758856 epoch: 19 step: 0 cls_loss= 1.05863 (139581 samples/sec)
2024-03-10 00:26:45.763152 epoch: 19 step: 100 cls_loss= 0.99532 (9985 samples/sec)
saving....
2024-03-10 00:26:49.533229------------------------------------------------------ Precision@1: 59.99% 

[59.65, 59.73, 59.76, 59.66, 59.83, 59.78, 59.87, 59.72, 59.84, 59.97, 60.0, 59.59, 59.85, 59.63, 59.85, 59.82, 60.0, 59.7, 59.78, 59.99]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 00:26:52.257880 epoch: 0 step: 0 cls_loss= 1.08246 (50034 samples/sec)
2024-03-10 00:26:55.295723 epoch: 0 step: 100 cls_loss= 0.99370 (9875 samples/sec)
saving....
2024-03-10 00:26:59.309431------------------------------------------------------ Precision@1: 59.68% 

[59.68]

Epoch: 1
2024-03-10 00:26:59.521735 epoch: 1 step: 0 cls_loss= 0.93052 (141991 samples/sec)
2024-03-10 00:27:02.557006 epoch: 1 step: 100 cls_loss= 1.06900 (9884 samples/sec)
saving....
2024-03-10 00:27:06.329675------------------------------------------------------ Precision@1: 59.70% 

[59.68, 59.7]

Epoch: 2
2024-03-10 00:27:06.525093 epoch: 2 step: 0 cls_loss= 0.97070 (154423 samples/sec)
2024-03-10 00:27:09.453106 epoch: 2 step: 100 cls_loss= 0.98314 (10246 samples/sec)
saving....
2024-03-10 00:27:13.154472------------------------------------------------------ Precision@1: 59.52% 

[59.68, 59.7, 59.52]

Epoch: 3
2024-03-10 00:27:13.354402 epoch: 3 step: 0 cls_loss= 1.03728 (150789 samples/sec)
2024-03-10 00:27:16.408942 epoch: 3 step: 100 cls_loss= 0.96758 (9821 samples/sec)
saving....
2024-03-10 00:27:20.192864------------------------------------------------------ Precision@1: 59.66% 

[59.68, 59.7, 59.52, 59.66]

Epoch: 4
2024-03-10 00:27:20.398708 epoch: 4 step: 0 cls_loss= 0.97933 (146477 samples/sec)
2024-03-10 00:27:23.416648 epoch: 4 step: 100 cls_loss= 1.07830 (9940 samples/sec)
saving....
2024-03-10 00:27:27.254330------------------------------------------------------ Precision@1: 59.61% 

[59.68, 59.7, 59.52, 59.66, 59.61]

Epoch: 5
2024-03-10 00:27:27.438403 epoch: 5 step: 0 cls_loss= 0.92822 (163914 samples/sec)
2024-03-10 00:27:30.594819 epoch: 5 step: 100 cls_loss= 1.05892 (9504 samples/sec)
saving....
2024-03-10 00:27:34.443918------------------------------------------------------ Precision@1: 59.75% 

[59.68, 59.7, 59.52, 59.66, 59.61, 59.75]

Epoch: 6
2024-03-10 00:27:34.673658 epoch: 6 step: 0 cls_loss= 0.90616 (131313 samples/sec)
2024-03-10 00:27:37.759101 epoch: 6 step: 100 cls_loss= 0.98052 (9723 samples/sec)
saving....
2024-03-10 00:27:41.576780------------------------------------------------------ Precision@1: 59.80% 

[59.68, 59.7, 59.52, 59.66, 59.61, 59.75, 59.8]

Epoch: 7
2024-03-10 00:27:41.795108 epoch: 7 step: 0 cls_loss= 0.92310 (138042 samples/sec)
2024-03-10 00:27:44.839574 epoch: 7 step: 100 cls_loss= 0.98661 (9854 samples/sec)
saving....
2024-03-10 00:27:48.707396------------------------------------------------------ Precision@1: 59.66% 

[59.68, 59.7, 59.52, 59.66, 59.61, 59.75, 59.8, 59.66]

Epoch: 8
2024-03-10 00:27:48.917390 epoch: 8 step: 0 cls_loss= 0.92212 (143701 samples/sec)
2024-03-10 00:27:52.006059 epoch: 8 step: 100 cls_loss= 0.93606 (9713 samples/sec)
saving....
2024-03-10 00:27:55.797366------------------------------------------------------ Precision@1: 59.71% 

[59.68, 59.7, 59.52, 59.66, 59.61, 59.75, 59.8, 59.66, 59.71]

Epoch: 9
2024-03-10 00:27:56.001322 epoch: 9 step: 0 cls_loss= 0.95449 (147823 samples/sec)
2024-03-10 00:27:59.104094 epoch: 9 step: 100 cls_loss= 1.03014 (9669 samples/sec)
saving....
2024-03-10 00:28:02.964715------------------------------------------------------ Precision@1: 59.67% 

[59.68, 59.7, 59.52, 59.66, 59.61, 59.75, 59.8, 59.66, 59.71, 59.67]

Epoch: 10
2024-03-10 00:28:03.177999 epoch: 10 step: 0 cls_loss= 0.92170 (141423 samples/sec)
2024-03-10 00:28:06.287580 epoch: 10 step: 100 cls_loss= 1.02236 (9647 samples/sec)
saving....
2024-03-10 00:28:10.120714------------------------------------------------------ Precision@1: 59.67% 

[59.68, 59.7, 59.52, 59.66, 59.61, 59.75, 59.8, 59.66, 59.71, 59.67, 59.67]

Epoch: 11
2024-03-10 00:28:10.315210 epoch: 11 step: 0 cls_loss= 1.03494 (155000 samples/sec)
2024-03-10 00:28:13.442395 epoch: 11 step: 100 cls_loss= 0.83173 (9593 samples/sec)
saving....
2024-03-10 00:28:17.307788------------------------------------------------------ Precision@1: 59.59% 

[59.68, 59.7, 59.52, 59.66, 59.61, 59.75, 59.8, 59.66, 59.71, 59.67, 59.67, 59.59]

Epoch: 12
2024-03-10 00:28:17.509279 epoch: 12 step: 0 cls_loss= 0.96000 (149709 samples/sec)
2024-03-10 00:28:20.594764 epoch: 12 step: 100 cls_loss= 1.00707 (9723 samples/sec)
saving....
2024-03-10 00:28:24.401765------------------------------------------------------ Precision@1: 59.80% 

[59.68, 59.7, 59.52, 59.66, 59.61, 59.75, 59.8, 59.66, 59.71, 59.67, 59.67, 59.59, 59.8]

Epoch: 13
2024-03-10 00:28:24.624159 epoch: 13 step: 0 cls_loss= 0.88068 (135465 samples/sec)
2024-03-10 00:28:27.728876 epoch: 13 step: 100 cls_loss= 0.99758 (9662 samples/sec)
saving....
2024-03-10 00:28:31.611624------------------------------------------------------ Precision@1: 59.92% 

[59.68, 59.7, 59.52, 59.66, 59.61, 59.75, 59.8, 59.66, 59.71, 59.67, 59.67, 59.59, 59.8, 59.92]

Epoch: 14
2024-03-10 00:28:31.810136 epoch: 14 step: 0 cls_loss= 1.00320 (151968 samples/sec)
2024-03-10 00:28:34.928886 epoch: 14 step: 100 cls_loss= 1.02114 (9619 samples/sec)
saving....
2024-03-10 00:28:38.706295------------------------------------------------------ Precision@1: 59.70% 

[59.68, 59.7, 59.52, 59.66, 59.61, 59.75, 59.8, 59.66, 59.71, 59.67, 59.67, 59.59, 59.8, 59.92, 59.7]

Epoch: 15
2024-03-10 00:28:38.923344 epoch: 15 step: 0 cls_loss= 1.01240 (138940 samples/sec)
2024-03-10 00:28:42.019883 epoch: 15 step: 100 cls_loss= 0.88581 (9688 samples/sec)
saving....
2024-03-10 00:28:45.887077------------------------------------------------------ Precision@1: 59.68% 

[59.68, 59.7, 59.52, 59.66, 59.61, 59.75, 59.8, 59.66, 59.71, 59.67, 59.67, 59.59, 59.8, 59.92, 59.7, 59.68]

Epoch: 16
2024-03-10 00:28:46.088167 epoch: 16 step: 0 cls_loss= 0.99011 (150036 samples/sec)
2024-03-10 00:28:49.173458 epoch: 16 step: 100 cls_loss= 0.96768 (9723 samples/sec)
saving....
2024-03-10 00:28:52.973115------------------------------------------------------ Precision@1: 60.00% 

[59.68, 59.7, 59.52, 59.66, 59.61, 59.75, 59.8, 59.66, 59.71, 59.67, 59.67, 59.59, 59.8, 59.92, 59.7, 59.68, 60.0]

Epoch: 17
2024-03-10 00:28:53.170295 epoch: 17 step: 0 cls_loss= 0.95151 (153045 samples/sec)
2024-03-10 00:28:56.273761 epoch: 17 step: 100 cls_loss= 1.02966 (9666 samples/sec)
saving....
2024-03-10 00:29:00.132684------------------------------------------------------ Precision@1: 59.81% 

[59.68, 59.7, 59.52, 59.66, 59.61, 59.75, 59.8, 59.66, 59.71, 59.67, 59.67, 59.59, 59.8, 59.92, 59.7, 59.68, 60.0, 59.81]

Epoch: 18
2024-03-10 00:29:00.340020 epoch: 18 step: 0 cls_loss= 1.01642 (145319 samples/sec)
2024-03-10 00:29:03.374793 epoch: 18 step: 100 cls_loss= 1.05999 (9885 samples/sec)
saving....
2024-03-10 00:29:07.176001------------------------------------------------------ Precision@1: 59.83% 

[59.68, 59.7, 59.52, 59.66, 59.61, 59.75, 59.8, 59.66, 59.71, 59.67, 59.67, 59.59, 59.8, 59.92, 59.7, 59.68, 60.0, 59.81, 59.83]

Epoch: 19
2024-03-10 00:29:07.378202 epoch: 19 step: 0 cls_loss= 0.87042 (149198 samples/sec)
2024-03-10 00:29:10.387086 epoch: 19 step: 100 cls_loss= 0.96519 (9970 samples/sec)
saving....
2024-03-10 00:29:14.110084------------------------------------------------------ Precision@1: 59.65% 

[59.68, 59.7, 59.52, 59.66, 59.61, 59.75, 59.8, 59.66, 59.71, 59.67, 59.67, 59.59, 59.8, 59.92, 59.7, 59.68, 60.0, 59.81, 59.83, 59.65]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 00:29:16.847227 epoch: 0 step: 0 cls_loss= 0.96756 (48375 samples/sec)
2024-03-10 00:29:19.907570 epoch: 0 step: 100 cls_loss= 1.02501 (9803 samples/sec)
saving....
2024-03-10 00:29:24.116904------------------------------------------------------ Precision@1: 59.64% 

[59.64]

Epoch: 1
2024-03-10 00:29:24.333082 epoch: 1 step: 0 cls_loss= 1.08967 (139414 samples/sec)
2024-03-10 00:29:27.322111 epoch: 1 step: 100 cls_loss= 1.03552 (10036 samples/sec)
saving....
2024-03-10 00:29:31.090955------------------------------------------------------ Precision@1: 59.85% 

[59.64, 59.85]

Epoch: 2
2024-03-10 00:29:31.295140 epoch: 2 step: 0 cls_loss= 1.00300 (147718 samples/sec)
2024-03-10 00:29:34.310016 epoch: 2 step: 100 cls_loss= 1.21917 (9950 samples/sec)
saving....
2024-03-10 00:29:38.165002------------------------------------------------------ Precision@1: 59.88% 

[59.64, 59.85, 59.88]

Epoch: 3
2024-03-10 00:29:38.358634 epoch: 3 step: 0 cls_loss= 1.01980 (155795 samples/sec)
2024-03-10 00:29:41.428454 epoch: 3 step: 100 cls_loss= 1.01786 (9772 samples/sec)
saving....
2024-03-10 00:29:45.218969------------------------------------------------------ Precision@1: 59.58% 

[59.64, 59.85, 59.88, 59.58]

Epoch: 4
2024-03-10 00:29:45.415227 epoch: 4 step: 0 cls_loss= 1.14964 (153472 samples/sec)
2024-03-10 00:29:48.480207 epoch: 4 step: 100 cls_loss= 1.06383 (9789 samples/sec)
saving....
2024-03-10 00:29:52.287787------------------------------------------------------ Precision@1: 59.39% 

[59.64, 59.85, 59.88, 59.58, 59.39]

Epoch: 5
2024-03-10 00:29:52.479661 epoch: 5 step: 0 cls_loss= 0.98366 (157314 samples/sec)
2024-03-10 00:29:55.460041 epoch: 5 step: 100 cls_loss= 0.98150 (10066 samples/sec)
saving....
2024-03-10 00:29:59.184950------------------------------------------------------ Precision@1: 59.56% 

[59.64, 59.85, 59.88, 59.58, 59.39, 59.56]

Epoch: 6
2024-03-10 00:29:59.403188 epoch: 6 step: 0 cls_loss= 1.03274 (138213 samples/sec)
2024-03-10 00:30:02.520533 epoch: 6 step: 100 cls_loss= 1.09471 (9623 samples/sec)
saving....
2024-03-10 00:30:06.319056------------------------------------------------------ Precision@1: 59.76% 

[59.64, 59.85, 59.88, 59.58, 59.39, 59.56, 59.76]

Epoch: 7
2024-03-10 00:30:06.526678 epoch: 7 step: 0 cls_loss= 1.01181 (145195 samples/sec)
2024-03-10 00:30:09.695503 epoch: 7 step: 100 cls_loss= 1.07696 (9467 samples/sec)
saving....
2024-03-10 00:30:13.536279------------------------------------------------------ Precision@1: 59.62% 

[59.64, 59.85, 59.88, 59.58, 59.39, 59.56, 59.76, 59.62]

Epoch: 8
2024-03-10 00:30:13.734795 epoch: 8 step: 0 cls_loss= 0.86040 (151907 samples/sec)
2024-03-10 00:30:16.696221 epoch: 8 step: 100 cls_loss= 0.97493 (10130 samples/sec)
saving....
2024-03-10 00:30:20.414956------------------------------------------------------ Precision@1: 59.54% 

[59.64, 59.85, 59.88, 59.58, 59.39, 59.56, 59.76, 59.62, 59.54]

Epoch: 9
2024-03-10 00:30:20.609794 epoch: 9 step: 0 cls_loss= 1.08839 (154887 samples/sec)
2024-03-10 00:30:23.675074 epoch: 9 step: 100 cls_loss= 0.95126 (9787 samples/sec)
saving....
2024-03-10 00:30:27.523947------------------------------------------------------ Precision@1: 59.82% 

[59.64, 59.85, 59.88, 59.58, 59.39, 59.56, 59.76, 59.62, 59.54, 59.82]

Epoch: 10
2024-03-10 00:30:27.713099 epoch: 10 step: 0 cls_loss= 1.01983 (159589 samples/sec)
2024-03-10 00:30:30.748277 epoch: 10 step: 100 cls_loss= 1.00647 (9884 samples/sec)
saving....
2024-03-10 00:30:34.512928------------------------------------------------------ Precision@1: 59.85% 

[59.64, 59.85, 59.88, 59.58, 59.39, 59.56, 59.76, 59.62, 59.54, 59.82, 59.85]

Epoch: 11
2024-03-10 00:30:34.717782 epoch: 11 step: 0 cls_loss= 0.87681 (147257 samples/sec)
2024-03-10 00:30:37.889745 epoch: 11 step: 100 cls_loss= 1.15665 (9458 samples/sec)
saving....
2024-03-10 00:30:41.729249------------------------------------------------------ Precision@1: 59.80% 

[59.64, 59.85, 59.88, 59.58, 59.39, 59.56, 59.76, 59.62, 59.54, 59.82, 59.85, 59.8]

Epoch: 12
2024-03-10 00:30:41.942984 epoch: 12 step: 0 cls_loss= 1.06110 (140991 samples/sec)
2024-03-10 00:30:44.956988 epoch: 12 step: 100 cls_loss= 0.88320 (9953 samples/sec)
saving....
2024-03-10 00:30:48.713916------------------------------------------------------ Precision@1: 60.00% 

[59.64, 59.85, 59.88, 59.58, 59.39, 59.56, 59.76, 59.62, 59.54, 59.82, 59.85, 59.8, 60.0]

Epoch: 13
2024-03-10 00:30:48.923436 epoch: 13 step: 0 cls_loss= 0.85742 (143789 samples/sec)
2024-03-10 00:30:52.034160 epoch: 13 step: 100 cls_loss= 1.02266 (9644 samples/sec)
saving....
2024-03-10 00:30:55.887159------------------------------------------------------ Precision@1: 59.68% 

[59.64, 59.85, 59.88, 59.58, 59.39, 59.56, 59.76, 59.62, 59.54, 59.82, 59.85, 59.8, 60.0, 59.68]

Epoch: 14
2024-03-10 00:30:56.078967 epoch: 14 step: 0 cls_loss= 1.09679 (157163 samples/sec)
2024-03-10 00:30:59.202119 epoch: 14 step: 100 cls_loss= 0.95832 (9605 samples/sec)
saving....
2024-03-10 00:31:03.037287------------------------------------------------------ Precision@1: 59.90% 

[59.64, 59.85, 59.88, 59.58, 59.39, 59.56, 59.76, 59.62, 59.54, 59.82, 59.85, 59.8, 60.0, 59.68, 59.9]

Epoch: 15
2024-03-10 00:31:03.258110 epoch: 15 step: 0 cls_loss= 0.97500 (136567 samples/sec)
2024-03-10 00:31:06.393698 epoch: 15 step: 100 cls_loss= 0.91624 (9567 samples/sec)
saving....
2024-03-10 00:31:10.196334------------------------------------------------------ Precision@1: 59.90% 

[59.64, 59.85, 59.88, 59.58, 59.39, 59.56, 59.76, 59.62, 59.54, 59.82, 59.85, 59.8, 60.0, 59.68, 59.9, 59.9]

Epoch: 16
2024-03-10 00:31:10.393314 epoch: 16 step: 0 cls_loss= 1.04931 (153206 samples/sec)
2024-03-10 00:31:13.483503 epoch: 16 step: 100 cls_loss= 1.02679 (9708 samples/sec)
saving....
2024-03-10 00:31:17.342308------------------------------------------------------ Precision@1: 59.73% 

[59.64, 59.85, 59.88, 59.58, 59.39, 59.56, 59.76, 59.62, 59.54, 59.82, 59.85, 59.8, 60.0, 59.68, 59.9, 59.9, 59.73]

Epoch: 17
2024-03-10 00:31:17.543943 epoch: 17 step: 0 cls_loss= 1.02759 (149530 samples/sec)
2024-03-10 00:31:20.644898 epoch: 17 step: 100 cls_loss= 0.85497 (9674 samples/sec)
saving....
2024-03-10 00:31:24.445429------------------------------------------------------ Precision@1: 59.96% 

[59.64, 59.85, 59.88, 59.58, 59.39, 59.56, 59.76, 59.62, 59.54, 59.82, 59.85, 59.8, 60.0, 59.68, 59.9, 59.9, 59.73, 59.96]

Epoch: 18
2024-03-10 00:31:24.661955 epoch: 18 step: 0 cls_loss= 0.94197 (139283 samples/sec)
2024-03-10 00:31:27.726095 epoch: 18 step: 100 cls_loss= 0.96955 (9790 samples/sec)
saving....
2024-03-10 00:31:31.470147------------------------------------------------------ Precision@1: 60.08% 

[59.64, 59.85, 59.88, 59.58, 59.39, 59.56, 59.76, 59.62, 59.54, 59.82, 59.85, 59.8, 60.0, 59.68, 59.9, 59.9, 59.73, 59.96, 60.08]

Epoch: 19
2024-03-10 00:31:31.652899 epoch: 19 step: 0 cls_loss= 1.07799 (165127 samples/sec)
2024-03-10 00:31:34.729661 epoch: 19 step: 100 cls_loss= 0.86530 (9751 samples/sec)
saving....
2024-03-10 00:31:38.534926------------------------------------------------------ Precision@1: 60.10% 

[59.64, 59.85, 59.88, 59.58, 59.39, 59.56, 59.76, 59.62, 59.54, 59.82, 59.85, 59.8, 60.0, 59.68, 59.9, 59.9, 59.73, 59.96, 60.08, 60.1]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 00:31:41.269746 epoch: 0 step: 0 cls_loss= 1.05004 (48508 samples/sec)
2024-03-10 00:31:44.364151 epoch: 0 step: 100 cls_loss= 1.07323 (9695 samples/sec)
saving....
2024-03-10 00:31:48.541387------------------------------------------------------ Precision@1: 59.63% 

[59.63]

Epoch: 1
2024-03-10 00:31:48.737220 epoch: 1 step: 0 cls_loss= 1.08679 (154124 samples/sec)
2024-03-10 00:31:51.815095 epoch: 1 step: 100 cls_loss= 1.09371 (9747 samples/sec)
saving....
2024-03-10 00:31:55.647464------------------------------------------------------ Precision@1: 59.60% 

[59.63, 59.6]

Epoch: 2
2024-03-10 00:31:55.844604 epoch: 2 step: 0 cls_loss= 1.13947 (153002 samples/sec)
2024-03-10 00:31:58.842984 epoch: 2 step: 100 cls_loss= 1.07013 (10005 samples/sec)
saving....
2024-03-10 00:32:02.676191------------------------------------------------------ Precision@1: 59.76% 

[59.63, 59.6, 59.76]

Epoch: 3
2024-03-10 00:32:02.876072 epoch: 3 step: 0 cls_loss= 1.01572 (150954 samples/sec)
2024-03-10 00:32:05.919872 epoch: 3 step: 100 cls_loss= 1.04564 (9856 samples/sec)
saving....
2024-03-10 00:32:09.664325------------------------------------------------------ Precision@1: 59.87% 

[59.63, 59.6, 59.76, 59.87]

Epoch: 4
2024-03-10 00:32:09.869089 epoch: 4 step: 0 cls_loss= 0.98755 (147343 samples/sec)
2024-03-10 00:32:12.939073 epoch: 4 step: 100 cls_loss= 1.01346 (9772 samples/sec)
saving....
2024-03-10 00:32:16.680275------------------------------------------------------ Precision@1: 59.91% 

[59.63, 59.6, 59.76, 59.87, 59.91]

Epoch: 5
2024-03-10 00:32:16.868656 epoch: 5 step: 0 cls_loss= 0.88599 (160182 samples/sec)
2024-03-10 00:32:20.065000 epoch: 5 step: 100 cls_loss= 1.00214 (9385 samples/sec)
saving....
2024-03-10 00:32:23.940558------------------------------------------------------ Precision@1: 59.69% 

[59.63, 59.6, 59.76, 59.87, 59.91, 59.69]

Epoch: 6
2024-03-10 00:32:24.142271 epoch: 6 step: 0 cls_loss= 1.06675 (149447 samples/sec)
2024-03-10 00:32:27.285395 epoch: 6 step: 100 cls_loss= 0.98757 (9544 samples/sec)
saving....
2024-03-10 00:32:31.099854------------------------------------------------------ Precision@1: 59.78% 

[59.63, 59.6, 59.76, 59.87, 59.91, 59.69, 59.78]

Epoch: 7
2024-03-10 00:32:31.320740 epoch: 7 step: 0 cls_loss= 1.01671 (136311 samples/sec)
2024-03-10 00:32:34.322716 epoch: 7 step: 100 cls_loss= 1.08220 (9993 samples/sec)
saving....
2024-03-10 00:32:38.063604------------------------------------------------------ Precision@1: 59.71% 

[59.63, 59.6, 59.76, 59.87, 59.91, 59.69, 59.78, 59.71]

Epoch: 8
2024-03-10 00:32:38.263773 epoch: 8 step: 0 cls_loss= 0.98349 (150629 samples/sec)
2024-03-10 00:32:41.473554 epoch: 8 step: 100 cls_loss= 0.97055 (9346 samples/sec)
saving....
2024-03-10 00:32:45.342150------------------------------------------------------ Precision@1: 59.86% 

[59.63, 59.6, 59.76, 59.87, 59.91, 59.69, 59.78, 59.71, 59.86]

Epoch: 9
2024-03-10 00:32:45.529562 epoch: 9 step: 0 cls_loss= 1.08782 (161012 samples/sec)
2024-03-10 00:32:48.527334 epoch: 9 step: 100 cls_loss= 0.95384 (10007 samples/sec)
saving....
2024-03-10 00:32:52.260176------------------------------------------------------ Precision@1: 59.60% 

[59.63, 59.6, 59.76, 59.87, 59.91, 59.69, 59.78, 59.71, 59.86, 59.6]

Epoch: 10
2024-03-10 00:32:52.470514 epoch: 10 step: 0 cls_loss= 0.95694 (143178 samples/sec)
2024-03-10 00:32:55.652619 epoch: 10 step: 100 cls_loss= 1.01714 (9427 samples/sec)
saving....
2024-03-10 00:32:59.496623------------------------------------------------------ Precision@1: 59.76% 

[59.63, 59.6, 59.76, 59.87, 59.91, 59.69, 59.78, 59.71, 59.86, 59.6, 59.76]

Epoch: 11
2024-03-10 00:32:59.692491 epoch: 11 step: 0 cls_loss= 0.95363 (154040 samples/sec)
2024-03-10 00:33:02.811367 epoch: 11 step: 100 cls_loss= 1.06572 (9619 samples/sec)
saving....
2024-03-10 00:33:06.647094------------------------------------------------------ Precision@1: 59.67% 

[59.63, 59.6, 59.76, 59.87, 59.91, 59.69, 59.78, 59.71, 59.86, 59.6, 59.76, 59.67]

Epoch: 12
2024-03-10 00:33:06.857834 epoch: 12 step: 0 cls_loss= 0.93221 (143223 samples/sec)
2024-03-10 00:33:09.892571 epoch: 12 step: 100 cls_loss= 0.95510 (9885 samples/sec)
saving....
2024-03-10 00:33:13.638849------------------------------------------------------ Precision@1: 59.73% 

[59.63, 59.6, 59.76, 59.87, 59.91, 59.69, 59.78, 59.71, 59.86, 59.6, 59.76, 59.67, 59.73]

Epoch: 13
2024-03-10 00:33:13.835000 epoch: 13 step: 0 cls_loss= 0.97620 (153897 samples/sec)
2024-03-10 00:33:16.817087 epoch: 13 step: 100 cls_loss= 1.02047 (10060 samples/sec)
saving....
2024-03-10 00:33:20.595260------------------------------------------------------ Precision@1: 59.80% 

[59.63, 59.6, 59.76, 59.87, 59.91, 59.69, 59.78, 59.71, 59.86, 59.6, 59.76, 59.67, 59.73, 59.8]

Epoch: 14
2024-03-10 00:33:20.809670 epoch: 14 step: 0 cls_loss= 0.94266 (140652 samples/sec)
2024-03-10 00:33:23.914498 epoch: 14 step: 100 cls_loss= 1.05302 (9662 samples/sec)
saving....
2024-03-10 00:33:27.651351------------------------------------------------------ Precision@1: 59.72% 

[59.63, 59.6, 59.76, 59.87, 59.91, 59.69, 59.78, 59.71, 59.86, 59.6, 59.76, 59.67, 59.73, 59.8, 59.72]

Epoch: 15
2024-03-10 00:33:27.869127 epoch: 15 step: 0 cls_loss= 0.95278 (138445 samples/sec)
2024-03-10 00:33:30.901725 epoch: 15 step: 100 cls_loss= 0.98835 (9892 samples/sec)
saving....
2024-03-10 00:33:34.646355------------------------------------------------------ Precision@1: 59.68% 

[59.63, 59.6, 59.76, 59.87, 59.91, 59.69, 59.78, 59.71, 59.86, 59.6, 59.76, 59.67, 59.73, 59.8, 59.72, 59.68]

Epoch: 16
2024-03-10 00:33:34.879192 epoch: 16 step: 0 cls_loss= 0.80851 (129421 samples/sec)
2024-03-10 00:33:38.000977 epoch: 16 step: 100 cls_loss= 1.09585 (9610 samples/sec)
saving....
2024-03-10 00:33:41.989075------------------------------------------------------ Precision@1: 59.69% 

[59.63, 59.6, 59.76, 59.87, 59.91, 59.69, 59.78, 59.71, 59.86, 59.6, 59.76, 59.67, 59.73, 59.8, 59.72, 59.68, 59.69]

Epoch: 17
2024-03-10 00:33:42.195869 epoch: 17 step: 0 cls_loss= 1.04232 (145771 samples/sec)
2024-03-10 00:33:45.185636 epoch: 17 step: 100 cls_loss= 1.01088 (10034 samples/sec)
saving....
2024-03-10 00:33:48.924378------------------------------------------------------ Precision@1: 59.88% 

[59.63, 59.6, 59.76, 59.87, 59.91, 59.69, 59.78, 59.71, 59.86, 59.6, 59.76, 59.67, 59.73, 59.8, 59.72, 59.68, 59.69, 59.88]

Epoch: 18
2024-03-10 00:33:49.139280 epoch: 18 step: 0 cls_loss= 0.91429 (140158 samples/sec)
2024-03-10 00:33:52.259885 epoch: 18 step: 100 cls_loss= 0.92735 (9613 samples/sec)
saving....
2024-03-10 00:33:56.068475------------------------------------------------------ Precision@1: 59.58% 

[59.63, 59.6, 59.76, 59.87, 59.91, 59.69, 59.78, 59.71, 59.86, 59.6, 59.76, 59.67, 59.73, 59.8, 59.72, 59.68, 59.69, 59.88, 59.58]

Epoch: 19
2024-03-10 00:33:56.280344 epoch: 19 step: 0 cls_loss= 0.93061 (142366 samples/sec)
2024-03-10 00:33:59.365095 epoch: 19 step: 100 cls_loss= 0.91547 (9725 samples/sec)
saving....
2024-03-10 00:34:03.209289------------------------------------------------------ Precision@1: 59.80% 

[59.63, 59.6, 59.76, 59.87, 59.91, 59.69, 59.78, 59.71, 59.86, 59.6, 59.76, 59.67, 59.73, 59.8, 59.72, 59.68, 59.69, 59.88, 59.58, 59.8]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 00:34:05.971070 epoch: 0 step: 0 cls_loss= 1.11574 (47341 samples/sec)
2024-03-10 00:34:09.049831 epoch: 0 step: 100 cls_loss= 1.10755 (9744 samples/sec)
saving....
2024-03-10 00:34:13.036982------------------------------------------------------ Precision@1: 59.77% 

[59.77]

Epoch: 1
2024-03-10 00:34:13.236702 epoch: 1 step: 0 cls_loss= 1.06736 (151062 samples/sec)
2024-03-10 00:34:16.408327 epoch: 1 step: 100 cls_loss= 1.21355 (9459 samples/sec)
saving....
2024-03-10 00:34:20.440600------------------------------------------------------ Precision@1: 59.67% 

[59.77, 59.67]

Epoch: 2
2024-03-10 00:34:20.635353 epoch: 2 step: 0 cls_loss= 1.05944 (154973 samples/sec)
2024-03-10 00:34:23.656793 epoch: 2 step: 100 cls_loss= 1.00432 (9929 samples/sec)
saving....
2024-03-10 00:34:27.425032------------------------------------------------------ Precision@1: 59.62% 

[59.77, 59.67, 59.62]

Epoch: 3
2024-03-10 00:34:27.639493 epoch: 3 step: 0 cls_loss= 0.93896 (140690 samples/sec)
2024-03-10 00:34:30.711893 epoch: 3 step: 100 cls_loss= 1.07423 (9764 samples/sec)
saving....
2024-03-10 00:34:34.531238------------------------------------------------------ Precision@1: 60.03% 

[59.77, 59.67, 59.62, 60.03]

Epoch: 4
2024-03-10 00:34:34.745673 epoch: 4 step: 0 cls_loss= 0.95483 (140580 samples/sec)
2024-03-10 00:34:37.833262 epoch: 4 step: 100 cls_loss= 1.01341 (9716 samples/sec)
saving....
2024-03-10 00:34:41.652213------------------------------------------------------ Precision@1: 59.73% 

[59.77, 59.67, 59.62, 60.03, 59.73]

Epoch: 5
2024-03-10 00:34:41.847578 epoch: 5 step: 0 cls_loss= 0.96361 (154422 samples/sec)
2024-03-10 00:34:44.928430 epoch: 5 step: 100 cls_loss= 0.99284 (9737 samples/sec)
saving....
2024-03-10 00:34:48.760378------------------------------------------------------ Precision@1: 59.79% 

[59.77, 59.67, 59.62, 60.03, 59.73, 59.79]

Epoch: 6
2024-03-10 00:34:48.957130 epoch: 6 step: 0 cls_loss= 1.00485 (153230 samples/sec)
2024-03-10 00:34:52.035030 epoch: 6 step: 100 cls_loss= 0.97379 (9747 samples/sec)
saving....
2024-03-10 00:34:55.835635------------------------------------------------------ Precision@1: 59.67% 

[59.77, 59.67, 59.62, 60.03, 59.73, 59.79, 59.67]

Epoch: 7
2024-03-10 00:34:56.045760 epoch: 7 step: 0 cls_loss= 0.96721 (143529 samples/sec)
2024-03-10 00:34:59.070336 epoch: 7 step: 100 cls_loss= 1.00191 (9918 samples/sec)
saving....
2024-03-10 00:35:02.908388------------------------------------------------------ Precision@1: 59.80% 

[59.77, 59.67, 59.62, 60.03, 59.73, 59.79, 59.67, 59.8]

Epoch: 8
2024-03-10 00:35:03.134092 epoch: 8 step: 0 cls_loss= 0.92229 (133517 samples/sec)
2024-03-10 00:35:06.130693 epoch: 8 step: 100 cls_loss= 1.02004 (10011 samples/sec)
saving....
2024-03-10 00:35:09.833085------------------------------------------------------ Precision@1: 59.75% 

[59.77, 59.67, 59.62, 60.03, 59.73, 59.79, 59.67, 59.8, 59.75]

Epoch: 9
2024-03-10 00:35:10.052180 epoch: 9 step: 0 cls_loss= 1.01403 (137644 samples/sec)
2024-03-10 00:35:13.186898 epoch: 9 step: 100 cls_loss= 0.94186 (9570 samples/sec)
saving....
2024-03-10 00:35:16.899854------------------------------------------------------ Precision@1: 59.95% 

[59.77, 59.67, 59.62, 60.03, 59.73, 59.79, 59.67, 59.8, 59.75, 59.95]

Epoch: 10
2024-03-10 00:35:17.105971 epoch: 10 step: 0 cls_loss= 0.91948 (146125 samples/sec)
2024-03-10 00:35:20.218812 epoch: 10 step: 100 cls_loss= 0.94465 (9637 samples/sec)
saving....
2024-03-10 00:35:24.052162------------------------------------------------------ Precision@1: 59.93% 

[59.77, 59.67, 59.62, 60.03, 59.73, 59.79, 59.67, 59.8, 59.75, 59.95, 59.93]

Epoch: 11
2024-03-10 00:35:24.256044 epoch: 11 step: 0 cls_loss= 0.93018 (147937 samples/sec)
2024-03-10 00:35:27.355221 epoch: 11 step: 100 cls_loss= 0.97139 (9680 samples/sec)
saving....
2024-03-10 00:35:31.157798------------------------------------------------------ Precision@1: 59.45% 

[59.77, 59.67, 59.62, 60.03, 59.73, 59.79, 59.67, 59.8, 59.75, 59.95, 59.93, 59.45]

Epoch: 12
2024-03-10 00:35:31.359019 epoch: 12 step: 0 cls_loss= 1.06843 (149709 samples/sec)
2024-03-10 00:35:34.594285 epoch: 12 step: 100 cls_loss= 1.04413 (9273 samples/sec)
saving....
2024-03-10 00:35:38.439047------------------------------------------------------ Precision@1: 59.66% 

[59.77, 59.67, 59.62, 60.03, 59.73, 59.79, 59.67, 59.8, 59.75, 59.95, 59.93, 59.45, 59.66]

Epoch: 13
2024-03-10 00:35:38.644418 epoch: 13 step: 0 cls_loss= 0.95727 (146723 samples/sec)
2024-03-10 00:35:41.745155 epoch: 13 step: 100 cls_loss= 1.07555 (9675 samples/sec)
saving....
2024-03-10 00:35:45.491785------------------------------------------------------ Precision@1: 59.87% 

[59.77, 59.67, 59.62, 60.03, 59.73, 59.79, 59.67, 59.8, 59.75, 59.95, 59.93, 59.45, 59.66, 59.87]

Epoch: 14
2024-03-10 00:35:45.696376 epoch: 14 step: 0 cls_loss= 1.08475 (147519 samples/sec)
2024-03-10 00:35:48.851981 epoch: 14 step: 100 cls_loss= 1.04290 (9507 samples/sec)
saving....
2024-03-10 00:35:52.685953------------------------------------------------------ Precision@1: 59.92% 

[59.77, 59.67, 59.62, 60.03, 59.73, 59.79, 59.67, 59.8, 59.75, 59.95, 59.93, 59.45, 59.66, 59.87, 59.92]

Epoch: 15
2024-03-10 00:35:52.886856 epoch: 15 step: 0 cls_loss= 0.97314 (150168 samples/sec)
2024-03-10 00:35:55.998423 epoch: 15 step: 100 cls_loss= 0.99457 (9641 samples/sec)
saving....
2024-03-10 00:35:59.774248------------------------------------------------------ Precision@1: 59.87% 

[59.77, 59.67, 59.62, 60.03, 59.73, 59.79, 59.67, 59.8, 59.75, 59.95, 59.93, 59.45, 59.66, 59.87, 59.92, 59.87]

Epoch: 16
2024-03-10 00:35:59.998754 epoch: 16 step: 0 cls_loss= 0.98004 (134282 samples/sec)
2024-03-10 00:36:03.138104 epoch: 16 step: 100 cls_loss= 1.02163 (9556 samples/sec)
saving....
2024-03-10 00:36:07.010383------------------------------------------------------ Precision@1: 59.72% 

[59.77, 59.67, 59.62, 60.03, 59.73, 59.79, 59.67, 59.8, 59.75, 59.95, 59.93, 59.45, 59.66, 59.87, 59.92, 59.87, 59.72]

Epoch: 17
2024-03-10 00:36:07.201884 epoch: 17 step: 0 cls_loss= 0.81032 (157581 samples/sec)
2024-03-10 00:36:10.352843 epoch: 17 step: 100 cls_loss= 1.05891 (9521 samples/sec)
saving....
2024-03-10 00:36:14.192641------------------------------------------------------ Precision@1: 59.96% 

[59.77, 59.67, 59.62, 60.03, 59.73, 59.79, 59.67, 59.8, 59.75, 59.95, 59.93, 59.45, 59.66, 59.87, 59.92, 59.87, 59.72, 59.96]

Epoch: 18
2024-03-10 00:36:14.390125 epoch: 18 step: 0 cls_loss= 0.96079 (152641 samples/sec)
2024-03-10 00:36:17.547382 epoch: 18 step: 100 cls_loss= 0.95175 (9502 samples/sec)
saving....
2024-03-10 00:36:21.353347------------------------------------------------------ Precision@1: 60.01% 

[59.77, 59.67, 59.62, 60.03, 59.73, 59.79, 59.67, 59.8, 59.75, 59.95, 59.93, 59.45, 59.66, 59.87, 59.92, 59.87, 59.72, 59.96, 60.01]

Epoch: 19
2024-03-10 00:36:21.545683 epoch: 19 step: 0 cls_loss= 0.99038 (156837 samples/sec)
2024-03-10 00:36:24.623386 epoch: 19 step: 100 cls_loss= 1.05008 (9747 samples/sec)
saving....
2024-03-10 00:36:28.488529------------------------------------------------------ Precision@1: 60.00% 

[59.77, 59.67, 59.62, 60.03, 59.73, 59.79, 59.67, 59.8, 59.75, 59.95, 59.93, 59.45, 59.66, 59.87, 59.92, 59.87, 59.72, 59.96, 60.01, 60.0]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 00:36:31.209620 epoch: 0 step: 0 cls_loss= 0.99919 (50284 samples/sec)
2024-03-10 00:36:34.247898 epoch: 0 step: 100 cls_loss= 0.99318 (9874 samples/sec)
saving....
2024-03-10 00:36:38.340935------------------------------------------------------ Precision@1: 60.01% 

[60.01]

Epoch: 1
2024-03-10 00:36:38.543222 epoch: 1 step: 0 cls_loss= 0.96491 (149097 samples/sec)
2024-03-10 00:36:41.662615 epoch: 1 step: 100 cls_loss= 0.93350 (9617 samples/sec)
saving....
2024-03-10 00:36:45.613265------------------------------------------------------ Precision@1: 59.67% 

[60.01, 59.67]

Epoch: 2
2024-03-10 00:36:45.819768 epoch: 2 step: 0 cls_loss= 1.02196 (146056 samples/sec)
2024-03-10 00:36:48.935140 epoch: 2 step: 100 cls_loss= 1.08653 (9629 samples/sec)
saving....
2024-03-10 00:36:52.726919------------------------------------------------------ Precision@1: 59.61% 

[60.01, 59.67, 59.61]

Epoch: 3
2024-03-10 00:36:52.915120 epoch: 3 step: 0 cls_loss= 0.93354 (160300 samples/sec)
2024-03-10 00:36:56.016544 epoch: 3 step: 100 cls_loss= 1.00524 (9673 samples/sec)
saving....
2024-03-10 00:36:59.941755------------------------------------------------------ Precision@1: 59.91% 

[60.01, 59.67, 59.61, 59.91]

Epoch: 4
2024-03-10 00:37:00.156735 epoch: 4 step: 0 cls_loss= 0.99937 (140188 samples/sec)
2024-03-10 00:37:03.164618 epoch: 4 step: 100 cls_loss= 0.99055 (9974 samples/sec)
saving....
2024-03-10 00:37:06.964240------------------------------------------------------ Precision@1: 59.57% 

[60.01, 59.67, 59.61, 59.91, 59.57]

Epoch: 5
2024-03-10 00:37:07.174361 epoch: 5 step: 0 cls_loss= 1.04897 (143633 samples/sec)
2024-03-10 00:37:10.174126 epoch: 5 step: 100 cls_loss= 1.07268 (10001 samples/sec)
saving....
2024-03-10 00:37:13.907153------------------------------------------------------ Precision@1: 59.87% 

[60.01, 59.67, 59.61, 59.91, 59.57, 59.87]

Epoch: 6
2024-03-10 00:37:14.109180 epoch: 6 step: 0 cls_loss= 1.18609 (149263 samples/sec)
2024-03-10 00:37:17.170154 epoch: 6 step: 100 cls_loss= 1.03821 (9801 samples/sec)
saving....
2024-03-10 00:37:20.921259------------------------------------------------------ Precision@1: 59.95% 

[60.01, 59.67, 59.61, 59.91, 59.57, 59.87, 59.95]

Epoch: 7
2024-03-10 00:37:21.133902 epoch: 7 step: 0 cls_loss= 1.03133 (141809 samples/sec)
2024-03-10 00:37:24.138036 epoch: 7 step: 100 cls_loss= 0.94970 (9986 samples/sec)
saving....
2024-03-10 00:37:27.957101------------------------------------------------------ Precision@1: 59.83% 

[60.01, 59.67, 59.61, 59.91, 59.57, 59.87, 59.95, 59.83]

Epoch: 8
2024-03-10 00:37:28.155227 epoch: 8 step: 0 cls_loss= 0.94314 (152353 samples/sec)
2024-03-10 00:37:31.159716 epoch: 8 step: 100 cls_loss= 1.01534 (9985 samples/sec)
saving....
2024-03-10 00:37:34.890010------------------------------------------------------ Precision@1: 60.03% 

[60.01, 59.67, 59.61, 59.91, 59.57, 59.87, 59.95, 59.83, 60.03]

Epoch: 9
2024-03-10 00:37:35.083580 epoch: 9 step: 0 cls_loss= 1.02916 (155690 samples/sec)
2024-03-10 00:37:38.089339 epoch: 9 step: 100 cls_loss= 0.95045 (9981 samples/sec)
saving....
2024-03-10 00:37:41.877387------------------------------------------------------ Precision@1: 60.02% 

[60.01, 59.67, 59.61, 59.91, 59.57, 59.87, 59.95, 59.83, 60.03, 60.02]

Epoch: 10
2024-03-10 00:37:42.083354 epoch: 10 step: 0 cls_loss= 0.97123 (146519 samples/sec)
2024-03-10 00:37:45.125518 epoch: 10 step: 100 cls_loss= 0.87931 (9861 samples/sec)
saving....
2024-03-10 00:37:48.935381------------------------------------------------------ Precision@1: 59.85% 

[60.01, 59.67, 59.61, 59.91, 59.57, 59.87, 59.95, 59.83, 60.03, 60.02, 59.85]

Epoch: 11
2024-03-10 00:37:49.130661 epoch: 11 step: 0 cls_loss= 0.95007 (154457 samples/sec)
2024-03-10 00:37:52.239767 epoch: 11 step: 100 cls_loss= 0.97142 (9649 samples/sec)
saving....
2024-03-10 00:37:56.104220------------------------------------------------------ Precision@1: 59.80% 

[60.01, 59.67, 59.61, 59.91, 59.57, 59.87, 59.95, 59.83, 60.03, 60.02, 59.85, 59.8]

Epoch: 12
2024-03-10 00:37:56.329406 epoch: 12 step: 0 cls_loss= 0.85872 (133823 samples/sec)
2024-03-10 00:37:59.403134 epoch: 12 step: 100 cls_loss= 1.00532 (9760 samples/sec)
saving....
2024-03-10 00:38:03.150539------------------------------------------------------ Precision@1: 59.87% 

[60.01, 59.67, 59.61, 59.91, 59.57, 59.87, 59.95, 59.83, 60.03, 60.02, 59.85, 59.8, 59.87]

Epoch: 13
2024-03-10 00:38:03.376320 epoch: 13 step: 0 cls_loss= 0.93603 (133496 samples/sec)
2024-03-10 00:38:06.427682 epoch: 13 step: 100 cls_loss= 1.14160 (9831 samples/sec)
saving....
2024-03-10 00:38:10.266434------------------------------------------------------ Precision@1: 60.09% 

[60.01, 59.67, 59.61, 59.91, 59.57, 59.87, 59.95, 59.83, 60.03, 60.02, 59.85, 59.8, 59.87, 60.09]

Epoch: 14
2024-03-10 00:38:10.464142 epoch: 14 step: 0 cls_loss= 1.02163 (152605 samples/sec)
2024-03-10 00:38:13.484497 epoch: 14 step: 100 cls_loss= 0.96064 (9932 samples/sec)
saving....
2024-03-10 00:38:17.240458------------------------------------------------------ Precision@1: 59.96% 

[60.01, 59.67, 59.61, 59.91, 59.57, 59.87, 59.95, 59.83, 60.03, 60.02, 59.85, 59.8, 59.87, 60.09, 59.96]

Epoch: 15
2024-03-10 00:38:17.438119 epoch: 15 step: 0 cls_loss= 1.02119 (152540 samples/sec)
2024-03-10 00:38:20.372069 epoch: 15 step: 100 cls_loss= 0.92185 (10225 samples/sec)
saving....
2024-03-10 00:38:24.026738------------------------------------------------------ Precision@1: 59.63% 

[60.01, 59.67, 59.61, 59.91, 59.57, 59.87, 59.95, 59.83, 60.03, 60.02, 59.85, 59.8, 59.87, 60.09, 59.96, 59.63]

Epoch: 16
2024-03-10 00:38:24.218025 epoch: 16 step: 0 cls_loss= 1.03043 (157773 samples/sec)
2024-03-10 00:38:27.236594 epoch: 16 step: 100 cls_loss= 0.98710 (9938 samples/sec)
saving....
2024-03-10 00:38:31.062628------------------------------------------------------ Precision@1: 59.89% 

[60.01, 59.67, 59.61, 59.91, 59.57, 59.87, 59.95, 59.83, 60.03, 60.02, 59.85, 59.8, 59.87, 60.09, 59.96, 59.63, 59.89]

Epoch: 17
2024-03-10 00:38:31.278223 epoch: 17 step: 0 cls_loss= 1.02999 (139897 samples/sec)
2024-03-10 00:38:34.388313 epoch: 17 step: 100 cls_loss= 0.97892 (9646 samples/sec)
saving....
2024-03-10 00:38:38.191458------------------------------------------------------ Precision@1: 59.60% 

[60.01, 59.67, 59.61, 59.91, 59.57, 59.87, 59.95, 59.83, 60.03, 60.02, 59.85, 59.8, 59.87, 60.09, 59.96, 59.63, 59.89, 59.6]

Epoch: 18
2024-03-10 00:38:38.387894 epoch: 18 step: 0 cls_loss= 1.00085 (153581 samples/sec)
2024-03-10 00:38:41.536907 epoch: 18 step: 100 cls_loss= 0.83463 (9527 samples/sec)
saving....
2024-03-10 00:38:45.378502------------------------------------------------------ Precision@1: 59.85% 

[60.01, 59.67, 59.61, 59.91, 59.57, 59.87, 59.95, 59.83, 60.03, 60.02, 59.85, 59.8, 59.87, 60.09, 59.96, 59.63, 59.89, 59.6, 59.85]

Epoch: 19
2024-03-10 00:38:45.577764 epoch: 19 step: 0 cls_loss= 0.97763 (151324 samples/sec)
2024-03-10 00:38:48.596609 epoch: 19 step: 100 cls_loss= 0.94243 (9937 samples/sec)
saving....
2024-03-10 00:38:52.350331------------------------------------------------------ Precision@1: 59.80% 

[60.01, 59.67, 59.61, 59.91, 59.57, 59.87, 59.95, 59.83, 60.03, 60.02, 59.85, 59.8, 59.87, 60.09, 59.96, 59.63, 59.89, 59.6, 59.85, 59.8]
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$[K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ exit

Script done on 2024-03-10 00:39:10+08:00 [COMMAND_EXIT_CODE="0"]
