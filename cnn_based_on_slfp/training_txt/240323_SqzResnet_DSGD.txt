Script started on 2024-03-23 20:21:04+08:00 [TERM="xterm-256color" TTY="/dev/pts/0" COLUMNS="214" LINES="17"]
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ bash bash_train.sh
=> creating model SqueezeNet ...
 learning rate =  5e-06
SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/workspaces/pytorch-dev/SLFP_CNNs/utils/optimizer.py:178: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-23 20:21:24.917622 epoch: 1 step: 0 cls_loss= 2.69790 (9789 samples/sec)
2024-03-23 20:26:31.762302 epoch: 1 step: 500 cls_loss= 2.26520 (52 samples/sec)
2024-03-23 20:31:40.325439 epoch: 1 step: 1000 cls_loss= 2.86535 (51 samples/sec)
2024-03-23 20:36:49.194805 epoch: 1 step: 1500 cls_loss= 2.38939 (51 samples/sec)
2024-03-23 20:41:52.452938 epoch: 1 step: 2000 cls_loss= 2.38049 (52 samples/sec)
2024-03-23 20:47:01.562927 epoch: 1 step: 2500 cls_loss= 2.58856 (51 samples/sec)
2024-03-23 20:52:05.814210 epoch: 1 step: 3000 cls_loss= 1.76687 (52 samples/sec)
2024-03-23 20:58:22.440284------------------------------------------------------ Precision@1: 57.85%  Precision@1: 80.29%

top1: [57.848]
top5: [80.286]
2024-03-23 20:58:22.628544 epoch: 2 step: 0 cls_loss= 2.16270 (85487 samples/sec)
2024-03-23 20:59:20.137712 epoch: 2 step: 500 cls_loss= 2.11556 (278 samples/sec)
2024-03-23 21:00:17.059690 epoch: 2 step: 1000 cls_loss= 2.64483 (281 samples/sec)
2024-03-23 21:01:14.096246 epoch: 2 step: 1500 cls_loss= 3.22590 (280 samples/sec)
2024-03-23 21:02:10.963700 epoch: 2 step: 2000 cls_loss= 2.72132 (281 samples/sec)
2024-03-23 21:03:07.869129 epoch: 2 step: 2500 cls_loss= 1.62768 (281 samples/sec)
2024-03-23 21:04:04.727554 epoch: 2 step: 3000 cls_loss= 2.74276 (281 samples/sec)
2024-03-23 21:07:05.456917------------------------------------------------------ Precision@1: 57.71%  Precision@1: 80.25%

top1: [57.848, 57.706]
top5: [80.286, 80.252]
2024-03-23 21:07:05.642947 epoch: 3 step: 0 cls_loss= 2.81082 (86533 samples/sec)
2024-03-23 21:08:02.021968 epoch: 3 step: 500 cls_loss= 2.50394 (283 samples/sec)
2024-03-23 21:08:58.559264 epoch: 3 step: 1000 cls_loss= 1.92503 (283 samples/sec)
2024-03-23 21:09:54.124075 epoch: 3 step: 1500 cls_loss= 1.87778 (288 samples/sec)
2024-03-23 21:10:50.288083 epoch: 3 step: 2000 cls_loss= 2.01354 (284 samples/sec)
2024-03-23 21:11:46.759368 epoch: 3 step: 2500 cls_loss= 1.76716 (283 samples/sec)
2024-03-23 21:12:43.108010 epoch: 3 step: 3000 cls_loss= 2.37744 (284 samples/sec)
2024-03-23 21:15:45.015558------------------------------------------------------ Precision@1: 57.81%  Precision@1: 80.27%

top1: [57.848, 57.706, 57.808]
top5: [80.286, 80.252, 80.266]
2024-03-23 21:15:45.219512 epoch: 4 step: 0 cls_loss= 3.03581 (78835 samples/sec)
2024-03-23 21:16:41.283972 epoch: 4 step: 500 cls_loss= 2.02422 (285 samples/sec)
2024-03-23 21:17:37.239987 epoch: 4 step: 1000 cls_loss= 2.18896 (285 samples/sec)
2024-03-23 21:18:33.276109 epoch: 4 step: 1500 cls_loss= 1.97293 (285 samples/sec)
2024-03-23 21:19:28.881606 epoch: 4 step: 2000 cls_loss= 2.47383 (287 samples/sec)
2024-03-23 21:20:24.696949 epoch: 4 step: 2500 cls_loss= 2.13659 (286 samples/sec)
2024-03-23 21:21:20.660780 epoch: 4 step: 3000 cls_loss= 1.97739 (285 samples/sec)
2024-03-23 21:24:20.670943------------------------------------------------------ Precision@1: 57.73%  Precision@1: 80.31%

top1: [57.848, 57.706, 57.808, 57.726]
top5: [80.286, 80.252, 80.266, 80.306]
2024-03-23 21:24:20.860870 epoch: 5 step: 0 cls_loss= 1.74147 (84725 samples/sec)
2024-03-23 21:25:17.998261 epoch: 5 step: 500 cls_loss= 1.56487 (280 samples/sec)
2024-03-23 21:26:15.232231 epoch: 5 step: 1000 cls_loss= 1.90755 (279 samples/sec)
2024-03-23 21:27:11.725026 epoch: 5 step: 1500 cls_loss= 1.88217 (283 samples/sec)
2024-03-23 21:28:08.533044 epoch: 5 step: 2000 cls_loss= 2.28095 (281 samples/sec)
2024-03-23 21:29:05.608739 epoch: 5 step: 2500 cls_loss= 2.14250 (280 samples/sec)
2024-03-23 21:30:03.111380 epoch: 5 step: 3000 cls_loss= 2.59467 (278 samples/sec)
2024-03-23 21:33:04.943772------------------------------------------------------ Precision@1: 57.77%  Precision@1: 80.27%

top1: [57.848, 57.706, 57.808, 57.726, 57.772]
top5: [80.286, 80.252, 80.266, 80.306, 80.272]
2024-03-23 21:33:05.137235 epoch: 6 step: 0 cls_loss= 1.69926 (83151 samples/sec)
2024-03-23 21:34:02.077683 epoch: 6 step: 500 cls_loss= 1.47729 (281 samples/sec)
2024-03-23 21:34:58.090993 epoch: 6 step: 1000 cls_loss= 2.40806 (285 samples/sec)
2024-03-23 21:35:55.168759 epoch: 6 step: 1500 cls_loss= 2.61397 (280 samples/sec)
2024-03-23 21:36:51.337584 epoch: 6 step: 2000 cls_loss= 2.62608 (284 samples/sec)
2024-03-23 21:37:47.823491 epoch: 6 step: 2500 cls_loss= 2.60370 (283 samples/sec)
2024-03-23 21:38:44.743269 epoch: 6 step: 3000 cls_loss= 2.33449 (281 samples/sec)
2024-03-23 21:41:44.315407------------------------------------------------------ Precision@1: 57.83%  Precision@1: 80.28%

top1: [57.848, 57.706, 57.808, 57.726, 57.772, 57.826]
top5: [80.286, 80.252, 80.266, 80.306, 80.272, 80.282]
2024-03-23 21:41:44.501929 epoch: 7 step: 0 cls_loss= 2.51734 (86301 samples/sec)
2024-03-23 21:42:41.677536 epoch: 7 step: 500 cls_loss= 2.03988 (279 samples/sec)
2024-03-23 21:43:37.931640 epoch: 7 step: 1000 cls_loss= 2.69930 (284 samples/sec)
2024-03-23 21:44:35.054434 epoch: 7 step: 1500 cls_loss= 2.73416 (280 samples/sec)
2024-03-23 21:45:31.913125 epoch: 7 step: 2000 cls_loss= 2.94908 (281 samples/sec)
2024-03-23 21:46:27.813029 epoch: 7 step: 2500 cls_loss= 2.73614 (286 samples/sec)
2024-03-23 21:47:24.364085 epoch: 7 step: 3000 cls_loss= 2.39627 (282 samples/sec)
2024-03-23 21:50:21.648183------------------------------------------------------ Precision@1: 57.71%  Precision@1: 80.19%

top1: [57.848, 57.706, 57.808, 57.726, 57.772, 57.826, 57.71]
top5: [80.286, 80.252, 80.266, 80.306, 80.272, 80.282, 80.194]
2024-03-23 21:50:21.914966 epoch: 8 step: 0 cls_loss= 2.47025 (60237 samples/sec)
2024-03-23 21:51:18.433882 epoch: 8 step: 500 cls_loss= 3.32240 (283 samples/sec)
2024-03-23 21:52:15.383669 epoch: 8 step: 1000 cls_loss= 2.11625 (281 samples/sec)
2024-03-23 21:53:12.153168 epoch: 8 step: 1500 cls_loss= 3.18726 (281 samples/sec)
2024-03-23 21:54:08.978830 epoch: 8 step: 2000 cls_loss= 2.49167 (281 samples/sec)
2024-03-23 21:55:06.272345 epoch: 8 step: 2500 cls_loss= 3.14412 (279 samples/sec)
2024-03-23 21:56:03.480764 epoch: 8 step: 3000 cls_loss= 3.39689 (279 samples/sec)
2024-03-23 21:59:02.346909------------------------------------------------------ Precision@1: 57.71%  Precision@1: 80.20%

top1: [57.848, 57.706, 57.808, 57.726, 57.772, 57.826, 57.71, 57.71]
top5: [80.286, 80.252, 80.266, 80.306, 80.272, 80.282, 80.194, 80.202]
2024-03-23 21:59:02.539016 epoch: 9 step: 0 cls_loss= 2.35635 (83792 samples/sec)
2024-03-23 21:59:59.144372 epoch: 9 step: 500 cls_loss= 2.39016 (282 samples/sec)
2024-03-23 22:00:55.759435 epoch: 9 step: 1000 cls_loss= 2.20101 (282 samples/sec)
2024-03-23 22:01:52.590248 epoch: 9 step: 1500 cls_loss= 2.42780 (281 samples/sec)
2024-03-23 22:02:49.190258 epoch: 9 step: 2000 cls_loss= 3.10448 (282 samples/sec)
2024-03-23 22:03:46.153102 epoch: 9 step: 2500 cls_loss= 2.53662 (280 samples/sec)
2024-03-23 22:04:43.088674 epoch: 9 step: 3000 cls_loss= 2.46203 (281 samples/sec)
2024-03-23 22:07:41.254341------------------------------------------------------ Precision@1: 57.60%  Precision@1: 80.17%

top1: [57.848, 57.706, 57.808, 57.726, 57.772, 57.826, 57.71, 57.71, 57.598]
top5: [80.286, 80.252, 80.266, 80.306, 80.272, 80.282, 80.194, 80.202, 80.166]
2024-03-23 22:07:41.443042 epoch: 10 step: 0 cls_loss= 2.28794 (85288 samples/sec)
2024-03-23 22:08:37.194552 epoch: 10 step: 500 cls_loss= 2.12741 (287 samples/sec)
2024-03-23 22:09:33.183581 epoch: 10 step: 1000 cls_loss= 2.86132 (285 samples/sec)
2024-03-23 22:10:29.237424 epoch: 10 step: 1500 cls_loss= 2.70764 (285 samples/sec)
2024-03-23 22:11:25.345713 epoch: 10 step: 2000 cls_loss= 2.39758 (285 samples/sec)
2024-03-23 22:12:21.242942 epoch: 10 step: 2500 cls_loss= 2.32905 (286 samples/sec)
2024-03-23 22:13:17.754375 epoch: 10 step: 3000 cls_loss= 2.23215 (283 samples/sec)
2024-03-23 22:16:14.057867------------------------------------------------------ Precision@1: 57.82%  Precision@1: 80.18%

top1: [57.848, 57.706, 57.808, 57.726, 57.772, 57.826, 57.71, 57.71, 57.598, 57.82]
top5: [80.286, 80.252, 80.266, 80.306, 80.272, 80.282, 80.194, 80.202, 80.166, 80.18]
=> creating model ResNet-50 ...
 learning rate =  5e-06
SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/workspaces/pytorch-dev/SLFP_CNNs/utils/optimizer.py:178: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-23 22:16:17.600311 epoch: 1 step: 0 cls_loss= 1.13960 (9419 samples/sec)
2024-03-23 22:17:49.512714 epoch: 1 step: 500 cls_loss= 1.23342 (174 samples/sec)
2024-03-23 22:19:21.667711 epoch: 1 step: 1000 cls_loss= 0.61809 (173 samples/sec)
2024-03-23 22:20:53.788564 epoch: 1 step: 1500 cls_loss= 0.78974 (173 samples/sec)
2024-03-23 22:22:26.128019 epoch: 1 step: 2000 cls_loss= 1.29400 (173 samples/sec)
2024-03-23 22:23:58.546800 epoch: 1 step: 2500 cls_loss= 1.13343 (173 samples/sec)
2024-03-23 22:25:31.012762 epoch: 1 step: 3000 cls_loss= 0.71771 (173 samples/sec)
2024-03-23 22:29:37.457289------------------------------------------------------ Precision@1: 75.99%  Precision@1: 92.87%

top1: [75.99]
top5: [92.866]
2024-03-23 22:29:37.794774 epoch: 2 step: 0 cls_loss= 1.07955 (47608 samples/sec)
2024-03-23 22:31:10.444426 epoch: 2 step: 500 cls_loss= 1.14638 (172 samples/sec)
2024-03-23 22:32:43.264942 epoch: 2 step: 1000 cls_loss= 1.11143 (172 samples/sec)
2024-03-23 22:34:15.952478 epoch: 2 step: 1500 cls_loss= 1.00687 (172 samples/sec)
2024-03-23 22:35:48.593127 epoch: 2 step: 2000 cls_loss= 0.97298 (172 samples/sec)
2024-03-23 22:37:21.231090 epoch: 2 step: 2500 cls_loss= 0.82504 (172 samples/sec)
2024-03-23 22:38:53.739113 epoch: 2 step: 3000 cls_loss= 1.18624 (172 samples/sec)
2024-03-23 22:43:00.054552------------------------------------------------------ Precision@1: 76.06%  Precision@1: 92.91%

top1: [75.99, 76.062]
top5: [92.866, 92.91]
2024-03-23 22:43:00.404595 epoch: 3 step: 0 cls_loss= 1.02334 (45900 samples/sec)
2024-03-23 22:44:32.728337 epoch: 3 step: 500 cls_loss= 1.39043 (173 samples/sec)
2024-03-23 22:46:05.082299 epoch: 3 step: 1000 cls_loss= 1.23952 (173 samples/sec)
2024-03-23 22:47:37.349652 epoch: 3 step: 1500 cls_loss= 1.43139 (173 samples/sec)
2024-03-23 22:49:09.603205 epoch: 3 step: 2000 cls_loss= 0.86828 (173 samples/sec)
2024-03-23 22:50:42.046288 epoch: 3 step: 2500 cls_loss= 0.73734 (173 samples/sec)
2024-03-23 22:52:14.699466 epoch: 3 step: 3000 cls_loss= 1.07541 (172 samples/sec)
2024-03-23 22:56:20.870329------------------------------------------------------ Precision@1: 76.12%  Precision@1: 92.85%

top1: [75.99, 76.062, 76.116]
top5: [92.866, 92.91, 92.854]
2024-03-23 22:56:21.210601 epoch: 4 step: 0 cls_loss= 1.48008 (47196 samples/sec)
2024-03-23 22:57:53.621816 epoch: 4 step: 500 cls_loss= 1.29334 (173 samples/sec)
2024-03-23 22:59:26.151730 epoch: 4 step: 1000 cls_loss= 1.22342 (172 samples/sec)
2024-03-23 23:00:58.739701 epoch: 4 step: 1500 cls_loss= 0.82270 (172 samples/sec)
2024-03-23 23:02:31.262962 epoch: 4 step: 2000 cls_loss= 0.91627 (172 samples/sec)
2024-03-23 23:04:03.806993 epoch: 4 step: 2500 cls_loss= 1.24186 (172 samples/sec)
2024-03-23 23:05:36.211834 epoch: 4 step: 3000 cls_loss= 1.30345 (173 samples/sec)
2024-03-23 23:09:42.192829------------------------------------------------------ Precision@1: 76.23%  Precision@1: 92.82%

top1: [75.99, 76.062, 76.116, 76.226]
top5: [92.866, 92.91, 92.854, 92.822]
2024-03-23 23:09:42.530130 epoch: 5 step: 0 cls_loss= 0.59205 (47590 samples/sec)
2024-03-23 23:11:15.119306 epoch: 5 step: 500 cls_loss= 1.28693 (172 samples/sec)
2024-03-23 23:12:47.491942 epoch: 5 step: 1000 cls_loss= 1.58295 (173 samples/sec)
2024-03-23 23:14:19.863788 epoch: 5 step: 1500 cls_loss= 1.53410 (173 samples/sec)
2024-03-23 23:15:52.188444 epoch: 5 step: 2000 cls_loss= 0.98700 (173 samples/sec)
2024-03-23 23:17:24.457581 epoch: 5 step: 2500 cls_loss= 1.10980 (173 samples/sec)
2024-03-23 23:18:56.747463 epoch: 5 step: 3000 cls_loss= 1.28474 (173 samples/sec)
2024-03-23 23:23:02.852248------------------------------------------------------ Precision@1: 76.27%  Precision@1: 92.91%

top1: [75.99, 76.062, 76.116, 76.226, 76.272]
top5: [92.866, 92.91, 92.854, 92.822, 92.906]
2024-03-23 23:23:03.207037 epoch: 6 step: 0 cls_loss= 0.89257 (45266 samples/sec)
2024-03-23 23:24:35.571590 epoch: 6 step: 500 cls_loss= 0.84220 (173 samples/sec)
2024-03-23 23:26:07.969592 epoch: 6 step: 1000 cls_loss= 1.71734 (173 samples/sec)
2024-03-23 23:27:40.372582 epoch: 6 step: 1500 cls_loss= 0.89411 (173 samples/sec)
2024-03-23 23:29:12.631043 epoch: 6 step: 2000 cls_loss= 0.94153 (173 samples/sec)
2024-03-23 23:30:44.933106 epoch: 6 step: 2500 cls_loss= 0.62233 (173 samples/sec)
2024-03-23 23:32:17.345347 epoch: 6 step: 3000 cls_loss= 1.23156 (173 samples/sec)
2024-03-23 23:36:23.734688------------------------------------------------------ Precision@1: 76.23%  Precision@1: 92.94%

top1: [75.99, 76.062, 76.116, 76.226, 76.272, 76.234]
top5: [92.866, 92.91, 92.854, 92.822, 92.906, 92.94200000000001]
2024-03-23 23:36:24.062651 epoch: 7 step: 0 cls_loss= 1.61297 (48983 samples/sec)
2024-03-23 23:37:56.465417 epoch: 7 step: 500 cls_loss= 0.98011 (173 samples/sec)
2024-03-23 23:39:29.044354 epoch: 7 step: 1000 cls_loss= 0.75922 (172 samples/sec)
2024-03-23 23:41:01.575725 epoch: 7 step: 1500 cls_loss= 1.01110 (172 samples/sec)
2024-03-23 23:42:34.203162 epoch: 7 step: 2000 cls_loss= 0.77285 (172 samples/sec)
2024-03-23 23:44:06.749443 epoch: 7 step: 2500 cls_loss= 0.85494 (172 samples/sec)
2024-03-23 23:45:39.418631 epoch: 7 step: 3000 cls_loss= 1.75986 (172 samples/sec)
2024-03-23 23:49:45.678219------------------------------------------------------ Precision@1: 76.28%  Precision@1: 92.89%

top1: [75.99, 76.062, 76.116, 76.226, 76.272, 76.234, 76.28]
top5: [92.866, 92.91, 92.854, 92.822, 92.906, 92.94200000000001, 92.89]
2024-03-23 23:49:46.018826 epoch: 8 step: 0 cls_loss= 1.15105 (47117 samples/sec)
2024-03-23 23:51:18.385045 epoch: 8 step: 500 cls_loss= 0.57634 (173 samples/sec)
2024-03-23 23:52:50.931527 epoch: 8 step: 1000 cls_loss= 0.68999 (172 samples/sec)
2024-03-23 23:54:23.448733 epoch: 8 step: 1500 cls_loss= 0.88087 (172 samples/sec)
2024-03-23 23:55:56.051849 epoch: 8 step: 2000 cls_loss= 1.31104 (172 samples/sec)
2024-03-23 23:57:28.738080 epoch: 8 step: 2500 cls_loss= 0.87149 (172 samples/sec)
2024-03-23 23:59:01.560894 epoch: 8 step: 3000 cls_loss= 1.17640 (172 samples/sec)
2024-03-24 00:03:07.681475------------------------------------------------------ Precision@1: 76.30%  Precision@1: 92.93%

top1: [75.99, 76.062, 76.116, 76.226, 76.272, 76.234, 76.28, 76.298]
top5: [92.866, 92.91, 92.854, 92.822, 92.906, 92.94200000000001, 92.89, 92.926]
2024-03-24 00:03:08.017724 epoch: 9 step: 0 cls_loss= 0.85465 (47775 samples/sec)
2024-03-24 00:04:40.368503 epoch: 9 step: 500 cls_loss= 0.75360 (173 samples/sec)
2024-03-24 00:06:12.623603 epoch: 9 step: 1000 cls_loss= 0.92403 (173 samples/sec)
2024-03-24 00:07:44.895720 epoch: 9 step: 1500 cls_loss= 0.85601 (173 samples/sec)
2024-03-24 00:09:17.202318 epoch: 9 step: 2000 cls_loss= 1.37056 (173 samples/sec)
2024-03-24 00:10:49.530195 epoch: 9 step: 2500 cls_loss= 0.71343 (173 samples/sec)
2024-03-24 00:12:21.965885 epoch: 9 step: 3000 cls_loss= 0.97367 (173 samples/sec)
2024-03-24 00:16:28.175547------------------------------------------------------ Precision@1: 76.26%  Precision@1: 92.93%

top1: [75.99, 76.062, 76.116, 76.226, 76.272, 76.234, 76.28, 76.298, 76.256]
top5: [92.866, 92.91, 92.854, 92.822, 92.906, 92.94200000000001, 92.89, 92.926, 92.93]
2024-03-24 00:16:28.517357 epoch: 10 step: 0 cls_loss= 0.93807 (47001 samples/sec)
2024-03-24 00:18:01.034446 epoch: 10 step: 500 cls_loss= 1.42543 (172 samples/sec)
2024-03-24 00:19:33.241589 epoch: 10 step: 1000 cls_loss= 1.19096 (173 samples/sec)
2024-03-24 00:21:05.676493 epoch: 10 step: 1500 cls_loss= 1.36173 (173 samples/sec)
2024-03-24 00:22:38.035693 epoch: 10 step: 2000 cls_loss= 1.38261 (173 samples/sec)
2024-03-24 00:24:10.304212 epoch: 10 step: 2500 cls_loss= 0.96321 (173 samples/sec)
2024-03-24 00:25:42.598718 epoch: 10 step: 3000 cls_loss= 1.21476 (173 samples/sec)
2024-03-24 00:29:48.574032------------------------------------------------------ Precision@1: 76.27%  Precision@1: 93.00%

top1: [75.99, 76.062, 76.116, 76.226, 76.272, 76.234, 76.28, 76.298, 76.256, 76.266]
top5: [92.866, 92.91, 92.854, 92.822, 92.906, 92.94200000000001, 92.89, 92.926, 92.93, 92.998]
=> creating model SqueezeNet ...
 learning rate =  5e-06
SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/workspaces/pytorch-dev/SLFP_CNNs/utils/optimizer.py:178: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-24 00:29:51.074352 epoch: 1 step: 0 cls_loss= 1.94624 (15005 samples/sec)
2024-03-24 00:30:48.149826 epoch: 1 step: 500 cls_loss= 1.62280 (280 samples/sec)
2024-03-24 00:31:45.587326 epoch: 1 step: 1000 cls_loss= 2.97884 (278 samples/sec)
2024-03-24 00:32:43.112204 epoch: 1 step: 1500 cls_loss= 2.64103 (278 samples/sec)
2024-03-24 00:33:40.460627 epoch: 1 step: 2000 cls_loss= 2.12838 (279 samples/sec)
2024-03-24 00:34:37.676765 epoch: 1 step: 2500 cls_loss= 1.87928 (279 samples/sec)
2024-03-24 00:35:35.055956 epoch: 1 step: 3000 cls_loss= 3.82871 (278 samples/sec)
2024-03-24 00:38:34.036726------------------------------------------------------ Precision@1: 57.33%  Precision@1: 79.92%

top1: [57.326]
top5: [79.918]
2024-03-24 00:38:34.229313 epoch: 2 step: 0 cls_loss= 2.27647 (83550 samples/sec)
2024-03-24 00:39:30.757431 epoch: 2 step: 500 cls_loss= 2.23537 (283 samples/sec)
2024-03-24 00:40:27.267824 epoch: 2 step: 1000 cls_loss= 2.59278 (283 samples/sec)
2024-03-24 00:41:22.683525 epoch: 2 step: 1500 cls_loss= 2.89941 (288 samples/sec)
2024-03-24 00:42:17.762883 epoch: 2 step: 2000 cls_loss= 2.64677 (290 samples/sec)
2024-03-24 00:43:13.660350 epoch: 2 step: 2500 cls_loss= 3.15349 (286 samples/sec)
2024-03-24 00:44:09.225695 epoch: 2 step: 3000 cls_loss= 2.31542 (288 samples/sec)
2024-03-24 00:47:05.796663------------------------------------------------------ Precision@1: 57.39%  Precision@1: 79.95%

top1: [57.326, 57.386]
top5: [79.918, 79.946]
2024-03-24 00:47:05.988339 epoch: 3 step: 0 cls_loss= 2.00891 (83932 samples/sec)
2024-03-24 00:48:02.159626 epoch: 3 step: 500 cls_loss= 1.84758 (284 samples/sec)
2024-03-24 00:48:59.130888 epoch: 3 step: 1000 cls_loss= 1.87016 (280 samples/sec)
2024-03-24 00:49:55.826210 epoch: 3 step: 1500 cls_loss= 2.35745 (282 samples/sec)
2024-03-24 00:50:52.719453 epoch: 3 step: 2000 cls_loss= 2.50265 (281 samples/sec)
2024-03-24 00:51:48.657001 epoch: 3 step: 2500 cls_loss= 3.60875 (286 samples/sec)
2024-03-24 00:52:45.326456 epoch: 3 step: 3000 cls_loss= 3.60588 (282 samples/sec)
2024-03-24 00:55:45.712865------------------------------------------------------ Precision@1: 57.42%  Precision@1: 79.96%

top1: [57.326, 57.386, 57.418]
top5: [79.918, 79.946, 79.958]
2024-03-24 00:55:45.901764 epoch: 4 step: 0 cls_loss= 2.35182 (85195 samples/sec)
2024-03-24 00:56:42.214703 epoch: 4 step: 500 cls_loss= 2.19740 (284 samples/sec)
2024-03-24 00:57:38.787724 epoch: 4 step: 1000 cls_loss= 1.98557 (282 samples/sec)
2024-03-24 00:58:34.444228 epoch: 4 step: 1500 cls_loss= 2.70727 (287 samples/sec)
2024-03-24 00:59:31.166976 epoch: 4 step: 2000 cls_loss= 3.14544 (282 samples/sec)
2024-03-24 01:00:27.673591 epoch: 4 step: 2500 cls_loss= 2.09809 (283 samples/sec)
2024-03-24 01:01:24.121658 epoch: 4 step: 3000 cls_loss= 2.31569 (283 samples/sec)
2024-03-24 01:04:21.303369------------------------------------------------------ Precision@1: 57.30%  Precision@1: 79.96%

top1: [57.326, 57.386, 57.418, 57.298]
top5: [79.918, 79.946, 79.958, 79.962]
2024-03-24 01:04:21.487847 epoch: 5 step: 0 cls_loss= 2.04449 (87216 samples/sec)
2024-03-24 01:05:18.262709 epoch: 5 step: 500 cls_loss= 2.48067 (281 samples/sec)
2024-03-24 01:06:14.837215 epoch: 5 step: 1000 cls_loss= 3.28032 (282 samples/sec)
2024-03-24 01:07:11.298889 epoch: 5 step: 1500 cls_loss= 2.33569 (283 samples/sec)
2024-03-24 01:08:08.337365 epoch: 5 step: 2000 cls_loss= 2.99599 (280 samples/sec)
2024-03-24 01:09:05.150100 epoch: 5 step: 2500 cls_loss= 2.17049 (281 samples/sec)
2024-03-24 01:10:01.949533 epoch: 5 step: 3000 cls_loss= 2.28962 (281 samples/sec)
2024-03-24 01:13:01.572963------------------------------------------------------ Precision@1: 57.47%  Precision@1: 79.95%

top1: [57.326, 57.386, 57.418, 57.298, 57.47]
top5: [79.918, 79.946, 79.958, 79.962, 79.952]
2024-03-24 01:13:01.761728 epoch: 6 step: 0 cls_loss= 2.21949 (85233 samples/sec)
2024-03-24 01:13:58.509174 epoch: 6 step: 500 cls_loss= 2.62207 (281 samples/sec)
2024-03-24 01:14:55.787349 epoch: 6 step: 1000 cls_loss= 2.67641 (279 samples/sec)
2024-03-24 01:15:51.712097 epoch: 6 step: 1500 cls_loss= 2.16950 (286 samples/sec)
2024-03-24 01:16:46.760510 epoch: 6 step: 2000 cls_loss= 2.86562 (290 samples/sec)
2024-03-24 01:17:43.795456 epoch: 6 step: 2500 cls_loss= 2.43058 (280 samples/sec)
2024-03-24 01:18:40.754868 epoch: 6 step: 3000 cls_loss= 2.92465 (280 samples/sec)
2024-03-24 01:21:41.805787------------------------------------------------------ Precision@1: 57.30%  Precision@1: 79.91%

top1: [57.326, 57.386, 57.418, 57.298, 57.47, 57.300000000000004]
top5: [79.918, 79.946, 79.958, 79.962, 79.952, 79.91]
2024-03-24 01:21:41.996227 epoch: 7 step: 0 cls_loss= 2.11064 (84450 samples/sec)
2024-03-24 01:22:38.433378 epoch: 7 step: 500 cls_loss= 1.79248 (283 samples/sec)
2024-03-24 01:23:34.152022 epoch: 7 step: 1000 cls_loss= 2.85743 (287 samples/sec)
2024-03-24 01:24:30.347022 epoch: 7 step: 1500 cls_loss= 2.32341 (284 samples/sec)
2024-03-24 01:25:26.303862 epoch: 7 step: 2000 cls_loss= 2.59984 (285 samples/sec)
2024-03-24 01:26:21.714953 epoch: 7 step: 2500 cls_loss= 2.43089 (288 samples/sec)
2024-03-24 01:27:18.102429 epoch: 7 step: 3000 cls_loss= 3.04888 (283 samples/sec)
2024-03-24 01:30:15.955251------------------------------------------------------ Precision@1: 57.41%  Precision@1: 79.96%

top1: [57.326, 57.386, 57.418, 57.298, 57.47, 57.300000000000004, 57.408]
top5: [79.918, 79.946, 79.958, 79.962, 79.952, 79.91, 79.964]
2024-03-24 01:30:16.154380 epoch: 8 step: 0 cls_loss= 1.68653 (80794 samples/sec)
2024-03-24 01:31:12.823633 epoch: 8 step: 500 cls_loss= 2.54017 (282 samples/sec)
2024-03-24 01:32:10.030491 epoch: 8 step: 1000 cls_loss= 3.02382 (279 samples/sec)
2024-03-24 01:33:06.483346 epoch: 8 step: 1500 cls_loss= 2.71329 (283 samples/sec)
2024-03-24 01:34:03.256483 epoch: 8 step: 2000 cls_loss= 2.12911 (281 samples/sec)
2024-03-24 01:35:00.256846 epoch: 8 step: 2500 cls_loss= 2.23320 (280 samples/sec)
2024-03-24 01:35:56.016086 epoch: 8 step: 3000 cls_loss= 2.84224 (287 samples/sec)
2024-03-24 01:38:56.204755------------------------------------------------------ Precision@1: 57.38%  Precision@1: 79.95%

top1: [57.326, 57.386, 57.418, 57.298, 57.47, 57.300000000000004, 57.408, 57.376]
top5: [79.918, 79.946, 79.958, 79.962, 79.952, 79.91, 79.964, 79.946]
2024-03-24 01:38:56.393486 epoch: 9 step: 0 cls_loss= 1.77461 (85248 samples/sec)
2024-03-24 01:39:52.319651 epoch: 9 step: 500 cls_loss= 2.50876 (286 samples/sec)
2024-03-24 01:40:48.240911 epoch: 9 step: 1000 cls_loss= 2.17632 (286 samples/sec)
2024-03-24 01:41:44.346386 epoch: 9 step: 1500 cls_loss= 2.35862 (285 samples/sec)
2024-03-24 01:42:40.180887 epoch: 9 step: 2000 cls_loss= 1.87249 (286 samples/sec)
2024-03-24 01:43:36.878536 epoch: 9 step: 2500 cls_loss= 2.39072 (282 samples/sec)
2024-03-24 01:44:33.699525 epoch: 9 step: 3000 cls_loss= 2.37917 (281 samples/sec)
2024-03-24 01:47:30.461090------------------------------------------------------ Precision@1: 57.43%  Precision@1: 79.96%

top1: [57.326, 57.386, 57.418, 57.298, 57.47, 57.300000000000004, 57.408, 57.376, 57.426]
top5: [79.918, 79.946, 79.958, 79.962, 79.952, 79.91, 79.964, 79.946, 79.964]
2024-03-24 01:47:30.655040 epoch: 10 step: 0 cls_loss= 2.17533 (82935 samples/sec)
2024-03-24 01:48:27.224184 epoch: 10 step: 500 cls_loss= 2.44267 (282 samples/sec)
2024-03-24 01:49:24.123441 epoch: 10 step: 1000 cls_loss= 2.63007 (281 samples/sec)
2024-03-24 01:50:20.210996 epoch: 10 step: 1500 cls_loss= 2.41715 (285 samples/sec)
2024-03-24 01:51:16.998664 epoch: 10 step: 2000 cls_loss= 2.60516 (281 samples/sec)
2024-03-24 01:52:13.851817 epoch: 10 step: 2500 cls_loss= 1.89494 (281 samples/sec)
2024-03-24 01:53:10.971625 epoch: 10 step: 3000 cls_loss= 2.55667 (280 samples/sec)
2024-03-24 01:56:09.582611------------------------------------------------------ Precision@1: 57.37%  Precision@1: 79.91%

top1: [57.326, 57.386, 57.418, 57.298, 57.47, 57.300000000000004, 57.408, 57.376, 57.426, 57.366]
top5: [79.918, 79.946, 79.958, 79.962, 79.952, 79.91, 79.964, 79.946, 79.964, 79.912]
=> creating model ResNet-50 ...
 learning rate =  5e-06
SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/workspaces/pytorch-dev/SLFP_CNNs/utils/optimizer.py:178: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-24 01:56:12.995757 epoch: 1 step: 0 cls_loss= 0.73228 (10121 samples/sec)
2024-03-24 01:57:35.588128 epoch: 1 step: 500 cls_loss= 1.76818 (193 samples/sec)
2024-03-24 01:58:58.300682 epoch: 1 step: 1000 cls_loss= 0.96761 (193 samples/sec)
2024-03-24 02:00:21.065672 epoch: 1 step: 1500 cls_loss= 0.72417 (193 samples/sec)
2024-03-24 02:01:43.938472 epoch: 1 step: 2000 cls_loss= 1.33977 (193 samples/sec)
2024-03-24 02:03:06.812126 epoch: 1 step: 2500 cls_loss= 0.88282 (193 samples/sec)
2024-03-24 02:04:29.717272 epoch: 1 step: 3000 cls_loss= 0.66710 (192 samples/sec)
2024-03-24 02:07:58.650380------------------------------------------------------ Precision@1: 75.75%  Precision@1: 92.69%

top1: [75.75]
top5: [92.69200000000001]
2024-03-24 02:07:58.961342 epoch: 2 step: 0 cls_loss= 0.87014 (51689 samples/sec)
2024-03-24 02:09:21.897117 epoch: 2 step: 500 cls_loss= 1.56442 (192 samples/sec)
2024-03-24 02:10:44.906695 epoch: 2 step: 1000 cls_loss= 1.51034 (192 samples/sec)
2024-03-24 02:12:07.885880 epoch: 2 step: 1500 cls_loss= 1.44829 (192 samples/sec)
2024-03-24 02:13:30.763553 epoch: 2 step: 2000 cls_loss= 1.55738 (193 samples/sec)
2024-03-24 02:14:53.643828 epoch: 2 step: 2500 cls_loss= 1.75052 (193 samples/sec)
2024-03-24 02:16:16.589269 epoch: 2 step: 3000 cls_loss= 0.90512 (192 samples/sec)
2024-03-24 02:19:45.191278------------------------------------------------------ Precision@1: 75.79%  Precision@1: 92.66%

top1: [75.75, 75.786]
top5: [92.69200000000001, 92.656]
2024-03-24 02:19:45.510865 epoch: 3 step: 0 cls_loss= 1.15348 (50273 samples/sec)
2024-03-24 02:21:08.416292 epoch: 3 step: 500 cls_loss= 0.90453 (192 samples/sec)
2024-03-24 02:22:31.354628 epoch: 3 step: 1000 cls_loss= 1.23606 (192 samples/sec)
2024-03-24 02:23:54.278273 epoch: 3 step: 1500 cls_loss= 0.71896 (192 samples/sec)
2024-03-24 02:25:17.174437 epoch: 3 step: 2000 cls_loss= 1.09137 (193 samples/sec)
2024-03-24 02:26:40.116214 epoch: 3 step: 2500 cls_loss= 0.99788 (192 samples/sec)
2024-03-24 02:28:03.150485 epoch: 3 step: 3000 cls_loss= 1.13607 (192 samples/sec)
2024-03-24 02:31:31.458957------------------------------------------------------ Precision@1: 75.96%  Precision@1: 92.77%

top1: [75.75, 75.786, 75.962]
top5: [92.69200000000001, 92.656, 92.774]
2024-03-24 02:31:31.794192 epoch: 4 step: 0 cls_loss= 0.91273 (47914 samples/sec)
2024-03-24 02:32:54.628757 epoch: 4 step: 500 cls_loss= 0.93435 (193 samples/sec)
2024-03-24 02:34:17.514681 epoch: 4 step: 1000 cls_loss= 1.29211 (193 samples/sec)
2024-03-24 02:35:40.426335 epoch: 4 step: 1500 cls_loss= 1.62549 (192 samples/sec)
2024-03-24 02:37:03.263437 epoch: 4 step: 2000 cls_loss= 1.20475 (193 samples/sec)
2024-03-24 02:38:26.057072 epoch: 4 step: 2500 cls_loss= 0.68894 (193 samples/sec)
2024-03-24 02:39:48.880446 epoch: 4 step: 3000 cls_loss= 0.86523 (193 samples/sec)
2024-03-24 02:43:16.948087------------------------------------------------------ Precision@1: 75.92%  Precision@1: 92.79%

top1: [75.75, 75.786, 75.962, 75.918]
top5: [92.69200000000001, 92.656, 92.774, 92.792]
2024-03-24 02:43:17.262619 epoch: 5 step: 0 cls_loss= 1.11786 (51081 samples/sec)
2024-03-24 02:44:40.090359 epoch: 5 step: 500 cls_loss= 1.19309 (193 samples/sec)
2024-03-24 02:46:03.042630 epoch: 5 step: 1000 cls_loss= 0.77896 (192 samples/sec)
2024-03-24 02:47:26.055777 epoch: 5 step: 1500 cls_loss= 1.94235 (192 samples/sec)
2024-03-24 02:48:48.901770 epoch: 5 step: 2000 cls_loss= 0.87828 (193 samples/sec)
2024-03-24 02:50:11.730489 epoch: 5 step: 2500 cls_loss= 0.92749 (193 samples/sec)
2024-03-24 02:51:34.726097 epoch: 5 step: 3000 cls_loss= 0.95768 (192 samples/sec)
2024-03-24 02:55:03.340318------------------------------------------------------ Precision@1: 75.91%  Precision@1: 92.77%

top1: [75.75, 75.786, 75.962, 75.918, 75.906]
top5: [92.69200000000001, 92.656, 92.774, 92.792, 92.768]
2024-03-24 02:55:03.673832 epoch: 6 step: 0 cls_loss= 1.27803 (48174 samples/sec)
2024-03-24 02:56:26.487126 epoch: 6 step: 500 cls_loss= 0.99986 (193 samples/sec)
2024-03-24 02:57:49.434896 epoch: 6 step: 1000 cls_loss= 1.41920 (192 samples/sec)
2024-03-24 02:59:12.435010 epoch: 6 step: 1500 cls_loss= 1.56786 (192 samples/sec)
2024-03-24 03:00:35.458985 epoch: 6 step: 2000 cls_loss= 0.77822 (192 samples/sec)
2024-03-24 03:01:58.477462 epoch: 6 step: 2500 cls_loss= 0.71200 (192 samples/sec)
2024-03-24 03:03:21.386650 epoch: 6 step: 3000 cls_loss= 0.69145 (192 samples/sec)
2024-03-24 03:06:50.158348------------------------------------------------------ Precision@1: 76.00%  Precision@1: 92.79%

top1: [75.75, 75.786, 75.962, 75.918, 75.906, 76.0]
top5: [92.69200000000001, 92.656, 92.774, 92.792, 92.768, 92.786]
2024-03-24 03:06:50.477414 epoch: 7 step: 0 cls_loss= 0.96247 (50363 samples/sec)
2024-03-24 03:08:13.305743 epoch: 7 step: 500 cls_loss= 0.88160 (193 samples/sec)
2024-03-24 03:09:36.165460 epoch: 7 step: 1000 cls_loss= 1.45759 (193 samples/sec)
2024-03-24 03:10:59.010318 epoch: 7 step: 1500 cls_loss= 0.94011 (193 samples/sec)
2024-03-24 03:12:22.070964 epoch: 7 step: 2000 cls_loss= 0.79717 (192 samples/sec)
2024-03-24 03:13:45.084770 epoch: 7 step: 2500 cls_loss= 1.18895 (192 samples/sec)
2024-03-24 03:15:08.103262 epoch: 7 step: 3000 cls_loss= 0.74180 (192 samples/sec)
2024-03-24 03:18:36.875944------------------------------------------------------ Precision@1: 75.86%  Precision@1: 92.76%

top1: [75.75, 75.786, 75.962, 75.918, 75.906, 76.0, 75.86]
top5: [92.69200000000001, 92.656, 92.774, 92.792, 92.768, 92.786, 92.76]
2024-03-24 03:18:37.191023 epoch: 8 step: 0 cls_loss= 1.22816 (50962 samples/sec)
2024-03-24 03:19:59.947587 epoch: 8 step: 500 cls_loss= 1.01786 (193 samples/sec)
2024-03-24 03:21:22.775004 epoch: 8 step: 1000 cls_loss= 0.90936 (193 samples/sec)
2024-03-24 03:22:45.698177 epoch: 8 step: 1500 cls_loss= 1.12239 (192 samples/sec)
2024-03-24 03:24:08.734855 epoch: 8 step: 2000 cls_loss= 0.80057 (192 samples/sec)
2024-03-24 03:25:31.795172 epoch: 8 step: 2500 cls_loss= 1.59458 (192 samples/sec)
2024-03-24 03:26:54.732401 epoch: 8 step: 3000 cls_loss= 0.77862 (192 samples/sec)
2024-03-24 03:30:23.724668------------------------------------------------------ Precision@1: 75.94%  Precision@1: 92.78%

top1: [75.75, 75.786, 75.962, 75.918, 75.906, 76.0, 75.86, 75.94200000000001]
top5: [92.69200000000001, 92.656, 92.774, 92.792, 92.768, 92.786, 92.76, 92.778]
2024-03-24 03:30:24.032833 epoch: 9 step: 0 cls_loss= 0.81584 (52157 samples/sec)
2024-03-24 03:31:46.848888 epoch: 9 step: 500 cls_loss= 0.89410 (193 samples/sec)
2024-03-24 03:33:09.670432 epoch: 9 step: 1000 cls_loss= 1.15765 (193 samples/sec)
2024-03-24 03:34:32.549136 epoch: 9 step: 1500 cls_loss= 0.95346 (193 samples/sec)
2024-03-24 03:35:55.514411 epoch: 9 step: 2000 cls_loss= 1.07387 (192 samples/sec)
2024-03-24 03:37:18.400094 epoch: 9 step: 2500 cls_loss= 1.44357 (193 samples/sec)
2024-03-24 03:38:41.332789 epoch: 9 step: 3000 cls_loss= 1.12715 (192 samples/sec)
2024-03-24 03:42:09.833715------------------------------------------------------ Precision@1: 75.93%  Precision@1: 92.81%

top1: [75.75, 75.786, 75.962, 75.918, 75.906, 76.0, 75.86, 75.94200000000001, 75.934]
top5: [92.69200000000001, 92.656, 92.774, 92.792, 92.768, 92.786, 92.76, 92.778, 92.81400000000001]
2024-03-24 03:42:10.148754 epoch: 10 step: 0 cls_loss= 0.75803 (51017 samples/sec)
2024-03-24 03:43:33.023194 epoch: 10 step: 500 cls_loss= 0.93490 (193 samples/sec)
2024-03-24 03:44:55.794446 epoch: 10 step: 1000 cls_loss= 1.09779 (193 samples/sec)
2024-03-24 03:46:18.586981 epoch: 10 step: 1500 cls_loss= 1.61905 (193 samples/sec)
2024-03-24 03:47:41.445976 epoch: 10 step: 2000 cls_loss= 1.28828 (193 samples/sec)
2024-03-24 03:49:04.364717 epoch: 10 step: 2500 cls_loss= 0.93858 (192 samples/sec)
2024-03-24 03:50:27.235587 epoch: 10 step: 3000 cls_loss= 0.84896 (193 samples/sec)
2024-03-24 03:53:55.987157------------------------------------------------------ Precision@1: 75.94%  Precision@1: 92.81%

top1: [75.75, 75.786, 75.962, 75.918, 75.906, 76.0, 75.86, 75.94200000000001, 75.934, 75.936]
top5: [92.69200000000001, 92.656, 92.774, 92.792, 92.768, 92.786, 92.76, 92.778, 92.81400000000001, 92.81]
=> creating model mobilenet_m1 ...
 learning rate =  1e-05
Traceback (most recent call last):
  File "./imgnet_train_eval.py", line 332, in <module>
    main()
  File "./imgnet_train_eval.py", line 107, in main
    model = MobileNetV1_Q(ch_in=3, wbit=cfg.Wbits, abit=cfg.Abits).cuda()
  File "/workspaces/pytorch-dev/SLFP_CNNs/nets/mobilenetv1.py", line 19, in __init__
    Linear = linear_Q_fn(w_bit=wbit)
TypeError: linear_Q_fn() missing 2 required positional arguments: 'in_features' and 'out_features'
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ python ./imgnet_train_eval.py --Wbits 8 --Abit 8 --lr 1e-5 --wd 1e-4  --pretrain --max_epochs 11  --if_train 1 --all_validate 1  --log_interval 500 --net mobilenet_m1 --optimizer DSGD
=> creating model mobilenet_m1 ...
 learning rate =  1e-05
Traceback (most recent call last):
  File "./imgnet_train_eval.py", line 332, in <module>
    main()
  File "./imgnet_train_eval.py", line 107, in main
    model = MobileNetV1_Q(ch_in=3, wbit=cfg.Wbits, abit=cfg.Abits).cuda()
  File "/workspaces/pytorch-dev/SLFP_CNNs/nets/mobilenetv1.py", line 19, in __init__
    Linear = linear_Q_fn(w_bit=wbit)
TypeError: linear_Q_fn() missing 2 required positional arguments: 'in_features' and 'out_features'
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ python ./imgnet_train_eval.py --Wbits 8 --Abit 8 --lr 1e-5 --wd 1e-4  --pretrain --max_epochs 11  --if_train 1 --all_validate 1  --log_interval 500 --net mobilenet_m1 --optimizer DSGD
=> creating model mobilenet_m1 ...
 learning rate =  1e-05
Traceback (most recent call last):
  File "./imgnet_train_eval.py", line 332, in <module>
    main()
  File "./imgnet_train_eval.py", line 107, in main
    model = MobileNetV1_Q(ch_in=3, wbit=cfg.Wbits, abit=cfg.Abits).cuda()
  File "/workspaces/pytorch-dev/SLFP_CNNs/nets/mobilenetv1.py", line 63, in __init__
    self.fc = Linear(1024, 1000)
TypeError: __init__() takes from 1 to 2 positional arguments but 3 were given
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ python ./imgnet_train_eval.py --Wbits 8 --Abit 8 --lr 1e-5 --wd 1e-4  --pretrain --max_epochs 11  --if_train 1 --all_validate 1  --log_interval 500 --net mobilenet_m1 --optimizer DSGD
=> creating model mobilenet_m1 ...
 learning rate =  1e-05
DSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/workspaces/pytorch-dev/SLFP_CNNs/utils/optimizer.py:110: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-24 14:22:26.518423 epoch: 1 step: 0 cls_loss= 1.90630 (18596 samples/sec)
^CTraceback (most recent call last):
  File "./imgnet_train_eval.py", line 332, in <module>
    main()
  File "./imgnet_train_eval.py", line 324, in main
    train(epoch)
  File "./imgnet_train_eval.py", line 156, in train
    loss.backward()
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor.py", line 484, in backward
    torch.autograd.backward(
  File "/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt

]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ python ./imgnet_train_eval.py --Wbits 8 --Abit 8 --lr 1e-5 --wd 1e-4  --pretrain --max_epochs 11  --if_train 1 --all_validate 1  --log_interval 500 --net mobilenet_m1 --optimizer DSGD[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[K
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cbs[Kash bash_train.sh
=> creating model mobilenet_m1 ...
 learning rate =  1e-05
DSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2024-03-24 14:28:26.746753------------------------------------------------------ Precision@1: 67.09%  Precision@1: 87.35%

top1: [67.086]
top5: [87.35000000000001]
=> creating model mobilenet_m1 ...
 learning rate =  1e-05
DSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2024-03-24 14:31:11.152082------------------------------------------------------ Precision@1: 60.65%  Precision@1: 82.89%

top1: [60.646]
top5: [82.886]
=> creating model mobilenet_m1 ...
 learning rate =  1e-05
DSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/workspaces/pytorch-dev/SLFP_CNNs/utils/optimizer.py:110: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-24 14:31:13.390865 epoch: 1 step: 0 cls_loss= 2.12509 (18910 samples/sec)
2024-03-24 14:32:15.952335 epoch: 1 step: 500 cls_loss= 2.33632 (255 samples/sec)
2024-03-24 14:33:18.487047 epoch: 1 step: 1000 cls_loss= 1.45502 (255 samples/sec)
2024-03-24 14:34:21.063584 epoch: 1 step: 1500 cls_loss= 1.34244 (255 samples/sec)
2024-03-24 14:35:23.610786 epoch: 1 step: 2000 cls_loss= 1.47118 (255 samples/sec)
2024-03-24 14:36:26.047854 epoch: 1 step: 2500 cls_loss= 2.38181 (256 samples/sec)
2024-03-24 14:37:28.688414 epoch: 1 step: 3000 cls_loss= 1.53296 (255 samples/sec)
2024-03-24 14:40:31.425545------------------------------------------------------ Precision@1: 68.08%  Precision@1: 88.24%

top1: [68.084]
top5: [88.242]
2024-03-24 14:40:31.700590 epoch: 2 step: 0 cls_loss= 1.49868 (58428 samples/sec)
2024-03-24 14:41:34.133191 epoch: 2 step: 500 cls_loss= 2.06329 (256 samples/sec)
2024-03-24 14:42:36.537094 epoch: 2 step: 1000 cls_loss= 1.34366 (256 samples/sec)
2024-03-24 14:43:38.915227 epoch: 2 step: 1500 cls_loss= 2.43316 (256 samples/sec)
2024-03-24 14:44:41.267718 epoch: 2 step: 2000 cls_loss= 1.73829 (256 samples/sec)
2024-03-24 14:45:43.827623 epoch: 2 step: 2500 cls_loss= 1.43810 (255 samples/sec)
2024-03-24 14:46:46.225671 epoch: 2 step: 3000 cls_loss= 1.64804 (256 samples/sec)
2024-03-24 14:49:46.830460------------------------------------------------------ Precision@1: 68.00%  Precision@1: 88.20%

top1: [68.084, 68.004]
top5: [88.242, 88.20400000000001]
2024-03-24 14:49:47.097955 epoch: 3 step: 0 cls_loss= 1.97284 (60069 samples/sec)
2024-03-24 14:50:49.546262 epoch: 3 step: 500 cls_loss= 1.83349 (256 samples/sec)
2024-03-24 14:51:51.978774 epoch: 3 step: 1000 cls_loss= 1.79736 (256 samples/sec)
2024-03-24 14:52:54.619071 epoch: 3 step: 1500 cls_loss= 2.54955 (255 samples/sec)
2024-03-24 14:53:57.212713 epoch: 3 step: 2000 cls_loss= 1.45174 (255 samples/sec)
2024-03-24 14:54:59.679842 epoch: 3 step: 2500 cls_loss= 2.48770 (256 samples/sec)
2024-03-24 14:56:02.175127 epoch: 3 step: 3000 cls_loss= 1.97902 (256 samples/sec)
2024-03-24 14:59:04.725263------------------------------------------------------ Precision@1: 67.97%  Precision@1: 88.16%

top1: [68.084, 68.004, 67.96600000000001]
top5: [88.242, 88.20400000000001, 88.164]
2024-03-24 14:59:04.987171 epoch: 4 step: 0 cls_loss= 1.41733 (61355 samples/sec)
2024-03-24 15:00:07.847639 epoch: 4 step: 500 cls_loss= 2.09677 (254 samples/sec)
2024-03-24 15:01:10.654627 epoch: 4 step: 1000 cls_loss= 1.94598 (254 samples/sec)
2024-03-24 15:02:13.289295 epoch: 4 step: 1500 cls_loss= 1.62565 (255 samples/sec)
2024-03-24 15:03:16.111365 epoch: 4 step: 2000 cls_loss= 2.37795 (254 samples/sec)
2024-03-24 15:04:18.983635 epoch: 4 step: 2500 cls_loss= 1.46770 (254 samples/sec)
2024-03-24 15:05:21.779196 epoch: 4 step: 3000 cls_loss= 1.94631 (254 samples/sec)
2024-03-24 15:08:24.521707------------------------------------------------------ Precision@1: 67.89%  Precision@1: 88.16%

top1: [68.084, 68.004, 67.96600000000001, 67.892]
top5: [88.242, 88.20400000000001, 88.164, 88.156]
2024-03-24 15:08:24.791262 epoch: 5 step: 0 cls_loss= 1.97908 (59595 samples/sec)
2024-03-24 15:09:27.531627 epoch: 5 step: 500 cls_loss= 0.97358 (255 samples/sec)
2024-03-24 15:10:29.964031 epoch: 5 step: 1000 cls_loss= 1.38639 (256 samples/sec)
2024-03-24 15:11:32.504937 epoch: 5 step: 1500 cls_loss= 2.97881 (255 samples/sec)
2024-03-24 15:12:35.105250 epoch: 5 step: 2000 cls_loss= 2.66493 (255 samples/sec)
2024-03-24 15:13:37.660813 epoch: 5 step: 2500 cls_loss= 1.62577 (255 samples/sec)
2024-03-24 15:14:40.182166 epoch: 5 step: 3000 cls_loss= 1.57524 (255 samples/sec)
2024-03-24 15:17:44.919415------------------------------------------------------ Precision@1: 67.97%  Precision@1: 88.05%

top1: [68.084, 68.004, 67.96600000000001, 67.892, 67.974]
top5: [88.242, 88.20400000000001, 88.164, 88.156, 88.052]
2024-03-24 15:17:45.192134 epoch: 6 step: 0 cls_loss= 1.49057 (58934 samples/sec)
2024-03-24 15:18:47.906840 epoch: 6 step: 500 cls_loss= 1.92937 (255 samples/sec)
2024-03-24 15:19:50.475618 epoch: 6 step: 1000 cls_loss= 1.74009 (255 samples/sec)
2024-03-24 15:20:52.984035 epoch: 6 step: 1500 cls_loss= 1.56488 (255 samples/sec)
2024-03-24 15:21:55.530567 epoch: 6 step: 2000 cls_loss= 2.29466 (255 samples/sec)
2024-03-24 15:22:58.165747 epoch: 6 step: 2500 cls_loss= 1.98807 (255 samples/sec)
2024-03-24 15:24:00.694572 epoch: 6 step: 3000 cls_loss= 1.50145 (255 samples/sec)
2024-03-24 15:26:59.305032------------------------------------------------------ Precision@1: 68.04%  Precision@1: 88.11%

top1: [68.084, 68.004, 67.96600000000001, 67.892, 67.974, 68.036]
top5: [88.242, 88.20400000000001, 88.164, 88.156, 88.052, 88.112]
2024-03-24 15:26:59.570965 epoch: 7 step: 0 cls_loss= 1.19534 (60419 samples/sec)
2024-03-24 15:28:02.015750 epoch: 7 step: 500 cls_loss= 1.45419 (256 samples/sec)
2024-03-24 15:29:04.367983 epoch: 7 step: 1000 cls_loss= 1.83575 (256 samples/sec)
2024-03-24 15:30:06.921111 epoch: 7 step: 1500 cls_loss= 1.43204 (255 samples/sec)
2024-03-24 15:31:09.608290 epoch: 7 step: 2000 cls_loss= 1.38451 (255 samples/sec)
2024-03-24 15:32:12.121466 epoch: 7 step: 2500 cls_loss= 1.63163 (255 samples/sec)
2024-03-24 15:33:14.652249 epoch: 7 step: 3000 cls_loss= 1.93372 (255 samples/sec)
2024-03-24 15:36:17.745825------------------------------------------------------ Precision@1: 67.83%  Precision@1: 87.99%

top1: [68.084, 68.004, 67.96600000000001, 67.892, 67.974, 68.036, 67.828]
top5: [88.242, 88.20400000000001, 88.164, 88.156, 88.052, 88.112, 87.99]
2024-03-24 15:36:18.016844 epoch: 8 step: 0 cls_loss= 1.66772 (59304 samples/sec)
2024-03-24 15:37:20.654732 epoch: 8 step: 500 cls_loss= 1.42802 (255 samples/sec)
2024-03-24 15:38:23.288723 epoch: 8 step: 1000 cls_loss= 2.00112 (255 samples/sec)
2024-03-24 15:39:26.030789 epoch: 8 step: 1500 cls_loss= 1.31566 (255 samples/sec)
2024-03-24 15:40:28.679587 epoch: 8 step: 2000 cls_loss= 1.88902 (255 samples/sec)
2024-03-24 15:41:31.294279 epoch: 8 step: 2500 cls_loss= 1.31224 (255 samples/sec)
2024-03-24 15:42:33.735718 epoch: 8 step: 3000 cls_loss= 2.01175 (256 samples/sec)
2024-03-24 15:45:37.899317------------------------------------------------------ Precision@1: 68.02%  Precision@1: 88.20%

top1: [68.084, 68.004, 67.96600000000001, 67.892, 67.974, 68.036, 67.828, 68.02]
top5: [88.242, 88.20400000000001, 88.164, 88.156, 88.052, 88.112, 87.99, 88.196]
2024-03-24 15:45:38.171451 epoch: 9 step: 0 cls_loss= 1.55964 (59034 samples/sec)
2024-03-24 15:46:40.994987 epoch: 9 step: 500 cls_loss= 1.76389 (254 samples/sec)
2024-03-24 15:47:43.658057 epoch: 9 step: 1000 cls_loss= 1.37391 (255 samples/sec)
2024-03-24 15:48:46.483441 epoch: 9 step: 1500 cls_loss= 1.73634 (254 samples/sec)
2024-03-24 15:49:49.234520 epoch: 9 step: 2000 cls_loss= 2.36459 (254 samples/sec)
2024-03-24 15:50:51.918843 epoch: 9 step: 2500 cls_loss= 1.56839 (255 samples/sec)
2024-03-24 15:51:54.556671 epoch: 9 step: 3000 cls_loss= 1.65753 (255 samples/sec)
2024-03-24 15:54:58.748847------------------------------------------------------ Precision@1: 67.84%  Precision@1: 88.03%

top1: [68.084, 68.004, 67.96600000000001, 67.892, 67.974, 68.036, 67.828, 68.02, 67.842]
top5: [88.242, 88.20400000000001, 88.164, 88.156, 88.052, 88.112, 87.99, 88.196, 88.032]
2024-03-24 15:54:59.016930 epoch: 10 step: 0 cls_loss= 1.24847 (59947 samples/sec)
2024-03-24 15:56:01.433559 epoch: 10 step: 500 cls_loss= 1.51460 (256 samples/sec)
2024-03-24 15:57:03.901380 epoch: 10 step: 1000 cls_loss= 2.17807 (256 samples/sec)
2024-03-24 15:58:06.412414 epoch: 10 step: 1500 cls_loss= 1.65002 (255 samples/sec)
2024-03-24 15:59:09.129913 epoch: 10 step: 2000 cls_loss= 2.12756 (255 samples/sec)
2024-03-24 16:00:11.487639 epoch: 10 step: 2500 cls_loss= 1.38141 (256 samples/sec)
2024-03-24 16:01:13.862126 epoch: 10 step: 3000 cls_loss= 1.34488 (256 samples/sec)
2024-03-24 16:04:16.524959------------------------------------------------------ Precision@1: 67.82%  Precision@1: 88.05%

top1: [68.084, 68.004, 67.96600000000001, 67.892, 67.974, 68.036, 67.828, 68.02, 67.842, 67.824]
top5: [88.242, 88.20400000000001, 88.164, 88.156, 88.052, 88.112, 87.99, 88.196, 88.032, 88.05]
=> creating model mobilenet_m1 ...
 learning rate =  5e-06
SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/workspaces/pytorch-dev/SLFP_CNNs/utils/optimizer.py:178: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-24 16:04:18.859663 epoch: 1 step: 0 cls_loss= 2.13856 (18329 samples/sec)
2024-03-24 16:05:15.115522 epoch: 1 step: 500 cls_loss= 1.94227 (284 samples/sec)
2024-03-24 16:06:11.943493 epoch: 1 step: 1000 cls_loss= 1.88927 (281 samples/sec)
2024-03-24 16:07:08.312653 epoch: 1 step: 1500 cls_loss= 1.69641 (283 samples/sec)
2024-03-24 16:08:05.238432 epoch: 1 step: 2000 cls_loss= 2.30146 (281 samples/sec)
2024-03-24 16:09:01.589478 epoch: 1 step: 2500 cls_loss= 1.63113 (284 samples/sec)
2024-03-24 16:09:58.424511 epoch: 1 step: 3000 cls_loss= 1.25009 (281 samples/sec)
2024-03-24 16:12:56.357941------------------------------------------------------ Precision@1: 67.89%  Precision@1: 88.10%

top1: [67.888]
top5: [88.102]
2024-03-24 16:12:56.567594 epoch: 2 step: 0 cls_loss= 1.33620 (76735 samples/sec)
2024-03-24 16:13:53.168136 epoch: 2 step: 500 cls_loss= 2.34496 (282 samples/sec)
2024-03-24 16:14:50.132136 epoch: 2 step: 1000 cls_loss= 1.53396 (280 samples/sec)
2024-03-24 16:15:46.883163 epoch: 2 step: 1500 cls_loss= 1.91669 (282 samples/sec)
2024-03-24 16:16:43.731906 epoch: 2 step: 2000 cls_loss= 1.18192 (281 samples/sec)
2024-03-24 16:17:40.176467 epoch: 2 step: 2500 cls_loss= 1.53206 (283 samples/sec)
2024-03-24 16:18:36.386518 epoch: 2 step: 3000 cls_loss= 2.10560 (284 samples/sec)
2024-03-24 16:21:36.882890------------------------------------------------------ Precision@1: 67.96%  Precision@1: 88.26%

top1: [67.888, 67.958]
top5: [88.102, 88.258]
2024-03-24 16:21:37.098452 epoch: 3 step: 0 cls_loss= 1.38600 (74590 samples/sec)
2024-03-24 16:22:33.658476 epoch: 3 step: 500 cls_loss= 1.40836 (282 samples/sec)
2024-03-24 16:23:30.649795 epoch: 3 step: 1000 cls_loss= 1.98453 (280 samples/sec)
2024-03-24 16:24:27.584392 epoch: 3 step: 1500 cls_loss= 1.55776 (281 samples/sec)
2024-03-24 16:25:24.585482 epoch: 3 step: 2000 cls_loss= 1.47134 (280 samples/sec)
2024-03-24 16:26:22.252851 epoch: 3 step: 2500 cls_loss= 2.12406 (277 samples/sec)
2024-03-24 16:27:19.873811 epoch: 3 step: 3000 cls_loss= 2.22778 (277 samples/sec)
2024-03-24 16:30:18.822607------------------------------------------------------ Precision@1: 67.91%  Precision@1: 88.22%

top1: [67.888, 67.958, 67.908]
top5: [88.102, 88.258, 88.21600000000001]
2024-03-24 16:30:19.030900 epoch: 4 step: 0 cls_loss= 1.44486 (77216 samples/sec)
2024-03-24 16:31:15.337109 epoch: 4 step: 500 cls_loss= 1.59344 (284 samples/sec)
2024-03-24 16:32:12.401883 epoch: 4 step: 1000 cls_loss= 1.45534 (280 samples/sec)
2024-03-24 16:33:09.904348 epoch: 4 step: 1500 cls_loss= 1.24675 (278 samples/sec)
2024-03-24 16:34:07.241597 epoch: 4 step: 2000 cls_loss= 1.83338 (279 samples/sec)
2024-03-24 16:35:04.527554 epoch: 4 step: 2500 cls_loss= 1.64822 (279 samples/sec)
2024-03-24 16:36:01.170777 epoch: 4 step: 3000 cls_loss= 1.89937 (282 samples/sec)
2024-03-24 16:38:59.534796------------------------------------------------------ Precision@1: 68.01%  Precision@1: 88.17%

top1: [67.888, 67.958, 67.908, 68.008]
top5: [88.102, 88.258, 88.21600000000001, 88.166]
2024-03-24 16:38:59.749234 epoch: 5 step: 0 cls_loss= 1.33521 (75025 samples/sec)
2024-03-24 16:39:55.728381 epoch: 5 step: 500 cls_loss= 1.71092 (285 samples/sec)
2024-03-24 16:40:51.910166 epoch: 5 step: 1000 cls_loss= 1.94381 (284 samples/sec)
2024-03-24 16:41:48.510964 epoch: 5 step: 1500 cls_loss= 2.66556 (282 samples/sec)
2024-03-24 16:42:44.934130 epoch: 5 step: 2000 cls_loss= 2.02118 (283 samples/sec)
2024-03-24 16:43:42.031853 epoch: 5 step: 2500 cls_loss= 1.69441 (280 samples/sec)
2024-03-24 16:44:38.547993 epoch: 5 step: 3000 cls_loss= 1.20134 (283 samples/sec)
2024-03-24 16:47:37.824040------------------------------------------------------ Precision@1: 68.17%  Precision@1: 88.34%

top1: [67.888, 67.958, 67.908, 68.008, 68.172]
top5: [88.102, 88.258, 88.21600000000001, 88.166, 88.336]
2024-03-24 16:47:38.027467 epoch: 6 step: 0 cls_loss= 2.00534 (79064 samples/sec)
2024-03-24 16:48:34.262835 epoch: 6 step: 500 cls_loss= 1.40285 (284 samples/sec)
2024-03-24 16:49:31.558634 epoch: 6 step: 1000 cls_loss= 1.25503 (279 samples/sec)
2024-03-24 16:50:29.296468 epoch: 6 step: 1500 cls_loss= 2.05185 (277 samples/sec)
2024-03-24 16:51:27.055399 epoch: 6 step: 2000 cls_loss= 2.21164 (277 samples/sec)
2024-03-24 16:52:24.404556 epoch: 6 step: 2500 cls_loss= 1.41924 (279 samples/sec)
2024-03-24 16:53:22.042043 epoch: 6 step: 3000 cls_loss= 1.99962 (277 samples/sec)
2024-03-24 16:56:20.851826------------------------------------------------------ Precision@1: 68.08%  Precision@1: 88.22%

top1: [67.888, 67.958, 67.908, 68.008, 68.172, 68.078]
top5: [88.102, 88.258, 88.21600000000001, 88.166, 88.336, 88.22]
2024-03-24 16:56:21.061062 epoch: 7 step: 0 cls_loss= 2.02563 (76887 samples/sec)
2024-03-24 16:57:17.882635 epoch: 7 step: 500 cls_loss= 1.62156 (281 samples/sec)
2024-03-24 16:58:14.532074 epoch: 7 step: 1000 cls_loss= 1.42148 (282 samples/sec)
2024-03-24 16:59:11.408681 epoch: 7 step: 1500 cls_loss= 1.44944 (281 samples/sec)
2024-03-24 17:00:07.944432 epoch: 7 step: 2000 cls_loss= 2.07945 (283 samples/sec)
2024-03-24 17:01:04.772661 epoch: 7 step: 2500 cls_loss= 1.69811 (281 samples/sec)
2024-03-24 17:02:03.164652 epoch: 7 step: 3000 cls_loss= 1.53025 (274 samples/sec)
2024-03-24 17:05:03.510990------------------------------------------------------ Precision@1: 68.09%  Precision@1: 88.27%

top1: [67.888, 67.958, 67.908, 68.008, 68.172, 68.078, 68.086]
top5: [88.102, 88.258, 88.21600000000001, 88.166, 88.336, 88.22, 88.27]
2024-03-24 17:05:03.720673 epoch: 8 step: 0 cls_loss= 2.21311 (76765 samples/sec)
2024-03-24 17:06:00.240570 epoch: 8 step: 500 cls_loss= 2.15899 (283 samples/sec)
2024-03-24 17:06:57.121475 epoch: 8 step: 1000 cls_loss= 1.71171 (281 samples/sec)
2024-03-24 17:07:53.661209 epoch: 8 step: 1500 cls_loss= 1.70355 (283 samples/sec)
2024-03-24 17:08:50.439569 epoch: 8 step: 2000 cls_loss= 0.98637 (281 samples/sec)
2024-03-24 17:09:46.801408 epoch: 8 step: 2500 cls_loss= 2.63683 (283 samples/sec)
2024-03-24 17:10:43.255778 epoch: 8 step: 3000 cls_loss= 1.53763 (283 samples/sec)
2024-03-24 17:13:43.364389------------------------------------------------------ Precision@1: 67.96%  Precision@1: 88.22%

top1: [67.888, 67.958, 67.908, 68.008, 68.172, 68.078, 68.086, 67.964]
top5: [88.102, 88.258, 88.21600000000001, 88.166, 88.336, 88.22, 88.27, 88.218]
2024-03-24 17:13:43.580485 epoch: 9 step: 0 cls_loss= 2.00605 (74446 samples/sec)
2024-03-24 17:14:40.680044 epoch: 9 step: 500 cls_loss= 1.31434 (280 samples/sec)
2024-03-24 17:15:38.830760 epoch: 9 step: 1000 cls_loss= 1.88712 (275 samples/sec)
2024-03-24 17:16:36.200590 epoch: 9 step: 1500 cls_loss= 1.92242 (278 samples/sec)
2024-03-24 17:17:33.230326 epoch: 9 step: 2000 cls_loss= 1.86401 (280 samples/sec)
2024-03-24 17:18:30.464949 epoch: 9 step: 2500 cls_loss= 1.94229 (279 samples/sec)
2024-03-24 17:19:28.096625 epoch: 9 step: 3000 cls_loss= 1.82777 (277 samples/sec)
2024-03-24 17:22:29.150174------------------------------------------------------ Precision@1: 68.07%  Precision@1: 88.29%

top1: [67.888, 67.958, 67.908, 68.008, 68.172, 68.078, 68.086, 67.964, 68.072]
top5: [88.102, 88.258, 88.21600000000001, 88.166, 88.336, 88.22, 88.27, 88.218, 88.286]
2024-03-24 17:22:29.362772 epoch: 10 step: 0 cls_loss= 1.61346 (75670 samples/sec)
2024-03-24 17:23:26.322933 epoch: 10 step: 500 cls_loss= 1.91427 (280 samples/sec)
2024-03-24 17:24:22.792179 epoch: 10 step: 1000 cls_loss= 1.52368 (283 samples/sec)
2024-03-24 17:25:19.820768 epoch: 10 step: 1500 cls_loss= 1.74139 (280 samples/sec)
2024-03-24 17:26:16.619660 epoch: 10 step: 2000 cls_loss= 1.98728 (281 samples/sec)
2024-03-24 17:27:13.213300 epoch: 10 step: 2500 cls_loss= 1.89234 (282 samples/sec)
2024-03-24 17:28:10.088772 epoch: 10 step: 3000 cls_loss= 1.11552 (281 samples/sec)
2024-03-24 17:31:11.961678------------------------------------------------------ Precision@1: 68.04%  Precision@1: 88.21%

top1: [67.888, 67.958, 67.908, 68.008, 68.172, 68.078, 68.086, 67.964, 68.072, 68.042]
top5: [88.102, 88.258, 88.21600000000001, 88.166, 88.336, 88.22, 88.27, 88.218, 88.286, 88.212]
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ bash bash_train.sh[Cpython ./imgnet_train_eval.py --Wbits 8 --Abit 8 --lr 1e-5 --wd 1e-4  --pretrain --max_epochs 11  --if_train 1 --all_validate 1  --log_interval 500 --net mobilenet_m1 --optimizer DSGD
=> creating model mobilenet_m1 ...
 learning rate =  1e-05
DSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/workspaces/pytorch-dev/SLFP_CNNs/utils/optimizer.py:110: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
Number of zeros: 0
Number of twos: 864
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 288
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 2048
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 576
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 8192
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 1152
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 16384
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 1152
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 32768
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 2304
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 1
Number of twos: 65535
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 2304
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 131072
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 524288
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 9216
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1048576
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024000
Number of zeros: 0
Number of twos: 1000
2024-03-24 19:43:46.515145 epoch: 1 step: 0 cls_loss= 2.17745 (18427 samples/sec)
Number of zeros: 0
Number of twos: 864
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 288
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 2048
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 576
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 8192
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 1152
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 16384
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 1152
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 32768
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 2304
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 65536
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 2304
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 131072
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 1
Number of twos: 511
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 524288
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 9216
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1048576
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024000
Number of zeros: 0
Number of twos: 1000
Number of zeros: 0
Number of twos: 864
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 288
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 2048
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 576
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 8192
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 1152
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 16384
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 1152
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 32768
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 2304
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 65536
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 2304
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 131072
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 1
Number of twos: 524287
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 9216
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1048576
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024000
Number of zeros: 0
Number of twos: 1000
Number of zeros: 0
Number of twos: 864
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 288
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 2048
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 576
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 8192
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 1152
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 16384
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 1152
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 32768
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 2304
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 65536
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 2304
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 131072
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 1
Number of twos: 262143
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 2
Number of twos: 510
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 1
Number of twos: 262143
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 524288
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 9216
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1048576
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024000
Number of zeros: 0
Number of twos: 1000
Number of zeros: 0
Number of twos: 864
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 288
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 2048
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 576
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 8192
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 1152
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 16384
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 1152
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 32768
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 2304
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 65536
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 2304
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 131072
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 524288
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 9216
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1048576
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 1
Number of twos: 1023999
Number of zeros: 0
Number of twos: 1000
Number of zeros: 0
Number of twos: 864
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 288
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 2048
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 576
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 8192
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 1152
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 1
Number of twos: 16383
Number of zeros: 0
Number of twos: 128
Number of zeros: 1
Number of twos: 127
Number of zeros: 0
Number of twos: 1152
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 32768
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 2304
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 65536
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 2304
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 1
Number of twos: 131071
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 2
Number of twos: 262142
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 1
Number of twos: 511
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 524288
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 9216
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1048576
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 1
Number of twos: 1023999
Number of zeros: 0
Number of twos: 1000
Number of zeros: 0
Number of twos: 864
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 288
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 2048
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 576
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 8192
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 1152
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 16384
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 1152
Number of zeros: 1
Number of twos: 127
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 32768
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 2304
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 65536
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 2304
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 131072
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 2
Number of twos: 262142
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 524288
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 9216
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1048576
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024000
Number of zeros: 0
Number of twos: 1000
Number of zeros: 0
Number of twos: 864
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 288
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 2048
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 576
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 8192
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 1152
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 16384
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 1152
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 32768
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 2304
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 65536
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 2304
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 131072
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 1
Number of twos: 4607
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 1
Number of twos: 262143
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 524288
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 9216
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1048576
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024000
Number of zeros: 0
Number of twos: 1000
Number of zeros: 0
Number of twos: 864
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 32
Number of zeros: 2
Number of twos: 286
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 2048
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 576
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 8192
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 1152
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 16384
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 1152
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 32768
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 2304
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 65536
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 2304
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 131072
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 1
Number of twos: 511
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 1
Number of twos: 511
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 524288
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 9216
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1048576
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024000
Number of zeros: 0
Number of twos: 1000
Number of zeros: 0
Number of twos: 864
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 288
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 2048
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 576
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 8192
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 1152
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 16384
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 1152
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 32768
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 2304
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 65536
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 2304
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 131072
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 1
Number of twos: 511
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 1
Number of twos: 524287
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 9216
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 1
Number of twos: 1048575
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 1
Number of twos: 1023999
Number of zeros: 0
Number of twos: 1000
Number of zeros: 0
Number of twos: 864
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 288
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 2048
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 576
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 8192
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 1152
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 16384
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 1152
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 1
Number of twos: 32767
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 2304
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 65536
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 2304
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 1
Number of twos: 131071
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 1
Number of twos: 262143
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 2
Number of twos: 262142
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 1
Number of twos: 511
Number of zeros: 0
Number of twos: 512
Number of zeros: 1
Number of twos: 524287
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 9216
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1048576
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 1
Number of twos: 1023999
Number of zeros: 0
Number of twos: 1000
Number of zeros: 0
Number of twos: 864
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 288
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 2048
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 576
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 8192
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 1152
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 16384
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 1152
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 128
Number of zeros: 0
Number of twos: 32768
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 2304
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 65536
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 2304
Number of zeros: 0
Number of twos: 256
Number of zeros: 0
Number of twos: 256
Number of zeros: 1
Number of twos: 131071
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 1
Number of twos: 511
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 1
Number of twos: 511
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 262144
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 4608
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 512
Number of zeros: 0
Number of twos: 524288
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 9216
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1048576
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024
Number of zeros: 0
Number of twos: 1024000
Number of zeros: 0
Number of twos: 1000
Number of zeros: 0
Number of twos: 864
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 288
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 32
Number of zeros: 0
Number of twos: 2048
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 576
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 64
Number of zeros: 0
Number of twos: 8192
^CTraceback (most recent call last):
  File "./imgnet_train_eval.py", line 332, in <module>
    main()
  File "./imgnet_train_eval.py", line 324, in main
    train(epoch)
  File "./imgnet_train_eval.py", line 157, in train
    optimizer.step()
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py", line 198, in wrapper
    out = func(*args, **kwargs)
  File "/workspaces/pytorch-dev/SLFP_CNNs/utils/optimizer.py", line 122, in step
    weight_before_update = self.quantize_fn(p.data.clone())
KeyboardInterrupt

]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ exit

Script done on 2024-03-24 19:43:50+08:00 [COMMAND_EXIT_CODE="130"]
