Script started on 2024-03-25 17:05:47+08:00 [TERM="xterm-256color" TTY="/dev/pts/0" COLUMNS="214" LINES="13"]
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ bash bash_train.sh
=> creating model mobilenet_m1 ...
 learning rate =  0.0001
DSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/workspaces/pytorch-dev/SLFP_CNNs/utils/optimizer.py:111: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-25 17:05:56.228631 epoch: 1 step: 0 cls_loss= 1.67958 (10825 samples/sec)
^CTraceback (most recent call last):
  File "./imgnet_train_eval.py", line 341, in <module>
    main()
  File "./imgnet_train_eval.py", line 333, in main
    train(epoch)
  File "./imgnet_train_eval.py", line 162, in train
    optimizer.step()
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py", line 198, in wrapper
    out = func(*args, **kwargs)
  File "/workspaces/pytorch-dev/SLFP_CNNs/utils/optimizer.py", line 124, in step
    p.data.add_(-group['lr']*d_p)  # SGD update
KeyboardInterrupt

]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ bash bash_train.sh
=> creating model mobilenet_m1 ...
 learning rate =  1e-05
DSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/workspaces/pytorch-dev/SLFP_CNNs/utils/optimizer.py:111: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-25 17:06:09.321042 epoch: 1 step: 0 cls_loss= 2.34504 (11181 samples/sec)
2024-03-25 17:10:14.563905 epoch: 1 step: 500 cls_loss= 1.89041 (65 samples/sec)
2024-03-25 17:14:56.266376 epoch: 1 step: 1000 cls_loss= 2.25976 (56 samples/sec)
2024-03-25 17:20:05.063451 epoch: 1 step: 1500 cls_loss= 1.84001 (51 samples/sec)
2024-03-25 17:25:17.800148 epoch: 1 step: 2000 cls_loss= 1.95644 (51 samples/sec)
2024-03-25 17:30:44.051119 epoch: 1 step: 2500 cls_loss= 1.96265 (49 samples/sec)
2024-03-25 17:36:10.989153 epoch: 1 step: 3000 cls_loss= 1.59722 (48 samples/sec)
SGD updated: 11978
SGD not updated: 13224913022
SSGD updated: 35266
2024-03-25 17:42:44.008941------------------------------------------------------ Precision@1: 67.09%  Precision@1: 87.49%

top1: [67.09]
top5: [87.49]
2024-03-25 17:42:44.919298 epoch: 2 step: 0 cls_loss= 2.13530 (17594 samples/sec)
2024-03-25 17:48:07.734475 epoch: 2 step: 500 cls_loss= 1.73035 (49 samples/sec)
2024-03-25 17:53:31.468239 epoch: 2 step: 1000 cls_loss= 1.71460 (49 samples/sec)
2024-03-25 17:58:53.023380 epoch: 2 step: 1500 cls_loss= 1.55082 (49 samples/sec)
2024-03-25 18:04:21.536590 epoch: 2 step: 2000 cls_loss= 1.78891 (48 samples/sec)
2024-03-25 18:09:49.832006 epoch: 2 step: 2500 cls_loss= 2.16276 (48 samples/sec)
2024-03-25 18:15:20.095587 epoch: 2 step: 3000 cls_loss= 2.67731 (48 samples/sec)
SGD updated: 11815
SGD not updated: 13224913185
SSGD updated: 34503
2024-03-25 18:21:41.773430------------------------------------------------------ Precision@1: 67.15%  Precision@1: 87.48%

top1: [67.09, 67.146]
top5: [87.49, 87.48]
2024-03-25 18:21:42.697020 epoch: 3 step: 0 cls_loss= 1.42452 (17348 samples/sec)
2024-03-25 18:27:15.977584 epoch: 3 step: 500 cls_loss= 2.27276 (48 samples/sec)
2024-03-25 18:32:50.119784 epoch: 3 step: 1000 cls_loss= 1.61909 (47 samples/sec)
2024-03-25 18:38:17.086698 epoch: 3 step: 1500 cls_loss= 2.56145 (48 samples/sec)
2024-03-25 18:43:51.378146 epoch: 3 step: 2000 cls_loss= 1.49600 (47 samples/sec)
2024-03-25 18:49:26.637349 epoch: 3 step: 2500 cls_loss= 1.92078 (47 samples/sec)
2024-03-25 18:55:21.756192 epoch: 3 step: 3000 cls_loss= 1.97310 (45 samples/sec)
SGD updated: 11596
SGD not updated: 13224913404
SSGD updated: 34090
2024-03-25 19:01:41.565249------------------------------------------------------ Precision@1: 67.24%  Precision@1: 87.75%

top1: [67.09, 67.146, 67.238]
top5: [87.49, 87.48, 87.752]
2024-03-25 19:01:41.853669 epoch: 4 step: 0 cls_loss= 1.63402 (55753 samples/sec)
2024-03-25 19:02:54.027687 epoch: 4 step: 500 cls_loss= 1.90696 (221 samples/sec)
2024-03-25 19:04:06.133506 epoch: 4 step: 1000 cls_loss= 1.35429 (221 samples/sec)
2024-03-25 19:05:18.903823 epoch: 4 step: 1500 cls_loss= 1.11133 (219 samples/sec)
2024-03-25 19:06:31.941023 epoch: 4 step: 2000 cls_loss= 1.41121 (219 samples/sec)
2024-03-25 19:07:46.047255 epoch: 4 step: 2500 cls_loss= 1.96496 (215 samples/sec)
2024-03-25 19:08:59.061671 epoch: 4 step: 3000 cls_loss= 1.98053 (219 samples/sec)
SGD updated: 11535
SGD not updated: 13224913465
SSGD updated: 33896
2024-03-25 19:12:05.318332------------------------------------------------------ Precision@1: 67.17%  Precision@1: 87.78%

top1: [67.09, 67.146, 67.238, 67.166]
top5: [87.49, 87.48, 87.752, 87.78]
2024-03-25 19:12:05.608754 epoch: 5 step: 0 cls_loss= 1.78067 (55325 samples/sec)
2024-03-25 19:13:18.096607 epoch: 5 step: 500 cls_loss= 1.96798 (220 samples/sec)
2024-03-25 19:14:30.675269 epoch: 5 step: 1000 cls_loss= 2.10914 (220 samples/sec)
2024-03-25 19:15:43.278176 epoch: 5 step: 1500 cls_loss= 1.82217 (220 samples/sec)
2024-03-25 19:16:55.843921 epoch: 5 step: 2000 cls_loss= 1.99949 (220 samples/sec)
2024-03-25 19:18:08.584980 epoch: 5 step: 2500 cls_loss= 1.79545 (219 samples/sec)
2024-03-25 19:19:21.295428 epoch: 5 step: 3000 cls_loss= 1.95058 (220 samples/sec)
SGD updated: 11384
SGD not updated: 13224913616
SSGD updated: 33673
2024-03-25 19:22:31.281814------------------------------------------------------ Precision@1: 67.19%  Precision@1: 87.71%

top1: [67.09, 67.146, 67.238, 67.166, 67.19200000000001]
top5: [87.49, 87.48, 87.752, 87.78, 87.708]
2024-03-25 19:22:31.573309 epoch: 6 step: 0 cls_loss= 1.60338 (55122 samples/sec)
2024-03-25 19:23:44.509737 epoch: 6 step: 500 cls_loss= 1.59386 (219 samples/sec)
2024-03-25 19:24:57.270903 epoch: 6 step: 1000 cls_loss= 1.37565 (219 samples/sec)
2024-03-25 19:26:10.542564 epoch: 6 step: 1500 cls_loss= 1.90481 (218 samples/sec)
2024-03-25 19:27:23.423216 epoch: 6 step: 2000 cls_loss= 1.82795 (219 samples/sec)
2024-03-25 19:28:36.272716 epoch: 6 step: 2500 cls_loss= 1.88131 (219 samples/sec)
2024-03-25 19:29:49.164445 epoch: 6 step: 3000 cls_loss= 1.54198 (219 samples/sec)
SGD updated: 11683
SGD not updated: 13224913317
SSGD updated: 34287
2024-03-25 19:32:52.899611------------------------------------------------------ Precision@1: 67.27%  Precision@1: 87.70%

top1: [67.09, 67.146, 67.238, 67.166, 67.19200000000001, 67.274]
top5: [87.49, 87.48, 87.752, 87.78, 87.708, 87.702]
2024-03-25 19:32:53.197809 epoch: 7 step: 0 cls_loss= 1.71125 (53874 samples/sec)
2024-03-25 19:34:06.046511 epoch: 7 step: 500 cls_loss= 2.32919 (219 samples/sec)
2024-03-25 19:35:18.596108 epoch: 7 step: 1000 cls_loss= 1.41236 (220 samples/sec)
2024-03-25 19:36:31.207412 epoch: 7 step: 1500 cls_loss= 2.99154 (220 samples/sec)
2024-03-25 19:37:43.632612 epoch: 7 step: 2000 cls_loss= 1.87478 (220 samples/sec)
2024-03-25 19:38:56.192908 epoch: 7 step: 2500 cls_loss= 1.91097 (220 samples/sec)
2024-03-25 19:40:09.160442 epoch: 7 step: 3000 cls_loss= 2.19044 (219 samples/sec)
SGD updated: 11696
SGD not updated: 13224913304
SSGD updated: 34453
2024-03-25 19:43:15.524397------------------------------------------------------ Precision@1: 67.24%  Precision@1: 87.87%

top1: [67.09, 67.146, 67.238, 67.166, 67.19200000000001, 67.274, 67.238]
top5: [87.49, 87.48, 87.752, 87.78, 87.708, 87.702, 87.868]
2024-03-25 19:43:15.817520 epoch: 8 step: 0 cls_loss= 1.79799 (54740 samples/sec)
2024-03-25 19:44:28.288490 epoch: 8 step: 500 cls_loss= 1.54409 (220 samples/sec)
2024-03-25 19:45:40.779898 epoch: 8 step: 1000 cls_loss= 2.54283 (220 samples/sec)
2024-03-25 19:46:53.362665 epoch: 8 step: 1500 cls_loss= 1.38385 (220 samples/sec)
2024-03-25 19:48:06.136711 epoch: 8 step: 2000 cls_loss= 1.82433 (219 samples/sec)
2024-03-25 19:49:18.957970 epoch: 8 step: 2500 cls_loss= 1.64863 (219 samples/sec)
2024-03-25 19:50:32.478565 epoch: 8 step: 3000 cls_loss= 1.63194 (217 samples/sec)
SGD updated: 11611
SGD not updated: 13224913389
SSGD updated: 34175
2024-03-25 19:53:34.946878------------------------------------------------------ Precision@1: 67.22%  Precision@1: 87.72%

top1: [67.09, 67.146, 67.238, 67.166, 67.19200000000001, 67.274, 67.238, 67.224]
top5: [87.49, 87.48, 87.752, 87.78, 87.708, 87.702, 87.868, 87.72]
2024-03-25 19:53:35.252367 epoch: 9 step: 0 cls_loss= 2.23344 (52595 samples/sec)
2024-03-25 19:54:50.748887 epoch: 9 step: 500 cls_loss= 2.06804 (211 samples/sec)
2024-03-25 19:56:04.395772 epoch: 9 step: 1000 cls_loss= 1.89461 (217 samples/sec)
2024-03-25 19:57:17.584481 epoch: 9 step: 1500 cls_loss= 1.70795 (218 samples/sec)
2024-03-25 19:58:30.764744 epoch: 9 step: 2000 cls_loss= 0.97722 (218 samples/sec)
2024-03-25 19:59:44.163593 epoch: 9 step: 2500 cls_loss= 1.08467 (217 samples/sec)
2024-03-25 20:00:57.856842 epoch: 9 step: 3000 cls_loss= 1.39847 (217 samples/sec)
SGD updated: 11749
SGD not updated: 13224913251
SSGD updated: 34777
2024-03-25 20:04:02.991731------------------------------------------------------ Precision@1: 67.12%  Precision@1: 87.71%

top1: [67.09, 67.146, 67.238, 67.166, 67.19200000000001, 67.274, 67.238, 67.224, 67.116]
top5: [87.49, 87.48, 87.752, 87.78, 87.708, 87.702, 87.868, 87.72, 87.714]
2024-03-25 20:04:03.280590 epoch: 10 step: 0 cls_loss= 1.39686 (55631 samples/sec)
2024-03-25 20:05:16.866260 epoch: 10 step: 500 cls_loss= 1.86691 (217 samples/sec)
2024-03-25 20:06:30.404779 epoch: 10 step: 1000 cls_loss= 2.01191 (217 samples/sec)
2024-03-25 20:07:43.704260 epoch: 10 step: 1500 cls_loss= 1.98381 (218 samples/sec)
2024-03-25 20:08:57.078936 epoch: 10 step: 2000 cls_loss= 1.30745 (218 samples/sec)
2024-03-25 20:10:10.473028 epoch: 10 step: 2500 cls_loss= 1.49946 (218 samples/sec)
2024-03-25 20:11:24.006940 epoch: 10 step: 3000 cls_loss= 1.61892 (217 samples/sec)
SGD updated: 11736
SGD not updated: 13224913264
SSGD updated: 34810
2024-03-25 20:14:32.440252------------------------------------------------------ Precision@1: 66.57%  Precision@1: 87.30%

top1: [67.09, 67.146, 67.238, 67.166, 67.19200000000001, 67.274, 67.238, 67.224, 67.116, 66.566]
top5: [87.49, 87.48, 87.752, 87.78, 87.708, 87.702, 87.868, 87.72, 87.714, 87.296]
=> creating model mobilenet_m1 ...
 learning rate =  1e-05
Traceback (most recent call last):
  File "./imgnet_train_eval.py", line 341, in <module>
    main()
  File "./imgnet_train_eval.py", line 140, in main
    lr_schedu = optim.lr_scheduler.MultiStepLR(optimizer, [100], gamma=0.3)
UnboundLocalError: local variable 'optimizer' referenced before assignment
=> creating model mobilenet_m1 ...
 learning rate =  1e-05
SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/workspaces/pytorch-dev/SLFP_CNNs/utils/optimizer.py:182: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
Traceback (most recent call last):
  File "./imgnet_train_eval.py", line 341, in <module>
    main()
  File "./imgnet_train_eval.py", line 333, in main
    train(epoch)
  File "./imgnet_train_eval.py", line 162, in train
    optimizer.step()
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py", line 198, in wrapper
    out = func(*args, **kwargs)
  File "/workspaces/pytorch-dev/SLFP_CNNs/utils/optimizer.py", line 195, in step
    weight_before_update = self.quantize_fn(p.data.clone())
AttributeError: 'SSGD' object has no attribute 'quantize_fn'
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ bash bash_train.sh
=> creating model mobilenet_m1 ...
 learning rate =  1e-05
SGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2024-03-25 22:00:08.411294 epoch: 1 step: 0 cls_loss= 1.72274 (7896 samples/sec)
2024-03-25 22:05:46.947821 epoch: 1 step: 500 cls_loss= 1.27404 (47 samples/sec)
^CTraceback (most recent call last):
  File "./imgnet_train_eval.py", line 344, in <module>
    main()
  File "./imgnet_train_eval.py", line 336, in main
    train(epoch)
  File "./imgnet_train_eval.py", line 157, in train
    for batch_idx, (inputs, targets) in enumerate(train_loader):
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py", line 635, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py", line 1330, in _next_data
    idx, data = self._get_data()
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py", line 1286, in _get_data
    success, data = self._try_get_data()
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py", line 1134, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/usr/lib/python3.8/queue.py", line 179, in get
    self.not_empty.wait(remaining)
  File "/usr/lib/python3.8/threading.py", line 306, in wait
    gotit = waiter.acquire(True, timeout)
KeyboardInterrupt

]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ bash bash_train.sh
=> creating model mobilenet_m1 ...
 learning rate =  1e-05
SGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2024-03-25 22:11:26.628390 epoch: 1 step: 0 cls_loss= 2.06954 (10399 samples/sec)
^CTraceback (most recent call last):
  File "./imgnet_train_eval.py", line 344, in <module>
    main()
  File "./imgnet_train_eval.py", line 336, in main
    train(epoch)
  File "./imgnet_train_eval.py", line 157, in train
    for batch_idx, (inputs, targets) in enumerate(train_loader):
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py", line 635, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py", line 1330, in _next_data
    idx, data = self._get_data()
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py", line 1286, in _get_data
    success, data = self._try_get_data()
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py", line 1134, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/usr/lib/python3.8/queue.py", line 179, in get
    self.not_empty.wait(remaining)
  File "/usr/lib/python3.8/threading.py", line 306, in wait
    gotit = waiter.acquire(True, timeout)
KeyboardInterrupt

]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ bash bash_train.sh
=> creating model mobilenet_m1 ...
 learning rate =  1e-05
SGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2024-03-25 22:11:35.405242 epoch: 1 step: 0 cls_loss= 2.06707 (10458 samples/sec)
2024-03-25 22:16:27.238423 epoch: 1 step: 500 cls_loss= 1.68274 (54 samples/sec)
2024-03-25 22:21:48.814827 epoch: 1 step: 1000 cls_loss= 2.10202 (49 samples/sec)
2024-03-25 22:27:19.313707 epoch: 1 step: 1500 cls_loss= 1.33642 (48 samples/sec)
2024-03-25 22:32:45.335313 epoch: 1 step: 2000 cls_loss= 1.81463 (49 samples/sec)
2024-03-25 22:38:11.880072 epoch: 1 step: 2500 cls_loss= 1.58643 (49 samples/sec)
2024-03-25 22:43:39.111035 epoch: 1 step: 3000 cls_loss= 2.24976 (48 samples/sec)
SGD updated: 0
SGD not updated: 0
SSGD updated: 0
2024-03-25 22:50:08.474052------------------------------------------------------ Precision@1: 67.04%  Precision@1: 87.51%

top1: [67.042]
top5: [87.508]
2024-03-25 22:50:09.232940 epoch: 2 step: 0 cls_loss= 1.67449 (21117 samples/sec)
2024-03-25 22:55:37.241975 epoch: 2 step: 500 cls_loss= 1.91766 (48 samples/sec)
2024-03-25 23:01:02.742276 epoch: 2 step: 1000 cls_loss= 1.46626 (49 samples/sec)
2024-03-25 23:06:30.355572 epoch: 2 step: 1500 cls_loss= 1.64534 (48 samples/sec)
2024-03-25 23:11:56.953191 epoch: 2 step: 2000 cls_loss= 1.92663 (48 samples/sec)
2024-03-25 23:17:29.254649 epoch: 2 step: 2500 cls_loss= 2.22708 (48 samples/sec)
2024-03-25 23:23:04.808629 epoch: 2 step: 3000 cls_loss= 2.17996 (47 samples/sec)
SGD updated: 0
SGD not updated: 0
SSGD updated: 0
2024-03-25 23:29:24.953690------------------------------------------------------ Precision@1: 67.02%  Precision@1: 87.47%

top1: [67.042, 67.024]
top5: [87.508, 87.47]
2024-03-25 23:29:25.754717 epoch: 3 step: 0 cls_loss= 1.84983 (20004 samples/sec)
2024-03-25 23:34:55.247099 epoch: 3 step: 500 cls_loss= 1.06963 (48 samples/sec)
2024-03-25 23:40:21.796353 epoch: 3 step: 1000 cls_loss= 1.84116 (48 samples/sec)
2024-03-25 23:45:57.043782 epoch: 3 step: 1500 cls_loss= 2.39875 (47 samples/sec)
2024-03-25 23:51:31.360201 epoch: 3 step: 2000 cls_loss= 1.94190 (47 samples/sec)
2024-03-25 23:56:58.785740 epoch: 3 step: 2500 cls_loss= 2.14486 (48 samples/sec)
2024-03-26 00:02:21.890192 epoch: 3 step: 3000 cls_loss= 2.30151 (49 samples/sec)
SGD updated: 0
SGD not updated: 0
SSGD updated: 0
2024-03-26 00:08:48.854255------------------------------------------------------ Precision@1: 67.05%  Precision@1: 87.61%

top1: [67.042, 67.024, 67.05]
top5: [87.508, 87.47, 87.614]
2024-03-26 00:08:49.699702 epoch: 4 step: 0 cls_loss= 1.42040 (18951 samples/sec)
2024-03-26 00:14:24.170482 epoch: 4 step: 500 cls_loss= 1.59532 (47 samples/sec)
2024-03-26 00:19:54.473528 epoch: 4 step: 1000 cls_loss= 1.88913 (48 samples/sec)
2024-03-26 00:25:30.277050 epoch: 4 step: 1500 cls_loss= 2.01318 (47 samples/sec)
2024-03-26 00:31:13.008426 epoch: 4 step: 2000 cls_loss= 2.64358 (46 samples/sec)
2024-03-26 00:36:57.995572 epoch: 4 step: 2500 cls_loss= 1.75558 (46 samples/sec)
2024-03-26 00:42:44.183565 epoch: 4 step: 3000 cls_loss= 1.55642 (46 samples/sec)
SGD updated: 0
SGD not updated: 0
SSGD updated: 0
2024-03-26 00:50:26.620020------------------------------------------------------ Precision@1: 67.32%  Precision@1: 87.58%

top1: [67.042, 67.024, 67.05, 67.322]
top5: [87.508, 87.47, 87.614, 87.58]
2024-03-26 00:50:27.445513 epoch: 5 step: 0 cls_loss= 1.38734 (19423 samples/sec)
2024-03-26 00:56:13.108853 epoch: 5 step: 500 cls_loss= 2.20760 (46 samples/sec)
2024-03-26 01:01:55.417177 epoch: 5 step: 1000 cls_loss= 1.28186 (46 samples/sec)
2024-03-26 01:07:38.914541 epoch: 5 step: 1500 cls_loss= 2.19113 (46 samples/sec)
2024-03-26 01:13:24.418282 epoch: 5 step: 2000 cls_loss= 2.28327 (46 samples/sec)
2024-03-26 01:19:02.600161 epoch: 5 step: 2500 cls_loss= 1.29282 (47 samples/sec)
2024-03-26 01:24:32.687016 epoch: 5 step: 3000 cls_loss= 1.60157 (48 samples/sec)
SGD updated: 0
SGD not updated: 0
SSGD updated: 0
2024-03-26 01:30:54.362961------------------------------------------------------ Precision@1: 67.22%  Precision@1: 87.74%

top1: [67.042, 67.024, 67.05, 67.322, 67.224]
top5: [87.508, 87.47, 87.614, 87.58, 87.738]
2024-03-26 01:30:55.063575 epoch: 6 step: 0 cls_loss= 2.13973 (22877 samples/sec)
2024-03-26 01:36:23.076880 epoch: 6 step: 500 cls_loss= 2.36651 (48 samples/sec)
2024-03-26 01:41:51.603225 epoch: 6 step: 1000 cls_loss= 1.06302 (48 samples/sec)
2024-03-26 01:47:27.671841 epoch: 6 step: 1500 cls_loss= 1.43222 (47 samples/sec)
2024-03-26 01:52:58.187630 epoch: 6 step: 2000 cls_loss= 2.12919 (48 samples/sec)
2024-03-26 01:58:31.695783 epoch: 6 step: 2500 cls_loss= 1.35766 (47 samples/sec)
2024-03-26 02:04:01.101912 epoch: 6 step: 3000 cls_loss= 2.12316 (48 samples/sec)
SGD updated: 0
SGD not updated: 0
SSGD updated: 0
2024-03-26 02:10:22.704072------------------------------------------------------ Precision@1: 66.78%  Precision@1: 87.30%

top1: [67.042, 67.024, 67.05, 67.322, 67.224, 66.782]
top5: [87.508, 87.47, 87.614, 87.58, 87.738, 87.304]
2024-03-26 02:10:23.523177 epoch: 7 step: 0 cls_loss= 1.52759 (19564 samples/sec)
2024-03-26 02:15:55.746970 epoch: 7 step: 500 cls_loss= 1.32716 (48 samples/sec)
2024-03-26 02:21:25.272398 epoch: 7 step: 1000 cls_loss= 1.68588 (48 samples/sec)
2024-03-26 02:26:56.442020 epoch: 7 step: 1500 cls_loss= 1.58344 (48 samples/sec)
2024-03-26 02:32:23.367844 epoch: 7 step: 2000 cls_loss= 2.30759 (48 samples/sec)
2024-03-26 02:37:53.943608 epoch: 7 step: 2500 cls_loss= 2.34022 (48 samples/sec)
2024-03-26 02:43:27.541386 epoch: 7 step: 3000 cls_loss= 1.71246 (47 samples/sec)
SGD updated: 0
SGD not updated: 0
SSGD updated: 0
2024-03-26 02:50:12.100721------------------------------------------------------ Precision@1: 67.31%  Precision@1: 87.71%

top1: [67.042, 67.024, 67.05, 67.322, 67.224, 66.782, 67.306]
top5: [87.508, 87.47, 87.614, 87.58, 87.738, 87.304, 87.712]
2024-03-26 02:50:12.895033 epoch: 8 step: 0 cls_loss= 1.68007 (20175 samples/sec)
2024-03-26 02:55:35.340949 epoch: 8 step: 500 cls_loss= 2.08026 (49 samples/sec)
2024-03-26 03:01:06.492079 epoch: 8 step: 1000 cls_loss= 1.84468 (48 samples/sec)
2024-03-26 03:06:40.458821 epoch: 8 step: 1500 cls_loss= 1.83792 (47 samples/sec)
2024-03-26 03:12:07.858317 epoch: 8 step: 2000 cls_loss= 1.97660 (48 samples/sec)
2024-03-26 03:17:34.290136 epoch: 8 step: 2500 cls_loss= 2.20422 (49 samples/sec)
2024-03-26 03:23:07.258196 epoch: 8 step: 3000 cls_loss= 1.77525 (48 samples/sec)
SGD updated: 0
SGD not updated: 0
SSGD updated: 0
2024-03-26 03:29:26.600418------------------------------------------------------ Precision@1: 67.25%  Precision@1: 87.67%

top1: [67.042, 67.024, 67.05, 67.322, 67.224, 66.782, 67.306, 67.248]
top5: [87.508, 87.47, 87.614, 87.58, 87.738, 87.304, 87.712, 87.672]
2024-03-26 03:29:27.359674 epoch: 9 step: 0 cls_loss= 1.65206 (21107 samples/sec)
2024-03-26 03:35:06.344184 epoch: 9 step: 500 cls_loss= 2.02320 (47 samples/sec)
2024-03-26 03:40:55.880691 epoch: 9 step: 1000 cls_loss= 1.90647 (45 samples/sec)
2024-03-26 03:46:34.934023 epoch: 9 step: 1500 cls_loss= 1.61121 (47 samples/sec)
2024-03-26 03:52:10.744875 epoch: 9 step: 2000 cls_loss= 1.26309 (47 samples/sec)
2024-03-26 03:57:47.364083 epoch: 9 step: 2500 cls_loss= 2.18025 (47 samples/sec)
2024-03-26 04:03:25.534076 epoch: 9 step: 3000 cls_loss= 2.82160 (47 samples/sec)
SGD updated: 0
SGD not updated: 0
SSGD updated: 0
2024-03-26 04:09:24.796816------------------------------------------------------ Precision@1: 67.20%  Precision@1: 87.62%

top1: [67.042, 67.024, 67.05, 67.322, 67.224, 66.782, 67.306, 67.248, 67.19800000000001]
top5: [87.508, 87.47, 87.614, 87.58, 87.738, 87.304, 87.712, 87.672, 87.618]
2024-03-26 04:09:25.005941 epoch: 10 step: 0 cls_loss= 1.99364 (76930 samples/sec)
2024-03-26 04:10:20.912442 epoch: 10 step: 500 cls_loss= 1.74232 (286 samples/sec)
2024-03-26 04:11:16.561858 epoch: 10 step: 1000 cls_loss= 1.89202 (287 samples/sec)
2024-03-26 04:12:12.407290 epoch: 10 step: 1500 cls_loss= 1.97324 (286 samples/sec)
2024-03-26 04:13:08.530013 epoch: 10 step: 2000 cls_loss= 1.85594 (285 samples/sec)
2024-03-26 04:14:04.283528 epoch: 10 step: 2500 cls_loss= 1.18781 (287 samples/sec)
2024-03-26 04:15:00.310334 epoch: 10 step: 3000 cls_loss= 2.23183 (285 samples/sec)
SGD updated: 0
SGD not updated: 0
SSGD updated: 0
2024-03-26 04:17:58.905516------------------------------------------------------ Precision@1: 67.27%  Precision@1: 87.69%

top1: [67.042, 67.024, 67.05, 67.322, 67.224, 66.782, 67.306, 67.248, 67.19800000000001, 67.268]
top5: [87.508, 87.47, 87.614, 87.58, 87.738, 87.304, 87.712, 87.672, 87.618, 87.694]
=> creating model mobilenet_m1 ...
 learning rate =  1e-05
SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/workspaces/pytorch-dev/SLFP_CNNs/utils/optimizer.py:182: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
Traceback (most recent call last):
  File "./imgnet_train_eval.py", line 344, in <module>
    main()
  File "./imgnet_train_eval.py", line 336, in main
    train(epoch)
  File "./imgnet_train_eval.py", line 165, in train
    optimizer.step()
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py", line 198, in wrapper
    out = func(*args, **kwargs)
  File "/workspaces/pytorch-dev/SLFP_CNNs/utils/optimizer.py", line 195, in step
    weight_before_update = self.quantize_fn(p.data.clone())
AttributeError: 'SSGD' object has no attribute 'quantize_fn'
=> creating model mobilenet_m1 ...
 learning rate =  1e-05
DSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/workspaces/pytorch-dev/SLFP_CNNs/utils/optimizer.py:111: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-26 04:18:05.507504 epoch: 1 step: 0 cls_loss= 2.10876 (18772 samples/sec)
2024-03-26 04:19:18.001030 epoch: 1 step: 500 cls_loss= 1.51980 (220 samples/sec)
2024-03-26 04:20:30.779305 epoch: 1 step: 1000 cls_loss= 1.80581 (219 samples/sec)
2024-03-26 04:21:43.327382 epoch: 1 step: 1500 cls_loss= 1.86354 (220 samples/sec)
2024-03-26 04:22:55.958115 epoch: 1 step: 2000 cls_loss= 1.94836 (220 samples/sec)
2024-03-26 04:24:08.447939 epoch: 1 step: 2500 cls_loss= 2.20942 (220 samples/sec)
2024-03-26 04:25:20.797132 epoch: 1 step: 3000 cls_loss= 1.97706 (221 samples/sec)
SGD updated: 11952
SGD not updated: 13224913048
SSGD updated: 35158
2024-03-26 04:28:21.487383------------------------------------------------------ Precision@1: 66.93%  Precision@1: 87.43%

top1: [66.926]
top5: [87.432]
2024-03-26 04:28:21.774624 epoch: 2 step: 0 cls_loss= 1.66345 (55917 samples/sec)
2024-03-26 04:29:34.570472 epoch: 2 step: 500 cls_loss= 1.61150 (219 samples/sec)
2024-03-26 04:30:47.226106 epoch: 2 step: 1000 cls_loss= 2.19123 (220 samples/sec)
2024-03-26 04:31:59.875484 epoch: 2 step: 1500 cls_loss= 1.90013 (220 samples/sec)
2024-03-26 04:33:12.369009 epoch: 2 step: 2000 cls_loss= 1.74410 (220 samples/sec)
2024-03-26 04:34:24.916708 epoch: 2 step: 2500 cls_loss= 1.65309 (220 samples/sec)
2024-03-26 04:35:37.775474 epoch: 2 step: 3000 cls_loss= 2.49392 (219 samples/sec)
SGD updated: 11875
SGD not updated: 13224913125
SSGD updated: 35162
2024-03-26 04:38:41.773925------------------------------------------------------ Precision@1: 67.21%  Precision@1: 87.72%

top1: [66.926, 67.214]
top5: [87.432, 87.718]
2024-03-26 04:38:42.059172 epoch: 3 step: 0 cls_loss= 2.19604 (56324 samples/sec)
2024-03-26 04:39:54.809669 epoch: 3 step: 500 cls_loss= 1.67858 (219 samples/sec)
2024-03-26 04:41:07.393462 epoch: 3 step: 1000 cls_loss= 1.43629 (220 samples/sec)
2024-03-26 04:42:19.744179 epoch: 3 step: 1500 cls_loss= 1.34379 (221 samples/sec)
2024-03-26 04:43:32.178625 epoch: 3 step: 2000 cls_loss= 2.17779 (220 samples/sec)
2024-03-26 04:44:44.662684 epoch: 3 step: 2500 cls_loss= 1.46836 (220 samples/sec)
2024-03-26 04:45:57.075272 epoch: 3 step: 3000 cls_loss= 2.64304 (220 samples/sec)
SGD updated: 11645
SGD not updated: 13224913355
SSGD updated: 33952
2024-03-26 04:48:59.300895------------------------------------------------------ Precision@1: 67.30%  Precision@1: 87.75%

top1: [66.926, 67.214, 67.3]
top5: [87.432, 87.718, 87.746]
2024-03-26 04:48:59.627902 epoch: 4 step: 0 cls_loss= 1.97593 (49089 samples/sec)
2024-03-26 04:50:12.111633 epoch: 4 step: 500 cls_loss= 1.30108 (220 samples/sec)
2024-03-26 04:51:24.600933 epoch: 4 step: 1000 cls_loss= 1.02991 (220 samples/sec)
2024-03-26 04:52:37.414023 epoch: 4 step: 1500 cls_loss= 2.56448 (219 samples/sec)
2024-03-26 04:53:49.942636 epoch: 4 step: 2000 cls_loss= 1.83002 (220 samples/sec)
2024-03-26 04:55:02.325893 epoch: 4 step: 2500 cls_loss= 2.83750 (221 samples/sec)
2024-03-26 04:56:15.358473 epoch: 4 step: 3000 cls_loss= 2.22880 (219 samples/sec)
SGD updated: 11596
SGD not updated: 13224913404
SSGD updated: 34025
2024-03-26 04:59:18.987479------------------------------------------------------ Precision@1: 67.20%  Precision@1: 87.64%

top1: [66.926, 67.214, 67.3, 67.20400000000001]
top5: [87.432, 87.718, 87.746, 87.644]
2024-03-26 04:59:19.278086 epoch: 5 step: 0 cls_loss= 1.89459 (55290 samples/sec)
2024-03-26 05:00:31.932637 epoch: 5 step: 500 cls_loss= 1.59528 (220 samples/sec)
2024-03-26 05:01:44.475056 epoch: 5 step: 1000 cls_loss= 1.75695 (220 samples/sec)
2024-03-26 05:02:57.104437 epoch: 5 step: 1500 cls_loss= 1.81367 (220 samples/sec)
2024-03-26 05:04:09.481757 epoch: 5 step: 2000 cls_loss= 2.02826 (221 samples/sec)
2024-03-26 05:05:21.769002 epoch: 5 step: 2500 cls_loss= 2.08424 (221 samples/sec)
2024-03-26 05:06:34.632082 epoch: 5 step: 3000 cls_loss= 1.72254 (219 samples/sec)
SGD updated: 11473
SGD not updated: 13224913527
SSGD updated: 33663
2024-03-26 05:09:38.551076------------------------------------------------------ Precision@1: 67.30%  Precision@1: 87.70%

top1: [66.926, 67.214, 67.3, 67.20400000000001, 67.296]
top5: [87.432, 87.718, 87.746, 87.644, 87.70400000000001]
2024-03-26 05:09:38.835186 epoch: 6 step: 0 cls_loss= 1.60881 (56541 samples/sec)
2024-03-26 05:10:51.656133 epoch: 6 step: 500 cls_loss= 1.60305 (219 samples/sec)
2024-03-26 05:12:04.306146 epoch: 6 step: 1000 cls_loss= 1.72794 (220 samples/sec)
2024-03-26 05:13:16.884771 epoch: 6 step: 1500 cls_loss= 1.50194 (220 samples/sec)
2024-03-26 05:14:29.701299 epoch: 6 step: 2000 cls_loss= 2.38657 (219 samples/sec)
2024-03-26 05:15:42.484183 epoch: 6 step: 2500 cls_loss= 2.11952 (219 samples/sec)
2024-03-26 05:16:55.192056 epoch: 6 step: 3000 cls_loss= 1.75587 (220 samples/sec)
SGD updated: 11432
SGD not updated: 13224913568
SSGD updated: 33760
2024-03-26 05:19:59.679688------------------------------------------------------ Precision@1: 67.29%  Precision@1: 87.73%

top1: [66.926, 67.214, 67.3, 67.20400000000001, 67.296, 67.292]
top5: [87.432, 87.718, 87.746, 87.644, 87.70400000000001, 87.73]
2024-03-26 05:19:59.966187 epoch: 7 step: 0 cls_loss= 1.36648 (56100 samples/sec)
2024-03-26 05:21:12.543844 epoch: 7 step: 500 cls_loss= 1.29336 (220 samples/sec)
2024-03-26 05:22:25.222692 epoch: 7 step: 1000 cls_loss= 1.11020 (220 samples/sec)
2024-03-26 05:23:37.714024 epoch: 7 step: 1500 cls_loss= 1.57627 (220 samples/sec)
2024-03-26 05:24:50.383228 epoch: 7 step: 2000 cls_loss= 1.59360 (220 samples/sec)
2024-03-26 05:26:03.107622 epoch: 7 step: 2500 cls_loss= 2.07224 (220 samples/sec)
2024-03-26 05:27:15.827378 epoch: 7 step: 3000 cls_loss= 1.50553 (220 samples/sec)
SGD updated: 11149
SGD not updated: 13224913851
SSGD updated: 32948
2024-03-26 05:30:18.305829------------------------------------------------------ Precision@1: 66.99%  Precision@1: 87.44%

top1: [66.926, 67.214, 67.3, 67.20400000000001, 67.296, 67.292, 66.988]
top5: [87.432, 87.718, 87.746, 87.644, 87.70400000000001, 87.73, 87.436]
2024-03-26 05:30:18.594461 epoch: 8 step: 0 cls_loss= 1.49729 (55658 samples/sec)
2024-03-26 05:31:31.033674 epoch: 8 step: 500 cls_loss= 1.51329 (220 samples/sec)
2024-03-26 05:32:43.457054 epoch: 8 step: 1000 cls_loss= 1.55850 (220 samples/sec)
2024-03-26 05:33:56.244540 epoch: 8 step: 1500 cls_loss= 1.84235 (219 samples/sec)
2024-03-26 05:35:08.886653 epoch: 8 step: 2000 cls_loss= 1.68717 (220 samples/sec)
2024-03-26 05:36:21.663413 epoch: 8 step: 2500 cls_loss= 2.27656 (219 samples/sec)
2024-03-26 05:37:34.346560 epoch: 8 step: 3000 cls_loss= 1.60221 (220 samples/sec)
SGD updated: 11292
SGD not updated: 13224913708
SSGD updated: 33438
2024-03-26 05:40:36.581727------------------------------------------------------ Precision@1: 67.16%  Precision@1: 87.61%

top1: [66.926, 67.214, 67.3, 67.20400000000001, 67.296, 67.292, 66.988, 67.158]
top5: [87.432, 87.718, 87.746, 87.644, 87.70400000000001, 87.73, 87.436, 87.608]
2024-03-26 05:40:36.878621 epoch: 9 step: 0 cls_loss= 1.60036 (54099 samples/sec)
2024-03-26 05:41:49.647817 epoch: 9 step: 500 cls_loss= 1.80035 (219 samples/sec)
2024-03-26 05:43:02.185025 epoch: 9 step: 1000 cls_loss= 1.46621 (220 samples/sec)
2024-03-26 05:44:14.920582 epoch: 9 step: 1500 cls_loss= 1.95141 (219 samples/sec)
2024-03-26 05:45:27.513580 epoch: 9 step: 2000 cls_loss= 2.38438 (220 samples/sec)
2024-03-26 05:46:40.244593 epoch: 9 step: 2500 cls_loss= 1.77655 (219 samples/sec)
2024-03-26 05:47:52.977637 epoch: 9 step: 3000 cls_loss= 1.71389 (219 samples/sec)
SGD updated: 11275
SGD not updated: 13224913725
SSGD updated: 33466
2024-03-26 05:50:54.596910------------------------------------------------------ Precision@1: 67.16%  Precision@1: 87.76%

top1: [66.926, 67.214, 67.3, 67.20400000000001, 67.296, 67.292, 66.988, 67.158, 67.164]
top5: [87.432, 87.718, 87.746, 87.644, 87.70400000000001, 87.73, 87.436, 87.608, 87.762]
2024-03-26 05:50:54.895942 epoch: 10 step: 0 cls_loss= 2.04489 (53724 samples/sec)
2024-03-26 05:52:07.953732 epoch: 10 step: 500 cls_loss= 1.59447 (219 samples/sec)
2024-03-26 05:53:20.395701 epoch: 10 step: 1000 cls_loss= 1.96634 (220 samples/sec)
2024-03-26 05:54:33.713900 epoch: 10 step: 1500 cls_loss= 1.66864 (218 samples/sec)
2024-03-26 05:55:46.515331 epoch: 10 step: 2000 cls_loss= 2.08027 (219 samples/sec)
2024-03-26 05:56:59.548028 epoch: 10 step: 2500 cls_loss= 2.02516 (219 samples/sec)
2024-03-26 05:58:12.292007 epoch: 10 step: 3000 cls_loss= 2.07253 (219 samples/sec)
SGD updated: 11653
SGD not updated: 13224913347
SSGD updated: 34377
2024-03-26 06:01:14.070542------------------------------------------------------ Precision@1: 67.31%  Precision@1: 87.79%

top1: [66.926, 67.214, 67.3, 67.20400000000001, 67.296, 67.292, 66.988, 67.158, 67.164, 67.31400000000001]
top5: [87.432, 87.718, 87.746, 87.644, 87.70400000000001, 87.73, 87.436, 87.608, 87.762, 87.786]
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ python ./imgnet_train_eval.py --Wbits 7 --Abit 7 --lr 1e-5 --wd 1e-4  --pretrain --max_epochs 11  --if_train 1 --all_validate 1  --log_interval 500 --net mobilenet_m1 --optimizer SSGD
=> creating model mobilenet_m1 ...
 learning rate =  1e-05
SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/workspaces/pytorch-dev/SLFP_CNNs/utils/optimizer.py:184: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
Traceback (most recent call last):
  File "./imgnet_train_eval.py", line 344, in <module>
    main()
  File "./imgnet_train_eval.py", line 336, in main
    train(epoch)
  File "./imgnet_train_eval.py", line 165, in train
    optimizer.step()
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py", line 198, in wrapper
    out = func(*args, **kwargs)
  File "/workspaces/pytorch-dev/SLFP_CNNs/utils/optimizer.py", line 210, in step
    scale[abs(weight_before_update - weight_after_DSGD) > 0.0001 ] = 3  #updated
NameError: name 'weight_after_DSGD' is not defined
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ python ./imgnet_train_eval.py --Wbits 7 --Abit 7 --lr 1e-5 --wd 1e-4  --pretrain --max_epochs 11  --if_train 1 --all_validate 1  --log_interval 500 --net mobilenet_m1 --optimizer SSGD
=> creating model mobilenet_m1 ...
 learning rate =  1e-05
SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/workspaces/pytorch-dev/SLFP_CNNs/utils/optimizer.py:184: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-26 07:01:03.744119 epoch: 1 step: 0 cls_loss= 1.75977 (18367 samples/sec)
2024-03-26 07:02:15.154907 epoch: 1 step: 500 cls_loss= 1.42083 (224 samples/sec)
2024-03-26 07:03:26.521833 epoch: 1 step: 1000 cls_loss= 2.21271 (224 samples/sec)
2024-03-26 07:04:37.966533 epoch: 1 step: 1500 cls_loss= 1.46986 (223 samples/sec)
2024-03-26 07:05:49.482694 epoch: 1 step: 2000 cls_loss= 2.24311 (223 samples/sec)
2024-03-26 07:07:00.991796 epoch: 1 step: 2500 cls_loss= 2.46349 (223 samples/sec)
2024-03-26 07:08:12.458771 epoch: 1 step: 3000 cls_loss= 1.95993 (223 samples/sec)
SGD updated: 12087
SGD not updated: 13224912913
SSGD updated: 13649
2024-03-26 07:11:16.978183------------------------------------------------------ Precision@1: 66.18%  Precision@1: 86.83%

top1: [66.184]
top5: [86.828]
2024-03-26 07:11:17.269954 epoch: 2 step: 0 cls_loss= 1.90114 (55064 samples/sec)
2024-03-26 07:12:29.372755 epoch: 2 step: 500 cls_loss= 1.71051 (221 samples/sec)
2024-03-26 07:13:41.076321 epoch: 2 step: 1000 cls_loss= 1.66965 (223 samples/sec)
2024-03-26 07:14:52.795544 epoch: 2 step: 1500 cls_loss= 1.32679 (223 samples/sec)
2024-03-26 07:16:04.697239 epoch: 2 step: 2000 cls_loss= 1.69793 (222 samples/sec)
2024-03-26 07:17:16.664288 epoch: 2 step: 2500 cls_loss= 1.76728 (222 samples/sec)
2024-03-26 07:18:28.578473 epoch: 2 step: 3000 cls_loss= 2.00876 (222 samples/sec)
SGD updated: 11482
SGD not updated: 13224913518
SSGD updated: 12874
2024-03-26 07:21:33.024965------------------------------------------------------ Precision@1: 67.21%  Precision@1: 87.71%

top1: [66.184, 67.212]
top5: [86.828, 87.706]
2024-03-26 07:21:33.316714 epoch: 3 step: 0 cls_loss= 1.46079 (55049 samples/sec)
2024-03-26 07:22:45.304498 epoch: 3 step: 500 cls_loss= 1.79384 (222 samples/sec)
2024-03-26 07:23:57.085206 epoch: 3 step: 1000 cls_loss= 1.27881 (222 samples/sec)
2024-03-26 07:25:08.829853 epoch: 3 step: 1500 cls_loss= 1.79827 (223 samples/sec)
2024-03-26 07:26:20.766322 epoch: 3 step: 2000 cls_loss= 1.65965 (222 samples/sec)
2024-03-26 07:27:32.941379 epoch: 3 step: 2500 cls_loss= 1.79878 (221 samples/sec)
2024-03-26 07:28:45.261144 epoch: 3 step: 3000 cls_loss= 1.90191 (221 samples/sec)
SGD updated: 11626
SGD not updated: 13224913374
SSGD updated: 13016
2024-03-26 07:31:49.622797------------------------------------------------------ Precision@1: 67.12%  Precision@1: 87.57%

top1: [66.184, 67.212, 67.124]
top5: [86.828, 87.706, 87.566]
2024-03-26 07:31:49.910420 epoch: 4 step: 0 cls_loss= 2.47884 (55842 samples/sec)
2024-03-26 07:33:02.045446 epoch: 4 step: 500 cls_loss= 1.91505 (221 samples/sec)
2024-03-26 07:34:14.005815 epoch: 4 step: 1000 cls_loss= 1.85761 (222 samples/sec)
2024-03-26 07:35:25.839342 epoch: 4 step: 1500 cls_loss= 1.83872 (222 samples/sec)
2024-03-26 07:36:37.875538 epoch: 4 step: 2000 cls_loss= 1.76715 (222 samples/sec)
2024-03-26 07:37:49.693640 epoch: 4 step: 2500 cls_loss= 1.70755 (222 samples/sec)
2024-03-26 07:39:01.567430 epoch: 4 step: 3000 cls_loss= 1.76121 (222 samples/sec)
SGD updated: 11601
SGD not updated: 13224913399
SSGD updated: 13055
2024-03-26 07:42:06.005240------------------------------------------------------ Precision@1: 67.34%  Precision@1: 87.73%

top1: [66.184, 67.212, 67.124, 67.336]
top5: [86.828, 87.706, 87.566, 87.732]
2024-03-26 07:42:06.287657 epoch: 5 step: 0 cls_loss= 1.71366 (56891 samples/sec)
2024-03-26 07:43:18.669650 epoch: 5 step: 500 cls_loss= 1.55904 (221 samples/sec)
2024-03-26 07:44:30.778926 epoch: 5 step: 1000 cls_loss= 1.75024 (221 samples/sec)
2024-03-26 07:45:42.772003 epoch: 5 step: 1500 cls_loss= 1.25735 (222 samples/sec)
2024-03-26 07:46:54.677354 epoch: 5 step: 2000 cls_loss= 2.26823 (222 samples/sec)
2024-03-26 07:48:06.538782 epoch: 5 step: 2500 cls_loss= 1.91639 (222 samples/sec)
2024-03-26 07:49:18.756165 epoch: 5 step: 3000 cls_loss= 1.67895 (221 samples/sec)
SGD updated: 11931
SGD not updated: 13224913069
SSGD updated: 13476
2024-03-26 07:52:22.690738------------------------------------------------------ Precision@1: 66.72%  Precision@1: 87.27%

top1: [66.184, 67.212, 67.124, 67.336, 66.72]
top5: [86.828, 87.706, 87.566, 87.732, 87.266]
2024-03-26 07:52:22.990726 epoch: 6 step: 0 cls_loss= 1.49992 (53565 samples/sec)
2024-03-26 07:53:34.597007 epoch: 6 step: 500 cls_loss= 2.41870 (223 samples/sec)
2024-03-26 07:54:46.219862 epoch: 6 step: 1000 cls_loss= 2.03391 (223 samples/sec)
2024-03-26 07:55:58.187856 epoch: 6 step: 1500 cls_loss= 1.57755 (222 samples/sec)
2024-03-26 07:57:09.817353 epoch: 6 step: 2000 cls_loss= 1.86087 (223 samples/sec)
2024-03-26 07:58:21.378799 epoch: 6 step: 2500 cls_loss= 2.01022 (223 samples/sec)
2024-03-26 07:59:33.079940 epoch: 6 step: 3000 cls_loss= 2.43289 (223 samples/sec)
SGD updated: 11213
SGD not updated: 13224913787
SSGD updated: 12618
2024-03-26 08:02:37.360804------------------------------------------------------ Precision@1: 67.20%  Precision@1: 87.65%

top1: [66.184, 67.212, 67.124, 67.336, 66.72, 67.2]
top5: [86.828, 87.706, 87.566, 87.732, 87.266, 87.65]
2024-03-26 08:02:37.656874 epoch: 7 step: 0 cls_loss= 1.78217 (54260 samples/sec)
2024-03-26 08:03:50.108646 epoch: 7 step: 500 cls_loss= 1.97670 (220 samples/sec)
2024-03-26 08:05:02.188088 epoch: 7 step: 1000 cls_loss= 1.79291 (221 samples/sec)
2024-03-26 08:06:14.498079 epoch: 7 step: 1500 cls_loss= 1.36754 (221 samples/sec)
2024-03-26 08:07:26.742758 epoch: 7 step: 2000 cls_loss= 1.75341 (221 samples/sec)
2024-03-26 08:08:38.800434 epoch: 7 step: 2500 cls_loss= 1.62721 (222 samples/sec)
2024-03-26 08:09:50.877061 epoch: 7 step: 3000 cls_loss= 2.28790 (221 samples/sec)
SGD updated: 10901
SGD not updated: 13224914099
SSGD updated: 12266
2024-03-26 08:12:54.635890------------------------------------------------------ Precision@1: 67.24%  Precision@1: 87.69%

top1: [66.184, 67.212, 67.124, 67.336, 66.72, 67.2, 67.244]
top5: [86.828, 87.706, 87.566, 87.732, 87.266, 87.65, 87.686]
2024-03-26 08:12:54.976606 epoch: 8 step: 0 cls_loss= 1.87237 (47128 samples/sec)
2024-03-26 08:14:07.074514 epoch: 8 step: 500 cls_loss= 1.38775 (221 samples/sec)
2024-03-26 08:15:19.282999 epoch: 8 step: 1000 cls_loss= 2.31087 (221 samples/sec)
2024-03-26 08:16:31.453438 epoch: 8 step: 1500 cls_loss= 2.76799 (221 samples/sec)
2024-03-26 08:17:43.539312 epoch: 8 step: 2000 cls_loss= 1.63162 (221 samples/sec)
2024-03-26 08:18:55.497979 epoch: 8 step: 2500 cls_loss= 1.62238 (222 samples/sec)
2024-03-26 08:20:07.525271 epoch: 8 step: 3000 cls_loss= 1.81487 (222 samples/sec)
SGD updated: 11019
SGD not updated: 13224913981
SSGD updated: 12379
2024-03-26 08:23:09.756480------------------------------------------------------ Precision@1: 67.22%  Precision@1: 87.66%

top1: [66.184, 67.212, 67.124, 67.336, 66.72, 67.2, 67.244, 67.218]
top5: [86.828, 87.706, 87.566, 87.732, 87.266, 87.65, 87.686, 87.656]
2024-03-26 08:23:10.048541 epoch: 9 step: 0 cls_loss= 1.28968 (55007 samples/sec)
2024-03-26 08:24:22.371329 epoch: 9 step: 500 cls_loss= 1.54344 (221 samples/sec)
2024-03-26 08:25:34.735752 epoch: 9 step: 1000 cls_loss= 1.51642 (221 samples/sec)
2024-03-26 08:26:47.019632 epoch: 9 step: 1500 cls_loss= 1.93439 (221 samples/sec)
2024-03-26 08:27:59.539275 epoch: 9 step: 2000 cls_loss= 1.59329 (220 samples/sec)
2024-03-26 08:29:11.706425 epoch: 9 step: 2500 cls_loss= 1.89009 (221 samples/sec)
2024-03-26 08:30:23.705025 epoch: 9 step: 3000 cls_loss= 1.63264 (222 samples/sec)
SGD updated: 10992
SGD not updated: 13224914008
SSGD updated: 12413
2024-03-26 08:33:25.675306------------------------------------------------------ Precision@1: 66.90%  Precision@1: 87.40%

top1: [66.184, 67.212, 67.124, 67.336, 66.72, 67.2, 67.244, 67.218, 66.9]
top5: [86.828, 87.706, 87.566, 87.732, 87.266, 87.65, 87.686, 87.656, 87.402]
2024-03-26 08:33:25.970155 epoch: 10 step: 0 cls_loss= 1.11340 (54473 samples/sec)
2024-03-26 08:34:38.038851 epoch: 10 step: 500 cls_loss= 1.33615 (222 samples/sec)
2024-03-26 08:35:50.176768 epoch: 10 step: 1000 cls_loss= 1.73380 (221 samples/sec)
2024-03-26 08:37:02.382782 epoch: 10 step: 1500 cls_loss= 1.63621 (221 samples/sec)
2024-03-26 08:38:14.570548 epoch: 10 step: 2000 cls_loss= 1.80584 (221 samples/sec)
2024-03-26 08:39:27.138618 epoch: 10 step: 2500 cls_loss= 1.12227 (220 samples/sec)
2024-03-26 08:40:39.217748 epoch: 10 step: 3000 cls_loss= 1.67234 (221 samples/sec)
SGD updated: 11235
SGD not updated: 13224913765
SSGD updated: 12608
2024-03-26 08:43:41.027133------------------------------------------------------ Precision@1: 67.36%  Precision@1: 87.74%

top1: [66.184, 67.212, 67.124, 67.336, 66.72, 67.2, 67.244, 67.218, 66.9, 67.362]
top5: [86.828, 87.706, 87.566, 87.732, 87.266, 87.65, 87.686, 87.656, 87.402, 87.742]
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ python ./imgnet_train_eval.py --Wbits 7 --Abit 7 --lr 1e-5 --wd 1e-4  --pretrain --max_epochs 11  --if_train 1 --all_validate 1  --log_interval 500 --net mobilenet_m1 --optimizer DSGD
=> creating model mobilenet_m1 ...
 learning rate =  1e-05
DSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/workspaces/pytorch-dev/SLFP_CNNs/utils/optimizer.py:111: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-26 16:36:35.411252 epoch: 1 step: 0 cls_loss= 1.72335 (10746 samples/sec)
2024-03-26 16:42:10.083809 epoch: 1 step: 500 cls_loss= 2.76231 (47 samples/sec)
2024-03-26 16:48:02.500420 epoch: 1 step: 1000 cls_loss= 1.53031 (45 samples/sec)
2024-03-26 16:54:00.832299 epoch: 1 step: 1500 cls_loss= 1.59981 (44 samples/sec)
2024-03-26 17:00:03.113984 epoch: 1 step: 2000 cls_loss= 1.41202 (44 samples/sec)
2024-03-26 17:06:04.097304 epoch: 1 step: 2500 cls_loss= 1.45442 (44 samples/sec)
2024-03-26 17:12:03.933811 epoch: 1 step: 3000 cls_loss= 1.33255 (44 samples/sec)
SGD updated: 11870
SGD not updated: 13224913130
SSGD updated: 34769
2024-03-26 17:18:59.602574------------------------------------------------------ Precision@1: 67.14%  Precision@1: 87.47%

top1: [67.138]
top5: [87.46600000000001]
2024-03-26 17:19:00.485386 epoch: 2 step: 0 cls_loss= 1.94627 (18149 samples/sec)
2024-03-26 17:24:31.840691 epoch: 2 step: 500 cls_loss= 2.06717 (48 samples/sec)
2024-03-26 17:30:06.879438 epoch: 2 step: 1000 cls_loss= 2.03600 (47 samples/sec)
2024-03-26 17:35:42.669708 epoch: 2 step: 1500 cls_loss= 2.25891 (47 samples/sec)
2024-03-26 17:41:12.617883 epoch: 2 step: 2000 cls_loss= 1.71879 (48 samples/sec)
2024-03-26 17:46:43.095836 epoch: 2 step: 2500 cls_loss= 1.52417 (48 samples/sec)
2024-03-26 17:52:18.819207 epoch: 2 step: 3000 cls_loss= 2.37059 (47 samples/sec)
SGD updated: 11911
SGD not updated: 13224913089
SSGD updated: 35127
2024-03-26 18:00:06.255119------------------------------------------------------ Precision@1: 67.29%  Precision@1: 87.65%

top1: [67.138, 67.29]
top5: [87.46600000000001, 87.646]
2024-03-26 18:00:07.125210 epoch: 3 step: 0 cls_loss= 1.79161 (18484 samples/sec)
2024-03-26 18:05:37.254356 epoch: 3 step: 500 cls_loss= 1.74282 (48 samples/sec)
2024-03-26 18:11:05.523544 epoch: 3 step: 1000 cls_loss= 1.76466 (48 samples/sec)
2024-03-26 18:16:39.020002 epoch: 3 step: 1500 cls_loss= 1.56901 (47 samples/sec)
2024-03-26 18:22:10.232796 epoch: 3 step: 2000 cls_loss= 1.66999 (48 samples/sec)
2024-03-26 18:27:43.008572 epoch: 3 step: 2500 cls_loss= 1.73153 (48 samples/sec)
2024-03-26 18:33:15.777104 epoch: 3 step: 3000 cls_loss= 1.53261 (48 samples/sec)
SGD updated: 11791
SGD not updated: 13224913209
SSGD updated: 34563
2024-03-26 18:41:01.512489------------------------------------------------------ Precision@1: 67.19%  Precision@1: 87.65%

top1: [67.138, 67.29, 67.19200000000001]
top5: [87.46600000000001, 87.646, 87.654]
2024-03-26 18:41:02.438529 epoch: 4 step: 0 cls_loss= 2.31072 (17321 samples/sec)
2024-03-26 18:46:29.640000 epoch: 4 step: 500 cls_loss= 1.83519 (48 samples/sec)
2024-03-26 18:51:55.545038 epoch: 4 step: 1000 cls_loss= 1.09379 (49 samples/sec)
2024-03-26 18:57:22.532007 epoch: 4 step: 1500 cls_loss= 1.97339 (48 samples/sec)
2024-03-26 19:02:50.102367 epoch: 4 step: 2000 cls_loss= 1.65906 (48 samples/sec)
2024-03-26 19:08:19.546607 epoch: 4 step: 2500 cls_loss= 1.12954 (48 samples/sec)
2024-03-26 19:13:51.254925 epoch: 4 step: 3000 cls_loss= 1.08099 (48 samples/sec)
SGD updated: 11380
SGD not updated: 13224913620
SSGD updated: 34031
2024-03-26 19:21:39.577947------------------------------------------------------ Precision@1: 67.24%  Precision@1: 87.56%

top1: [67.138, 67.29, 67.19200000000001, 67.244]
top5: [87.46600000000001, 87.646, 87.654, 87.556]
2024-03-26 19:21:40.450251 epoch: 5 step: 0 cls_loss= 1.34858 (18377 samples/sec)
2024-03-26 19:27:02.908187 epoch: 5 step: 500 cls_loss= 2.64746 (49 samples/sec)
2024-03-26 19:32:24.353643 epoch: 5 step: 1000 cls_loss= 0.99923 (49 samples/sec)
2024-03-26 19:37:46.722807 epoch: 5 step: 1500 cls_loss= 1.91260 (49 samples/sec)
2024-03-26 19:42:58.296093 epoch: 5 step: 2000 cls_loss= 1.65486 (51 samples/sec)
2024-03-26 19:48:19.819859 epoch: 5 step: 2500 cls_loss= 1.82297 (49 samples/sec)
2024-03-26 19:53:33.675041 epoch: 5 step: 3000 cls_loss= 2.29533 (50 samples/sec)
SGD updated: 12039
SGD not updated: 13224912961
SSGD updated: 35053
2024-03-26 19:59:46.107774------------------------------------------------------ Precision@1: 67.23%  Precision@1: 87.56%

top1: [67.138, 67.29, 67.19200000000001, 67.244, 67.234]
top5: [87.46600000000001, 87.646, 87.654, 87.556, 87.558]
2024-03-26 19:59:46.565847 epoch: 6 step: 0 cls_loss= 1.40636 (35038 samples/sec)
2024-03-26 20:01:54.111318 epoch: 6 step: 500 cls_loss= 1.43911 (125 samples/sec)
2024-03-26 20:04:03.701351 epoch: 6 step: 1000 cls_loss= 2.14057 (123 samples/sec)
2024-03-26 20:06:12.089776 epoch: 6 step: 1500 cls_loss= 1.91634 (124 samples/sec)
2024-03-26 20:08:19.436909 epoch: 6 step: 2000 cls_loss= 2.30107 (125 samples/sec)
2024-03-26 20:10:27.092928 epoch: 6 step: 2500 cls_loss= 1.87138 (125 samples/sec)
2024-03-26 20:12:37.362491 epoch: 6 step: 3000 cls_loss= 2.53439 (122 samples/sec)
SGD updated: 11512
SGD not updated: 13224913488
SSGD updated: 34428
2024-03-26 20:15:58.049492------------------------------------------------------ Precision@1: 67.16%  Precision@1: 87.72%

top1: [67.138, 67.29, 67.19200000000001, 67.244, 67.234, 67.164]
top5: [87.46600000000001, 87.646, 87.654, 87.556, 87.558, 87.72]
2024-03-26 20:15:58.345959 epoch: 7 step: 0 cls_loss= 1.73215 (54193 samples/sec)
2024-03-26 20:17:12.342200 epoch: 7 step: 500 cls_loss= 2.04063 (216 samples/sec)
2024-03-26 20:18:25.678039 epoch: 7 step: 1000 cls_loss= 1.70289 (218 samples/sec)
2024-03-26 20:19:39.329877 epoch: 7 step: 1500 cls_loss= 1.64521 (217 samples/sec)
2024-03-26 20:20:52.684750 epoch: 7 step: 2000 cls_loss= 1.91430 (218 samples/sec)
2024-03-26 20:22:06.088043 epoch: 7 step: 2500 cls_loss= 1.51748 (217 samples/sec)
2024-03-26 20:23:19.640303 epoch: 7 step: 3000 cls_loss= 1.48893 (217 samples/sec)
SGD updated: 11572
SGD not updated: 13224913428
SSGD updated: 33725
2024-03-26 20:26:26.746377------------------------------------------------------ Precision@1: 67.32%  Precision@1: 87.82%

top1: [67.138, 67.29, 67.19200000000001, 67.244, 67.234, 67.164, 67.322]
top5: [87.46600000000001, 87.646, 87.654, 87.556, 87.558, 87.72, 87.818]
2024-03-26 20:26:27.046691 epoch: 8 step: 0 cls_loss= 2.37960 (53462 samples/sec)
2024-03-26 20:27:40.330050 epoch: 8 step: 500 cls_loss= 1.10808 (218 samples/sec)
2024-03-26 20:28:53.205477 epoch: 8 step: 1000 cls_loss= 1.70192 (219 samples/sec)
2024-03-26 20:30:06.293718 epoch: 8 step: 1500 cls_loss= 1.20859 (218 samples/sec)
2024-03-26 20:31:19.251246 epoch: 8 step: 2000 cls_loss= 2.09561 (219 samples/sec)
2024-03-26 20:32:32.177480 epoch: 8 step: 2500 cls_loss= 1.61710 (219 samples/sec)
2024-03-26 20:33:45.013138 epoch: 8 step: 3000 cls_loss= 1.41585 (219 samples/sec)
SGD updated: 11221
SGD not updated: 13224913779
SSGD updated: 33299
2024-03-26 20:36:53.023742------------------------------------------------------ Precision@1: 67.31%  Precision@1: 87.70%

top1: [67.138, 67.29, 67.19200000000001, 67.244, 67.234, 67.164, 67.322, 67.312]
top5: [87.46600000000001, 87.646, 87.654, 87.556, 87.558, 87.72, 87.818, 87.69800000000001]
2024-03-26 20:36:53.312355 epoch: 9 step: 0 cls_loss= 1.33853 (55643 samples/sec)
2024-03-26 20:38:06.250563 epoch: 9 step: 500 cls_loss= 1.92312 (219 samples/sec)
2024-03-26 20:39:19.158414 epoch: 9 step: 1000 cls_loss= 2.34103 (219 samples/sec)
2024-03-26 20:40:32.038421 epoch: 9 step: 1500 cls_loss= 1.34738 (219 samples/sec)
2024-03-26 20:41:44.994278 epoch: 9 step: 2000 cls_loss= 1.72949 (219 samples/sec)
2024-03-26 20:42:57.946324 epoch: 9 step: 2500 cls_loss= 2.29727 (219 samples/sec)
2024-03-26 20:44:10.857964 epoch: 9 step: 3000 cls_loss= 1.94994 (219 samples/sec)
SGD updated: 11063
SGD not updated: 13224913937
SSGD updated: 32808
2024-03-26 20:47:16.851627------------------------------------------------------ Precision@1: 67.11%  Precision@1: 87.58%

top1: [67.138, 67.29, 67.19200000000001, 67.244, 67.234, 67.164, 67.322, 67.312, 67.106]
top5: [87.46600000000001, 87.646, 87.654, 87.556, 87.558, 87.72, 87.818, 87.69800000000001, 87.584]
2024-03-26 20:47:17.157162 epoch: 10 step: 0 cls_loss= 2.13000 (52570 samples/sec)
2024-03-26 20:48:30.126932 epoch: 10 step: 500 cls_loss= 1.37206 (219 samples/sec)
2024-03-26 20:49:43.621764 epoch: 10 step: 1000 cls_loss= 1.75180 (217 samples/sec)
2024-03-26 20:50:57.167073 epoch: 10 step: 1500 cls_loss= 1.66458 (217 samples/sec)
2024-03-26 20:52:10.583583 epoch: 10 step: 2000 cls_loss= 1.36349 (217 samples/sec)
2024-03-26 20:53:24.002477 epoch: 10 step: 2500 cls_loss= 2.18522 (217 samples/sec)
2024-03-26 20:54:36.918054 epoch: 10 step: 3000 cls_loss= 1.96975 (219 samples/sec)
SGD updated: 11420
SGD not updated: 13224913580
SSGD updated: 33582
2024-03-26 20:57:44.038098------------------------------------------------------ Precision@1: 67.22%  Precision@1: 87.66%

top1: [67.138, 67.29, 67.19200000000001, 67.244, 67.234, 67.164, 67.322, 67.312, 67.106, 67.22]
top5: [87.46600000000001, 87.646, 87.654, 87.556, 87.558, 87.72, 87.818, 87.69800000000001, 87.584, 87.662]
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ python ./imgnet_train_eval.py --Wbits 7 --Abit 7 --lr 1e-5 --wd 1e-4  --pretrain --max_epochs 11  --if_train 1 --all_validate 1  --log_interval 500 --net mobilenet_m1 --optimizer DSGD
=> creating model mobilenet_m1 ...
 learning rate =  1e-05
DSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/workspaces/pytorch-dev/SLFP_CNNs/utils/optimizer.py:111: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-26 21:16:55.666823 epoch: 1 step: 0 cls_loss= 1.33818 (8576 samples/sec)
2024-03-26 21:18:04.093054 epoch: 1 step: 500 cls_loss= 2.36631 (233 samples/sec)
2024-03-26 21:19:12.444802 epoch: 1 step: 1000 cls_loss= 1.69368 (234 samples/sec)
2024-03-26 21:20:20.783632 epoch: 1 step: 1500 cls_loss= 2.15734 (234 samples/sec)
2024-03-26 21:21:29.125500 epoch: 1 step: 2000 cls_loss= 2.12282 (234 samples/sec)
2024-03-26 21:22:37.641081 epoch: 1 step: 2500 cls_loss= 1.89096 (233 samples/sec)
2024-03-26 21:23:45.946339 epoch: 1 step: 3000 cls_loss= 2.24022 (234 samples/sec)
SGD updated: 8362
SGD not updated: 13224916638
SSGD updated: 24612
2024-03-26 21:26:47.576757------------------------------------------------------ Precision@1: 67.13%  Precision@1: 87.65%

top1: [67.134]
top5: [87.648]
2024-03-26 21:26:47.850134 epoch: 2 step: 0 cls_loss= 1.30800 (58708 samples/sec)
2024-03-26 21:27:56.315518 epoch: 2 step: 500 cls_loss= 2.07554 (233 samples/sec)
2024-03-26 21:29:04.750876 epoch: 2 step: 1000 cls_loss= 1.62942 (233 samples/sec)
2024-03-26 21:30:13.214838 epoch: 2 step: 1500 cls_loss= 1.62056 (233 samples/sec)
2024-03-26 21:31:21.505387 epoch: 2 step: 2000 cls_loss= 2.13919 (234 samples/sec)
2024-03-26 21:32:29.839148 epoch: 2 step: 2500 cls_loss= 1.49144 (234 samples/sec)
2024-03-26 21:33:38.329633 epoch: 2 step: 3000 cls_loss= 1.68487 (233 samples/sec)
SGD updated: 8504
SGD not updated: 13224916496
SSGD updated: 24733
2024-03-26 21:36:42.568719------------------------------------------------------ Precision@1: 67.24%  Precision@1: 87.77%

top1: [67.134, 67.24]
top5: [87.648, 87.768]
2024-03-26 21:36:42.879226 epoch: 3 step: 0 cls_loss= 1.65086 (51720 samples/sec)
2024-03-26 21:37:51.011998 epoch: 3 step: 500 cls_loss= 1.37540 (234 samples/sec)
2024-03-26 21:38:59.208738 epoch: 3 step: 1000 cls_loss= 1.56291 (234 samples/sec)
2024-03-26 21:40:07.606275 epoch: 3 step: 1500 cls_loss= 1.69989 (233 samples/sec)
2024-03-26 21:41:16.031067 epoch: 3 step: 2000 cls_loss= 1.82688 (233 samples/sec)
2024-03-26 21:42:24.579983 epoch: 3 step: 2500 cls_loss= 2.14936 (233 samples/sec)
2024-03-26 21:43:32.997128 epoch: 3 step: 3000 cls_loss= 1.99240 (233 samples/sec)
SGD updated: 7888
SGD not updated: 13224917112
SSGD updated: 23243
2024-03-26 21:47:35.375940------------------------------------------------------ Precision@1: 67.25%  Precision@1: 87.67%

top1: [67.134, 67.24, 67.248]
top5: [87.648, 87.768, 87.67]
2024-03-26 21:47:36.488064 epoch: 4 step: 0 cls_loss= 1.79535 (14431 samples/sec)
2024-03-26 21:52:34.121157 epoch: 4 step: 500 cls_loss= 1.44613 (53 samples/sec)
2024-03-26 21:57:22.035222 epoch: 4 step: 1000 cls_loss= 1.28677 (55 samples/sec)
2024-03-26 22:02:09.402766 epoch: 4 step: 1500 cls_loss= 2.21624 (55 samples/sec)
2024-03-26 22:06:59.529736 epoch: 4 step: 2000 cls_loss= 1.74177 (55 samples/sec)
2024-03-26 22:11:51.086263 epoch: 4 step: 2500 cls_loss= 1.37931 (54 samples/sec)
2024-03-26 22:16:56.515567 epoch: 4 step: 3000 cls_loss= 2.25274 (52 samples/sec)
SGD updated: 7782
SGD not updated: 13224917218
SSGD updated: 22876
2024-03-26 22:23:58.306771------------------------------------------------------ Precision@1: 67.03%  Precision@1: 87.51%

top1: [67.134, 67.24, 67.248, 67.032]
top5: [87.648, 87.768, 87.67, 87.506]
2024-03-26 22:23:59.130919 epoch: 5 step: 0 cls_loss= 2.31768 (19451 samples/sec)
2024-03-26 22:29:10.380437 epoch: 5 step: 500 cls_loss= 1.61297 (51 samples/sec)
2024-03-26 22:34:31.957970 epoch: 5 step: 1000 cls_loss= 1.75589 (49 samples/sec)
2024-03-26 22:39:51.753600 epoch: 5 step: 1500 cls_loss= 1.45918 (50 samples/sec)
2024-03-26 22:45:12.394441 epoch: 5 step: 2000 cls_loss= 1.58194 (49 samples/sec)
2024-03-26 22:50:31.070283 epoch: 5 step: 2500 cls_loss= 2.03930 (50 samples/sec)
2024-03-26 22:55:50.567537 epoch: 5 step: 3000 cls_loss= 2.45087 (50 samples/sec)
SGD updated: 7642
SGD not updated: 13224917358
SSGD updated: 22312
2024-03-26 23:03:31.900293------------------------------------------------------ Precision@1: 67.24%  Precision@1: 87.67%

top1: [67.134, 67.24, 67.248, 67.032, 67.236]
top5: [87.648, 87.768, 87.67, 87.506, 87.666]
2024-03-26 23:03:32.681070 epoch: 6 step: 0 cls_loss= 1.26421 (20543 samples/sec)
2024-03-26 23:08:29.794744 epoch: 6 step: 500 cls_loss= 1.38034 (53 samples/sec)
2024-03-26 23:13:49.350961 epoch: 6 step: 1000 cls_loss= 1.79538 (50 samples/sec)
2024-03-26 23:19:06.709943 epoch: 6 step: 1500 cls_loss= 2.42348 (50 samples/sec)
2024-03-26 23:24:28.678439 epoch: 6 step: 2000 cls_loss= 1.65925 (49 samples/sec)
2024-03-26 23:29:49.014096 epoch: 6 step: 2500 cls_loss= 1.91679 (49 samples/sec)
2024-03-26 23:35:13.330711 epoch: 6 step: 3000 cls_loss= 2.04754 (49 samples/sec)
SGD updated: 7736
SGD not updated: 13224917264
SSGD updated: 22818
2024-03-26 23:42:50.577499------------------------------------------------------ Precision@1: 67.32%  Precision@1: 87.71%

top1: [67.134, 67.24, 67.248, 67.032, 67.236, 67.318]
top5: [87.648, 87.768, 87.67, 87.506, 87.666, 87.714]
2024-03-26 23:42:51.342972 epoch: 7 step: 0 cls_loss= 1.61619 (20973 samples/sec)
2024-03-26 23:47:41.496011 epoch: 7 step: 500 cls_loss= 1.78011 (55 samples/sec)
2024-03-26 23:52:58.336071 epoch: 7 step: 1000 cls_loss= 1.50733 (50 samples/sec)
2024-03-26 23:58:19.030289 epoch: 7 step: 1500 cls_loss= 1.55528 (49 samples/sec)
2024-03-27 00:03:42.201021 epoch: 7 step: 2000 cls_loss= 2.67736 (49 samples/sec)
2024-03-27 00:09:08.123967 epoch: 7 step: 2500 cls_loss= 1.51320 (49 samples/sec)
2024-03-27 00:14:49.174658 epoch: 7 step: 3000 cls_loss= 1.88523 (46 samples/sec)
SGD updated: 7834
SGD not updated: 13224917166
SSGD updated: 22930
2024-03-27 00:21:02.461327------------------------------------------------------ Precision@1: 67.15%  Precision@1: 87.78%

top1: [67.134, 67.24, 67.248, 67.032, 67.236, 67.318, 67.15]
top5: [87.648, 87.768, 87.67, 87.506, 87.666, 87.714, 87.78]
2024-03-27 00:21:02.882116 epoch: 8 step: 0 cls_loss= 2.04147 (38123 samples/sec)
2024-03-27 00:23:28.010832 epoch: 8 step: 500 cls_loss= 1.78187 (110 samples/sec)
2024-03-27 00:25:51.767036 epoch: 8 step: 1000 cls_loss= 2.02275 (111 samples/sec)
2024-03-27 00:28:13.595693 epoch: 8 step: 1500 cls_loss= 1.96968 (112 samples/sec)
2024-03-27 00:30:38.155196 epoch: 8 step: 2000 cls_loss= 1.55809 (110 samples/sec)
2024-03-27 00:33:04.290500 epoch: 8 step: 2500 cls_loss= 1.45787 (109 samples/sec)
2024-03-27 00:35:29.488117 epoch: 8 step: 3000 cls_loss= 2.26665 (110 samples/sec)
SGD updated: 7622
SGD not updated: 13224917378
SSGD updated: 22323
2024-03-27 00:38:48.865753------------------------------------------------------ Precision@1: 67.14%  Precision@1: 87.59%

top1: [67.134, 67.24, 67.248, 67.032, 67.236, 67.318, 67.15, 67.142]
top5: [87.648, 87.768, 87.67, 87.506, 87.666, 87.714, 87.78, 87.59400000000001]
2024-03-27 00:38:49.148789 epoch: 9 step: 0 cls_loss= 2.31232 (56766 samples/sec)
2024-03-27 00:39:56.404316 epoch: 9 step: 500 cls_loss= 2.33991 (237 samples/sec)
2024-03-27 00:41:03.838026 epoch: 9 step: 1000 cls_loss= 1.89190 (237 samples/sec)
2024-03-27 00:42:11.258044 epoch: 9 step: 1500 cls_loss= 1.76620 (237 samples/sec)
2024-03-27 00:43:19.029222 epoch: 9 step: 2000 cls_loss= 1.78050 (236 samples/sec)
2024-03-27 00:44:26.700267 epoch: 9 step: 2500 cls_loss= 1.92762 (236 samples/sec)
2024-03-27 00:45:34.207994 epoch: 9 step: 3000 cls_loss= 2.04686 (237 samples/sec)
SGD updated: 7831
SGD not updated: 13224917169
SSGD updated: 23057
2024-03-27 00:48:38.633916------------------------------------------------------ Precision@1: 67.02%  Precision@1: 87.61%

top1: [67.134, 67.24, 67.248, 67.032, 67.236, 67.318, 67.15, 67.142, 67.016]
top5: [87.648, 87.768, 87.67, 87.506, 87.666, 87.714, 87.78, 87.59400000000001, 87.60600000000001]
2024-03-27 00:48:38.935985 epoch: 10 step: 0 cls_loss= 1.58091 (53173 samples/sec)
2024-03-27 00:49:46.212196 epoch: 10 step: 500 cls_loss= 1.82062 (237 samples/sec)
2024-03-27 00:50:53.508768 epoch: 10 step: 1000 cls_loss= 1.69765 (237 samples/sec)
2024-03-27 00:52:00.773398 epoch: 10 step: 1500 cls_loss= 1.65112 (237 samples/sec)
2024-03-27 00:53:08.092759 epoch: 10 step: 2000 cls_loss= 2.34101 (237 samples/sec)
2024-03-27 00:54:15.358707 epoch: 10 step: 2500 cls_loss= 1.71353 (237 samples/sec)
2024-03-27 00:55:22.646336 epoch: 10 step: 3000 cls_loss= 2.50164 (237 samples/sec)
SGD updated: 7961
SGD not updated: 13224917039
SSGD updated: 23458
2024-03-27 00:58:24.012102------------------------------------------------------ Precision@1: 67.22%  Precision@1: 87.72%

top1: [67.134, 67.24, 67.248, 67.032, 67.236, 67.318, 67.15, 67.142, 67.016, 67.22200000000001]
top5: [87.648, 87.768, 87.67, 87.506, 87.666, 87.714, 87.78, 87.59400000000001, 87.60600000000001, 87.72]
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ [K]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ python ./imgnet_train_eval.py --Wbits 7 --Abit 7 --lr 1e-5 --wd 1e-4  --pretrain --max_epochs 11  --if_train 1 --all_validate 1  --log_interval 500 --net mobilenet_m1 --optimizer DSGD
=> creating model mobilenet_m1 ...
 learning rate =  1e-05
DSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/workspaces/pytorch-dev/SLFP_CNNs/utils/optimizer.py:111: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-27 05:17:56.919325 epoch: 1 step: 0 cls_loss= 1.86995 (14865 samples/sec)
2024-03-27 05:19:03.878566 epoch: 1 step: 500 cls_loss= 1.95141 (238 samples/sec)
2024-03-27 05:20:10.560409 epoch: 1 step: 1000 cls_loss= 1.87506 (239 samples/sec)
2024-03-27 05:21:17.489058 epoch: 1 step: 1500 cls_loss= 1.81984 (239 samples/sec)
2024-03-27 05:22:24.360913 epoch: 1 step: 2000 cls_loss= 1.50517 (239 samples/sec)
2024-03-27 05:23:31.209613 epoch: 1 step: 2500 cls_loss= 1.90214 (239 samples/sec)
2024-03-27 05:24:38.180888 epoch: 1 step: 3000 cls_loss= 0.99397 (238 samples/sec)
SGD updated: 8216
SGD not updated: 13224916784
SSGD updated: 24292
2024-03-27 05:27:37.518541------------------------------------------------------ Precision@1: 67.15%  Precision@1: 87.61%

top1: [67.146]
top5: [87.60600000000001]
2024-03-27 05:27:37.796121 epoch: 2 step: 0 cls_loss= 1.87219 (57877 samples/sec)
2024-03-27 05:28:45.074306 epoch: 2 step: 500 cls_loss= 2.28572 (237 samples/sec)
2024-03-27 05:29:52.314067 epoch: 2 step: 1000 cls_loss= 2.10453 (237 samples/sec)
2024-03-27 05:30:59.454080 epoch: 2 step: 1500 cls_loss= 1.60132 (238 samples/sec)
2024-03-27 05:32:06.425698 epoch: 2 step: 2000 cls_loss= 2.07428 (238 samples/sec)
2024-03-27 05:33:13.425878 epoch: 2 step: 2500 cls_loss= 1.90043 (238 samples/sec)
2024-03-27 05:34:20.425497 epoch: 2 step: 3000 cls_loss= 1.48657 (238 samples/sec)
SGD updated: 8135
SGD not updated: 13224916865
SSGD updated: 24090
2024-03-27 05:37:19.298407------------------------------------------------------ Precision@1: 67.07%  Precision@1: 87.61%

top1: [67.146, 67.07000000000001]
top5: [87.60600000000001, 87.614]
2024-03-27 05:37:19.572010 epoch: 3 step: 0 cls_loss= 1.64313 (58741 samples/sec)
2024-03-27 05:38:26.535376 epoch: 3 step: 500 cls_loss= 1.87946 (238 samples/sec)
2024-03-27 05:39:33.435488 epoch: 3 step: 1000 cls_loss= 2.31019 (239 samples/sec)
2024-03-27 05:40:40.267727 epoch: 3 step: 1500 cls_loss= 1.93367 (239 samples/sec)
2024-03-27 05:41:47.091002 epoch: 3 step: 2000 cls_loss= 1.42386 (239 samples/sec)
2024-03-27 05:42:53.935969 epoch: 3 step: 2500 cls_loss= 1.31188 (239 samples/sec)
2024-03-27 05:44:00.932832 epoch: 3 step: 3000 cls_loss= 2.07962 (238 samples/sec)
SGD updated: 8237
SGD not updated: 13224916763
SSGD updated: 24405
2024-03-27 05:47:06.034600------------------------------------------------------ Precision@1: 67.03%  Precision@1: 87.72%

top1: [67.146, 67.07000000000001, 67.026]
top5: [87.60600000000001, 87.614, 87.72200000000001]
2024-03-27 05:47:06.303449 epoch: 4 step: 0 cls_loss= 1.87449 (59758 samples/sec)
2024-03-27 05:48:13.324865 epoch: 4 step: 500 cls_loss= 2.31102 (238 samples/sec)
2024-03-27 05:49:20.113917 epoch: 4 step: 1000 cls_loss= 2.03348 (239 samples/sec)
2024-03-27 05:50:26.910753 epoch: 4 step: 1500 cls_loss= 1.27048 (239 samples/sec)
2024-03-27 05:51:33.617266 epoch: 4 step: 2000 cls_loss= 1.86456 (239 samples/sec)
2024-03-27 05:52:40.460155 epoch: 4 step: 2500 cls_loss= 2.32411 (239 samples/sec)
2024-03-27 05:53:47.332268 epoch: 4 step: 3000 cls_loss= 2.16284 (239 samples/sec)
SGD updated: 8092
SGD not updated: 13224916908
SSGD updated: 23738
2024-03-27 05:56:50.689314------------------------------------------------------ Precision@1: 67.30%  Precision@1: 87.74%

top1: [67.146, 67.07000000000001, 67.026, 67.296]
top5: [87.60600000000001, 87.614, 87.72200000000001, 87.736]
2024-03-27 05:56:50.970014 epoch: 5 step: 0 cls_loss= 1.17961 (57245 samples/sec)
2024-03-27 05:57:57.938620 epoch: 5 step: 500 cls_loss= 1.77001 (238 samples/sec)
2024-03-27 05:59:04.933491 epoch: 5 step: 1000 cls_loss= 1.03476 (238 samples/sec)
2024-03-27 06:00:11.881268 epoch: 5 step: 1500 cls_loss= 0.92271 (238 samples/sec)
2024-03-27 06:01:18.684376 epoch: 5 step: 2000 cls_loss= 1.38698 (239 samples/sec)
2024-03-27 06:02:25.430528 epoch: 5 step: 2500 cls_loss= 1.53602 (239 samples/sec)
2024-03-27 06:03:32.300725 epoch: 5 step: 3000 cls_loss= 2.70526 (239 samples/sec)
SGD updated: 7738
SGD not updated: 13224917262
SSGD updated: 23135
2024-03-27 06:06:38.805047------------------------------------------------------ Precision@1: 67.54%  Precision@1: 87.68%

top1: [67.146, 67.07000000000001, 67.026, 67.296, 67.542]
top5: [87.60600000000001, 87.614, 87.72200000000001, 87.736, 87.676]
2024-03-27 06:06:39.080764 epoch: 6 step: 0 cls_loss= 1.71679 (58254 samples/sec)
2024-03-27 06:07:45.975104 epoch: 6 step: 500 cls_loss= 1.66403 (239 samples/sec)
2024-03-27 06:08:53.022659 epoch: 6 step: 1000 cls_loss= 1.74220 (238 samples/sec)
2024-03-27 06:10:00.122737 epoch: 6 step: 1500 cls_loss= 2.07811 (238 samples/sec)
2024-03-27 06:11:07.050090 epoch: 6 step: 2000 cls_loss= 1.56528 (239 samples/sec)
2024-03-27 06:12:13.876195 epoch: 6 step: 2500 cls_loss= 2.00426 (239 samples/sec)
2024-03-27 06:13:20.760308 epoch: 6 step: 3000 cls_loss= 1.24296 (239 samples/sec)
SGD updated: 7829
SGD not updated: 13224917171
SSGD updated: 22881
2024-03-27 06:16:25.189910------------------------------------------------------ Precision@1: 67.15%  Precision@1: 87.59%

top1: [67.146, 67.07000000000001, 67.026, 67.296, 67.542, 67.146]
top5: [87.60600000000001, 87.614, 87.72200000000001, 87.736, 87.676, 87.59]
2024-03-27 06:16:25.469405 epoch: 7 step: 0 cls_loss= 1.76978 (57495 samples/sec)
2024-03-27 06:17:32.323434 epoch: 7 step: 500 cls_loss= 2.37286 (239 samples/sec)
2024-03-27 06:18:39.177966 epoch: 7 step: 1000 cls_loss= 2.11911 (239 samples/sec)
2024-03-27 06:19:45.864501 epoch: 7 step: 1500 cls_loss= 1.57537 (239 samples/sec)
2024-03-27 06:20:52.715206 epoch: 7 step: 2000 cls_loss= 1.91993 (239 samples/sec)
2024-03-27 06:21:59.580384 epoch: 7 step: 2500 cls_loss= 1.59064 (239 samples/sec)
2024-03-27 06:23:06.488238 epoch: 7 step: 3000 cls_loss= 1.78623 (239 samples/sec)
SGD updated: 7895
SGD not updated: 13224917105
SSGD updated: 23285
2024-03-27 06:26:11.906034------------------------------------------------------ Precision@1: 67.27%  Precision@1: 87.62%

top1: [67.146, 67.07000000000001, 67.026, 67.296, 67.542, 67.146, 67.274]
top5: [87.60600000000001, 87.614, 87.72200000000001, 87.736, 87.676, 87.59, 87.622]
2024-03-27 06:26:12.211648 epoch: 8 step: 0 cls_loss= 1.87788 (52564 samples/sec)
2024-03-27 06:27:19.138821 epoch: 8 step: 500 cls_loss= 1.58063 (239 samples/sec)
2024-03-27 06:28:25.937195 epoch: 8 step: 1000 cls_loss= 1.61616 (239 samples/sec)
2024-03-27 06:29:32.854540 epoch: 8 step: 1500 cls_loss= 1.40020 (239 samples/sec)
2024-03-27 06:30:39.892362 epoch: 8 step: 2000 cls_loss= 1.95305 (238 samples/sec)
2024-03-27 06:31:47.112299 epoch: 8 step: 2500 cls_loss= 1.81275 (238 samples/sec)
2024-03-27 06:32:54.169106 epoch: 8 step: 3000 cls_loss= 1.96875 (238 samples/sec)
SGD updated: 8217
SGD not updated: 13224916783
SSGD updated: 23568
2024-03-27 06:35:57.564936------------------------------------------------------ Precision@1: 67.20%  Precision@1: 87.68%

top1: [67.146, 67.07000000000001, 67.026, 67.296, 67.542, 67.146, 67.274, 67.2]
top5: [87.60600000000001, 87.614, 87.72200000000001, 87.736, 87.676, 87.59, 87.622, 87.678]
2024-03-27 06:35:57.850121 epoch: 9 step: 0 cls_loss= 1.83082 (56360 samples/sec)
2024-03-27 06:37:05.095720 epoch: 9 step: 500 cls_loss= 1.24393 (237 samples/sec)
2024-03-27 06:38:12.429065 epoch: 9 step: 1000 cls_loss= 1.60095 (237 samples/sec)
2024-03-27 06:39:19.739397 epoch: 9 step: 1500 cls_loss= 1.74214 (237 samples/sec)
2024-03-27 06:40:27.081022 epoch: 9 step: 2000 cls_loss= 2.84191 (237 samples/sec)
2024-03-27 06:41:34.278909 epoch: 9 step: 2500 cls_loss= 2.09172 (238 samples/sec)
2024-03-27 06:42:41.452109 epoch: 9 step: 3000 cls_loss= 1.68548 (238 samples/sec)
SGD updated: 7853
SGD not updated: 13224917147
SSGD updated: 23300
2024-03-27 06:45:47.064378------------------------------------------------------ Precision@1: 66.96%  Precision@1: 87.66%

top1: [67.146, 67.07000000000001, 67.026, 67.296, 67.542, 67.146, 67.274, 67.2, 66.962]
top5: [87.60600000000001, 87.614, 87.72200000000001, 87.736, 87.676, 87.59, 87.622, 87.678, 87.656]
2024-03-27 06:45:47.354610 epoch: 10 step: 0 cls_loss= 1.69352 (55352 samples/sec)
2024-03-27 06:46:54.475482 epoch: 10 step: 500 cls_loss= 1.76195 (238 samples/sec)
2024-03-27 06:48:01.395631 epoch: 10 step: 1000 cls_loss= 1.40046 (239 samples/sec)
2024-03-27 06:49:08.247623 epoch: 10 step: 1500 cls_loss= 1.78881 (239 samples/sec)
2024-03-27 06:50:15.192593 epoch: 10 step: 2000 cls_loss= 1.76234 (239 samples/sec)
2024-03-27 06:51:22.187706 epoch: 10 step: 2500 cls_loss= 1.29421 (238 samples/sec)
2024-03-27 06:52:29.345312 epoch: 10 step: 3000 cls_loss= 2.05416 (238 samples/sec)
SGD updated: 7939
SGD not updated: 13224917061
SSGD updated: 23366
2024-03-27 06:55:32.775693------------------------------------------------------ Precision@1: 67.01%  Precision@1: 87.58%

top1: [67.146, 67.07000000000001, 67.026, 67.296, 67.542, 67.146, 67.274, 67.2, 66.962, 67.014]
top5: [87.60600000000001, 87.614, 87.72200000000001, 87.736, 87.676, 87.59, 87.622, 87.678, 87.656, 87.58200000000001]
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ exit

Script done on 2024-03-27 06:59:18+08:00 [COMMAND_EXIT_CODE="0"]
