Script started on 2024-03-09 20:51:58+08:00 [TERM="xterm-256color" TTY="/dev/pts/0" COLUMNS="195" LINES="11"]
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ bash bash_train.sh 
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
Traceback (most recent call last):
  File "./cifar100_train_eval.py", line 362, in <module>
    main()
  File "./cifar100_train_eval.py", line 187, in main
    optimizer = torch.optim.Adam(model.parameters(), cfg.lr, momentum=0.9, weight_decay=cfg.wd)
TypeError: __init__() got an unexpected keyword argument 'momentum'
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
Traceback (most recent call last):
  File "./cifar100_train_eval.py", line 362, in <module>
    main()
  File "./cifar100_train_eval.py", line 187, in main
    optimizer = torch.optim.Adam(model.parameters(), cfg.lr, momentum=0.9, weight_decay=cfg.wd)
TypeError: __init__() got an unexpected keyword argument 'momentum'
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
Traceback (most recent call last):
  File "./cifar100_train_eval.py", line 362, in <module>
    main()
  File "./cifar100_train_eval.py", line 187, in main
    optimizer = torch.optim.Adam(model.parameters(), cfg.lr, momentum=0.9, weight_decay=cfg.wd)
TypeError: __init__() got an unexpected keyword argument 'momentum'
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
Traceback (most recent call last):
  File "./cifar100_train_eval.py", line 362, in <module>
    main()
  File "./cifar100_train_eval.py", line 187, in main
    optimizer = torch.optim.Adam(model.parameters(), cfg.lr, momentum=0.9, weight_decay=cfg.wd)
TypeError: __init__() got an unexpected keyword argument 'momentum'
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
Traceback (most recent call last):
  File "./cifar100_train_eval.py", line 362, in <module>
    main()
  File "./cifar100_train_eval.py", line 187, in main
    optimizer = torch.optim.Adam(model.parameters(), cfg.lr, momentum=0.9, weight_decay=cfg.wd)
TypeError: __init__() got an unexpected keyword argument 'momentum'
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
Traceback (most recent call last):
  File "./cifar100_train_eval.py", line 362, in <module>
    main()
  File "./cifar100_train_eval.py", line 187, in main
    optimizer = torch.optim.Adam(model.parameters(), cfg.lr, momentum=0.9, weight_decay=cfg.wd)
TypeError: __init__() got an unexpected keyword argument 'momentum'
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
Traceback (most recent call last):
  File "./cifar100_train_eval.py", line 362, in <module>
    main()
  File "./cifar100_train_eval.py", line 187, in main
    optimizer = torch.optim.Adam(model.parameters(), cfg.lr, momentum=0.9, weight_decay=cfg.wd)
TypeError: __init__() got an unexpected keyword argument 'momentum'
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
Traceback (most recent call last):
  File "./cifar100_train_eval.py", line 362, in <module>
    main()
  File "./cifar100_train_eval.py", line 187, in main
    optimizer = torch.optim.Adam(model.parameters(), cfg.lr, momentum=0.9, weight_decay=cfg.wd)
TypeError: __init__() got an unexpected keyword argument 'momentum'
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
Traceback (most recent call last):
  File "./cifar100_train_eval.py", line 362, in <module>
    main()
  File "./cifar100_train_eval.py", line 187, in main
    optimizer = torch.optim.Adam(model.parameters(), cfg.lr, momentum=0.9, weight_decay=cfg.wd)
TypeError: __init__() got an unexpected keyword argument 'momentum'
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
Traceback (most recent call last):
  File "./cifar100_train_eval.py", line 362, in <module>
    main()
  File "./cifar100_train_eval.py", line 187, in main
    optimizer = torch.optim.Adam(model.parameters(), cfg.lr, momentum=0.9, weight_decay=cfg.wd)
TypeError: __init__() got an unexpected keyword argument 'momentum'
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ bash bash_train.sh 
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-09 20:53:56.252664 epoch: 0 step: 0 cls_loss= 0.99723 (39542 samples/sec)
2024-03-09 20:53:59.382446 epoch: 0 step: 100 cls_loss= 1.05440 (9585 samples/sec)
saving....
2024-03-09 20:54:03.432056------------------------------------------------------ Precision@1: 59.56% 

[59.56]
max acc : 59.56

Epoch: 1
2024-03-09 20:54:03.630063 epoch: 1 step: 0 cls_loss= 0.96908 (167482 samples/sec)
2024-03-09 20:54:06.628807 epoch: 1 step: 100 cls_loss= 1.05900 (10004 samples/sec)
saving....
2024-03-09 20:54:10.335877------------------------------------------------------ Precision@1: 59.66% 

[59.56, 59.66]
max acc : 59.66

Epoch: 2
2024-03-09 20:54:10.560332 epoch: 2 step: 0 cls_loss= 1.04170 (154042 samples/sec)
2024-03-09 20:54:13.654904 epoch: 2 step: 100 cls_loss= 1.02275 (9694 samples/sec)
saving....
2024-03-09 20:54:17.412287------------------------------------------------------ Precision@1: 59.55% 

[59.56, 59.66, 59.55]

Epoch: 3
2024-03-09 20:54:17.611701 epoch: 3 step: 0 cls_loss= 1.04827 (151343 samples/sec)
2024-03-09 20:54:20.665705 epoch: 3 step: 100 cls_loss= 1.05961 (9823 samples/sec)
saving....
2024-03-09 20:54:24.421390------------------------------------------------------ Precision@1: 59.60% 

[59.56, 59.66, 59.55, 59.6]

Epoch: 4
2024-03-09 20:54:24.633567 epoch: 4 step: 0 cls_loss= 0.91688 (142019 samples/sec)
2024-03-09 20:54:27.642639 epoch: 4 step: 100 cls_loss= 0.95601 (9970 samples/sec)
saving....
2024-03-09 20:54:31.324120------------------------------------------------------ Precision@1: 59.90% 

[59.56, 59.66, 59.55, 59.6, 59.9]
max acc : 59.9

Epoch: 5
2024-03-09 20:54:31.536714 epoch: 5 step: 0 cls_loss= 0.90469 (163722 samples/sec)
2024-03-09 20:54:34.623162 epoch: 5 step: 100 cls_loss= 1.00463 (9720 samples/sec)
saving....
2024-03-09 20:54:38.419915------------------------------------------------------ Precision@1: 59.85% 

[59.56, 59.66, 59.55, 59.6, 59.9, 59.85]

Epoch: 6
2024-03-09 20:54:38.632553 epoch: 6 step: 0 cls_loss= 0.90454 (141773 samples/sec)
2024-03-09 20:54:41.628610 epoch: 6 step: 100 cls_loss= 1.06744 (10013 samples/sec)
saving....
2024-03-09 20:54:45.370121------------------------------------------------------ Precision@1: 59.70% 

[59.56, 59.66, 59.55, 59.6, 59.9, 59.85, 59.7]

Epoch: 7
2024-03-09 20:54:45.571069 epoch: 7 step: 0 cls_loss= 1.00961 (150178 samples/sec)
2024-03-09 20:54:48.562180 epoch: 7 step: 100 cls_loss= 0.92977 (10029 samples/sec)
saving....
2024-03-09 20:54:52.309554------------------------------------------------------ Precision@1: 59.81% 

[59.56, 59.66, 59.55, 59.6, 59.9, 59.85, 59.7, 59.81]

Epoch: 8
2024-03-09 20:54:52.518142 epoch: 8 step: 0 cls_loss= 1.03101 (144577 samples/sec)
2024-03-09 20:54:55.538217 epoch: 8 step: 100 cls_loss= 0.90260 (9933 samples/sec)
saving....
2024-03-09 20:54:59.283854------------------------------------------------------ Precision@1: 59.65% 

[59.56, 59.66, 59.55, 59.6, 59.9, 59.85, 59.7, 59.81, 59.65]

Epoch: 9
2024-03-09 20:54:59.487211 epoch: 9 step: 0 cls_loss= 1.05489 (148331 samples/sec)
2024-03-09 20:55:02.590370 epoch: 9 step: 100 cls_loss= 1.04597 (9667 samples/sec)
saving....
2024-03-09 20:55:06.311210------------------------------------------------------ Precision@1: 59.64% 

[59.56, 59.66, 59.55, 59.6, 59.9, 59.85, 59.7, 59.81, 59.65, 59.64]

Epoch: 10
2024-03-09 20:55:06.519595 epoch: 10 step: 0 cls_loss= 0.97439 (144697 samples/sec)
2024-03-09 20:55:09.612509 epoch: 10 step: 100 cls_loss= 1.11170 (9699 samples/sec)
saving....
2024-03-09 20:55:13.351474------------------------------------------------------ Precision@1: 59.59% 

[59.56, 59.66, 59.55, 59.6, 59.9, 59.85, 59.7, 59.81, 59.65, 59.64, 59.59]

Epoch: 11
2024-03-09 20:55:13.550514 epoch: 11 step: 0 cls_loss= 0.91723 (151630 samples/sec)
2024-03-09 20:55:16.550422 epoch: 11 step: 100 cls_loss= 0.98696 (10000 samples/sec)
saving....
2024-03-09 20:55:20.299885------------------------------------------------------ Precision@1: 59.85% 

[59.56, 59.66, 59.55, 59.6, 59.9, 59.85, 59.7, 59.81, 59.65, 59.64, 59.59, 59.85]

Epoch: 12
2024-03-09 20:55:20.498567 epoch: 12 step: 0 cls_loss= 0.94449 (151823 samples/sec)
2024-03-09 20:55:23.479412 epoch: 12 step: 100 cls_loss= 0.86063 (10064 samples/sec)
saving....
2024-03-09 20:55:27.242752------------------------------------------------------ Precision@1: 59.79% 

[59.56, 59.66, 59.55, 59.6, 59.9, 59.85, 59.7, 59.81, 59.65, 59.64, 59.59, 59.85, 59.79]

Epoch: 13
2024-03-09 20:55:27.456509 epoch: 13 step: 0 cls_loss= 0.86071 (140941 samples/sec)
2024-03-09 20:55:30.433715 epoch: 13 step: 100 cls_loss= 0.94756 (10076 samples/sec)
saving....
2024-03-09 20:55:34.143457------------------------------------------------------ Precision@1: 59.96% 

[59.56, 59.66, 59.55, 59.6, 59.9, 59.85, 59.7, 59.81, 59.65, 59.64, 59.59, 59.85, 59.79, 59.96]
max acc : 59.96

Epoch: 14
2024-03-09 20:55:34.359240 epoch: 14 step: 0 cls_loss= 0.87031 (161004 samples/sec)
2024-03-09 20:55:37.335155 epoch: 14 step: 100 cls_loss= 1.04388 (10081 samples/sec)
saving....
2024-03-09 20:55:41.037156------------------------------------------------------ Precision@1: 59.50% 

[59.56, 59.66, 59.55, 59.6, 59.9, 59.85, 59.7, 59.81, 59.65, 59.64, 59.59, 59.85, 59.79, 59.96, 59.5]

Epoch: 15
2024-03-09 20:55:41.249871 epoch: 15 step: 0 cls_loss= 0.93214 (141775 samples/sec)
2024-03-09 20:55:44.194755 epoch: 15 step: 100 cls_loss= 0.90767 (10187 samples/sec)
saving....
2024-03-09 20:55:47.856794------------------------------------------------------ Precision@1: 59.61% 

[59.56, 59.66, 59.55, 59.6, 59.9, 59.85, 59.7, 59.81, 59.65, 59.64, 59.59, 59.85, 59.79, 59.96, 59.5, 59.61]

Epoch: 16
2024-03-09 20:55:48.044128 epoch: 16 step: 0 cls_loss= 0.88410 (161051 samples/sec)
2024-03-09 20:55:51.150168 epoch: 16 step: 100 cls_loss= 0.97735 (9658 samples/sec)
saving....
2024-03-09 20:55:54.912045------------------------------------------------------ Precision@1: 59.66% 

[59.56, 59.66, 59.55, 59.6, 59.9, 59.85, 59.7, 59.81, 59.65, 59.64, 59.59, 59.85, 59.79, 59.96, 59.5, 59.61, 59.66]

Epoch: 17
2024-03-09 20:55:55.105284 epoch: 17 step: 0 cls_loss= 0.96817 (155941 samples/sec)
2024-03-09 20:55:58.171658 epoch: 17 step: 100 cls_loss= 1.09362 (9783 samples/sec)
saving....
2024-03-09 20:56:01.935161------------------------------------------------------ Precision@1: 59.72% 

[59.56, 59.66, 59.55, 59.6, 59.9, 59.85, 59.7, 59.81, 59.65, 59.64, 59.59, 59.85, 59.79, 59.96, 59.5, 59.61, 59.66, 59.72]

Epoch: 18
2024-03-09 20:56:02.120087 epoch: 18 step: 0 cls_loss= 0.93011 (163188 samples/sec)
2024-03-09 20:56:05.232212 epoch: 18 step: 100 cls_loss= 0.91861 (9639 samples/sec)
saving....
2024-03-09 20:56:08.986974------------------------------------------------------ Precision@1: 59.70% 

[59.56, 59.66, 59.55, 59.6, 59.9, 59.85, 59.7, 59.81, 59.65, 59.64, 59.59, 59.85, 59.79, 59.96, 59.5, 59.61, 59.66, 59.72, 59.7]

Epoch: 19
2024-03-09 20:56:09.181359 epoch: 19 step: 0 cls_loss= 0.98187 (155149 samples/sec)
2024-03-09 20:56:12.296253 epoch: 19 step: 100 cls_loss= 0.95111 (9631 samples/sec)
saving....
2024-03-09 20:56:16.088119------------------------------------------------------ Precision@1: 60.05% 

[59.56, 59.66, 59.55, 59.6, 59.9, 59.85, 59.7, 59.81, 59.65, 59.64, 59.59, 59.85, 59.79, 59.96, 59.5, 59.61, 59.66, 59.72, 59.7, 60.05]
max acc : 60.05
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-09 20:56:18.806359 epoch: 0 step: 0 cls_loss= 0.96877 (50604 samples/sec)
2024-03-09 20:56:21.816384 epoch: 0 step: 100 cls_loss= 1.14386 (9966 samples/sec)
saving....
2024-03-09 20:56:25.750981------------------------------------------------------ Precision@1: 59.75% 

[59.75]
max acc : 59.75

Epoch: 1
2024-03-09 20:56:25.999249 epoch: 1 step: 0 cls_loss= 1.09413 (137381 samples/sec)
2024-03-09 20:56:29.145149 epoch: 1 step: 100 cls_loss= 1.03228 (9536 samples/sec)
saving....
2024-03-09 20:56:32.966586------------------------------------------------------ Precision@1: 59.74% 

[59.75, 59.74]

Epoch: 2
2024-03-09 20:56:33.180399 epoch: 2 step: 0 cls_loss= 0.96913 (141097 samples/sec)
2024-03-09 20:56:36.279700 epoch: 2 step: 100 cls_loss= 0.99929 (9679 samples/sec)
saving....
2024-03-09 20:56:40.056501------------------------------------------------------ Precision@1: 59.62% 

[59.75, 59.74, 59.62]

Epoch: 3
2024-03-09 20:56:40.256123 epoch: 3 step: 0 cls_loss= 0.91943 (151129 samples/sec)
2024-03-09 20:56:43.378746 epoch: 3 step: 100 cls_loss= 1.04852 (9607 samples/sec)
saving....
2024-03-09 20:56:47.144606------------------------------------------------------ Precision@1: 59.94% 

[59.75, 59.74, 59.62, 59.94]
max acc : 59.94

Epoch: 4
2024-03-09 20:56:47.375990 epoch: 4 step: 0 cls_loss= 1.05836 (148145 samples/sec)
2024-03-09 20:56:50.421367 epoch: 4 step: 100 cls_loss= 1.05859 (9851 samples/sec)
saving....
2024-03-09 20:56:54.206075------------------------------------------------------ Precision@1: 59.92% 

[59.75, 59.74, 59.62, 59.94, 59.92]

Epoch: 5
2024-03-09 20:56:54.401827 epoch: 5 step: 0 cls_loss= 1.18511 (154223 samples/sec)
2024-03-09 20:56:57.431715 epoch: 5 step: 100 cls_loss= 0.99720 (9901 samples/sec)
saving....
2024-03-09 20:57:01.272487------------------------------------------------------ Precision@1: 59.85% 

[59.75, 59.74, 59.62, 59.94, 59.92, 59.85]

Epoch: 6
2024-03-09 20:57:01.469965 epoch: 6 step: 0 cls_loss= 0.95582 (152701 samples/sec)
2024-03-09 20:57:04.465698 epoch: 6 step: 100 cls_loss= 1.01377 (10014 samples/sec)
saving....
2024-03-09 20:57:08.248380------------------------------------------------------ Precision@1: 60.02% 

[59.75, 59.74, 59.62, 59.94, 59.92, 59.85, 60.02]
max acc : 60.02

Epoch: 7
2024-03-09 20:57:08.465295 epoch: 7 step: 0 cls_loss= 1.00282 (160801 samples/sec)
2024-03-09 20:57:11.544934 epoch: 7 step: 100 cls_loss= 1.00746 (9741 samples/sec)
saving....
2024-03-09 20:57:15.315472------------------------------------------------------ Precision@1: 59.65% 

[59.75, 59.74, 59.62, 59.94, 59.92, 59.85, 60.02, 59.65]

Epoch: 8
2024-03-09 20:57:15.523935 epoch: 8 step: 0 cls_loss= 1.01030 (144613 samples/sec)
2024-03-09 20:57:18.679221 epoch: 8 step: 100 cls_loss= 0.90226 (9508 samples/sec)
saving....
2024-03-09 20:57:22.558586------------------------------------------------------ Precision@1: 59.74% 

[59.75, 59.74, 59.62, 59.94, 59.92, 59.85, 60.02, 59.65, 59.74]

Epoch: 9
2024-03-09 20:57:22.758500 epoch: 9 step: 0 cls_loss= 0.85037 (150835 samples/sec)
2024-03-09 20:57:25.772513 epoch: 9 step: 100 cls_loss= 1.03436 (9953 samples/sec)
saving....
2024-03-09 20:57:29.520430------------------------------------------------------ Precision@1: 59.94% 

[59.75, 59.74, 59.62, 59.94, 59.92, 59.85, 60.02, 59.65, 59.74, 59.94]

Epoch: 10
2024-03-09 20:57:29.719847 epoch: 10 step: 0 cls_loss= 0.79361 (151255 samples/sec)
2024-03-09 20:57:32.828016 epoch: 10 step: 100 cls_loss= 0.88185 (9652 samples/sec)
saving....
2024-03-09 20:57:36.560950------------------------------------------------------ Precision@1: 59.82% 

[59.75, 59.74, 59.62, 59.94, 59.92, 59.85, 60.02, 59.65, 59.74, 59.94, 59.82]

Epoch: 11
2024-03-09 20:57:36.757415 epoch: 11 step: 0 cls_loss= 0.90336 (153566 samples/sec)
2024-03-09 20:57:39.795362 epoch: 11 step: 100 cls_loss= 1.00054 (9875 samples/sec)
saving....
2024-03-09 20:57:43.583438------------------------------------------------------ Precision@1: 59.88% 

[59.75, 59.74, 59.62, 59.94, 59.92, 59.85, 60.02, 59.65, 59.74, 59.94, 59.82, 59.88]

Epoch: 12
2024-03-09 20:57:43.778866 epoch: 12 step: 0 cls_loss= 0.90987 (154483 samples/sec)
2024-03-09 20:57:46.921856 epoch: 12 step: 100 cls_loss= 0.95632 (9545 samples/sec)
saving....
2024-03-09 20:57:50.737685------------------------------------------------------ Precision@1: 60.02% 

[59.75, 59.74, 59.62, 59.94, 59.92, 59.85, 60.02, 59.65, 59.74, 59.94, 59.82, 59.88, 60.02]

Epoch: 13
2024-03-09 20:57:50.937630 epoch: 13 step: 0 cls_loss= 0.87588 (150987 samples/sec)
2024-03-09 20:57:53.952792 epoch: 13 step: 100 cls_loss= 1.07188 (9949 samples/sec)
saving....
2024-03-09 20:57:57.655710------------------------------------------------------ Precision@1: 59.92% 

[59.75, 59.74, 59.62, 59.94, 59.92, 59.85, 60.02, 59.65, 59.74, 59.94, 59.82, 59.88, 60.02, 59.92]

Epoch: 14
2024-03-09 20:57:57.855879 epoch: 14 step: 0 cls_loss= 0.83481 (150721 samples/sec)
2024-03-09 20:58:00.830933 epoch: 14 step: 100 cls_loss= 0.83853 (10084 samples/sec)
saving....
2024-03-09 20:58:04.512021------------------------------------------------------ Precision@1: 59.79% 

[59.75, 59.74, 59.62, 59.94, 59.92, 59.85, 60.02, 59.65, 59.74, 59.94, 59.82, 59.88, 60.02, 59.92, 59.79]

Epoch: 15
2024-03-09 20:58:04.715014 epoch: 15 step: 0 cls_loss= 0.95599 (148667 samples/sec)
2024-03-09 20:58:07.796605 epoch: 15 step: 100 cls_loss= 0.98221 (9735 samples/sec)
saving....
2024-03-09 20:58:11.618490------------------------------------------------------ Precision@1: 59.74% 

[59.75, 59.74, 59.62, 59.94, 59.92, 59.85, 60.02, 59.65, 59.74, 59.94, 59.82, 59.88, 60.02, 59.92, 59.79, 59.74]

Epoch: 16
2024-03-09 20:58:11.819014 epoch: 16 step: 0 cls_loss= 0.94215 (150466 samples/sec)
2024-03-09 20:58:14.968569 epoch: 16 step: 100 cls_loss= 0.82493 (9525 samples/sec)
saving....
2024-03-09 20:58:18.814604------------------------------------------------------ Precision@1: 59.80% 

[59.75, 59.74, 59.62, 59.94, 59.92, 59.85, 60.02, 59.65, 59.74, 59.94, 59.82, 59.88, 60.02, 59.92, 59.79, 59.74, 59.8]

Epoch: 17
2024-03-09 20:58:19.006630 epoch: 17 step: 0 cls_loss= 0.92855 (157200 samples/sec)
2024-03-09 20:58:21.935630 epoch: 17 step: 100 cls_loss= 0.94634 (10242 samples/sec)
saving....
2024-03-09 20:58:25.755331------------------------------------------------------ Precision@1: 59.83% 

[59.75, 59.74, 59.62, 59.94, 59.92, 59.85, 60.02, 59.65, 59.74, 59.94, 59.82, 59.88, 60.02, 59.92, 59.79, 59.74, 59.8, 59.83]

Epoch: 18
2024-03-09 20:58:25.961343 epoch: 18 step: 0 cls_loss= 0.84180 (146337 samples/sec)
2024-03-09 20:58:29.040719 epoch: 18 step: 100 cls_loss= 0.94893 (9742 samples/sec)
saving....
2024-03-09 20:58:32.830786------------------------------------------------------ Precision@1: 59.78% 

[59.75, 59.74, 59.62, 59.94, 59.92, 59.85, 60.02, 59.65, 59.74, 59.94, 59.82, 59.88, 60.02, 59.92, 59.79, 59.74, 59.8, 59.83, 59.78]

Epoch: 19
2024-03-09 20:58:33.024952 epoch: 19 step: 0 cls_loss= 0.85107 (155252 samples/sec)
2024-03-09 20:58:36.124355 epoch: 19 step: 100 cls_loss= 0.82116 (9679 samples/sec)
saving....
2024-03-09 20:58:39.946226------------------------------------------------------ Precision@1: 59.76% 

[59.75, 59.74, 59.62, 59.94, 59.92, 59.85, 60.02, 59.65, 59.74, 59.94, 59.82, 59.88, 60.02, 59.92, 59.79, 59.74, 59.8, 59.83, 59.78, 59.76]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-09 20:58:42.652296 epoch: 0 step: 0 cls_loss= 1.05752 (49606 samples/sec)
2024-03-09 20:58:45.694427 epoch: 0 step: 100 cls_loss= 1.18824 (9861 samples/sec)
saving....
2024-03-09 20:58:49.623511------------------------------------------------------ Precision@1: 59.51% 

[59.51]
max acc : 59.51

Epoch: 1
2024-03-09 20:58:49.865059 epoch: 1 step: 0 cls_loss= 1.02401 (141480 samples/sec)
2024-03-09 20:58:52.914756 epoch: 1 step: 100 cls_loss= 0.92098 (9837 samples/sec)
saving....
2024-03-09 20:58:56.671968------------------------------------------------------ Precision@1: 59.63% 

[59.51, 59.63]
max acc : 59.63

Epoch: 2
2024-03-09 20:58:56.919117 epoch: 2 step: 0 cls_loss= 0.94976 (137456 samples/sec)
2024-03-09 20:58:59.983948 epoch: 2 step: 100 cls_loss= 1.05804 (9788 samples/sec)
saving....
2024-03-09 20:59:03.722756------------------------------------------------------ Precision@1: 59.69% 

[59.51, 59.63, 59.69]
max acc : 59.69

Epoch: 3
2024-03-09 20:59:03.942463 epoch: 3 step: 0 cls_loss= 1.04470 (157668 samples/sec)
2024-03-09 20:59:07.068839 epoch: 3 step: 100 cls_loss= 0.98605 (9595 samples/sec)
saving....
2024-03-09 20:59:10.867605------------------------------------------------------ Precision@1: 59.73% 

[59.51, 59.63, 59.69, 59.73]
max acc : 59.73

Epoch: 4
2024-03-09 20:59:11.110816 epoch: 4 step: 0 cls_loss= 0.98398 (140146 samples/sec)
2024-03-09 20:59:14.232594 epoch: 4 step: 100 cls_loss= 1.08139 (9610 samples/sec)
saving....
2024-03-09 20:59:18.122993------------------------------------------------------ Precision@1: 59.88% 

[59.51, 59.63, 59.69, 59.73, 59.88]
max acc : 59.88

Epoch: 5
2024-03-09 20:59:18.355364 epoch: 5 step: 0 cls_loss= 1.00610 (148239 samples/sec)
2024-03-09 20:59:21.395839 epoch: 5 step: 100 cls_loss= 0.98954 (9867 samples/sec)
saving....
2024-03-09 20:59:25.195938------------------------------------------------------ Precision@1: 59.86% 

[59.51, 59.63, 59.69, 59.73, 59.88, 59.86]

Epoch: 6
2024-03-09 20:59:25.381647 epoch: 6 step: 0 cls_loss= 0.95975 (162525 samples/sec)
2024-03-09 20:59:28.495246 epoch: 6 step: 100 cls_loss= 0.89367 (9635 samples/sec)
saving....
2024-03-09 20:59:32.322230------------------------------------------------------ Precision@1: 59.86% 

[59.51, 59.63, 59.69, 59.73, 59.88, 59.86, 59.86]

Epoch: 7
2024-03-09 20:59:32.542158 epoch: 7 step: 0 cls_loss= 0.99098 (137041 samples/sec)
2024-03-09 20:59:35.555885 epoch: 7 step: 100 cls_loss= 0.88715 (9954 samples/sec)
saving....
2024-03-09 20:59:39.289026------------------------------------------------------ Precision@1: 59.88% 

[59.51, 59.63, 59.69, 59.73, 59.88, 59.86, 59.86, 59.88]

Epoch: 8
2024-03-09 20:59:39.490200 epoch: 8 step: 0 cls_loss= 0.93719 (149908 samples/sec)
2024-03-09 20:59:42.562301 epoch: 8 step: 100 cls_loss= 0.80556 (9765 samples/sec)
saving....
2024-03-09 20:59:46.445857------------------------------------------------------ Precision@1: 59.65% 

[59.51, 59.63, 59.69, 59.73, 59.88, 59.86, 59.86, 59.88, 59.65]

Epoch: 9
2024-03-09 20:59:46.655751 epoch: 9 step: 0 cls_loss= 1.02497 (143722 samples/sec)
2024-03-09 20:59:49.700110 epoch: 9 step: 100 cls_loss= 0.98603 (9854 samples/sec)
saving....
2024-03-09 20:59:53.503352------------------------------------------------------ Precision@1: 59.90% 

[59.51, 59.63, 59.69, 59.73, 59.88, 59.86, 59.86, 59.88, 59.65, 59.9]
max acc : 59.9

Epoch: 10
2024-03-09 20:59:53.729136 epoch: 10 step: 0 cls_loss= 0.93199 (150207 samples/sec)
2024-03-09 20:59:56.812409 epoch: 10 step: 100 cls_loss= 1.04798 (9730 samples/sec)
saving....
2024-03-09 21:00:00.614058------------------------------------------------------ Precision@1: 59.59% 

[59.51, 59.63, 59.69, 59.73, 59.88, 59.86, 59.86, 59.88, 59.65, 59.9, 59.59]

Epoch: 11
2024-03-09 21:00:00.808843 epoch: 11 step: 0 cls_loss= 0.83903 (154920 samples/sec)
2024-03-09 21:00:04.098853 epoch: 11 step: 100 cls_loss= 0.87179 (9118 samples/sec)
saving....
2024-03-09 21:00:08.000815------------------------------------------------------ Precision@1: 60.18% 

[59.51, 59.63, 59.69, 59.73, 59.88, 59.86, 59.86, 59.88, 59.65, 59.9, 59.59, 60.18]
max acc : 60.18

Epoch: 12
2024-03-09 21:00:08.226509 epoch: 12 step: 0 cls_loss= 0.95951 (150508 samples/sec)
2024-03-09 21:00:11.253332 epoch: 12 step: 100 cls_loss= 0.93397 (9911 samples/sec)
saving....
2024-03-09 21:00:15.013339------------------------------------------------------ Precision@1: 59.80% 

[59.51, 59.63, 59.69, 59.73, 59.88, 59.86, 59.86, 59.88, 59.65, 59.9, 59.59, 60.18, 59.8]

Epoch: 13
2024-03-09 21:00:15.221955 epoch: 13 step: 0 cls_loss= 0.94987 (144584 samples/sec)
2024-03-09 21:00:18.297284 epoch: 13 step: 100 cls_loss= 0.90423 (9755 samples/sec)
saving....
2024-03-09 21:00:22.068829------------------------------------------------------ Precision@1: 59.94% 

[59.51, 59.63, 59.69, 59.73, 59.88, 59.86, 59.86, 59.88, 59.65, 59.9, 59.59, 60.18, 59.8, 59.94]

Epoch: 14
2024-03-09 21:00:22.274961 epoch: 14 step: 0 cls_loss= 0.88766 (146217 samples/sec)
2024-03-09 21:00:25.347343 epoch: 14 step: 100 cls_loss= 0.84629 (9765 samples/sec)
saving....
2024-03-09 21:00:29.127493------------------------------------------------------ Precision@1: 59.84% 

[59.51, 59.63, 59.69, 59.73, 59.88, 59.86, 59.86, 59.88, 59.65, 59.9, 59.59, 60.18, 59.8, 59.94, 59.84]

Epoch: 15
2024-03-09 21:00:29.326811 epoch: 15 step: 0 cls_loss= 0.98789 (151341 samples/sec)
2024-03-09 21:00:32.324322 epoch: 15 step: 100 cls_loss= 0.89250 (10008 samples/sec)
saving....
2024-03-09 21:00:36.038771------------------------------------------------------ Precision@1: 59.81% 

[59.51, 59.63, 59.69, 59.73, 59.88, 59.86, 59.86, 59.88, 59.65, 59.9, 59.59, 60.18, 59.8, 59.94, 59.84, 59.81]

Epoch: 16
2024-03-09 21:00:36.233437 epoch: 16 step: 0 cls_loss= 0.82442 (155062 samples/sec)
2024-03-09 21:00:39.338418 epoch: 16 step: 100 cls_loss= 0.81309 (9662 samples/sec)
saving....
2024-03-09 21:00:43.058830------------------------------------------------------ Precision@1: 59.65% 

[59.51, 59.63, 59.69, 59.73, 59.88, 59.86, 59.86, 59.88, 59.65, 59.9, 59.59, 60.18, 59.8, 59.94, 59.84, 59.81, 59.65]

Epoch: 17
2024-03-09 21:00:43.265578 epoch: 17 step: 0 cls_loss= 0.87578 (145729 samples/sec)
2024-03-09 21:00:46.376318 epoch: 17 step: 100 cls_loss= 0.97669 (9644 samples/sec)
saving....
2024-03-09 21:00:50.164194------------------------------------------------------ Precision@1: 60.00% 

[59.51, 59.63, 59.69, 59.73, 59.88, 59.86, 59.86, 59.88, 59.65, 59.9, 59.59, 60.18, 59.8, 59.94, 59.84, 59.81, 59.65, 60.0]

Epoch: 18
2024-03-09 21:00:50.368653 epoch: 18 step: 0 cls_loss= 0.98299 (147575 samples/sec)
2024-03-09 21:00:53.370826 epoch: 18 step: 100 cls_loss= 0.75264 (9992 samples/sec)
saving....
2024-03-09 21:00:57.123133------------------------------------------------------ Precision@1: 59.93% 

[59.51, 59.63, 59.69, 59.73, 59.88, 59.86, 59.86, 59.88, 59.65, 59.9, 59.59, 60.18, 59.8, 59.94, 59.84, 59.81, 59.65, 60.0, 59.93]

Epoch: 19
2024-03-09 21:00:57.313999 epoch: 19 step: 0 cls_loss= 0.72588 (158154 samples/sec)
2024-03-09 21:01:00.287312 epoch: 19 step: 100 cls_loss= 0.98675 (10089 samples/sec)
saving....
2024-03-09 21:01:03.996425------------------------------------------------------ Precision@1: 59.80% 

[59.51, 59.63, 59.69, 59.73, 59.88, 59.86, 59.86, 59.88, 59.65, 59.9, 59.59, 60.18, 59.8, 59.94, 59.84, 59.81, 59.65, 60.0, 59.93, 59.8]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-09 21:01:06.713769 epoch: 0 step: 0 cls_loss= 1.04147 (49031 samples/sec)
2024-03-09 21:01:09.695681 epoch: 0 step: 100 cls_loss= 1.12500 (10060 samples/sec)
saving....
2024-03-09 21:01:13.608962------------------------------------------------------ Precision@1: 59.64% 

[59.64]
max acc : 59.64

Epoch: 1
2024-03-09 21:01:13.827992 epoch: 1 step: 0 cls_loss= 1.04061 (159212 samples/sec)
2024-03-09 21:01:16.925665 epoch: 1 step: 100 cls_loss= 0.94775 (9685 samples/sec)
saving....
2024-03-09 21:01:20.721633------------------------------------------------------ Precision@1: 59.55% 

[59.64, 59.55]

Epoch: 2
2024-03-09 21:01:20.911686 epoch: 2 step: 0 cls_loss= 1.10150 (158849 samples/sec)
2024-03-09 21:01:24.029522 epoch: 2 step: 100 cls_loss= 1.12101 (9622 samples/sec)
saving....
2024-03-09 21:01:27.830522------------------------------------------------------ Precision@1: 59.57% 

[59.64, 59.55, 59.57]

Epoch: 3
2024-03-09 21:01:28.052434 epoch: 3 step: 0 cls_loss= 1.07313 (135886 samples/sec)
2024-03-09 21:01:31.167295 epoch: 3 step: 100 cls_loss= 0.96926 (9631 samples/sec)
saving....
2024-03-09 21:01:34.952681------------------------------------------------------ Precision@1: 59.78% 

[59.64, 59.55, 59.57, 59.78]
max acc : 59.78

Epoch: 4
2024-03-09 21:01:35.180732 epoch: 4 step: 0 cls_loss= 0.84499 (150410 samples/sec)
2024-03-09 21:01:38.161015 epoch: 4 step: 100 cls_loss= 0.83778 (10066 samples/sec)
saving....
2024-03-09 21:01:42.018634------------------------------------------------------ Precision@1: 59.48% 

[59.64, 59.55, 59.57, 59.78, 59.48]

Epoch: 5
2024-03-09 21:01:42.229975 epoch: 5 step: 0 cls_loss= 0.98715 (142715 samples/sec)
2024-03-09 21:01:45.463413 epoch: 5 step: 100 cls_loss= 1.01975 (9278 samples/sec)
saving....
2024-03-09 21:01:49.484372------------------------------------------------------ Precision@1: 59.39% 

[59.64, 59.55, 59.57, 59.78, 59.48, 59.39]

Epoch: 6
2024-03-09 21:01:49.675994 epoch: 6 step: 0 cls_loss= 0.97557 (157345 samples/sec)
2024-03-09 21:01:52.660468 epoch: 6 step: 100 cls_loss= 0.83887 (10052 samples/sec)
saving....
2024-03-09 21:01:56.402444------------------------------------------------------ Precision@1: 60.02% 

[59.64, 59.55, 59.57, 59.78, 59.48, 59.39, 60.02]
max acc : 60.02

Epoch: 7
2024-03-09 21:01:56.630592 epoch: 7 step: 0 cls_loss= 0.92846 (151748 samples/sec)
2024-03-09 21:01:59.833387 epoch: 7 step: 100 cls_loss= 1.01422 (9367 samples/sec)
saving....
2024-03-09 21:02:03.654567------------------------------------------------------ Precision@1: 59.61% 

[59.64, 59.55, 59.57, 59.78, 59.48, 59.39, 60.02, 59.61]

Epoch: 8
2024-03-09 21:02:03.857909 epoch: 8 step: 0 cls_loss= 1.05020 (148327 samples/sec)
2024-03-09 21:02:06.900991 epoch: 8 step: 100 cls_loss= 1.10820 (9858 samples/sec)
saving....
2024-03-09 21:02:10.634839------------------------------------------------------ Precision@1: 59.89% 

[59.64, 59.55, 59.57, 59.78, 59.48, 59.39, 60.02, 59.61, 59.89]

Epoch: 9
2024-03-09 21:02:10.821826 epoch: 9 step: 0 cls_loss= 1.01635 (161453 samples/sec)
2024-03-09 21:02:13.948074 epoch: 9 step: 100 cls_loss= 0.92535 (9596 samples/sec)
saving....
2024-03-09 21:02:17.754787------------------------------------------------------ Precision@1: 59.66% 

[59.64, 59.55, 59.57, 59.78, 59.48, 59.39, 60.02, 59.61, 59.89, 59.66]

Epoch: 10
2024-03-09 21:02:17.971964 epoch: 10 step: 0 cls_loss= 0.89051 (138875 samples/sec)
2024-03-09 21:02:21.093908 epoch: 10 step: 100 cls_loss= 0.85093 (9609 samples/sec)
saving....
2024-03-09 21:02:24.891403------------------------------------------------------ Precision@1: 59.61% 

[59.64, 59.55, 59.57, 59.78, 59.48, 59.39, 60.02, 59.61, 59.89, 59.66, 59.61]

Epoch: 11
2024-03-09 21:02:25.101728 epoch: 11 step: 0 cls_loss= 0.84311 (143366 samples/sec)
2024-03-09 21:02:28.089598 epoch: 11 step: 100 cls_loss= 0.87900 (10040 samples/sec)
saving....
2024-03-09 21:02:31.807848------------------------------------------------------ Precision@1: 59.61% 

[59.64, 59.55, 59.57, 59.78, 59.48, 59.39, 60.02, 59.61, 59.89, 59.66, 59.61, 59.61]

Epoch: 12
2024-03-09 21:02:32.026189 epoch: 12 step: 0 cls_loss= 0.94263 (138048 samples/sec)
2024-03-09 21:02:35.070402 epoch: 12 step: 100 cls_loss= 1.03504 (9855 samples/sec)
saving....
2024-03-09 21:02:38.922807------------------------------------------------------ Precision@1: 59.68% 

[59.64, 59.55, 59.57, 59.78, 59.48, 59.39, 60.02, 59.61, 59.89, 59.66, 59.61, 59.61, 59.68]

Epoch: 13
2024-03-09 21:02:39.120794 epoch: 13 step: 0 cls_loss= 0.79905 (152382 samples/sec)
2024-03-09 21:02:42.235505 epoch: 13 step: 100 cls_loss= 0.82441 (9631 samples/sec)
saving....
2024-03-09 21:02:45.988815------------------------------------------------------ Precision@1: 59.80% 

[59.64, 59.55, 59.57, 59.78, 59.48, 59.39, 60.02, 59.61, 59.89, 59.66, 59.61, 59.61, 59.68, 59.8]

Epoch: 14
2024-03-09 21:02:46.196840 epoch: 14 step: 0 cls_loss= 0.89553 (145044 samples/sec)
2024-03-09 21:02:49.210326 epoch: 14 step: 100 cls_loss= 0.84104 (9955 samples/sec)
saving....
2024-03-09 21:02:52.926079------------------------------------------------------ Precision@1: 59.67% 

[59.64, 59.55, 59.57, 59.78, 59.48, 59.39, 60.02, 59.61, 59.89, 59.66, 59.61, 59.61, 59.68, 59.8, 59.67]

Epoch: 15
2024-03-09 21:02:53.118344 epoch: 15 step: 0 cls_loss= 0.85683 (156808 samples/sec)
2024-03-09 21:02:56.231252 epoch: 15 step: 100 cls_loss= 0.86997 (9637 samples/sec)
saving....
2024-03-09 21:03:00.009389------------------------------------------------------ Precision@1: 59.90% 

[59.64, 59.55, 59.57, 59.78, 59.48, 59.39, 60.02, 59.61, 59.89, 59.66, 59.61, 59.61, 59.68, 59.8, 59.67, 59.9]

Epoch: 16
2024-03-09 21:03:00.218151 epoch: 16 step: 0 cls_loss= 0.87627 (144387 samples/sec)
2024-03-09 21:03:03.201227 epoch: 16 step: 100 cls_loss= 0.73670 (10056 samples/sec)
saving....
2024-03-09 21:03:06.953441------------------------------------------------------ Precision@1: 59.97% 

[59.64, 59.55, 59.57, 59.78, 59.48, 59.39, 60.02, 59.61, 59.89, 59.66, 59.61, 59.61, 59.68, 59.8, 59.67, 59.9, 59.97]

Epoch: 17
2024-03-09 21:03:07.157468 epoch: 17 step: 0 cls_loss= 0.89443 (147737 samples/sec)
2024-03-09 21:03:10.238268 epoch: 17 step: 100 cls_loss= 0.92190 (9737 samples/sec)
saving....
2024-03-09 21:03:14.096822------------------------------------------------------ Precision@1: 59.67% 

[59.64, 59.55, 59.57, 59.78, 59.48, 59.39, 60.02, 59.61, 59.89, 59.66, 59.61, 59.61, 59.68, 59.8, 59.67, 59.9, 59.97, 59.67]

Epoch: 18
2024-03-09 21:03:14.307291 epoch: 18 step: 0 cls_loss= 0.70011 (143211 samples/sec)
2024-03-09 21:03:17.303717 epoch: 18 step: 100 cls_loss= 0.92672 (10012 samples/sec)
saving....
2024-03-09 21:03:21.106422------------------------------------------------------ Precision@1: 59.51% 

[59.64, 59.55, 59.57, 59.78, 59.48, 59.39, 60.02, 59.61, 59.89, 59.66, 59.61, 59.61, 59.68, 59.8, 59.67, 59.9, 59.97, 59.67, 59.51]

Epoch: 19
2024-03-09 21:03:21.307051 epoch: 19 step: 0 cls_loss= 0.76575 (150083 samples/sec)
2024-03-09 21:03:24.316652 epoch: 19 step: 100 cls_loss= 0.75987 (9968 samples/sec)
saving....
2024-03-09 21:03:27.962205------------------------------------------------------ Precision@1: 59.85% 

[59.64, 59.55, 59.57, 59.78, 59.48, 59.39, 60.02, 59.61, 59.89, 59.66, 59.61, 59.61, 59.68, 59.8, 59.67, 59.9, 59.97, 59.67, 59.51, 59.85]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-09 21:03:30.642874 epoch: 0 step: 0 cls_loss= 0.94270 (50960 samples/sec)
2024-03-09 21:03:33.633143 epoch: 0 step: 100 cls_loss= 1.01542 (10032 samples/sec)
saving....
2024-03-09 21:03:37.730804------------------------------------------------------ Precision@1: 59.76% 

[59.76]
max acc : 59.76

Epoch: 1
2024-03-09 21:03:37.951353 epoch: 1 step: 0 cls_loss= 0.83065 (159208 samples/sec)
2024-03-09 21:03:41.065203 epoch: 1 step: 100 cls_loss= 0.90358 (9634 samples/sec)
saving....
2024-03-09 21:03:44.912244------------------------------------------------------ Precision@1: 59.66% 

[59.76, 59.66]

Epoch: 2
2024-03-09 21:03:45.110067 epoch: 2 step: 0 cls_loss= 0.82397 (152365 samples/sec)
2024-03-09 21:03:48.123763 epoch: 2 step: 100 cls_loss= 0.88925 (9954 samples/sec)
saving....
2024-03-09 21:03:51.957047------------------------------------------------------ Precision@1: 59.63% 

[59.76, 59.66, 59.63]

Epoch: 3
2024-03-09 21:03:52.191934 epoch: 3 step: 0 cls_loss= 0.98866 (128366 samples/sec)
2024-03-09 21:03:55.434801 epoch: 3 step: 100 cls_loss= 0.95497 (9251 samples/sec)
saving....
2024-03-09 21:03:59.313622------------------------------------------------------ Precision@1: 59.73% 

[59.76, 59.66, 59.63, 59.73]

Epoch: 4
2024-03-09 21:03:59.511202 epoch: 4 step: 0 cls_loss= 0.91649 (152461 samples/sec)
2024-03-09 21:04:02.563011 epoch: 4 step: 100 cls_loss= 0.92922 (9830 samples/sec)
saving....
2024-03-09 21:04:06.339502------------------------------------------------------ Precision@1: 59.65% 

[59.76, 59.66, 59.63, 59.73, 59.65]

Epoch: 5
2024-03-09 21:04:06.555800 epoch: 5 step: 0 cls_loss= 1.04021 (139474 samples/sec)
2024-03-09 21:04:09.732014 epoch: 5 step: 100 cls_loss= 0.98727 (9445 samples/sec)
saving....
2024-03-09 21:04:13.533960------------------------------------------------------ Precision@1: 59.95% 

[59.76, 59.66, 59.63, 59.73, 59.65, 59.95]
max acc : 59.95

Epoch: 6
2024-03-09 21:04:13.754630 epoch: 6 step: 0 cls_loss= 1.01515 (154146 samples/sec)
2024-03-09 21:04:16.776593 epoch: 6 step: 100 cls_loss= 0.84799 (9927 samples/sec)
saving....
2024-03-09 21:04:20.623534------------------------------------------------------ Precision@1: 60.02% 

[59.76, 59.66, 59.63, 59.73, 59.65, 59.95, 60.02]
max acc : 60.02

Epoch: 7
2024-03-09 21:04:20.837439 epoch: 7 step: 0 cls_loss= 0.85454 (162407 samples/sec)
2024-03-09 21:04:23.814180 epoch: 7 step: 100 cls_loss= 0.88118 (10078 samples/sec)
saving....
2024-03-09 21:04:27.594540------------------------------------------------------ Precision@1: 59.73% 

[59.76, 59.66, 59.63, 59.73, 59.65, 59.95, 60.02, 59.73]

Epoch: 8
2024-03-09 21:04:27.790037 epoch: 8 step: 0 cls_loss= 0.90857 (154220 samples/sec)
2024-03-09 21:04:30.833636 epoch: 8 step: 100 cls_loss= 0.76554 (9857 samples/sec)
saving....
2024-03-09 21:04:34.672293------------------------------------------------------ Precision@1: 59.63% 

[59.76, 59.66, 59.63, 59.73, 59.65, 59.95, 60.02, 59.73, 59.63]

Epoch: 9
2024-03-09 21:04:34.874596 epoch: 9 step: 0 cls_loss= 0.92166 (149033 samples/sec)
2024-03-09 21:04:37.981154 epoch: 9 step: 100 cls_loss= 0.87987 (9657 samples/sec)
saving....
2024-03-09 21:04:41.874957------------------------------------------------------ Precision@1: 59.56% 

[59.76, 59.66, 59.63, 59.73, 59.65, 59.95, 60.02, 59.73, 59.63, 59.56]

Epoch: 10
2024-03-09 21:04:42.076868 epoch: 10 step: 0 cls_loss= 1.02132 (149426 samples/sec)
2024-03-09 21:04:45.168095 epoch: 10 step: 100 cls_loss= 0.92927 (9705 samples/sec)
saving....
2024-03-09 21:04:48.982850------------------------------------------------------ Precision@1: 59.51% 

[59.76, 59.66, 59.63, 59.73, 59.65, 59.95, 60.02, 59.73, 59.63, 59.56, 59.51]

Epoch: 11
2024-03-09 21:04:49.185750 epoch: 11 step: 0 cls_loss= 0.98375 (148783 samples/sec)
2024-03-09 21:04:52.312380 epoch: 11 step: 100 cls_loss= 0.93018 (9595 samples/sec)
saving....
2024-03-09 21:04:56.180099------------------------------------------------------ Precision@1: 59.74% 

[59.76, 59.66, 59.63, 59.73, 59.65, 59.95, 60.02, 59.73, 59.63, 59.56, 59.51, 59.74]

Epoch: 12
2024-03-09 21:04:56.389600 epoch: 12 step: 0 cls_loss= 0.92416 (144060 samples/sec)
2024-03-09 21:04:59.479002 epoch: 12 step: 100 cls_loss= 0.81856 (9710 samples/sec)
saving....
2024-03-09 21:05:03.340099------------------------------------------------------ Precision@1: 59.77% 

[59.76, 59.66, 59.63, 59.73, 59.65, 59.95, 60.02, 59.73, 59.63, 59.56, 59.51, 59.74, 59.77]

Epoch: 13
2024-03-09 21:05:03.529582 epoch: 13 step: 0 cls_loss= 0.83069 (159246 samples/sec)
2024-03-09 21:05:06.625997 epoch: 13 step: 100 cls_loss= 0.89287 (9688 samples/sec)
saving....
2024-03-09 21:05:10.414166------------------------------------------------------ Precision@1: 59.97% 

[59.76, 59.66, 59.63, 59.73, 59.65, 59.95, 60.02, 59.73, 59.63, 59.56, 59.51, 59.74, 59.77, 59.97]

Epoch: 14
2024-03-09 21:05:10.608998 epoch: 14 step: 0 cls_loss= 0.83980 (154837 samples/sec)
2024-03-09 21:05:13.735976 epoch: 14 step: 100 cls_loss= 0.86088 (9594 samples/sec)
saving....
2024-03-09 21:05:17.540283------------------------------------------------------ Precision@1: 59.69% 

[59.76, 59.66, 59.63, 59.73, 59.65, 59.95, 60.02, 59.73, 59.63, 59.56, 59.51, 59.74, 59.77, 59.97, 59.69]

Epoch: 15
2024-03-09 21:05:17.747103 epoch: 15 step: 0 cls_loss= 0.87582 (145894 samples/sec)
2024-03-09 21:05:20.837752 epoch: 15 step: 100 cls_loss= 0.75512 (9706 samples/sec)
saving....
2024-03-09 21:05:24.619058------------------------------------------------------ Precision@1: 59.93% 

[59.76, 59.66, 59.63, 59.73, 59.65, 59.95, 60.02, 59.73, 59.63, 59.56, 59.51, 59.74, 59.77, 59.97, 59.69, 59.93]

Epoch: 16
2024-03-09 21:05:24.816848 epoch: 16 step: 0 cls_loss= 0.85362 (152582 samples/sec)
2024-03-09 21:05:27.835690 epoch: 16 step: 100 cls_loss= 0.97573 (9937 samples/sec)
saving....
2024-03-09 21:05:31.607252------------------------------------------------------ Precision@1: 59.52% 

[59.76, 59.66, 59.63, 59.73, 59.65, 59.95, 60.02, 59.73, 59.63, 59.56, 59.51, 59.74, 59.77, 59.97, 59.69, 59.93, 59.52]

Epoch: 17
2024-03-09 21:05:31.799431 epoch: 17 step: 0 cls_loss= 0.74088 (156927 samples/sec)
2024-03-09 21:05:34.798232 epoch: 17 step: 100 cls_loss= 0.84779 (10004 samples/sec)
saving....
2024-03-09 21:05:38.543728------------------------------------------------------ Precision@1: 59.76% 

[59.76, 59.66, 59.63, 59.73, 59.65, 59.95, 60.02, 59.73, 59.63, 59.56, 59.51, 59.74, 59.77, 59.97, 59.69, 59.93, 59.52, 59.76]

Epoch: 18
2024-03-09 21:05:38.764194 epoch: 18 step: 0 cls_loss= 0.73842 (136709 samples/sec)
2024-03-09 21:05:41.780476 epoch: 18 step: 100 cls_loss= 1.05399 (9946 samples/sec)
saving....
2024-03-09 21:05:45.628891------------------------------------------------------ Precision@1: 59.80% 

[59.76, 59.66, 59.63, 59.73, 59.65, 59.95, 60.02, 59.73, 59.63, 59.56, 59.51, 59.74, 59.77, 59.97, 59.69, 59.93, 59.52, 59.76, 59.8]

Epoch: 19
2024-03-09 21:05:45.837857 epoch: 19 step: 0 cls_loss= 0.87901 (144323 samples/sec)
2024-03-09 21:05:48.899160 epoch: 19 step: 100 cls_loss= 0.72406 (9799 samples/sec)
saving....
2024-03-09 21:05:52.756992------------------------------------------------------ Precision@1: 59.72% 

[59.76, 59.66, 59.63, 59.73, 59.65, 59.95, 60.02, 59.73, 59.63, 59.56, 59.51, 59.74, 59.77, 59.97, 59.69, 59.93, 59.52, 59.76, 59.8, 59.72]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-09 21:05:55.497406 epoch: 0 step: 0 cls_loss= 1.03647 (47715 samples/sec)
2024-03-09 21:05:58.637639 epoch: 0 step: 100 cls_loss= 0.96483 (9553 samples/sec)
saving....
2024-03-09 21:06:02.682369------------------------------------------------------ Precision@1: 59.76% 

[59.76]
max acc : 59.76

Epoch: 1
2024-03-09 21:06:02.897522 epoch: 1 step: 0 cls_loss= 1.03537 (162066 samples/sec)
2024-03-09 21:06:06.005514 epoch: 1 step: 100 cls_loss= 1.04945 (9652 samples/sec)
saving....
2024-03-09 21:06:09.822012------------------------------------------------------ Precision@1: 59.86% 

[59.76, 59.86]
max acc : 59.86

Epoch: 2
2024-03-09 21:06:10.054697 epoch: 2 step: 0 cls_loss= 1.05456 (147742 samples/sec)
2024-03-09 21:06:13.196678 epoch: 2 step: 100 cls_loss= 0.89519 (9548 samples/sec)
saving....
2024-03-09 21:06:17.061427------------------------------------------------------ Precision@1: 59.99% 

[59.76, 59.86, 59.99]
max acc : 59.99

Epoch: 3
2024-03-09 21:06:17.279651 epoch: 3 step: 0 cls_loss= 0.88160 (159001 samples/sec)
2024-03-09 21:06:20.382793 epoch: 3 step: 100 cls_loss= 1.03614 (9667 samples/sec)
saving....
2024-03-09 21:06:24.194731------------------------------------------------------ Precision@1: 59.64% 

[59.76, 59.86, 59.99, 59.64]

Epoch: 4
2024-03-09 21:06:24.399740 epoch: 4 step: 0 cls_loss= 0.91512 (147148 samples/sec)
2024-03-09 21:06:27.385211 epoch: 4 step: 100 cls_loss= 0.91328 (10049 samples/sec)
saving....
2024-03-09 21:06:31.135215------------------------------------------------------ Precision@1: 59.89% 

[59.76, 59.86, 59.99, 59.64, 59.89]

Epoch: 5
2024-03-09 21:06:31.339458 epoch: 5 step: 0 cls_loss= 0.98055 (147691 samples/sec)
2024-03-09 21:06:34.374024 epoch: 5 step: 100 cls_loss= 0.92733 (9886 samples/sec)
saving....
2024-03-09 21:06:38.199135------------------------------------------------------ Precision@1: 59.89% 

[59.76, 59.86, 59.99, 59.64, 59.89, 59.89]

Epoch: 6
2024-03-09 21:06:38.397455 epoch: 6 step: 0 cls_loss= 0.90472 (152006 samples/sec)
2024-03-09 21:06:41.460634 epoch: 6 step: 100 cls_loss= 0.87734 (9794 samples/sec)
saving....
2024-03-09 21:06:45.265681------------------------------------------------------ Precision@1: 59.78% 

[59.76, 59.86, 59.99, 59.64, 59.89, 59.89, 59.78]

Epoch: 7
2024-03-09 21:06:45.468535 epoch: 7 step: 0 cls_loss= 0.87492 (148613 samples/sec)
2024-03-09 21:06:48.508909 epoch: 7 step: 100 cls_loss= 0.88915 (9867 samples/sec)
saving....
2024-03-09 21:06:52.239955------------------------------------------------------ Precision@1: 60.07% 

[59.76, 59.86, 59.99, 59.64, 59.89, 59.89, 59.78, 60.07]
max acc : 60.07

Epoch: 8
2024-03-09 21:06:52.462965 epoch: 8 step: 0 cls_loss= 0.78247 (155538 samples/sec)
2024-03-09 21:06:55.476922 epoch: 8 step: 100 cls_loss= 0.97196 (9953 samples/sec)
saving....
2024-03-09 21:06:59.250787------------------------------------------------------ Precision@1: 59.81% 

[59.76, 59.86, 59.99, 59.64, 59.89, 59.89, 59.78, 60.07, 59.81]

Epoch: 9
2024-03-09 21:06:59.445452 epoch: 9 step: 0 cls_loss= 0.89388 (155000 samples/sec)
2024-03-09 21:07:02.473744 epoch: 9 step: 100 cls_loss= 0.83459 (9906 samples/sec)
saving....
2024-03-09 21:07:06.237770------------------------------------------------------ Precision@1: 59.65% 

[59.76, 59.86, 59.99, 59.64, 59.89, 59.89, 59.78, 60.07, 59.81, 59.65]

Epoch: 10
2024-03-09 21:07:06.440052 epoch: 10 step: 0 cls_loss= 0.80413 (149140 samples/sec)
2024-03-09 21:07:09.489418 epoch: 10 step: 100 cls_loss= 0.91650 (9838 samples/sec)
saving....
2024-03-09 21:07:13.229999------------------------------------------------------ Precision@1: 59.94% 

[59.76, 59.86, 59.99, 59.64, 59.89, 59.89, 59.78, 60.07, 59.81, 59.65, 59.94]

Epoch: 11
2024-03-09 21:07:13.416253 epoch: 11 step: 0 cls_loss= 0.77507 (162010 samples/sec)
2024-03-09 21:07:16.637312 epoch: 11 step: 100 cls_loss= 0.86574 (9313 samples/sec)
saving....
2024-03-09 21:07:20.496605------------------------------------------------------ Precision@1: 59.13% 

[59.76, 59.86, 59.99, 59.64, 59.89, 59.89, 59.78, 60.07, 59.81, 59.65, 59.94, 59.13]

Epoch: 12
2024-03-09 21:07:20.699629 epoch: 12 step: 0 cls_loss= 0.87608 (148588 samples/sec)
2024-03-09 21:07:23.943329 epoch: 12 step: 100 cls_loss= 0.85168 (9248 samples/sec)
saving....
2024-03-09 21:07:27.956595------------------------------------------------------ Precision@1: 59.87% 

[59.76, 59.86, 59.99, 59.64, 59.89, 59.89, 59.78, 60.07, 59.81, 59.65, 59.94, 59.13, 59.87]

Epoch: 13
2024-03-09 21:07:28.153823 epoch: 13 step: 0 cls_loss= 0.82462 (152868 samples/sec)
2024-03-09 21:07:31.201913 epoch: 13 step: 100 cls_loss= 0.74124 (9842 samples/sec)
saving....
2024-03-09 21:07:34.955224------------------------------------------------------ Precision@1: 59.59% 

[59.76, 59.86, 59.99, 59.64, 59.89, 59.89, 59.78, 60.07, 59.81, 59.65, 59.94, 59.13, 59.87, 59.59]

Epoch: 14
2024-03-09 21:07:35.147693 epoch: 14 step: 0 cls_loss= 0.83615 (156668 samples/sec)
2024-03-09 21:07:38.137190 epoch: 14 step: 100 cls_loss= 0.95778 (10035 samples/sec)
saving....
2024-03-09 21:07:41.868523------------------------------------------------------ Precision@1: 59.64% 

[59.76, 59.86, 59.99, 59.64, 59.89, 59.89, 59.78, 60.07, 59.81, 59.65, 59.94, 59.13, 59.87, 59.59, 59.64]

Epoch: 15
2024-03-09 21:07:42.074958 epoch: 15 step: 0 cls_loss= 0.75539 (146050 samples/sec)
2024-03-09 21:07:45.088020 epoch: 15 step: 100 cls_loss= 0.79068 (9956 samples/sec)
saving....
2024-03-09 21:07:48.839361------------------------------------------------------ Precision@1: 59.42% 

[59.76, 59.86, 59.99, 59.64, 59.89, 59.89, 59.78, 60.07, 59.81, 59.65, 59.94, 59.13, 59.87, 59.59, 59.64, 59.42]

Epoch: 16
2024-03-09 21:07:49.039595 epoch: 16 step: 0 cls_loss= 0.84274 (150604 samples/sec)
2024-03-09 21:07:51.964851 epoch: 16 step: 100 cls_loss= 0.91366 (10255 samples/sec)
saving....
2024-03-09 21:07:55.703778------------------------------------------------------ Precision@1: 59.72% 

[59.76, 59.86, 59.99, 59.64, 59.89, 59.89, 59.78, 60.07, 59.81, 59.65, 59.94, 59.13, 59.87, 59.59, 59.64, 59.42, 59.72]

Epoch: 17
2024-03-09 21:07:55.901201 epoch: 17 step: 0 cls_loss= 0.75790 (152918 samples/sec)
2024-03-09 21:07:58.948383 epoch: 17 step: 100 cls_loss= 0.80621 (9845 samples/sec)
saving....
2024-03-09 21:08:02.777828------------------------------------------------------ Precision@1: 59.53% 

[59.76, 59.86, 59.99, 59.64, 59.89, 59.89, 59.78, 60.07, 59.81, 59.65, 59.94, 59.13, 59.87, 59.59, 59.64, 59.42, 59.72, 59.53]

Epoch: 18
2024-03-09 21:08:02.979806 epoch: 18 step: 0 cls_loss= 0.82713 (149274 samples/sec)
2024-03-09 21:08:06.099444 epoch: 18 step: 100 cls_loss= 0.84000 (9616 samples/sec)
saving....
2024-03-09 21:08:09.912459------------------------------------------------------ Precision@1: 59.53% 

[59.76, 59.86, 59.99, 59.64, 59.89, 59.89, 59.78, 60.07, 59.81, 59.65, 59.94, 59.13, 59.87, 59.59, 59.64, 59.42, 59.72, 59.53, 59.53]

Epoch: 19
2024-03-09 21:08:10.100174 epoch: 19 step: 0 cls_loss= 0.90720 (160650 samples/sec)
2024-03-09 21:08:13.177681 epoch: 19 step: 100 cls_loss= 0.64672 (9748 samples/sec)
saving....
2024-03-09 21:08:16.942451------------------------------------------------------ Precision@1: 59.62% 

[59.76, 59.86, 59.99, 59.64, 59.89, 59.89, 59.78, 60.07, 59.81, 59.65, 59.94, 59.13, 59.87, 59.59, 59.64, 59.42, 59.72, 59.53, 59.53, 59.62]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-09 21:08:19.645012 epoch: 0 step: 0 cls_loss= 1.04364 (51519 samples/sec)
2024-03-09 21:08:22.681443 epoch: 0 step: 100 cls_loss= 1.07631 (9880 samples/sec)
saving....
2024-03-09 21:08:26.748765------------------------------------------------------ Precision@1: 59.31% 

[59.31]
max acc : 59.31

Epoch: 1
2024-03-09 21:08:26.967131 epoch: 1 step: 0 cls_loss= 1.11127 (157317 samples/sec)
2024-03-09 21:08:29.979934 epoch: 1 step: 100 cls_loss= 1.03512 (9957 samples/sec)
saving....
2024-03-09 21:08:33.666028------------------------------------------------------ Precision@1: 59.81% 

[59.31, 59.81]
max acc : 59.81

Epoch: 2
2024-03-09 21:08:33.893330 epoch: 2 step: 0 cls_loss= 1.12365 (149032 samples/sec)
2024-03-09 21:08:36.940577 epoch: 2 step: 100 cls_loss= 1.03162 (9845 samples/sec)
saving....
2024-03-09 21:08:40.727876------------------------------------------------------ Precision@1: 59.80% 

[59.31, 59.81, 59.8]

Epoch: 3
2024-03-09 21:08:40.924652 epoch: 3 step: 0 cls_loss= 1.14823 (153331 samples/sec)
2024-03-09 21:08:43.935836 epoch: 3 step: 100 cls_loss= 0.87335 (9963 samples/sec)
saving....
2024-03-09 21:08:47.856395------------------------------------------------------ Precision@1: 59.84% 

[59.31, 59.81, 59.8, 59.84]
max acc : 59.84

Epoch: 4
2024-03-09 21:08:48.082458 epoch: 4 step: 0 cls_loss= 0.93587 (153155 samples/sec)
2024-03-09 21:08:51.264938 epoch: 4 step: 100 cls_loss= 0.90136 (9426 samples/sec)
saving....
2024-03-09 21:08:55.184784------------------------------------------------------ Precision@1: 60.08% 

[59.31, 59.81, 59.8, 59.84, 60.08]
max acc : 60.08

Epoch: 5
2024-03-09 21:08:55.396852 epoch: 5 step: 0 cls_loss= 0.73149 (164090 samples/sec)
2024-03-09 21:08:58.524061 epoch: 5 step: 100 cls_loss= 1.08140 (9593 samples/sec)
saving....
2024-03-09 21:09:02.326217------------------------------------------------------ Precision@1: 59.56% 

[59.31, 59.81, 59.8, 59.84, 60.08, 59.56]

Epoch: 6
2024-03-09 21:09:02.528466 epoch: 6 step: 0 cls_loss= 0.83195 (149234 samples/sec)
2024-03-09 21:09:05.663197 epoch: 6 step: 100 cls_loss= 0.86391 (9570 samples/sec)
saving....
2024-03-09 21:09:09.470309------------------------------------------------------ Precision@1: 59.32% 

[59.31, 59.81, 59.8, 59.84, 60.08, 59.56, 59.32]

Epoch: 7
2024-03-09 21:09:09.663430 epoch: 7 step: 0 cls_loss= 0.91632 (156262 samples/sec)
2024-03-09 21:09:12.668084 epoch: 7 step: 100 cls_loss= 1.07513 (9984 samples/sec)
saving....
2024-03-09 21:09:16.408953------------------------------------------------------ Precision@1: 59.81% 

[59.31, 59.81, 59.8, 59.84, 60.08, 59.56, 59.32, 59.81]

Epoch: 8
2024-03-09 21:09:16.598766 epoch: 8 step: 0 cls_loss= 0.90908 (158641 samples/sec)
2024-03-09 21:09:19.639153 epoch: 8 step: 100 cls_loss= 0.97339 (9867 samples/sec)
saving....
2024-03-09 21:09:23.347170------------------------------------------------------ Precision@1: 59.44% 

[59.31, 59.81, 59.8, 59.84, 60.08, 59.56, 59.32, 59.81, 59.44]

Epoch: 9
2024-03-09 21:09:23.568242 epoch: 9 step: 0 cls_loss= 0.98220 (136384 samples/sec)
2024-03-09 21:09:26.615692 epoch: 9 step: 100 cls_loss= 0.84276 (9844 samples/sec)
saving....
2024-03-09 21:09:30.431938------------------------------------------------------ Precision@1: 60.00% 

[59.31, 59.81, 59.8, 59.84, 60.08, 59.56, 59.32, 59.81, 59.44, 60.0]

Epoch: 10
2024-03-09 21:09:30.631623 epoch: 10 step: 0 cls_loss= 0.84501 (151186 samples/sec)
2024-03-09 21:09:33.637439 epoch: 10 step: 100 cls_loss= 0.83699 (9980 samples/sec)
saving....
2024-03-09 21:09:37.382942------------------------------------------------------ Precision@1: 59.80% 

[59.31, 59.81, 59.8, 59.84, 60.08, 59.56, 59.32, 59.81, 59.44, 60.0, 59.8]

Epoch: 11
2024-03-09 21:09:37.583817 epoch: 11 step: 0 cls_loss= 0.82539 (150190 samples/sec)
2024-03-09 21:09:40.681143 epoch: 11 step: 100 cls_loss= 0.97349 (9685 samples/sec)
saving....
2024-03-09 21:09:44.622846------------------------------------------------------ Precision@1: 59.61% 

[59.31, 59.81, 59.8, 59.84, 60.08, 59.56, 59.32, 59.81, 59.44, 60.0, 59.8, 59.61]

Epoch: 12
2024-03-09 21:09:44.829681 epoch: 12 step: 0 cls_loss= 0.77462 (145820 samples/sec)
2024-03-09 21:09:47.836933 epoch: 12 step: 100 cls_loss= 0.91118 (9976 samples/sec)
saving....
2024-03-09 21:09:51.567838------------------------------------------------------ Precision@1: 59.61% 

[59.31, 59.81, 59.8, 59.84, 60.08, 59.56, 59.32, 59.81, 59.44, 60.0, 59.8, 59.61, 59.61]

Epoch: 13
2024-03-09 21:09:51.754889 epoch: 13 step: 0 cls_loss= 0.90773 (161425 samples/sec)
2024-03-09 21:09:54.784581 epoch: 13 step: 100 cls_loss= 0.92390 (9902 samples/sec)
saving....
2024-03-09 21:09:58.642688------------------------------------------------------ Precision@1: 59.78% 

[59.31, 59.81, 59.8, 59.84, 60.08, 59.56, 59.32, 59.81, 59.44, 60.0, 59.8, 59.61, 59.61, 59.78]

Epoch: 14
2024-03-09 21:09:58.837872 epoch: 14 step: 0 cls_loss= 0.94055 (154374 samples/sec)
2024-03-09 21:10:01.757713 epoch: 14 step: 100 cls_loss= 0.81536 (10274 samples/sec)
saving....
2024-03-09 21:10:05.451361------------------------------------------------------ Precision@1: 59.36% 

[59.31, 59.81, 59.8, 59.84, 60.08, 59.56, 59.32, 59.81, 59.44, 60.0, 59.8, 59.61, 59.61, 59.78, 59.36]

Epoch: 15
2024-03-09 21:10:05.667009 epoch: 15 step: 0 cls_loss= 0.73995 (139822 samples/sec)
2024-03-09 21:10:08.849832 epoch: 15 step: 100 cls_loss= 0.85141 (9425 samples/sec)
saving....
2024-03-09 21:10:12.673101------------------------------------------------------ Precision@1: 59.65% 

[59.31, 59.81, 59.8, 59.84, 60.08, 59.56, 59.32, 59.81, 59.44, 60.0, 59.8, 59.61, 59.61, 59.78, 59.36, 59.65]

Epoch: 16
2024-03-09 21:10:12.874309 epoch: 16 step: 0 cls_loss= 0.89744 (149925 samples/sec)
2024-03-09 21:10:15.999015 epoch: 16 step: 100 cls_loss= 0.87211 (9601 samples/sec)
saving....
2024-03-09 21:10:19.797810------------------------------------------------------ Precision@1: 59.66% 

[59.31, 59.81, 59.8, 59.84, 60.08, 59.56, 59.32, 59.81, 59.44, 60.0, 59.8, 59.61, 59.61, 59.78, 59.36, 59.65, 59.66]

Epoch: 17
2024-03-09 21:10:20.004672 epoch: 17 step: 0 cls_loss= 0.69234 (145804 samples/sec)
2024-03-09 21:10:23.170005 epoch: 17 step: 100 cls_loss= 0.76735 (9477 samples/sec)
saving....
2024-03-09 21:10:27.021858------------------------------------------------------ Precision@1: 59.48% 

[59.31, 59.81, 59.8, 59.84, 60.08, 59.56, 59.32, 59.81, 59.44, 60.0, 59.8, 59.61, 59.61, 59.78, 59.36, 59.65, 59.66, 59.48]

Epoch: 18
2024-03-09 21:10:27.218575 epoch: 18 step: 0 cls_loss= 0.76852 (153330 samples/sec)
2024-03-09 21:10:30.258049 epoch: 18 step: 100 cls_loss= 0.67219 (9870 samples/sec)
saving....
2024-03-09 21:10:34.095285------------------------------------------------------ Precision@1: 59.60% 

[59.31, 59.81, 59.8, 59.84, 60.08, 59.56, 59.32, 59.81, 59.44, 60.0, 59.8, 59.61, 59.61, 59.78, 59.36, 59.65, 59.66, 59.48, 59.6]

Epoch: 19
2024-03-09 21:10:34.298119 epoch: 19 step: 0 cls_loss= 0.79384 (148561 samples/sec)
2024-03-09 21:10:37.398552 epoch: 19 step: 100 cls_loss= 0.74598 (9676 samples/sec)
saving....
2024-03-09 21:10:41.315865------------------------------------------------------ Precision@1: 59.72% 

[59.31, 59.81, 59.8, 59.84, 60.08, 59.56, 59.32, 59.81, 59.44, 60.0, 59.8, 59.61, 59.61, 59.78, 59.36, 59.65, 59.66, 59.48, 59.6, 59.72]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-09 21:10:44.040098 epoch: 0 step: 0 cls_loss= 0.91592 (48180 samples/sec)
2024-03-09 21:10:47.166586 epoch: 0 step: 100 cls_loss= 0.99865 (9595 samples/sec)
saving....
2024-03-09 21:10:51.354679------------------------------------------------------ Precision@1: 59.64% 

[59.64]
max acc : 59.64

Epoch: 1
2024-03-09 21:10:51.570947 epoch: 1 step: 0 cls_loss= 1.11322 (161670 samples/sec)
2024-03-09 21:10:54.688259 epoch: 1 step: 100 cls_loss= 0.99930 (9623 samples/sec)
saving....
2024-03-09 21:10:58.492512------------------------------------------------------ Precision@1: 59.85% 

[59.64, 59.85]
max acc : 59.85

Epoch: 2
2024-03-09 21:10:58.712025 epoch: 2 step: 0 cls_loss= 0.83159 (157655 samples/sec)
2024-03-09 21:11:01.810076 epoch: 2 step: 100 cls_loss= 1.02458 (9683 samples/sec)
saving....
2024-03-09 21:11:05.609166------------------------------------------------------ Precision@1: 59.80% 

[59.64, 59.85, 59.8]

Epoch: 3
2024-03-09 21:11:05.804230 epoch: 3 step: 0 cls_loss= 0.86859 (154724 samples/sec)
2024-03-09 21:11:08.876498 epoch: 3 step: 100 cls_loss= 0.89588 (9764 samples/sec)
saving....
2024-03-09 21:11:12.633814------------------------------------------------------ Precision@1: 59.41% 

[59.64, 59.85, 59.8, 59.41]

Epoch: 4
2024-03-09 21:11:12.843977 epoch: 4 step: 0 cls_loss= 0.96865 (143473 samples/sec)
2024-03-09 21:11:15.931729 epoch: 4 step: 100 cls_loss= 0.87437 (9716 samples/sec)
saving....
2024-03-09 21:11:19.787852------------------------------------------------------ Precision@1: 59.56% 

[59.64, 59.85, 59.8, 59.41, 59.56]

Epoch: 5
2024-03-09 21:11:19.989267 epoch: 5 step: 0 cls_loss= 0.90045 (149740 samples/sec)
2024-03-09 21:11:23.020800 epoch: 5 step: 100 cls_loss= 0.89756 (9896 samples/sec)
saving....
2024-03-09 21:11:26.818842------------------------------------------------------ Precision@1: 59.45% 

[59.64, 59.85, 59.8, 59.41, 59.56, 59.45]

Epoch: 6
2024-03-09 21:11:27.018921 epoch: 6 step: 0 cls_loss= 0.91969 (150805 samples/sec)
2024-03-09 21:11:30.070031 epoch: 6 step: 100 cls_loss= 0.97662 (9832 samples/sec)
saving....
2024-03-09 21:11:33.976334------------------------------------------------------ Precision@1: 59.61% 

[59.64, 59.85, 59.8, 59.41, 59.56, 59.45, 59.61]

Epoch: 7
2024-03-09 21:11:34.178117 epoch: 7 step: 0 cls_loss= 0.87454 (149325 samples/sec)
2024-03-09 21:11:37.316440 epoch: 7 step: 100 cls_loss= 0.94518 (9559 samples/sec)
saving....
2024-03-09 21:11:41.312064------------------------------------------------------ Precision@1: 59.40% 

[59.64, 59.85, 59.8, 59.41, 59.56, 59.45, 59.61, 59.4]

Epoch: 8
2024-03-09 21:11:41.535930 epoch: 8 step: 0 cls_loss= 0.81634 (134693 samples/sec)
2024-03-09 21:11:44.629209 epoch: 8 step: 100 cls_loss= 0.82140 (9698 samples/sec)
saving....
2024-03-09 21:11:48.471327------------------------------------------------------ Precision@1: 59.55% 

[59.64, 59.85, 59.8, 59.41, 59.56, 59.45, 59.61, 59.4, 59.55]

Epoch: 9
2024-03-09 21:11:48.672627 epoch: 9 step: 0 cls_loss= 0.80964 (149892 samples/sec)
2024-03-09 21:11:51.797793 epoch: 9 step: 100 cls_loss= 0.80227 (9599 samples/sec)
saving....
2024-03-09 21:11:55.741753------------------------------------------------------ Precision@1: 59.37% 

[59.64, 59.85, 59.8, 59.41, 59.56, 59.45, 59.61, 59.4, 59.55, 59.37]

Epoch: 10
2024-03-09 21:11:55.935319 epoch: 10 step: 0 cls_loss= 0.87492 (155905 samples/sec)
2024-03-09 21:11:59.004801 epoch: 10 step: 100 cls_loss= 0.89005 (9773 samples/sec)
saving....
2024-03-09 21:12:02.830550------------------------------------------------------ Precision@1: 59.60% 

[59.64, 59.85, 59.8, 59.41, 59.56, 59.45, 59.61, 59.4, 59.55, 59.37, 59.6]

Epoch: 11
2024-03-09 21:12:03.032716 epoch: 11 step: 0 cls_loss= 0.80377 (149111 samples/sec)
2024-03-09 21:12:06.026922 epoch: 11 step: 100 cls_loss= 0.89406 (10019 samples/sec)
saving....
2024-03-09 21:12:09.749835------------------------------------------------------ Precision@1: 59.35% 

[59.64, 59.85, 59.8, 59.41, 59.56, 59.45, 59.61, 59.4, 59.55, 59.37, 59.6, 59.35]

Epoch: 12
2024-03-09 21:12:09.952681 epoch: 12 step: 0 cls_loss= 0.73338 (148727 samples/sec)
2024-03-09 21:12:12.994918 epoch: 12 step: 100 cls_loss= 0.81239 (9861 samples/sec)
saving....
2024-03-09 21:12:16.901366------------------------------------------------------ Precision@1: 59.44% 

[59.64, 59.85, 59.8, 59.41, 59.56, 59.45, 59.61, 59.4, 59.55, 59.37, 59.6, 59.35, 59.44]

Epoch: 13
2024-03-09 21:12:17.111400 epoch: 13 step: 0 cls_loss= 0.75601 (143604 samples/sec)
2024-03-09 21:12:20.203113 epoch: 13 step: 100 cls_loss= 0.93380 (9703 samples/sec)
saving....
2024-03-09 21:12:24.016117------------------------------------------------------ Precision@1: 59.13% 

[59.64, 59.85, 59.8, 59.41, 59.56, 59.45, 59.61, 59.4, 59.55, 59.37, 59.6, 59.35, 59.44, 59.13]

Epoch: 14
2024-03-09 21:12:24.218388 epoch: 14 step: 0 cls_loss= 0.79869 (148852 samples/sec)
2024-03-09 21:12:27.325292 epoch: 14 step: 100 cls_loss= 0.77608 (9656 samples/sec)
saving....
2024-03-09 21:12:31.150979------------------------------------------------------ Precision@1: 59.63% 

[59.64, 59.85, 59.8, 59.41, 59.56, 59.45, 59.61, 59.4, 59.55, 59.37, 59.6, 59.35, 59.44, 59.13, 59.63]

Epoch: 15
2024-03-09 21:12:31.349957 epoch: 15 step: 0 cls_loss= 0.80324 (151518 samples/sec)
2024-03-09 21:12:34.538596 epoch: 15 step: 100 cls_loss= 0.94446 (9408 samples/sec)
saving....
2024-03-09 21:12:38.388238------------------------------------------------------ Precision@1: 59.21% 

[59.64, 59.85, 59.8, 59.41, 59.56, 59.45, 59.61, 59.4, 59.55, 59.37, 59.6, 59.35, 59.44, 59.13, 59.63, 59.21]

Epoch: 16
2024-03-09 21:12:38.582064 epoch: 16 step: 0 cls_loss= 0.71836 (155635 samples/sec)
2024-03-09 21:12:41.679757 epoch: 16 step: 100 cls_loss= 0.73408 (9684 samples/sec)
saving....
2024-03-09 21:12:45.459249------------------------------------------------------ Precision@1: 59.27% 

[59.64, 59.85, 59.8, 59.41, 59.56, 59.45, 59.61, 59.4, 59.55, 59.37, 59.6, 59.35, 59.44, 59.13, 59.63, 59.21, 59.27]

Epoch: 17
2024-03-09 21:12:45.660687 epoch: 17 step: 0 cls_loss= 0.64893 (149649 samples/sec)
2024-03-09 21:12:48.790606 epoch: 17 step: 100 cls_loss= 0.75472 (9585 samples/sec)
saving....
2024-03-09 21:12:52.785502------------------------------------------------------ Precision@1: 59.26% 

[59.64, 59.85, 59.8, 59.41, 59.56, 59.45, 59.61, 59.4, 59.55, 59.37, 59.6, 59.35, 59.44, 59.13, 59.63, 59.21, 59.27, 59.26]

Epoch: 18
2024-03-09 21:12:52.988896 epoch: 18 step: 0 cls_loss= 0.74200 (148339 samples/sec)
2024-03-09 21:12:55.991798 epoch: 18 step: 100 cls_loss= 0.91784 (9990 samples/sec)
saving....
2024-03-09 21:12:59.727712------------------------------------------------------ Precision@1: 58.96% 

[59.64, 59.85, 59.8, 59.41, 59.56, 59.45, 59.61, 59.4, 59.55, 59.37, 59.6, 59.35, 59.44, 59.13, 59.63, 59.21, 59.27, 59.26, 58.96]

Epoch: 19
2024-03-09 21:12:59.938235 epoch: 19 step: 0 cls_loss= 0.69470 (143214 samples/sec)
2024-03-09 21:13:02.961031 epoch: 19 step: 100 cls_loss= 0.77086 (9924 samples/sec)
saving....
2024-03-09 21:13:06.729297------------------------------------------------------ Precision@1: 59.12% 

[59.64, 59.85, 59.8, 59.41, 59.56, 59.45, 59.61, 59.4, 59.55, 59.37, 59.6, 59.35, 59.44, 59.13, 59.63, 59.21, 59.27, 59.26, 58.96, 59.12]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-09 21:13:09.454737 epoch: 0 step: 0 cls_loss= 0.96992 (48395 samples/sec)
2024-03-09 21:13:12.538058 epoch: 0 step: 100 cls_loss= 1.06597 (9730 samples/sec)
saving....
2024-03-09 21:13:16.538545------------------------------------------------------ Precision@1: 59.38% 

[59.38]
max acc : 59.38

Epoch: 1
2024-03-09 21:13:16.750672 epoch: 1 step: 0 cls_loss= 0.99075 (164573 samples/sec)
2024-03-09 21:13:19.770000 epoch: 1 step: 100 cls_loss= 1.01994 (9936 samples/sec)
saving....
2024-03-09 21:13:23.584038------------------------------------------------------ Precision@1: 59.83% 

[59.38, 59.83]
max acc : 59.83

Epoch: 2
2024-03-09 21:13:23.800688 epoch: 2 step: 0 cls_loss= 0.94605 (159721 samples/sec)
2024-03-09 21:13:26.955334 epoch: 2 step: 100 cls_loss= 0.91491 (9509 samples/sec)
saving....
2024-03-09 21:13:30.894094------------------------------------------------------ Precision@1: 59.52% 

[59.38, 59.83, 59.52]

Epoch: 3
2024-03-09 21:13:31.099064 epoch: 3 step: 0 cls_loss= 0.90638 (147139 samples/sec)
2024-03-09 21:13:34.162794 epoch: 3 step: 100 cls_loss= 0.93710 (9792 samples/sec)
saving....
2024-03-09 21:13:38.036850------------------------------------------------------ Precision@1: 59.55% 

[59.38, 59.83, 59.52, 59.55]

Epoch: 4
2024-03-09 21:13:38.254742 epoch: 4 step: 0 cls_loss= 0.87310 (138331 samples/sec)
2024-03-09 21:13:41.411320 epoch: 4 step: 100 cls_loss= 1.07784 (9504 samples/sec)
saving....
2024-03-09 21:13:45.294899------------------------------------------------------ Precision@1: 59.40% 

[59.38, 59.83, 59.52, 59.55, 59.4]

Epoch: 5
2024-03-09 21:13:45.512816 epoch: 5 step: 0 cls_loss= 0.85812 (138285 samples/sec)
2024-03-09 21:13:48.737037 epoch: 5 step: 100 cls_loss= 1.06918 (9304 samples/sec)
saving....
2024-03-09 21:13:52.522788------------------------------------------------------ Precision@1: 59.74% 

[59.38, 59.83, 59.52, 59.55, 59.4, 59.74]

Epoch: 6
2024-03-09 21:13:52.722839 epoch: 6 step: 0 cls_loss= 0.86819 (150747 samples/sec)
2024-03-09 21:13:55.736377 epoch: 6 step: 100 cls_loss= 0.86588 (9955 samples/sec)
saving....
2024-03-09 21:13:59.632522------------------------------------------------------ Precision@1: 60.04% 

[59.38, 59.83, 59.52, 59.55, 59.4, 59.74, 60.04]
max acc : 60.04

Epoch: 7
2024-03-09 21:13:59.854316 epoch: 7 step: 0 cls_loss= 0.82542 (155994 samples/sec)
2024-03-09 21:14:03.023131 epoch: 7 step: 100 cls_loss= 0.80671 (9467 samples/sec)
saving....
2024-03-09 21:14:06.835311------------------------------------------------------ Precision@1: 59.64% 

[59.38, 59.83, 59.52, 59.55, 59.4, 59.74, 60.04, 59.64]

Epoch: 8
2024-03-09 21:14:07.037236 epoch: 8 step: 0 cls_loss= 0.76213 (149155 samples/sec)
2024-03-09 21:14:10.068472 epoch: 8 step: 100 cls_loss= 0.76070 (9897 samples/sec)
saving....
2024-03-09 21:14:13.893804------------------------------------------------------ Precision@1: 59.35% 

[59.38, 59.83, 59.52, 59.55, 59.4, 59.74, 60.04, 59.64, 59.35]

Epoch: 9
2024-03-09 21:14:14.078094 epoch: 9 step: 0 cls_loss= 0.95187 (163893 samples/sec)
2024-03-09 21:14:17.151395 epoch: 9 step: 100 cls_loss= 0.95352 (9761 samples/sec)
saving....
2024-03-09 21:14:21.160455------------------------------------------------------ Precision@1: 59.40% 

[59.38, 59.83, 59.52, 59.55, 59.4, 59.74, 60.04, 59.64, 59.35, 59.4]

Epoch: 10
2024-03-09 21:14:21.372418 epoch: 10 step: 0 cls_loss= 0.74991 (142209 samples/sec)
2024-03-09 21:14:24.511911 epoch: 10 step: 100 cls_loss= 0.87125 (9555 samples/sec)
saving....
2024-03-09 21:14:28.309044------------------------------------------------------ Precision@1: 59.42% 

[59.38, 59.83, 59.52, 59.55, 59.4, 59.74, 60.04, 59.64, 59.35, 59.4, 59.42]

Epoch: 11
2024-03-09 21:14:28.494197 epoch: 11 step: 0 cls_loss= 0.71914 (163092 samples/sec)
2024-03-09 21:14:31.599435 epoch: 11 step: 100 cls_loss= 0.83179 (9661 samples/sec)
saving....
2024-03-09 21:14:35.410197------------------------------------------------------ Precision@1: 59.46% 

[59.38, 59.83, 59.52, 59.55, 59.4, 59.74, 60.04, 59.64, 59.35, 59.4, 59.42, 59.46]

Epoch: 12
2024-03-09 21:14:35.608246 epoch: 12 step: 0 cls_loss= 1.01553 (152413 samples/sec)
2024-03-09 21:14:38.655653 epoch: 12 step: 100 cls_loss= 0.84183 (9844 samples/sec)
saving....
2024-03-09 21:14:42.427449------------------------------------------------------ Precision@1: 59.24% 

[59.38, 59.83, 59.52, 59.55, 59.4, 59.74, 60.04, 59.64, 59.35, 59.4, 59.42, 59.46, 59.24]

Epoch: 13
2024-03-09 21:14:42.624020 epoch: 13 step: 0 cls_loss= 0.73482 (153552 samples/sec)
2024-03-09 21:14:45.677450 epoch: 13 step: 100 cls_loss= 0.76971 (9825 samples/sec)
saving....
2024-03-09 21:14:49.423890------------------------------------------------------ Precision@1: 59.41% 

[59.38, 59.83, 59.52, 59.55, 59.4, 59.74, 60.04, 59.64, 59.35, 59.4, 59.42, 59.46, 59.24, 59.41]

Epoch: 14
2024-03-09 21:14:49.640210 epoch: 14 step: 0 cls_loss= 0.76145 (139441 samples/sec)
2024-03-09 21:14:52.678685 epoch: 14 step: 100 cls_loss= 0.84064 (9873 samples/sec)
saving....
2024-03-09 21:14:56.473492------------------------------------------------------ Precision@1: 59.17% 

[59.38, 59.83, 59.52, 59.55, 59.4, 59.74, 60.04, 59.64, 59.35, 59.4, 59.42, 59.46, 59.24, 59.41, 59.17]

Epoch: 15
2024-03-09 21:14:56.656638 epoch: 15 step: 0 cls_loss= 0.77666 (164897 samples/sec)
2024-03-09 21:14:59.620966 epoch: 15 step: 100 cls_loss= 0.76160 (10120 samples/sec)
saving....
2024-03-09 21:15:03.413681------------------------------------------------------ Precision@1: 59.25% 

[59.38, 59.83, 59.52, 59.55, 59.4, 59.74, 60.04, 59.64, 59.35, 59.4, 59.42, 59.46, 59.24, 59.41, 59.17, 59.25]

Epoch: 16
2024-03-09 21:15:03.617247 epoch: 16 step: 0 cls_loss= 0.65845 (148245 samples/sec)
2024-03-09 21:15:06.699961 epoch: 16 step: 100 cls_loss= 0.84734 (9731 samples/sec)
saving....
2024-03-09 21:15:10.445327------------------------------------------------------ Precision@1: 58.84% 

[59.38, 59.83, 59.52, 59.55, 59.4, 59.74, 60.04, 59.64, 59.35, 59.4, 59.42, 59.46, 59.24, 59.41, 59.17, 59.25, 58.84]

Epoch: 17
2024-03-09 21:15:10.651087 epoch: 17 step: 0 cls_loss= 0.76273 (146630 samples/sec)
2024-03-09 21:15:13.808397 epoch: 17 step: 100 cls_loss= 0.66448 (9501 samples/sec)
saving....
2024-03-09 21:15:17.685213------------------------------------------------------ Precision@1: 59.32% 

[59.38, 59.83, 59.52, 59.55, 59.4, 59.74, 60.04, 59.64, 59.35, 59.4, 59.42, 59.46, 59.24, 59.41, 59.17, 59.25, 58.84, 59.32]

Epoch: 18
2024-03-09 21:15:17.881348 epoch: 18 step: 0 cls_loss= 0.86967 (153739 samples/sec)
2024-03-09 21:15:21.013787 epoch: 18 step: 100 cls_loss= 0.82034 (9577 samples/sec)
saving....
2024-03-09 21:15:24.856888------------------------------------------------------ Precision@1: 58.86% 

[59.38, 59.83, 59.52, 59.55, 59.4, 59.74, 60.04, 59.64, 59.35, 59.4, 59.42, 59.46, 59.24, 59.41, 59.17, 59.25, 58.84, 59.32, 58.86]

Epoch: 19
2024-03-09 21:15:25.042061 epoch: 19 step: 0 cls_loss= 0.70290 (163004 samples/sec)
2024-03-09 21:15:28.057921 epoch: 19 step: 100 cls_loss= 0.81728 (9947 samples/sec)
saving....
2024-03-09 21:15:31.838103------------------------------------------------------ Precision@1: 59.13% 

[59.38, 59.83, 59.52, 59.55, 59.4, 59.74, 60.04, 59.64, 59.35, 59.4, 59.42, 59.46, 59.24, 59.41, 59.17, 59.25, 58.84, 59.32, 58.86, 59.13]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model mobilenet_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-09 21:15:34.573053 epoch: 0 step: 0 cls_loss= 1.07556 (49168 samples/sec)
2024-03-09 21:15:37.594131 epoch: 0 step: 100 cls_loss= 1.16959 (9930 samples/sec)
saving....
2024-03-09 21:15:41.590011------------------------------------------------------ Precision@1: 59.46% 

[59.46]
max acc : 59.46

Epoch: 1
2024-03-09 21:15:41.825909 epoch: 1 step: 0 cls_loss= 0.93954 (146337 samples/sec)
2024-03-09 21:15:45.078446 epoch: 1 step: 100 cls_loss= 1.10350 (9223 samples/sec)
saving....
2024-03-09 21:15:48.905333------------------------------------------------------ Precision@1: 59.33% 

[59.46, 59.33]

Epoch: 2
2024-03-09 21:15:49.105288 epoch: 2 step: 0 cls_loss= 0.77713 (150839 samples/sec)
2024-03-09 21:15:52.143920 epoch: 2 step: 100 cls_loss= 0.93843 (9873 samples/sec)
saving....
2024-03-09 21:15:56.000746------------------------------------------------------ Precision@1: 59.85% 

[59.46, 59.33, 59.85]
max acc : 59.85

Epoch: 3
2024-03-09 21:15:56.226981 epoch: 3 step: 0 cls_loss= 0.91546 (151428 samples/sec)
2024-03-09 21:15:59.253925 epoch: 3 step: 100 cls_loss= 1.01208 (9911 samples/sec)
saving....
2024-03-09 21:16:02.997547------------------------------------------------------ Precision@1: 59.56% 

[59.46, 59.33, 59.85, 59.56]

Epoch: 4
2024-03-09 21:16:03.204789 epoch: 4 step: 0 cls_loss= 0.91564 (145456 samples/sec)
2024-03-09 21:16:06.329892 epoch: 4 step: 100 cls_loss= 0.85839 (9599 samples/sec)
saving....
2024-03-09 21:16:10.351032------------------------------------------------------ Precision@1: 59.43% 

[59.46, 59.33, 59.85, 59.56, 59.43]

Epoch: 5
2024-03-09 21:16:10.547072 epoch: 5 step: 0 cls_loss= 0.95034 (153921 samples/sec)
2024-03-09 21:16:13.545500 epoch: 5 step: 100 cls_loss= 0.79066 (10005 samples/sec)
saving....
2024-03-09 21:16:17.309506------------------------------------------------------ Precision@1: 59.75% 

[59.46, 59.33, 59.85, 59.56, 59.43, 59.75]

Epoch: 6
2024-03-09 21:16:17.494417 epoch: 6 step: 0 cls_loss= 0.89265 (163131 samples/sec)
2024-03-09 21:16:20.504926 epoch: 6 step: 100 cls_loss= 0.89645 (9965 samples/sec)
saving....
2024-03-09 21:16:24.261658------------------------------------------------------ Precision@1: 59.80% 

[59.46, 59.33, 59.85, 59.56, 59.43, 59.75, 59.8]

Epoch: 7
2024-03-09 21:16:24.476158 epoch: 7 step: 0 cls_loss= 0.78268 (140451 samples/sec)
2024-03-09 21:16:27.569968 epoch: 7 step: 100 cls_loss= 0.82667 (9697 samples/sec)
saving....
2024-03-09 21:16:31.566527------------------------------------------------------ Precision@1: 59.56% 

[59.46, 59.33, 59.85, 59.56, 59.43, 59.75, 59.8, 59.56]

Epoch: 8
2024-03-09 21:16:31.764227 epoch: 8 step: 0 cls_loss= 0.80624 (152546 samples/sec)
2024-03-09 21:16:34.791892 epoch: 8 step: 100 cls_loss= 0.91195 (9908 samples/sec)
saving....
2024-03-09 21:16:38.613295------------------------------------------------------ Precision@1: 59.41% 

[59.46, 59.33, 59.85, 59.56, 59.43, 59.75, 59.8, 59.56, 59.41]

Epoch: 9
2024-03-09 21:16:38.830074 epoch: 9 step: 0 cls_loss= 0.79151 (139083 samples/sec)
2024-03-09 21:16:41.917710 epoch: 9 step: 100 cls_loss= 0.99864 (9716 samples/sec)
saving....
2024-03-09 21:16:45.784862------------------------------------------------------ Precision@1: 59.53% 

[59.46, 59.33, 59.85, 59.56, 59.43, 59.75, 59.8, 59.56, 59.41, 59.53]

Epoch: 10
2024-03-09 21:16:45.993471 epoch: 10 step: 0 cls_loss= 0.84059 (144605 samples/sec)
2024-03-09 21:16:49.047119 epoch: 10 step: 100 cls_loss= 0.87655 (9824 samples/sec)
saving....
2024-03-09 21:16:52.870252------------------------------------------------------ Precision@1: 59.58% 

[59.46, 59.33, 59.85, 59.56, 59.43, 59.75, 59.8, 59.56, 59.41, 59.53, 59.58]

Epoch: 11
2024-03-09 21:16:53.066902 epoch: 11 step: 0 cls_loss= 0.91298 (153385 samples/sec)
2024-03-09 21:16:56.208888 epoch: 11 step: 100 cls_loss= 0.91283 (9548 samples/sec)
saving....
2024-03-09 21:17:00.043440------------------------------------------------------ Precision@1: 59.35% 

[59.46, 59.33, 59.85, 59.56, 59.43, 59.75, 59.8, 59.56, 59.41, 59.53, 59.58, 59.35]

Epoch: 12
2024-03-09 21:17:00.237464 epoch: 12 step: 0 cls_loss= 0.73540 (155500 samples/sec)
2024-03-09 21:17:03.384287 epoch: 12 step: 100 cls_loss= 0.76087 (9533 samples/sec)
saving....
2024-03-09 21:17:07.250055------------------------------------------------------ Precision@1: 59.35% 

[59.46, 59.33, 59.85, 59.56, 59.43, 59.75, 59.8, 59.56, 59.41, 59.53, 59.58, 59.35, 59.35]

Epoch: 13
2024-03-09 21:17:07.444814 epoch: 13 step: 0 cls_loss= 0.74898 (154706 samples/sec)
2024-03-09 21:17:10.647320 epoch: 13 step: 100 cls_loss= 0.76938 (9367 samples/sec)
saving....
2024-03-09 21:17:14.500811------------------------------------------------------ Precision@1: 59.31% 

[59.46, 59.33, 59.85, 59.56, 59.43, 59.75, 59.8, 59.56, 59.41, 59.53, 59.58, 59.35, 59.35, 59.31]

Epoch: 14
2024-03-09 21:17:14.714253 epoch: 14 step: 0 cls_loss= 0.84113 (141348 samples/sec)
2024-03-09 21:17:17.834038 epoch: 14 step: 100 cls_loss= 0.78089 (9616 samples/sec)
saving....
2024-03-09 21:17:21.881994------------------------------------------------------ Precision@1: 59.38% 

[59.46, 59.33, 59.85, 59.56, 59.43, 59.75, 59.8, 59.56, 59.41, 59.53, 59.58, 59.35, 59.35, 59.31, 59.38]

Epoch: 15
2024-03-09 21:17:22.081196 epoch: 15 step: 0 cls_loss= 0.64076 (151462 samples/sec)
2024-03-09 21:17:25.294186 epoch: 15 step: 100 cls_loss= 0.90614 (9337 samples/sec)
saving....
2024-03-09 21:17:29.114759------------------------------------------------------ Precision@1: 59.25% 

[59.46, 59.33, 59.85, 59.56, 59.43, 59.75, 59.8, 59.56, 59.41, 59.53, 59.58, 59.35, 59.35, 59.31, 59.38, 59.25]

Epoch: 16
2024-03-09 21:17:29.320656 epoch: 16 step: 0 cls_loss= 0.75006 (146532 samples/sec)
2024-03-09 21:17:32.391274 epoch: 16 step: 100 cls_loss= 0.68235 (9770 samples/sec)
saving....
2024-03-09 21:17:36.219029------------------------------------------------------ Precision@1: 58.77% 

[59.46, 59.33, 59.85, 59.56, 59.43, 59.75, 59.8, 59.56, 59.41, 59.53, 59.58, 59.35, 59.35, 59.31, 59.38, 59.25, 58.77]

Epoch: 17
2024-03-09 21:17:36.428395 epoch: 17 step: 0 cls_loss= 0.72889 (144047 samples/sec)
2024-03-09 21:17:39.625891 epoch: 17 step: 100 cls_loss= 0.75357 (9382 samples/sec)
saving....
2024-03-09 21:17:43.589612------------------------------------------------------ Precision@1: 59.15% 

[59.46, 59.33, 59.85, 59.56, 59.43, 59.75, 59.8, 59.56, 59.41, 59.53, 59.58, 59.35, 59.35, 59.31, 59.38, 59.25, 58.77, 59.15]

Epoch: 18
2024-03-09 21:17:43.809889 epoch: 18 step: 0 cls_loss= 0.67866 (136772 samples/sec)
2024-03-09 21:17:47.051810 epoch: 18 step: 100 cls_loss= 0.60653 (9254 samples/sec)
saving....
2024-03-09 21:17:50.902234------------------------------------------------------ Precision@1: 58.82% 

[59.46, 59.33, 59.85, 59.56, 59.43, 59.75, 59.8, 59.56, 59.41, 59.53, 59.58, 59.35, 59.35, 59.31, 59.38, 59.25, 58.77, 59.15, 58.82]

Epoch: 19
2024-03-09 21:17:51.120940 epoch: 19 step: 0 cls_loss= 0.78985 (137825 samples/sec)
2024-03-09 21:17:54.128409 epoch: 19 step: 100 cls_loss= 0.81340 (9975 samples/sec)
saving....
2024-03-09 21:17:57.847543------------------------------------------------------ Precision@1: 59.36% 

[59.46, 59.33, 59.85, 59.56, 59.43, 59.75, 59.8, 59.56, 59.41, 59.53, 59.58, 59.35, 59.35, 59.31, 59.38, 59.25, 58.77, 59.15, 58.82, 59.36]
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ exit

Script done on 2024-03-09 21:33:32+08:00 [COMMAND_EXIT_CODE="0"]
