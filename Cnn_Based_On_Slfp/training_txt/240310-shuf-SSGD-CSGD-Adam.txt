Script started on 2024-03-10 00:41:35+08:00 [TERM="xterm-256color" TTY="/dev/pts/1" COLUMNS="208" LINES="11"]
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ bash bash_train.sh 

training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 00:41:51.458563 epoch: 0 step: 0 cls_loss= 0.23432 (41999 samples/sec)
2024-03-10 00:42:00.854629 epoch: 0 step: 100 cls_loss= 0.30100 (3192 samples/sec)
saving....
2024-03-10 00:42:11.055507------------------------------------------------------ Precision@1: 65.64% 

[65.64]

Epoch: 1
2024-03-10 00:42:11.320671 epoch: 1 step: 0 cls_loss= 0.35211 (113823 samples/sec)
2024-03-10 00:42:20.677774 epoch: 1 step: 100 cls_loss= 0.29862 (3206 samples/sec)
saving....
2024-03-10 00:42:30.560038------------------------------------------------------ Precision@1: 65.75% 

[65.64, 65.75]

Epoch: 2
2024-03-10 00:42:30.820796 epoch: 2 step: 0 cls_loss= 0.31454 (115691 samples/sec)
2024-03-10 00:42:40.193702 epoch: 2 step: 100 cls_loss= 0.28987 (3201 samples/sec)
saving....
2024-03-10 00:42:50.098588------------------------------------------------------ Precision@1: 65.87% 

[65.64, 65.75, 65.87]

Epoch: 3
2024-03-10 00:42:50.352775 epoch: 3 step: 0 cls_loss= 0.33107 (118684 samples/sec)
2024-03-10 00:42:59.725654 epoch: 3 step: 100 cls_loss= 0.34991 (3201 samples/sec)
saving....
2024-03-10 00:43:09.674247------------------------------------------------------ Precision@1: 65.85% 

[65.64, 65.75, 65.87, 65.85]

Epoch: 4
2024-03-10 00:43:09.928164 epoch: 4 step: 0 cls_loss= 0.31115 (118687 samples/sec)
2024-03-10 00:43:19.281902 epoch: 4 step: 100 cls_loss= 0.36067 (3207 samples/sec)
saving....
2024-03-10 00:43:29.185009------------------------------------------------------ Precision@1: 65.91% 

[65.64, 65.75, 65.87, 65.85, 65.91]

Epoch: 5
2024-03-10 00:43:29.444331 epoch: 5 step: 0 cls_loss= 0.33068 (116306 samples/sec)
2024-03-10 00:43:38.830509 epoch: 5 step: 100 cls_loss= 0.34963 (3196 samples/sec)
saving....
2024-03-10 00:43:48.733185------------------------------------------------------ Precision@1: 65.74% 

[65.64, 65.75, 65.87, 65.85, 65.91, 65.74]

Epoch: 6
2024-03-10 00:43:48.995782 epoch: 6 step: 0 cls_loss= 0.31378 (114978 samples/sec)
2024-03-10 00:43:58.370473 epoch: 6 step: 100 cls_loss= 0.30744 (3200 samples/sec)
saving....
2024-03-10 00:44:08.279844------------------------------------------------------ Precision@1: 65.80% 

[65.64, 65.75, 65.87, 65.85, 65.91, 65.74, 65.8]

Epoch: 7
2024-03-10 00:44:08.532864 epoch: 7 step: 0 cls_loss= 0.30139 (119249 samples/sec)
2024-03-10 00:44:17.903153 epoch: 7 step: 100 cls_loss= 0.31740 (3202 samples/sec)
saving....
2024-03-10 00:44:27.805604------------------------------------------------------ Precision@1: 65.69% 

[65.64, 65.75, 65.87, 65.85, 65.91, 65.74, 65.8, 65.69]

Epoch: 8
2024-03-10 00:44:28.077260 epoch: 8 step: 0 cls_loss= 0.36170 (111036 samples/sec)
2024-03-10 00:44:37.446685 epoch: 8 step: 100 cls_loss= 0.23699 (3202 samples/sec)
saving....
2024-03-10 00:44:47.395891------------------------------------------------------ Precision@1: 65.86% 

[65.64, 65.75, 65.87, 65.85, 65.91, 65.74, 65.8, 65.69, 65.86]

Epoch: 9
2024-03-10 00:44:47.661071 epoch: 9 step: 0 cls_loss= 0.36908 (113788 samples/sec)
2024-03-10 00:44:57.040104 epoch: 9 step: 100 cls_loss= 0.33673 (3199 samples/sec)
saving....
2024-03-10 00:45:07.029199------------------------------------------------------ Precision@1: 65.77% 

[65.64, 65.75, 65.87, 65.85, 65.91, 65.74, 65.8, 65.69, 65.86, 65.77]

Epoch: 10
2024-03-10 00:45:07.295799 epoch: 10 step: 0 cls_loss= 0.30537 (113077 samples/sec)
2024-03-10 00:45:16.637468 epoch: 10 step: 100 cls_loss= 0.28751 (3212 samples/sec)
saving....
2024-03-10 00:45:26.613131------------------------------------------------------ Precision@1: 65.85% 

[65.64, 65.75, 65.87, 65.85, 65.91, 65.74, 65.8, 65.69, 65.86, 65.77, 65.85]

Epoch: 11
2024-03-10 00:45:26.879603 epoch: 11 step: 0 cls_loss= 0.35329 (113241 samples/sec)
2024-03-10 00:45:36.336852 epoch: 11 step: 100 cls_loss= 0.35068 (3172 samples/sec)
saving....
2024-03-10 00:45:46.365282------------------------------------------------------ Precision@1: 65.74% 

[65.64, 65.75, 65.87, 65.85, 65.91, 65.74, 65.8, 65.69, 65.86, 65.77, 65.85, 65.74]

Epoch: 12
2024-03-10 00:45:46.639030 epoch: 12 step: 0 cls_loss= 0.31688 (109990 samples/sec)
2024-03-10 00:45:56.022973 epoch: 12 step: 100 cls_loss= 0.35570 (3197 samples/sec)
saving....
2024-03-10 00:46:05.958467------------------------------------------------------ Precision@1: 65.59% 

[65.64, 65.75, 65.87, 65.85, 65.91, 65.74, 65.8, 65.69, 65.86, 65.77, 65.85, 65.74, 65.59]

Epoch: 13
2024-03-10 00:46:06.232929 epoch: 13 step: 0 cls_loss= 0.34917 (109933 samples/sec)
2024-03-10 00:46:15.591047 epoch: 13 step: 100 cls_loss= 0.32959 (3206 samples/sec)
saving....
2024-03-10 00:46:25.515150------------------------------------------------------ Precision@1: 65.96% 

[65.64, 65.75, 65.87, 65.85, 65.91, 65.74, 65.8, 65.69, 65.86, 65.77, 65.85, 65.74, 65.59, 65.96]

Epoch: 14
2024-03-10 00:46:25.779918 epoch: 14 step: 0 cls_loss= 0.41954 (113951 samples/sec)
2024-03-10 00:46:35.134373 epoch: 14 step: 100 cls_loss= 0.25802 (3207 samples/sec)
saving....
2024-03-10 00:46:45.044872------------------------------------------------------ Precision@1: 65.59% 

[65.64, 65.75, 65.87, 65.85, 65.91, 65.74, 65.8, 65.69, 65.86, 65.77, 65.85, 65.74, 65.59, 65.96, 65.59]

Epoch: 15
2024-03-10 00:46:45.316823 epoch: 15 step: 0 cls_loss= 0.30146 (110899 samples/sec)
2024-03-10 00:46:54.712842 epoch: 15 step: 100 cls_loss= 0.36273 (3193 samples/sec)
saving....
2024-03-10 00:47:04.737644------------------------------------------------------ Precision@1: 65.82% 

[65.64, 65.75, 65.87, 65.85, 65.91, 65.74, 65.8, 65.69, 65.86, 65.77, 65.85, 65.74, 65.59, 65.96, 65.59, 65.82]

Epoch: 16
2024-03-10 00:47:05.005758 epoch: 16 step: 0 cls_loss= 0.33132 (112483 samples/sec)
2024-03-10 00:47:14.351453 epoch: 16 step: 100 cls_loss= 0.30589 (3210 samples/sec)
saving....
2024-03-10 00:47:24.246713------------------------------------------------------ Precision@1: 65.67% 

[65.64, 65.75, 65.87, 65.85, 65.91, 65.74, 65.8, 65.69, 65.86, 65.77, 65.85, 65.74, 65.59, 65.96, 65.59, 65.82, 65.67]

Epoch: 17
2024-03-10 00:47:24.498588 epoch: 17 step: 0 cls_loss= 0.37439 (119873 samples/sec)
2024-03-10 00:47:33.859668 epoch: 17 step: 100 cls_loss= 0.34345 (3205 samples/sec)
saving....
2024-03-10 00:47:43.745928------------------------------------------------------ Precision@1: 65.77% 

[65.64, 65.75, 65.87, 65.85, 65.91, 65.74, 65.8, 65.69, 65.86, 65.77, 65.85, 65.74, 65.59, 65.96, 65.59, 65.82, 65.67, 65.77]

Epoch: 18
2024-03-10 00:47:44.008366 epoch: 18 step: 0 cls_loss= 0.31609 (115027 samples/sec)
2024-03-10 00:47:53.457869 epoch: 18 step: 100 cls_loss= 0.36732 (3175 samples/sec)
saving....
2024-03-10 00:48:03.347795------------------------------------------------------ Precision@1: 65.75% 

[65.64, 65.75, 65.87, 65.85, 65.91, 65.74, 65.8, 65.69, 65.86, 65.77, 65.85, 65.74, 65.59, 65.96, 65.59, 65.82, 65.67, 65.77, 65.75]

Epoch: 19
2024-03-10 00:48:03.616131 epoch: 19 step: 0 cls_loss= 0.36514 (112344 samples/sec)
2024-03-10 00:48:12.958918 epoch: 19 step: 100 cls_loss= 0.33407 (3211 samples/sec)
saving....
2024-03-10 00:48:22.899803------------------------------------------------------ Precision@1: 65.63% 

[65.64, 65.75, 65.87, 65.85, 65.91, 65.74, 65.8, 65.69, 65.86, 65.77, 65.85, 65.74, 65.59, 65.96, 65.59, 65.82, 65.67, 65.77, 65.75, 65.63]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 00:48:25.757507 epoch: 0 step: 0 cls_loss= 0.34984 (40676 samples/sec)
2024-03-10 00:48:35.110302 epoch: 0 step: 100 cls_loss= 0.33252 (3207 samples/sec)
saving....
2024-03-10 00:48:45.353624------------------------------------------------------ Precision@1: 65.91% 

[65.91]

Epoch: 1
2024-03-10 00:48:45.608776 epoch: 1 step: 0 cls_loss= 0.39565 (118279 samples/sec)
2024-03-10 00:48:54.961887 epoch: 1 step: 100 cls_loss= 0.37100 (3208 samples/sec)
saving....
2024-03-10 00:49:04.847059------------------------------------------------------ Precision@1: 65.85% 

[65.91, 65.85]

Epoch: 2
2024-03-10 00:49:05.109851 epoch: 2 step: 0 cls_loss= 0.34240 (114812 samples/sec)
2024-03-10 00:49:14.485678 epoch: 2 step: 100 cls_loss= 0.27795 (3200 samples/sec)
saving....
2024-03-10 00:49:24.437599------------------------------------------------------ Precision@1: 65.64% 

[65.91, 65.85, 65.64]

Epoch: 3
2024-03-10 00:49:24.720709 epoch: 3 step: 0 cls_loss= 0.33337 (106522 samples/sec)
2024-03-10 00:49:34.125258 epoch: 3 step: 100 cls_loss= 0.27963 (3190 samples/sec)
saving....
2024-03-10 00:49:44.160703------------------------------------------------------ Precision@1: 65.76% 

[65.91, 65.85, 65.64, 65.76]

Epoch: 4
2024-03-10 00:49:44.410189 epoch: 4 step: 0 cls_loss= 0.26931 (120975 samples/sec)
2024-03-10 00:49:53.767365 epoch: 4 step: 100 cls_loss= 0.34198 (3206 samples/sec)
saving....
2024-03-10 00:50:03.768421------------------------------------------------------ Precision@1: 65.91% 

[65.91, 65.85, 65.64, 65.76, 65.91]

Epoch: 5
2024-03-10 00:50:04.023685 epoch: 5 step: 0 cls_loss= 0.30642 (118292 samples/sec)
2024-03-10 00:50:13.393486 epoch: 5 step: 100 cls_loss= 0.36399 (3202 samples/sec)
saving....
2024-03-10 00:50:23.457570------------------------------------------------------ Precision@1: 65.61% 

[65.91, 65.85, 65.64, 65.76, 65.91, 65.61]

Epoch: 6
2024-03-10 00:50:23.739693 epoch: 6 step: 0 cls_loss= 0.34956 (106896 samples/sec)
2024-03-10 00:50:33.096477 epoch: 6 step: 100 cls_loss= 0.27039 (3206 samples/sec)
saving....
2024-03-10 00:50:43.142383------------------------------------------------------ Precision@1: 65.90% 

[65.91, 65.85, 65.64, 65.76, 65.91, 65.61, 65.9]

Epoch: 7
2024-03-10 00:50:43.406800 epoch: 7 step: 0 cls_loss= 0.33207 (114179 samples/sec)
2024-03-10 00:50:52.791515 epoch: 7 step: 100 cls_loss= 0.31183 (3197 samples/sec)
saving....
2024-03-10 00:51:02.751544------------------------------------------------------ Precision@1: 65.72% 

[65.91, 65.85, 65.64, 65.76, 65.91, 65.61, 65.9, 65.72]

Epoch: 8
2024-03-10 00:51:03.033275 epoch: 8 step: 0 cls_loss= 0.26311 (107040 samples/sec)
2024-03-10 00:51:12.423032 epoch: 8 step: 100 cls_loss= 0.31895 (3195 samples/sec)
saving....
2024-03-10 00:51:22.571966------------------------------------------------------ Precision@1: 65.56% 

[65.91, 65.85, 65.64, 65.76, 65.91, 65.61, 65.9, 65.72, 65.56]

Epoch: 9
2024-03-10 00:51:22.845853 epoch: 9 step: 0 cls_loss= 0.30522 (110115 samples/sec)
2024-03-10 00:51:32.223958 epoch: 9 step: 100 cls_loss= 0.36613 (3199 samples/sec)
saving....
2024-03-10 00:51:42.257963------------------------------------------------------ Precision@1: 65.60% 

[65.91, 65.85, 65.64, 65.76, 65.91, 65.61, 65.9, 65.72, 65.56, 65.6]

Epoch: 10
2024-03-10 00:51:42.524206 epoch: 10 step: 0 cls_loss= 0.31194 (113382 samples/sec)
2024-03-10 00:51:51.912785 epoch: 10 step: 100 cls_loss= 0.33115 (3196 samples/sec)
saving....
2024-03-10 00:52:01.942888------------------------------------------------------ Precision@1: 65.47% 

[65.91, 65.85, 65.64, 65.76, 65.91, 65.61, 65.9, 65.72, 65.56, 65.6, 65.47]

Epoch: 11
2024-03-10 00:52:02.199945 epoch: 11 step: 0 cls_loss= 0.30987 (117415 samples/sec)
2024-03-10 00:52:11.585422 epoch: 11 step: 100 cls_loss= 0.29924 (3197 samples/sec)
saving....
2024-03-10 00:52:21.572562------------------------------------------------------ Precision@1: 65.75% 

[65.91, 65.85, 65.64, 65.76, 65.91, 65.61, 65.9, 65.72, 65.56, 65.6, 65.47, 65.75]

Epoch: 12
2024-03-10 00:52:21.844996 epoch: 12 step: 0 cls_loss= 0.30830 (110701 samples/sec)
2024-03-10 00:52:31.222975 epoch: 12 step: 100 cls_loss= 0.33605 (3199 samples/sec)
saving....
2024-03-10 00:52:41.202695------------------------------------------------------ Precision@1: 65.66% 

[65.91, 65.85, 65.64, 65.76, 65.91, 65.61, 65.9, 65.72, 65.56, 65.6, 65.47, 65.75, 65.66]

Epoch: 13
2024-03-10 00:52:41.464089 epoch: 13 step: 0 cls_loss= 0.32242 (115462 samples/sec)
2024-03-10 00:52:50.850064 epoch: 13 step: 100 cls_loss= 0.34405 (3196 samples/sec)
saving....
2024-03-10 00:53:00.811142------------------------------------------------------ Precision@1: 65.62% 

[65.91, 65.85, 65.64, 65.76, 65.91, 65.61, 65.9, 65.72, 65.56, 65.6, 65.47, 65.75, 65.66, 65.62]

Epoch: 14
2024-03-10 00:53:01.078026 epoch: 14 step: 0 cls_loss= 0.36501 (112951 samples/sec)
2024-03-10 00:53:10.468655 epoch: 14 step: 100 cls_loss= 0.31014 (3195 samples/sec)
saving....
2024-03-10 00:53:20.426891------------------------------------------------------ Precision@1: 65.69% 

[65.91, 65.85, 65.64, 65.76, 65.91, 65.61, 65.9, 65.72, 65.56, 65.6, 65.47, 65.75, 65.66, 65.62, 65.69]

Epoch: 15
2024-03-10 00:53:20.690703 epoch: 15 step: 0 cls_loss= 0.30402 (114397 samples/sec)
2024-03-10 00:53:30.111948 epoch: 15 step: 100 cls_loss= 0.30175 (3184 samples/sec)
saving....
2024-03-10 00:53:40.213340------------------------------------------------------ Precision@1: 65.90% 

[65.91, 65.85, 65.64, 65.76, 65.91, 65.61, 65.9, 65.72, 65.56, 65.6, 65.47, 65.75, 65.66, 65.62, 65.69, 65.9]

Epoch: 16
2024-03-10 00:53:40.487970 epoch: 16 step: 0 cls_loss= 0.29565 (109924 samples/sec)
2024-03-10 00:53:49.869561 epoch: 16 step: 100 cls_loss= 0.36878 (3198 samples/sec)
saving....
2024-03-10 00:53:59.892461------------------------------------------------------ Precision@1: 65.53% 

[65.91, 65.85, 65.64, 65.76, 65.91, 65.61, 65.9, 65.72, 65.56, 65.6, 65.47, 65.75, 65.66, 65.62, 65.69, 65.9, 65.53]

Epoch: 17
2024-03-10 00:54:00.156606 epoch: 17 step: 0 cls_loss= 0.32821 (114245 samples/sec)
2024-03-10 00:54:09.602215 epoch: 17 step: 100 cls_loss= 0.28962 (3176 samples/sec)
saving....
2024-03-10 00:54:19.635021------------------------------------------------------ Precision@1: 65.85% 

[65.91, 65.85, 65.64, 65.76, 65.91, 65.61, 65.9, 65.72, 65.56, 65.6, 65.47, 65.75, 65.66, 65.62, 65.69, 65.9, 65.53, 65.85]

Epoch: 18
2024-03-10 00:54:19.891004 epoch: 18 step: 0 cls_loss= 0.34289 (117875 samples/sec)
2024-03-10 00:54:29.280197 epoch: 18 step: 100 cls_loss= 0.41211 (3195 samples/sec)
saving....
2024-03-10 00:54:39.245947------------------------------------------------------ Precision@1: 65.79% 

[65.91, 65.85, 65.64, 65.76, 65.91, 65.61, 65.9, 65.72, 65.56, 65.6, 65.47, 65.75, 65.66, 65.62, 65.69, 65.9, 65.53, 65.85, 65.79]

Epoch: 19
2024-03-10 00:54:39.511426 epoch: 19 step: 0 cls_loss= 0.30934 (113731 samples/sec)
2024-03-10 00:54:49.036516 epoch: 19 step: 100 cls_loss= 0.33052 (3150 samples/sec)
saving....
2024-03-10 00:54:59.021162------------------------------------------------------ Precision@1: 65.81% 

[65.91, 65.85, 65.64, 65.76, 65.91, 65.61, 65.9, 65.72, 65.56, 65.6, 65.47, 65.75, 65.66, 65.62, 65.69, 65.9, 65.53, 65.85, 65.79, 65.81]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 00:55:01.890277 epoch: 0 step: 0 cls_loss= 0.29330 (41664 samples/sec)
2024-03-10 00:55:11.240885 epoch: 0 step: 100 cls_loss= 0.30401 (3208 samples/sec)
saving....
2024-03-10 00:55:21.426247------------------------------------------------------ Precision@1: 65.98% 

[65.98]

Epoch: 1
2024-03-10 00:55:21.672149 epoch: 1 step: 0 cls_loss= 0.29673 (122797 samples/sec)
2024-03-10 00:55:31.025771 epoch: 1 step: 100 cls_loss= 0.30296 (3207 samples/sec)
saving....
2024-03-10 00:55:40.945202------------------------------------------------------ Precision@1: 65.63% 

[65.98, 65.63]

Epoch: 2
2024-03-10 00:55:41.207121 epoch: 2 step: 0 cls_loss= 0.41616 (115216 samples/sec)
2024-03-10 00:55:50.611932 epoch: 2 step: 100 cls_loss= 0.31490 (3190 samples/sec)
saving....
2024-03-10 00:56:00.555326------------------------------------------------------ Precision@1: 65.97% 

[65.98, 65.63, 65.97]

Epoch: 3
2024-03-10 00:56:00.812011 epoch: 3 step: 0 cls_loss= 0.27995 (117601 samples/sec)
2024-03-10 00:56:10.155985 epoch: 3 step: 100 cls_loss= 0.32936 (3211 samples/sec)
saving....
2024-03-10 00:56:20.084463------------------------------------------------------ Precision@1: 65.58% 

[65.98, 65.63, 65.97, 65.58]

Epoch: 4
2024-03-10 00:56:20.343280 epoch: 4 step: 0 cls_loss= 0.33329 (116604 samples/sec)
2024-03-10 00:56:29.670688 epoch: 4 step: 100 cls_loss= 0.33141 (3216 samples/sec)
saving....
2024-03-10 00:56:39.612243------------------------------------------------------ Precision@1: 65.81% 

[65.98, 65.63, 65.97, 65.58, 65.81]

Epoch: 5
2024-03-10 00:56:39.872834 epoch: 5 step: 0 cls_loss= 0.29349 (115820 samples/sec)
2024-03-10 00:56:49.217169 epoch: 5 step: 100 cls_loss= 0.41041 (3211 samples/sec)
saving....
2024-03-10 00:56:59.177750------------------------------------------------------ Precision@1: 65.93% 

[65.98, 65.63, 65.97, 65.58, 65.81, 65.93]

Epoch: 6
2024-03-10 00:56:59.449091 epoch: 6 step: 0 cls_loss= 0.32094 (111105 samples/sec)
2024-03-10 00:57:08.820454 epoch: 6 step: 100 cls_loss= 0.31507 (3201 samples/sec)
saving....
2024-03-10 00:57:18.719643------------------------------------------------------ Precision@1: 65.89% 

[65.98, 65.63, 65.97, 65.58, 65.81, 65.93, 65.89]

Epoch: 7
2024-03-10 00:57:18.975369 epoch: 7 step: 0 cls_loss= 0.23684 (118072 samples/sec)
2024-03-10 00:57:28.452504 epoch: 7 step: 100 cls_loss= 0.34003 (3166 samples/sec)
saving....
2024-03-10 00:57:38.536696------------------------------------------------------ Precision@1: 65.98% 

[65.98, 65.63, 65.97, 65.58, 65.81, 65.93, 65.89, 65.98]

Epoch: 8
2024-03-10 00:57:38.814197 epoch: 8 step: 0 cls_loss= 0.29825 (108735 samples/sec)
2024-03-10 00:57:48.197251 epoch: 8 step: 100 cls_loss= 0.32135 (3197 samples/sec)
saving....
2024-03-10 00:57:58.077386------------------------------------------------------ Precision@1: 65.81% 

[65.98, 65.63, 65.97, 65.58, 65.81, 65.93, 65.89, 65.98, 65.81]

Epoch: 9
2024-03-10 00:57:58.341434 epoch: 9 step: 0 cls_loss= 0.32689 (114343 samples/sec)
2024-03-10 00:58:07.706800 epoch: 9 step: 100 cls_loss= 0.28896 (3203 samples/sec)
saving....
2024-03-10 00:58:17.714545------------------------------------------------------ Precision@1: 65.90% 

[65.98, 65.63, 65.97, 65.58, 65.81, 65.93, 65.89, 65.98, 65.81, 65.9]

Epoch: 10
2024-03-10 00:58:17.965568 epoch: 10 step: 0 cls_loss= 0.34157 (120289 samples/sec)
2024-03-10 00:58:27.356272 epoch: 10 step: 100 cls_loss= 0.26970 (3195 samples/sec)
saving....
2024-03-10 00:58:37.314776------------------------------------------------------ Precision@1: 65.90% 

[65.98, 65.63, 65.97, 65.58, 65.81, 65.93, 65.89, 65.98, 65.81, 65.9, 65.9]

Epoch: 11
2024-03-10 00:58:37.579857 epoch: 11 step: 0 cls_loss= 0.33408 (113823 samples/sec)
2024-03-10 00:58:46.991355 epoch: 11 step: 100 cls_loss= 0.33098 (3188 samples/sec)
saving....
2024-03-10 00:58:56.939729------------------------------------------------------ Precision@1: 65.59% 

[65.98, 65.63, 65.97, 65.58, 65.81, 65.93, 65.89, 65.98, 65.81, 65.9, 65.9, 65.59]

Epoch: 12
2024-03-10 00:58:57.209975 epoch: 12 step: 0 cls_loss= 0.31456 (111689 samples/sec)
2024-03-10 00:59:06.607691 epoch: 12 step: 100 cls_loss= 0.33736 (3192 samples/sec)
saving....
2024-03-10 00:59:16.565869------------------------------------------------------ Precision@1: 65.76% 

[65.98, 65.63, 65.97, 65.58, 65.81, 65.93, 65.89, 65.98, 65.81, 65.9, 65.9, 65.59, 65.76]

Epoch: 13
2024-03-10 00:59:16.837034 epoch: 13 step: 0 cls_loss= 0.28257 (111121 samples/sec)
2024-03-10 00:59:26.192918 epoch: 13 step: 100 cls_loss= 0.36971 (3207 samples/sec)
saving....
2024-03-10 00:59:36.039169------------------------------------------------------ Precision@1: 65.77% 

[65.98, 65.63, 65.97, 65.58, 65.81, 65.93, 65.89, 65.98, 65.81, 65.9, 65.9, 65.59, 65.76, 65.77]

Epoch: 14
2024-03-10 00:59:36.289561 epoch: 14 step: 0 cls_loss= 0.34359 (120576 samples/sec)
2024-03-10 00:59:45.750235 epoch: 14 step: 100 cls_loss= 0.35577 (3171 samples/sec)
saving....
2024-03-10 00:59:55.720576------------------------------------------------------ Precision@1: 65.74% 

[65.98, 65.63, 65.97, 65.58, 65.81, 65.93, 65.89, 65.98, 65.81, 65.9, 65.9, 65.59, 65.76, 65.77, 65.74]

Epoch: 15
2024-03-10 00:59:55.993896 epoch: 15 step: 0 cls_loss= 0.40568 (110301 samples/sec)
2024-03-10 01:00:05.394145 epoch: 15 step: 100 cls_loss= 0.34859 (3192 samples/sec)
saving....
2024-03-10 01:00:15.312613------------------------------------------------------ Precision@1: 65.75% 

[65.98, 65.63, 65.97, 65.58, 65.81, 65.93, 65.89, 65.98, 65.81, 65.9, 65.9, 65.59, 65.76, 65.77, 65.74, 65.75]

Epoch: 16
2024-03-10 01:00:15.586614 epoch: 16 step: 0 cls_loss= 0.29981 (110081 samples/sec)
2024-03-10 01:00:24.924565 epoch: 16 step: 100 cls_loss= 0.30557 (3213 samples/sec)
saving....
2024-03-10 01:00:34.790458------------------------------------------------------ Precision@1: 65.72% 

[65.98, 65.63, 65.97, 65.58, 65.81, 65.93, 65.89, 65.98, 65.81, 65.9, 65.9, 65.59, 65.76, 65.77, 65.74, 65.75, 65.72]

Epoch: 17
2024-03-10 01:00:35.038047 epoch: 17 step: 0 cls_loss= 0.25908 (121946 samples/sec)
2024-03-10 01:00:44.365266 epoch: 17 step: 100 cls_loss= 0.30752 (3217 samples/sec)
saving....
2024-03-10 01:00:54.243979------------------------------------------------------ Precision@1: 65.87% 

[65.98, 65.63, 65.97, 65.58, 65.81, 65.93, 65.89, 65.98, 65.81, 65.9, 65.9, 65.59, 65.76, 65.77, 65.74, 65.75, 65.72, 65.87]

Epoch: 18
2024-03-10 01:00:54.517916 epoch: 18 step: 0 cls_loss= 0.33881 (110124 samples/sec)
2024-03-10 01:01:03.838579 epoch: 18 step: 100 cls_loss= 0.31273 (3219 samples/sec)
saving....
2024-03-10 01:01:13.844942------------------------------------------------------ Precision@1: 65.65% 

[65.98, 65.63, 65.97, 65.58, 65.81, 65.93, 65.89, 65.98, 65.81, 65.9, 65.9, 65.59, 65.76, 65.77, 65.74, 65.75, 65.72, 65.87, 65.65]

Epoch: 19
2024-03-10 01:01:14.107282 epoch: 19 step: 0 cls_loss= 0.30126 (115099 samples/sec)
2024-03-10 01:01:23.490889 epoch: 19 step: 100 cls_loss= 0.29895 (3197 samples/sec)
saving....
2024-03-10 01:01:33.473376------------------------------------------------------ Precision@1: 65.91% 

[65.98, 65.63, 65.97, 65.58, 65.81, 65.93, 65.89, 65.98, 65.81, 65.9, 65.9, 65.59, 65.76, 65.77, 65.74, 65.75, 65.72, 65.87, 65.65, 65.91]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 01:01:36.347860 epoch: 0 step: 0 cls_loss= 0.28620 (40683 samples/sec)
2024-03-10 01:01:45.688458 epoch: 0 step: 100 cls_loss= 0.28368 (3211 samples/sec)
saving....
2024-03-10 01:01:55.889612------------------------------------------------------ Precision@1: 65.73% 

[65.73]

Epoch: 1
2024-03-10 01:01:56.150607 epoch: 1 step: 0 cls_loss= 0.39625 (115593 samples/sec)
2024-03-10 01:02:05.504997 epoch: 1 step: 100 cls_loss= 0.32174 (3207 samples/sec)
saving....
2024-03-10 01:02:15.381443------------------------------------------------------ Precision@1: 65.90% 

[65.73, 65.9]

Epoch: 2
2024-03-10 01:02:15.645354 epoch: 2 step: 0 cls_loss= 0.31934 (114325 samples/sec)
2024-03-10 01:02:24.997830 epoch: 2 step: 100 cls_loss= 0.29933 (3208 samples/sec)
saving....
2024-03-10 01:02:34.936902------------------------------------------------------ Precision@1: 65.84% 

[65.73, 65.9, 65.84]

Epoch: 3
2024-03-10 01:02:35.214830 epoch: 3 step: 0 cls_loss= 0.31060 (108480 samples/sec)
2024-03-10 01:02:44.574931 epoch: 3 step: 100 cls_loss= 0.31579 (3205 samples/sec)
saving....
2024-03-10 01:02:54.519332------------------------------------------------------ Precision@1: 65.88% 

[65.73, 65.9, 65.84, 65.88]

Epoch: 4
2024-03-10 01:02:54.789623 epoch: 4 step: 0 cls_loss= 0.33225 (111630 samples/sec)
2024-03-10 01:03:04.165985 epoch: 4 step: 100 cls_loss= 0.40838 (3200 samples/sec)
saving....
2024-03-10 01:03:14.109276------------------------------------------------------ Precision@1: 65.59% 

[65.73, 65.9, 65.84, 65.88, 65.59]

Epoch: 5
2024-03-10 01:03:14.378498 epoch: 5 step: 0 cls_loss= 0.35002 (111986 samples/sec)
2024-03-10 01:03:23.744005 epoch: 5 step: 100 cls_loss= 0.34131 (3203 samples/sec)
saving....
2024-03-10 01:03:33.697864------------------------------------------------------ Precision@1: 65.74% 

[65.73, 65.9, 65.84, 65.88, 65.59, 65.74]

Epoch: 6
2024-03-10 01:03:33.968180 epoch: 6 step: 0 cls_loss= 0.29171 (111563 samples/sec)
2024-03-10 01:03:43.335483 epoch: 6 step: 100 cls_loss= 0.36508 (3203 samples/sec)
saving....
2024-03-10 01:03:53.263770------------------------------------------------------ Precision@1: 65.70% 

[65.73, 65.9, 65.84, 65.88, 65.59, 65.74, 65.7]

Epoch: 7
2024-03-10 01:03:53.526974 epoch: 7 step: 0 cls_loss= 0.28616 (114664 samples/sec)
2024-03-10 01:04:02.915350 epoch: 7 step: 100 cls_loss= 0.32094 (3195 samples/sec)
saving....
2024-03-10 01:04:12.977035------------------------------------------------------ Precision@1: 65.66% 

[65.73, 65.9, 65.84, 65.88, 65.59, 65.74, 65.7, 65.66]

Epoch: 8
2024-03-10 01:04:13.239927 epoch: 8 step: 0 cls_loss= 0.38923 (114713 samples/sec)
2024-03-10 01:04:22.606690 epoch: 8 step: 100 cls_loss= 0.33746 (3203 samples/sec)
saving....
2024-03-10 01:04:32.525778------------------------------------------------------ Precision@1: 65.76% 

[65.73, 65.9, 65.84, 65.88, 65.59, 65.74, 65.7, 65.66, 65.76]

Epoch: 9
2024-03-10 01:04:32.795491 epoch: 9 step: 0 cls_loss= 0.32683 (111734 samples/sec)
2024-03-10 01:04:42.191613 epoch: 9 step: 100 cls_loss= 0.33319 (3193 samples/sec)
saving....
2024-03-10 01:04:52.141183------------------------------------------------------ Precision@1: 65.68% 

[65.73, 65.9, 65.84, 65.88, 65.59, 65.74, 65.7, 65.66, 65.76, 65.68]

Epoch: 10
2024-03-10 01:04:52.397656 epoch: 10 step: 0 cls_loss= 0.39566 (117665 samples/sec)
2024-03-10 01:05:01.775254 epoch: 10 step: 100 cls_loss= 0.30491 (3199 samples/sec)
saving....
2024-03-10 01:05:11.688702------------------------------------------------------ Precision@1: 65.76% 

[65.73, 65.9, 65.84, 65.88, 65.59, 65.74, 65.7, 65.66, 65.76, 65.68, 65.76]

Epoch: 11
2024-03-10 01:05:11.951269 epoch: 11 step: 0 cls_loss= 0.30353 (114966 samples/sec)
2024-03-10 01:05:21.304666 epoch: 11 step: 100 cls_loss= 0.37209 (3208 samples/sec)
saving....
2024-03-10 01:05:31.207069------------------------------------------------------ Precision@1: 65.66% 

[65.73, 65.9, 65.84, 65.88, 65.59, 65.74, 65.7, 65.66, 65.76, 65.68, 65.76, 65.66]

Epoch: 12
2024-03-10 01:05:31.467065 epoch: 12 step: 0 cls_loss= 0.26928 (115925 samples/sec)
2024-03-10 01:05:40.791564 epoch: 12 step: 100 cls_loss= 0.34570 (3217 samples/sec)
saving....
2024-03-10 01:05:50.742050------------------------------------------------------ Precision@1: 65.77% 

[65.73, 65.9, 65.84, 65.88, 65.59, 65.74, 65.7, 65.66, 65.76, 65.68, 65.76, 65.66, 65.77]

Epoch: 13
2024-03-10 01:05:51.000907 epoch: 13 step: 0 cls_loss= 0.29148 (116597 samples/sec)
2024-03-10 01:06:00.379769 epoch: 13 step: 100 cls_loss= 0.34839 (3199 samples/sec)
saving....
2024-03-10 01:06:10.299739------------------------------------------------------ Precision@1: 65.76% 

[65.73, 65.9, 65.84, 65.88, 65.59, 65.74, 65.7, 65.66, 65.76, 65.68, 65.76, 65.66, 65.77, 65.76]

Epoch: 14
2024-03-10 01:06:10.567339 epoch: 14 step: 0 cls_loss= 0.31153 (112741 samples/sec)
2024-03-10 01:06:19.963100 epoch: 14 step: 100 cls_loss= 0.36468 (3193 samples/sec)
saving....
2024-03-10 01:06:29.911102------------------------------------------------------ Precision@1: 65.85% 

[65.73, 65.9, 65.84, 65.88, 65.59, 65.74, 65.7, 65.66, 65.76, 65.68, 65.76, 65.66, 65.77, 65.76, 65.85]

Epoch: 15
2024-03-10 01:06:30.175068 epoch: 15 step: 0 cls_loss= 0.29612 (114388 samples/sec)
2024-03-10 01:06:39.534488 epoch: 15 step: 100 cls_loss= 0.35823 (3205 samples/sec)
saving....
2024-03-10 01:06:49.495586------------------------------------------------------ Precision@1: 65.67% 

[65.73, 65.9, 65.84, 65.88, 65.59, 65.74, 65.7, 65.66, 65.76, 65.68, 65.76, 65.66, 65.77, 65.76, 65.85, 65.67]

Epoch: 16
2024-03-10 01:06:49.751397 epoch: 16 step: 0 cls_loss= 0.27415 (117959 samples/sec)
2024-03-10 01:06:59.133390 epoch: 16 step: 100 cls_loss= 0.28237 (3198 samples/sec)
saving....
2024-03-10 01:07:09.048142------------------------------------------------------ Precision@1: 65.81% 

[65.73, 65.9, 65.84, 65.88, 65.59, 65.74, 65.7, 65.66, 65.76, 65.68, 65.76, 65.66, 65.77, 65.76, 65.85, 65.67, 65.81]

Epoch: 17
2024-03-10 01:07:09.320722 epoch: 17 step: 0 cls_loss= 0.30607 (110713 samples/sec)
2024-03-10 01:07:18.702259 epoch: 17 step: 100 cls_loss= 0.40516 (3198 samples/sec)
saving....
2024-03-10 01:07:28.643269------------------------------------------------------ Precision@1: 65.91% 

[65.73, 65.9, 65.84, 65.88, 65.59, 65.74, 65.7, 65.66, 65.76, 65.68, 65.76, 65.66, 65.77, 65.76, 65.85, 65.67, 65.81, 65.91]

Epoch: 18
2024-03-10 01:07:28.918998 epoch: 18 step: 0 cls_loss= 0.32670 (109404 samples/sec)
2024-03-10 01:07:38.303926 epoch: 18 step: 100 cls_loss= 0.30762 (3197 samples/sec)
saving....
2024-03-10 01:07:48.265112------------------------------------------------------ Precision@1: 65.67% 

[65.73, 65.9, 65.84, 65.88, 65.59, 65.74, 65.7, 65.66, 65.76, 65.68, 65.76, 65.66, 65.77, 65.76, 65.85, 65.67, 65.81, 65.91, 65.67]

Epoch: 19
2024-03-10 01:07:48.538124 epoch: 19 step: 0 cls_loss= 0.25742 (110510 samples/sec)
2024-03-10 01:07:57.919238 epoch: 19 step: 100 cls_loss= 0.33308 (3198 samples/sec)
saving....
2024-03-10 01:08:07.953102------------------------------------------------------ Precision@1: 65.59% 

[65.73, 65.9, 65.84, 65.88, 65.59, 65.74, 65.7, 65.66, 65.76, 65.68, 65.76, 65.66, 65.77, 65.76, 65.85, 65.67, 65.81, 65.91, 65.67, 65.59]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 01:08:10.771886 epoch: 0 step: 0 cls_loss= 0.31932 (43234 samples/sec)
2024-03-10 01:08:20.106934 epoch: 0 step: 100 cls_loss= 0.35430 (3213 samples/sec)
saving....
2024-03-10 01:08:30.303182------------------------------------------------------ Precision@1: 65.77% 

[65.77]

Epoch: 1
2024-03-10 01:08:30.547035 epoch: 1 step: 0 cls_loss= 0.32456 (123706 samples/sec)
2024-03-10 01:08:39.952614 epoch: 1 step: 100 cls_loss= 0.33897 (3190 samples/sec)
saving....
2024-03-10 01:08:49.923787------------------------------------------------------ Precision@1: 65.67% 

[65.77, 65.67]

Epoch: 2
2024-03-10 01:08:50.196032 epoch: 2 step: 0 cls_loss= 0.33413 (110837 samples/sec)
2024-03-10 01:08:59.548629 epoch: 2 step: 100 cls_loss= 0.30256 (3208 samples/sec)
saving....
2024-03-10 01:09:09.450823------------------------------------------------------ Precision@1: 65.68% 

[65.77, 65.67, 65.68]

Epoch: 3
2024-03-10 01:09:09.708680 epoch: 3 step: 0 cls_loss= 0.28845 (116974 samples/sec)
2024-03-10 01:09:19.140931 epoch: 3 step: 100 cls_loss= 0.29775 (3181 samples/sec)
saving....
2024-03-10 01:09:29.077160------------------------------------------------------ Precision@1: 65.81% 

[65.77, 65.67, 65.68, 65.81]

Epoch: 4
2024-03-10 01:09:29.345708 epoch: 4 step: 0 cls_loss= 0.37065 (112347 samples/sec)
2024-03-10 01:09:38.725398 epoch: 4 step: 100 cls_loss= 0.30689 (3199 samples/sec)
saving....
2024-03-10 01:09:48.685821------------------------------------------------------ Precision@1: 65.81% 

[65.77, 65.67, 65.68, 65.81, 65.81]

Epoch: 5
2024-03-10 01:09:48.966590 epoch: 5 step: 0 cls_loss= 0.31954 (107415 samples/sec)
2024-03-10 01:09:58.349103 epoch: 5 step: 100 cls_loss= 0.30720 (3198 samples/sec)
saving....
2024-03-10 01:10:08.313763------------------------------------------------------ Precision@1: 65.83% 

[65.77, 65.67, 65.68, 65.81, 65.81, 65.83]

Epoch: 6
2024-03-10 01:10:08.584713 epoch: 6 step: 0 cls_loss= 0.29499 (111347 samples/sec)
2024-03-10 01:10:17.926676 epoch: 6 step: 100 cls_loss= 0.25743 (3211 samples/sec)
saving....
2024-03-10 01:10:27.858793------------------------------------------------------ Precision@1: 65.87% 

[65.77, 65.67, 65.68, 65.81, 65.81, 65.83, 65.87]

Epoch: 7
2024-03-10 01:10:28.131379 epoch: 7 step: 0 cls_loss= 0.29695 (110732 samples/sec)
2024-03-10 01:10:37.538777 epoch: 7 step: 100 cls_loss= 0.32134 (3189 samples/sec)
saving....
2024-03-10 01:10:47.498379------------------------------------------------------ Precision@1: 65.94% 

[65.77, 65.67, 65.68, 65.81, 65.81, 65.83, 65.87, 65.94]

Epoch: 8
2024-03-10 01:10:47.764038 epoch: 8 step: 0 cls_loss= 0.32044 (113595 samples/sec)
2024-03-10 01:10:57.143338 epoch: 8 step: 100 cls_loss= 0.31906 (3199 samples/sec)
saving....
2024-03-10 01:11:07.088250------------------------------------------------------ Precision@1: 65.97% 

[65.77, 65.67, 65.68, 65.81, 65.81, 65.83, 65.87, 65.94, 65.97]

Epoch: 9
2024-03-10 01:11:07.346788 epoch: 9 step: 0 cls_loss= 0.36829 (116656 samples/sec)
2024-03-10 01:11:16.740581 epoch: 9 step: 100 cls_loss= 0.31152 (3194 samples/sec)
saving....
2024-03-10 01:11:26.724237------------------------------------------------------ Precision@1: 65.77% 

[65.77, 65.67, 65.68, 65.81, 65.81, 65.83, 65.87, 65.94, 65.97, 65.77]

Epoch: 10
2024-03-10 01:11:26.983071 epoch: 10 step: 0 cls_loss= 0.27077 (116548 samples/sec)
2024-03-10 01:11:36.339836 epoch: 10 step: 100 cls_loss= 0.35197 (3206 samples/sec)
saving....
2024-03-10 01:11:46.279799------------------------------------------------------ Precision@1: 65.71% 

[65.77, 65.67, 65.68, 65.81, 65.81, 65.83, 65.87, 65.94, 65.97, 65.77, 65.71]

Epoch: 11
2024-03-10 01:11:46.537005 epoch: 11 step: 0 cls_loss= 0.40864 (117309 samples/sec)
2024-03-10 01:11:55.906174 epoch: 11 step: 100 cls_loss= 0.35697 (3202 samples/sec)
saving....
2024-03-10 01:12:05.945911------------------------------------------------------ Precision@1: 65.56% 

[65.77, 65.67, 65.68, 65.81, 65.81, 65.83, 65.87, 65.94, 65.97, 65.77, 65.71, 65.56]

Epoch: 12
2024-03-10 01:12:06.203632 epoch: 12 step: 0 cls_loss= 0.33493 (117059 samples/sec)
2024-03-10 01:12:15.554536 epoch: 12 step: 100 cls_loss= 0.32992 (3208 samples/sec)
saving....
2024-03-10 01:12:25.568999------------------------------------------------------ Precision@1: 65.80% 

[65.77, 65.67, 65.68, 65.81, 65.81, 65.83, 65.87, 65.94, 65.97, 65.77, 65.71, 65.56, 65.8]

Epoch: 13
2024-03-10 01:12:25.820812 epoch: 13 step: 0 cls_loss= 0.33084 (119812 samples/sec)
2024-03-10 01:12:35.178705 epoch: 13 step: 100 cls_loss= 0.31585 (3206 samples/sec)
saving....
2024-03-10 01:12:45.136608------------------------------------------------------ Precision@1: 65.64% 

[65.77, 65.67, 65.68, 65.81, 65.81, 65.83, 65.87, 65.94, 65.97, 65.77, 65.71, 65.56, 65.8, 65.64]

Epoch: 14
2024-03-10 01:12:45.398423 epoch: 14 step: 0 cls_loss= 0.30244 (115061 samples/sec)
2024-03-10 01:12:54.767182 epoch: 14 step: 100 cls_loss= 0.36229 (3202 samples/sec)
saving....
2024-03-10 01:13:04.712190------------------------------------------------------ Precision@1: 65.79% 

[65.77, 65.67, 65.68, 65.81, 65.81, 65.83, 65.87, 65.94, 65.97, 65.77, 65.71, 65.56, 65.8, 65.64, 65.79]

Epoch: 15
2024-03-10 01:13:04.980131 epoch: 15 step: 0 cls_loss= 0.30567 (112567 samples/sec)
2024-03-10 01:13:14.356118 epoch: 15 step: 100 cls_loss= 0.30112 (3200 samples/sec)
saving....
2024-03-10 01:13:24.311097------------------------------------------------------ Precision@1: 65.61% 

[65.77, 65.67, 65.68, 65.81, 65.81, 65.83, 65.87, 65.94, 65.97, 65.77, 65.71, 65.56, 65.8, 65.64, 65.79, 65.61]

Epoch: 16
2024-03-10 01:13:24.586888 epoch: 16 step: 0 cls_loss= 0.34594 (109268 samples/sec)
2024-03-10 01:13:34.012941 epoch: 16 step: 100 cls_loss= 0.29196 (3183 samples/sec)
saving....
2024-03-10 01:13:43.963878------------------------------------------------------ Precision@1: 65.86% 

[65.77, 65.67, 65.68, 65.81, 65.81, 65.83, 65.87, 65.94, 65.97, 65.77, 65.71, 65.56, 65.8, 65.64, 65.79, 65.61, 65.86]

Epoch: 17
2024-03-10 01:13:44.210350 epoch: 17 step: 0 cls_loss= 0.29408 (122555 samples/sec)
2024-03-10 01:13:53.563466 epoch: 17 step: 100 cls_loss= 0.30225 (3208 samples/sec)
saving....
2024-03-10 01:14:03.546605------------------------------------------------------ Precision@1: 65.86% 

[65.77, 65.67, 65.68, 65.81, 65.81, 65.83, 65.87, 65.94, 65.97, 65.77, 65.71, 65.56, 65.8, 65.64, 65.79, 65.61, 65.86, 65.86]

Epoch: 18
2024-03-10 01:14:03.809497 epoch: 18 step: 0 cls_loss= 0.38703 (114749 samples/sec)
2024-03-10 01:14:13.233904 epoch: 18 step: 100 cls_loss= 0.33092 (3183 samples/sec)
saving....
2024-03-10 01:14:23.193406------------------------------------------------------ Precision@1: 65.72% 

[65.77, 65.67, 65.68, 65.81, 65.81, 65.83, 65.87, 65.94, 65.97, 65.77, 65.71, 65.56, 65.8, 65.64, 65.79, 65.61, 65.86, 65.86, 65.72]

Epoch: 19
2024-03-10 01:14:23.446056 epoch: 19 step: 0 cls_loss= 0.29124 (119466 samples/sec)
2024-03-10 01:14:32.825896 epoch: 19 step: 100 cls_loss= 0.38676 (3198 samples/sec)
saving....
2024-03-10 01:14:42.738959------------------------------------------------------ Precision@1: 65.80% 

[65.77, 65.67, 65.68, 65.81, 65.81, 65.83, 65.87, 65.94, 65.97, 65.77, 65.71, 65.56, 65.8, 65.64, 65.79, 65.61, 65.86, 65.86, 65.72, 65.8]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 01:14:45.581980 epoch: 0 step: 0 cls_loss= 0.34885 (42530 samples/sec)
2024-03-10 01:14:54.921993 epoch: 0 step: 100 cls_loss= 0.32027 (3212 samples/sec)
saving....
2024-03-10 01:15:05.128195------------------------------------------------------ Precision@1: 65.67% 

[65.67]

Epoch: 1
2024-03-10 01:15:05.394104 epoch: 1 step: 0 cls_loss= 0.29329 (113482 samples/sec)
2024-03-10 01:15:14.755286 epoch: 1 step: 100 cls_loss= 0.33407 (3205 samples/sec)
saving....
2024-03-10 01:15:24.650027------------------------------------------------------ Precision@1: 65.77% 

[65.67, 65.77]

Epoch: 2
2024-03-10 01:15:24.924850 epoch: 2 step: 0 cls_loss= 0.34008 (109793 samples/sec)
2024-03-10 01:15:34.320447 epoch: 2 step: 100 cls_loss= 0.32233 (3193 samples/sec)
saving....
2024-03-10 01:15:44.444179------------------------------------------------------ Precision@1: 65.72% 

[65.67, 65.77, 65.72]

Epoch: 3
2024-03-10 01:15:44.704929 epoch: 3 step: 0 cls_loss= 0.31602 (115685 samples/sec)
2024-03-10 01:15:54.067348 epoch: 3 step: 100 cls_loss= 0.33122 (3204 samples/sec)
saving....
2024-03-10 01:16:04.033400------------------------------------------------------ Precision@1: 65.81% 

[65.67, 65.77, 65.72, 65.81]

Epoch: 4
2024-03-10 01:16:04.297427 epoch: 4 step: 0 cls_loss= 0.35538 (114300 samples/sec)
2024-03-10 01:16:13.654141 epoch: 4 step: 100 cls_loss= 0.27055 (3206 samples/sec)
saving....
2024-03-10 01:16:23.599282------------------------------------------------------ Precision@1: 66.04% 

[65.67, 65.77, 65.72, 65.81, 66.04]

Epoch: 5
2024-03-10 01:16:23.865547 epoch: 5 step: 0 cls_loss= 0.42063 (113380 samples/sec)
2024-03-10 01:16:33.246512 epoch: 5 step: 100 cls_loss= 0.25528 (3198 samples/sec)
saving....
2024-03-10 01:16:43.223839------------------------------------------------------ Precision@1: 65.75% 

[65.67, 65.77, 65.72, 65.81, 66.04, 65.75]

Epoch: 6
2024-03-10 01:16:43.483369 epoch: 6 step: 0 cls_loss= 0.38003 (116327 samples/sec)
2024-03-10 01:16:52.881457 epoch: 6 step: 100 cls_loss= 0.39755 (3192 samples/sec)
saving....
2024-03-10 01:17:02.824840------------------------------------------------------ Precision@1: 65.67% 

[65.67, 65.77, 65.72, 65.81, 66.04, 65.75, 65.67]

Epoch: 7
2024-03-10 01:17:03.074761 epoch: 7 step: 0 cls_loss= 0.31949 (120791 samples/sec)
2024-03-10 01:17:12.483363 epoch: 7 step: 100 cls_loss= 0.30848 (3189 samples/sec)
saving....
2024-03-10 01:17:22.432077------------------------------------------------------ Precision@1: 65.81% 

[65.67, 65.77, 65.72, 65.81, 66.04, 65.75, 65.67, 65.81]

Epoch: 8
2024-03-10 01:17:22.719740 epoch: 8 step: 0 cls_loss= 0.33662 (104694 samples/sec)
2024-03-10 01:17:32.109980 epoch: 8 step: 100 cls_loss= 0.32526 (3195 samples/sec)
saving....
2024-03-10 01:17:42.040084------------------------------------------------------ Precision@1: 65.69% 

[65.67, 65.77, 65.72, 65.81, 66.04, 65.75, 65.67, 65.81, 65.69]

Epoch: 9
2024-03-10 01:17:42.294031 epoch: 9 step: 0 cls_loss= 0.28386 (118898 samples/sec)
2024-03-10 01:17:51.689090 epoch: 9 step: 100 cls_loss= 0.22288 (3193 samples/sec)
saving....
2024-03-10 01:18:01.615778------------------------------------------------------ Precision@1: 65.77% 

[65.67, 65.77, 65.72, 65.81, 66.04, 65.75, 65.67, 65.81, 65.69, 65.77]

Epoch: 10
2024-03-10 01:18:01.893737 epoch: 10 step: 0 cls_loss= 0.30230 (108405 samples/sec)
2024-03-10 01:18:11.312394 epoch: 10 step: 100 cls_loss= 0.35329 (3185 samples/sec)
saving....
2024-03-10 01:18:21.369833------------------------------------------------------ Precision@1: 65.67% 

[65.67, 65.77, 65.72, 65.81, 66.04, 65.75, 65.67, 65.81, 65.69, 65.77, 65.67]

Epoch: 11
2024-03-10 01:18:21.631667 epoch: 11 step: 0 cls_loss= 0.31813 (115325 samples/sec)
2024-03-10 01:18:31.127857 epoch: 11 step: 100 cls_loss= 0.31249 (3159 samples/sec)
saving....
2024-03-10 01:18:41.320163------------------------------------------------------ Precision@1: 65.71% 

[65.67, 65.77, 65.72, 65.81, 66.04, 65.75, 65.67, 65.81, 65.69, 65.77, 65.67, 65.71]

Epoch: 12
2024-03-10 01:18:41.579792 epoch: 12 step: 0 cls_loss= 0.24724 (116280 samples/sec)
2024-03-10 01:18:50.971860 epoch: 12 step: 100 cls_loss= 0.32381 (3194 samples/sec)
saving....
2024-03-10 01:19:00.952702------------------------------------------------------ Precision@1: 65.87% 

[65.67, 65.77, 65.72, 65.81, 66.04, 65.75, 65.67, 65.81, 65.69, 65.77, 65.67, 65.71, 65.87]

Epoch: 13
2024-03-10 01:19:01.220348 epoch: 13 step: 0 cls_loss= 0.30233 (112747 samples/sec)
2024-03-10 01:19:10.597472 epoch: 13 step: 100 cls_loss= 0.33249 (3199 samples/sec)
saving....
2024-03-10 01:19:20.546568------------------------------------------------------ Precision@1: 65.84% 

[65.67, 65.77, 65.72, 65.81, 66.04, 65.75, 65.67, 65.81, 65.69, 65.77, 65.67, 65.71, 65.87, 65.84]

Epoch: 14
2024-03-10 01:19:20.808987 epoch: 14 step: 0 cls_loss= 0.33368 (114998 samples/sec)
2024-03-10 01:19:30.157089 epoch: 14 step: 100 cls_loss= 0.32878 (3209 samples/sec)
saving....
2024-03-10 01:19:40.112240------------------------------------------------------ Precision@1: 65.81% 

[65.67, 65.77, 65.72, 65.81, 66.04, 65.75, 65.67, 65.81, 65.69, 65.77, 65.67, 65.71, 65.87, 65.84, 65.81]

Epoch: 15
2024-03-10 01:19:40.389219 epoch: 15 step: 0 cls_loss= 0.29080 (108840 samples/sec)
2024-03-10 01:19:49.755457 epoch: 15 step: 100 cls_loss= 0.37278 (3203 samples/sec)
saving....
2024-03-10 01:19:59.700208------------------------------------------------------ Precision@1: 65.89% 

[65.67, 65.77, 65.72, 65.81, 66.04, 65.75, 65.67, 65.81, 65.69, 65.77, 65.67, 65.71, 65.87, 65.84, 65.81, 65.89]

Epoch: 16
2024-03-10 01:19:59.970553 epoch: 16 step: 0 cls_loss= 0.30918 (111478 samples/sec)
2024-03-10 01:20:09.326057 epoch: 16 step: 100 cls_loss= 0.34497 (3207 samples/sec)
saving....
2024-03-10 01:20:19.259338------------------------------------------------------ Precision@1: 65.66% 

[65.67, 65.77, 65.72, 65.81, 66.04, 65.75, 65.67, 65.81, 65.69, 65.77, 65.67, 65.71, 65.87, 65.84, 65.81, 65.89, 65.66]

Epoch: 17
2024-03-10 01:20:19.538125 epoch: 17 step: 0 cls_loss= 0.40981 (108271 samples/sec)
2024-03-10 01:20:28.959587 epoch: 17 step: 100 cls_loss= 0.35758 (3184 samples/sec)
saving....
2024-03-10 01:20:38.962859------------------------------------------------------ Precision@1: 65.77% 

[65.67, 65.77, 65.72, 65.81, 66.04, 65.75, 65.67, 65.81, 65.69, 65.77, 65.67, 65.71, 65.87, 65.84, 65.81, 65.89, 65.66, 65.77]

Epoch: 18
2024-03-10 01:20:39.241510 epoch: 18 step: 0 cls_loss= 0.30395 (108171 samples/sec)
2024-03-10 01:20:48.597667 epoch: 18 step: 100 cls_loss= 0.27857 (3207 samples/sec)
saving....
2024-03-10 01:20:58.741635------------------------------------------------------ Precision@1: 65.92% 

[65.67, 65.77, 65.72, 65.81, 66.04, 65.75, 65.67, 65.81, 65.69, 65.77, 65.67, 65.71, 65.87, 65.84, 65.81, 65.89, 65.66, 65.77, 65.92]

Epoch: 19
2024-03-10 01:20:59.000496 epoch: 19 step: 0 cls_loss= 0.34810 (116416 samples/sec)
2024-03-10 01:21:08.366136 epoch: 19 step: 100 cls_loss= 0.32722 (3203 samples/sec)
saving....
2024-03-10 01:21:18.388838------------------------------------------------------ Precision@1: 65.73% 

[65.67, 65.77, 65.72, 65.81, 66.04, 65.75, 65.67, 65.81, 65.69, 65.77, 65.67, 65.71, 65.87, 65.84, 65.81, 65.89, 65.66, 65.77, 65.92, 65.73]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 01:21:21.278925 epoch: 0 step: 0 cls_loss= 0.34708 (40141 samples/sec)
2024-03-10 01:21:30.651055 epoch: 0 step: 100 cls_loss= 0.35224 (3201 samples/sec)
saving....
2024-03-10 01:21:40.847849------------------------------------------------------ Precision@1: 65.64% 

[65.64]

Epoch: 1
2024-03-10 01:21:41.095897 epoch: 1 step: 0 cls_loss= 0.30504 (121699 samples/sec)
2024-03-10 01:21:50.438897 epoch: 1 step: 100 cls_loss= 0.29194 (3211 samples/sec)
saving....
2024-03-10 01:22:00.363704------------------------------------------------------ Precision@1: 65.70% 

[65.64, 65.7]

Epoch: 2
2024-03-10 01:22:00.606661 epoch: 2 step: 0 cls_loss= 0.35046 (124253 samples/sec)
2024-03-10 01:22:09.965805 epoch: 2 step: 100 cls_loss= 0.35073 (3206 samples/sec)
saving....
2024-03-10 01:22:19.930695------------------------------------------------------ Precision@1: 65.73% 

[65.64, 65.7, 65.73]

Epoch: 3
2024-03-10 01:22:20.190735 epoch: 3 step: 0 cls_loss= 0.25620 (115970 samples/sec)
2024-03-10 01:22:29.561171 epoch: 3 step: 100 cls_loss= 0.37405 (3202 samples/sec)
saving....
2024-03-10 01:22:39.488484------------------------------------------------------ Precision@1: 65.78% 

[65.64, 65.7, 65.73, 65.78]

Epoch: 4
2024-03-10 01:22:39.738202 epoch: 4 step: 0 cls_loss= 0.31283 (120795 samples/sec)
2024-03-10 01:22:49.084778 epoch: 4 step: 100 cls_loss= 0.29474 (3210 samples/sec)
saving....
2024-03-10 01:22:59.003351------------------------------------------------------ Precision@1: 65.94% 

[65.64, 65.7, 65.73, 65.78, 65.94]

Epoch: 5
2024-03-10 01:22:59.266892 epoch: 5 step: 0 cls_loss= 0.27519 (114490 samples/sec)
2024-03-10 01:23:08.623468 epoch: 5 step: 100 cls_loss= 0.29288 (3206 samples/sec)
saving....
2024-03-10 01:23:18.639676------------------------------------------------------ Precision@1: 65.71% 

[65.64, 65.7, 65.73, 65.78, 65.94, 65.71]

Epoch: 6
2024-03-10 01:23:18.909056 epoch: 6 step: 0 cls_loss= 0.28444 (111934 samples/sec)
2024-03-10 01:23:28.296522 epoch: 6 step: 100 cls_loss= 0.27252 (3196 samples/sec)
saving....
2024-03-10 01:23:38.275249------------------------------------------------------ Precision@1: 65.72% 

[65.64, 65.7, 65.73, 65.78, 65.94, 65.71, 65.72]

Epoch: 7
2024-03-10 01:23:38.528968 epoch: 7 step: 0 cls_loss= 0.32147 (118968 samples/sec)
2024-03-10 01:23:47.885509 epoch: 7 step: 100 cls_loss= 0.29755 (3207 samples/sec)
saving....
2024-03-10 01:23:57.812862------------------------------------------------------ Precision@1: 65.89% 

[65.64, 65.7, 65.73, 65.78, 65.94, 65.71, 65.72, 65.89]

Epoch: 8
2024-03-10 01:23:58.065479 epoch: 8 step: 0 cls_loss= 0.26953 (119412 samples/sec)
2024-03-10 01:24:07.429151 epoch: 8 step: 100 cls_loss= 0.38422 (3204 samples/sec)
saving....
2024-03-10 01:24:17.342099------------------------------------------------------ Precision@1: 65.72% 

[65.64, 65.7, 65.73, 65.78, 65.94, 65.71, 65.72, 65.89, 65.72]

Epoch: 9
2024-03-10 01:24:17.593366 epoch: 9 step: 0 cls_loss= 0.32485 (120173 samples/sec)
2024-03-10 01:24:26.998557 epoch: 9 step: 100 cls_loss= 0.34801 (3190 samples/sec)
saving....
2024-03-10 01:24:36.862626------------------------------------------------------ Precision@1: 65.80% 

[65.64, 65.7, 65.73, 65.78, 65.94, 65.71, 65.72, 65.89, 65.72, 65.8]

Epoch: 10
2024-03-10 01:24:37.120228 epoch: 10 step: 0 cls_loss= 0.29484 (117184 samples/sec)
2024-03-10 01:24:46.455732 epoch: 10 step: 100 cls_loss= 0.37048 (3214 samples/sec)
saving....
2024-03-10 01:24:56.342460------------------------------------------------------ Precision@1: 65.91% 

[65.64, 65.7, 65.73, 65.78, 65.94, 65.71, 65.72, 65.89, 65.72, 65.8, 65.91]

Epoch: 11
2024-03-10 01:24:56.620955 epoch: 11 step: 0 cls_loss= 0.35340 (108155 samples/sec)
2024-03-10 01:25:05.982562 epoch: 11 step: 100 cls_loss= 0.33655 (3205 samples/sec)
saving....
2024-03-10 01:25:15.918905------------------------------------------------------ Precision@1: 65.74% 

[65.64, 65.7, 65.73, 65.78, 65.94, 65.71, 65.72, 65.89, 65.72, 65.8, 65.91, 65.74]

Epoch: 12
2024-03-10 01:25:16.177939 epoch: 12 step: 0 cls_loss= 0.35214 (116449 samples/sec)
2024-03-10 01:25:25.508983 epoch: 12 step: 100 cls_loss= 0.30116 (3215 samples/sec)
saving....
2024-03-10 01:25:35.430611------------------------------------------------------ Precision@1: 65.79% 

[65.64, 65.7, 65.73, 65.78, 65.94, 65.71, 65.72, 65.89, 65.72, 65.8, 65.91, 65.74, 65.79]

Epoch: 13
2024-03-10 01:25:35.707127 epoch: 13 step: 0 cls_loss= 0.31700 (109151 samples/sec)
2024-03-10 01:25:45.075136 epoch: 13 step: 100 cls_loss= 0.36315 (3203 samples/sec)
saving....
2024-03-10 01:25:55.073386------------------------------------------------------ Precision@1: 65.63% 

[65.64, 65.7, 65.73, 65.78, 65.94, 65.71, 65.72, 65.89, 65.72, 65.8, 65.91, 65.74, 65.79, 65.63]

Epoch: 14
2024-03-10 01:25:55.342842 epoch: 14 step: 0 cls_loss= 0.37037 (112047 samples/sec)
2024-03-10 01:26:04.693177 epoch: 14 step: 100 cls_loss= 0.34023 (3209 samples/sec)
saving....
2024-03-10 01:26:14.661213------------------------------------------------------ Precision@1: 65.69% 

[65.64, 65.7, 65.73, 65.78, 65.94, 65.71, 65.72, 65.89, 65.72, 65.8, 65.91, 65.74, 65.79, 65.63, 65.69]

Epoch: 15
2024-03-10 01:26:14.919681 epoch: 15 step: 0 cls_loss= 0.29436 (116851 samples/sec)
2024-03-10 01:26:24.274076 epoch: 15 step: 100 cls_loss= 0.29573 (3207 samples/sec)
saving....
2024-03-10 01:26:34.198053------------------------------------------------------ Precision@1: 65.80% 

[65.64, 65.7, 65.73, 65.78, 65.94, 65.71, 65.72, 65.89, 65.72, 65.8, 65.91, 65.74, 65.79, 65.63, 65.69, 65.8]

Epoch: 16
2024-03-10 01:26:34.455919 epoch: 16 step: 0 cls_loss= 0.33341 (117051 samples/sec)
2024-03-10 01:26:43.817476 epoch: 16 step: 100 cls_loss= 0.32288 (3205 samples/sec)
saving....
2024-03-10 01:26:53.739960------------------------------------------------------ Precision@1: 65.65% 

[65.64, 65.7, 65.73, 65.78, 65.94, 65.71, 65.72, 65.89, 65.72, 65.8, 65.91, 65.74, 65.79, 65.63, 65.69, 65.8, 65.65]

Epoch: 17
2024-03-10 01:26:53.998413 epoch: 17 step: 0 cls_loss= 0.32101 (116791 samples/sec)
2024-03-10 01:27:03.447147 epoch: 17 step: 100 cls_loss= 0.29525 (3175 samples/sec)
saving....
2024-03-10 01:27:13.422058------------------------------------------------------ Precision@1: 65.67% 

[65.64, 65.7, 65.73, 65.78, 65.94, 65.71, 65.72, 65.89, 65.72, 65.8, 65.91, 65.74, 65.79, 65.63, 65.69, 65.8, 65.65, 65.67]

Epoch: 18
2024-03-10 01:27:13.686280 epoch: 18 step: 0 cls_loss= 0.28861 (114273 samples/sec)
2024-03-10 01:27:23.059535 epoch: 18 step: 100 cls_loss= 0.37164 (3201 samples/sec)
saving....
2024-03-10 01:27:32.973779------------------------------------------------------ Precision@1: 65.73% 

[65.64, 65.7, 65.73, 65.78, 65.94, 65.71, 65.72, 65.89, 65.72, 65.8, 65.91, 65.74, 65.79, 65.63, 65.69, 65.8, 65.65, 65.67, 65.73]

Epoch: 19
2024-03-10 01:27:33.248783 epoch: 19 step: 0 cls_loss= 0.33989 (109728 samples/sec)
2024-03-10 01:27:42.629501 epoch: 19 step: 100 cls_loss= 0.29573 (3198 samples/sec)
saving....
2024-03-10 01:27:52.674518------------------------------------------------------ Precision@1: 65.82% 

[65.64, 65.7, 65.73, 65.78, 65.94, 65.71, 65.72, 65.89, 65.72, 65.8, 65.91, 65.74, 65.79, 65.63, 65.69, 65.8, 65.65, 65.67, 65.73, 65.82]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 01:27:55.527350 epoch: 0 step: 0 cls_loss= 0.29378 (42518 samples/sec)
2024-03-10 01:28:04.899500 epoch: 0 step: 100 cls_loss= 0.33298 (3200 samples/sec)
saving....
2024-03-10 01:28:15.063801------------------------------------------------------ Precision@1: 65.91% 

[65.91]

Epoch: 1
2024-03-10 01:28:15.320676 epoch: 1 step: 0 cls_loss= 0.33932 (117267 samples/sec)
2024-03-10 01:28:24.682292 epoch: 1 step: 100 cls_loss= 0.29704 (3205 samples/sec)
saving....
2024-03-10 01:28:34.696759------------------------------------------------------ Precision@1: 65.80% 

[65.91, 65.8]

Epoch: 2
2024-03-10 01:28:34.943761 epoch: 2 step: 0 cls_loss= 0.30311 (122172 samples/sec)
2024-03-10 01:28:44.306157 epoch: 2 step: 100 cls_loss= 0.41264 (3204 samples/sec)
saving....
2024-03-10 01:28:54.251404------------------------------------------------------ Precision@1: 65.93% 

[65.91, 65.8, 65.93]

Epoch: 3
2024-03-10 01:28:54.494135 epoch: 3 step: 0 cls_loss= 0.31693 (124385 samples/sec)
2024-03-10 01:29:03.832746 epoch: 3 step: 100 cls_loss= 0.32157 (3213 samples/sec)
saving....
2024-03-10 01:29:13.805226------------------------------------------------------ Precision@1: 65.56% 

[65.91, 65.8, 65.93, 65.56]

Epoch: 4
2024-03-10 01:29:14.056853 epoch: 4 step: 0 cls_loss= 0.32744 (119974 samples/sec)
2024-03-10 01:29:23.479786 epoch: 4 step: 100 cls_loss= 0.33232 (3184 samples/sec)
saving....
2024-03-10 01:29:33.423639------------------------------------------------------ Precision@1: 65.78% 

[65.91, 65.8, 65.93, 65.56, 65.78]

Epoch: 5
2024-03-10 01:29:33.667499 epoch: 5 step: 0 cls_loss= 0.34711 (123691 samples/sec)
2024-03-10 01:29:43.015411 epoch: 5 step: 100 cls_loss= 0.36826 (3209 samples/sec)
saving....
2024-03-10 01:29:52.927833------------------------------------------------------ Precision@1: 65.86% 

[65.91, 65.8, 65.93, 65.56, 65.78, 65.86]

Epoch: 6
2024-03-10 01:29:53.192850 epoch: 6 step: 0 cls_loss= 0.37240 (113877 samples/sec)
2024-03-10 01:30:02.563289 epoch: 6 step: 100 cls_loss= 0.33987 (3202 samples/sec)
saving....
2024-03-10 01:30:12.559494------------------------------------------------------ Precision@1: 65.87% 

[65.91, 65.8, 65.93, 65.56, 65.78, 65.86, 65.87]

Epoch: 7
2024-03-10 01:30:12.819456 epoch: 7 step: 0 cls_loss= 0.35298 (116084 samples/sec)
2024-03-10 01:30:22.180127 epoch: 7 step: 100 cls_loss= 0.30265 (3205 samples/sec)
saving....
2024-03-10 01:30:32.113888------------------------------------------------------ Precision@1: 65.81% 

[65.91, 65.8, 65.93, 65.56, 65.78, 65.86, 65.87, 65.81]

Epoch: 8
2024-03-10 01:30:32.367828 epoch: 8 step: 0 cls_loss= 0.36438 (118862 samples/sec)
2024-03-10 01:30:41.748208 epoch: 8 step: 100 cls_loss= 0.40293 (3198 samples/sec)
saving....
2024-03-10 01:30:51.696487------------------------------------------------------ Precision@1: 65.64% 

[65.91, 65.8, 65.93, 65.56, 65.78, 65.86, 65.87, 65.81, 65.64]

Epoch: 9
2024-03-10 01:30:51.963380 epoch: 9 step: 0 cls_loss= 0.32308 (113051 samples/sec)
2024-03-10 01:31:01.337809 epoch: 9 step: 100 cls_loss= 0.25627 (3200 samples/sec)
saving....
2024-03-10 01:31:11.280513------------------------------------------------------ Precision@1: 65.69% 

[65.91, 65.8, 65.93, 65.56, 65.78, 65.86, 65.87, 65.81, 65.64, 65.69]

Epoch: 10
2024-03-10 01:31:11.554390 epoch: 10 step: 0 cls_loss= 0.32850 (110053 samples/sec)
2024-03-10 01:31:20.925108 epoch: 10 step: 100 cls_loss= 0.35050 (3202 samples/sec)
saving....
2024-03-10 01:31:30.839804------------------------------------------------------ Precision@1: 65.59% 

[65.91, 65.8, 65.93, 65.56, 65.78, 65.86, 65.87, 65.81, 65.64, 65.69, 65.59]

Epoch: 11
2024-03-10 01:31:31.107495 epoch: 11 step: 0 cls_loss= 0.30848 (112753 samples/sec)
2024-03-10 01:31:40.464685 epoch: 11 step: 100 cls_loss= 0.29551 (3206 samples/sec)
saving....
2024-03-10 01:31:50.352309------------------------------------------------------ Precision@1: 65.72% 

[65.91, 65.8, 65.93, 65.56, 65.78, 65.86, 65.87, 65.81, 65.64, 65.69, 65.59, 65.72]

Epoch: 12
2024-03-10 01:31:50.607497 epoch: 12 step: 0 cls_loss= 0.31022 (118260 samples/sec)
2024-03-10 01:31:59.951433 epoch: 12 step: 100 cls_loss= 0.26600 (3211 samples/sec)
saving....
2024-03-10 01:32:09.853724------------------------------------------------------ Precision@1: 65.61% 

[65.91, 65.8, 65.93, 65.56, 65.78, 65.86, 65.87, 65.81, 65.64, 65.69, 65.59, 65.72, 65.61]

Epoch: 13
2024-03-10 01:32:10.110563 epoch: 13 step: 0 cls_loss= 0.30300 (117561 samples/sec)
2024-03-10 01:32:19.479913 epoch: 13 step: 100 cls_loss= 0.32407 (3202 samples/sec)
saving....
2024-03-10 01:32:29.371321------------------------------------------------------ Precision@1: 65.74% 

[65.91, 65.8, 65.93, 65.56, 65.78, 65.86, 65.87, 65.81, 65.64, 65.69, 65.59, 65.72, 65.61, 65.74]

Epoch: 14
2024-03-10 01:32:29.634337 epoch: 14 step: 0 cls_loss= 0.38478 (114751 samples/sec)
2024-03-10 01:32:38.965179 epoch: 14 step: 100 cls_loss= 0.36661 (3215 samples/sec)
saving....
2024-03-10 01:32:48.856801------------------------------------------------------ Precision@1: 65.53% 

[65.91, 65.8, 65.93, 65.56, 65.78, 65.86, 65.87, 65.81, 65.64, 65.69, 65.59, 65.72, 65.61, 65.74, 65.53]

Epoch: 15
2024-03-10 01:32:49.111824 epoch: 15 step: 0 cls_loss= 0.28614 (118343 samples/sec)
2024-03-10 01:32:58.450656 epoch: 15 step: 100 cls_loss= 0.36860 (3213 samples/sec)
saving....
2024-03-10 01:33:08.379890------------------------------------------------------ Precision@1: 65.69% 

[65.91, 65.8, 65.93, 65.56, 65.78, 65.86, 65.87, 65.81, 65.64, 65.69, 65.59, 65.72, 65.61, 65.74, 65.53, 65.69]

Epoch: 16
2024-03-10 01:33:08.648244 epoch: 16 step: 0 cls_loss= 0.31483 (112349 samples/sec)
2024-03-10 01:33:18.030759 epoch: 16 step: 100 cls_loss= 0.25523 (3197 samples/sec)
saving....
2024-03-10 01:33:27.995955------------------------------------------------------ Precision@1: 65.65% 

[65.91, 65.8, 65.93, 65.56, 65.78, 65.86, 65.87, 65.81, 65.64, 65.69, 65.59, 65.72, 65.61, 65.74, 65.53, 65.69, 65.65]

Epoch: 17
2024-03-10 01:33:28.261098 epoch: 17 step: 0 cls_loss= 0.34453 (113631 samples/sec)
2024-03-10 01:33:37.610251 epoch: 17 step: 100 cls_loss= 0.31357 (3209 samples/sec)
saving....
2024-03-10 01:33:47.523680------------------------------------------------------ Precision@1: 65.60% 

[65.91, 65.8, 65.93, 65.56, 65.78, 65.86, 65.87, 65.81, 65.64, 65.69, 65.59, 65.72, 65.61, 65.74, 65.53, 65.69, 65.65, 65.6]

Epoch: 18
2024-03-10 01:33:47.784149 epoch: 18 step: 0 cls_loss= 0.30071 (115823 samples/sec)
2024-03-10 01:33:57.152108 epoch: 18 step: 100 cls_loss= 0.30050 (3203 samples/sec)
saving....
2024-03-10 01:34:07.077735------------------------------------------------------ Precision@1: 65.82% 

[65.91, 65.8, 65.93, 65.56, 65.78, 65.86, 65.87, 65.81, 65.64, 65.69, 65.59, 65.72, 65.61, 65.74, 65.53, 65.69, 65.65, 65.6, 65.82]

Epoch: 19
2024-03-10 01:34:07.341218 epoch: 19 step: 0 cls_loss= 0.28534 (114489 samples/sec)
2024-03-10 01:34:16.682079 epoch: 19 step: 100 cls_loss= 0.30872 (3212 samples/sec)
saving....
2024-03-10 01:34:26.588017------------------------------------------------------ Precision@1: 65.70% 

[65.91, 65.8, 65.93, 65.56, 65.78, 65.86, 65.87, 65.81, 65.64, 65.69, 65.59, 65.72, 65.61, 65.74, 65.53, 65.69, 65.65, 65.6, 65.82, 65.7]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 01:34:29.464583 epoch: 0 step: 0 cls_loss= 0.29700 (40145 samples/sec)
2024-03-10 01:34:38.838522 epoch: 0 step: 100 cls_loss= 0.33631 (3200 samples/sec)
saving....
2024-03-10 01:34:49.121294------------------------------------------------------ Precision@1: 65.70% 

[65.7]

Epoch: 1
2024-03-10 01:34:49.393195 epoch: 1 step: 0 cls_loss= 0.37334 (110991 samples/sec)
2024-03-10 01:34:58.768728 epoch: 1 step: 100 cls_loss= 0.22977 (3200 samples/sec)
saving....
2024-03-10 01:35:08.675412------------------------------------------------------ Precision@1: 65.64% 

[65.7, 65.64]

Epoch: 2
2024-03-10 01:35:08.941567 epoch: 2 step: 0 cls_loss= 0.32763 (113259 samples/sec)
2024-03-10 01:35:18.321956 epoch: 2 step: 100 cls_loss= 0.26068 (3198 samples/sec)
saving....
2024-03-10 01:35:28.275544------------------------------------------------------ Precision@1: 65.79% 

[65.7, 65.64, 65.79]

Epoch: 3
2024-03-10 01:35:28.541014 epoch: 3 step: 0 cls_loss= 0.29202 (113666 samples/sec)
2024-03-10 01:35:37.915751 epoch: 3 step: 100 cls_loss= 0.35339 (3200 samples/sec)
saving....
2024-03-10 01:35:47.901727------------------------------------------------------ Precision@1: 65.73% 

[65.7, 65.64, 65.79, 65.73]

Epoch: 4
2024-03-10 01:35:48.143261 epoch: 4 step: 0 cls_loss= 0.31022 (125017 samples/sec)
2024-03-10 01:35:57.509584 epoch: 4 step: 100 cls_loss= 0.31396 (3203 samples/sec)
saving....
2024-03-10 01:36:07.453865------------------------------------------------------ Precision@1: 65.84% 

[65.7, 65.64, 65.79, 65.73, 65.84]

Epoch: 5
2024-03-10 01:36:07.706791 epoch: 5 step: 0 cls_loss= 0.33201 (119322 samples/sec)
2024-03-10 01:36:17.068874 epoch: 5 step: 100 cls_loss= 0.32915 (3205 samples/sec)
saving....
2024-03-10 01:36:27.029688------------------------------------------------------ Precision@1: 65.81% 

[65.7, 65.64, 65.79, 65.73, 65.84, 65.81]

Epoch: 6
2024-03-10 01:36:27.294453 epoch: 6 step: 0 cls_loss= 0.30535 (113816 samples/sec)
2024-03-10 01:36:36.667790 epoch: 6 step: 100 cls_loss= 0.30294 (3201 samples/sec)
saving....
2024-03-10 01:36:46.631097------------------------------------------------------ Precision@1: 65.76% 

[65.7, 65.64, 65.79, 65.73, 65.84, 65.81, 65.76]

Epoch: 7
2024-03-10 01:36:46.896641 epoch: 7 step: 0 cls_loss= 0.37613 (113666 samples/sec)
2024-03-10 01:36:56.265267 epoch: 7 step: 100 cls_loss= 0.31733 (3202 samples/sec)
saving....
2024-03-10 01:37:06.297425------------------------------------------------------ Precision@1: 65.81% 

[65.7, 65.64, 65.79, 65.73, 65.84, 65.81, 65.76, 65.81]

Epoch: 8
2024-03-10 01:37:06.547684 epoch: 8 step: 0 cls_loss= 0.35866 (120634 samples/sec)
2024-03-10 01:37:15.922139 epoch: 8 step: 100 cls_loss= 0.37026 (3200 samples/sec)
saving....
2024-03-10 01:37:25.941228------------------------------------------------------ Precision@1: 65.74% 

[65.7, 65.64, 65.79, 65.73, 65.84, 65.81, 65.76, 65.81, 65.74]

Epoch: 9
2024-03-10 01:37:26.211355 epoch: 9 step: 0 cls_loss= 0.36041 (111658 samples/sec)
2024-03-10 01:37:35.572489 epoch: 9 step: 100 cls_loss= 0.33255 (3205 samples/sec)
saving....
2024-03-10 01:37:45.525094------------------------------------------------------ Precision@1: 65.61% 

[65.7, 65.64, 65.79, 65.73, 65.84, 65.81, 65.76, 65.81, 65.74, 65.61]

Epoch: 10
2024-03-10 01:37:45.775503 epoch: 10 step: 0 cls_loss= 0.36760 (120486 samples/sec)
2024-03-10 01:37:55.153252 epoch: 10 step: 100 cls_loss= 0.40309 (3199 samples/sec)
saving....
2024-03-10 01:38:05.098998------------------------------------------------------ Precision@1: 65.89% 

[65.7, 65.64, 65.79, 65.73, 65.84, 65.81, 65.76, 65.81, 65.74, 65.61, 65.89]

Epoch: 11
2024-03-10 01:38:05.357972 epoch: 11 step: 0 cls_loss= 0.39378 (116329 samples/sec)
2024-03-10 01:38:14.738182 epoch: 11 step: 100 cls_loss= 0.30898 (3198 samples/sec)
saving....
2024-03-10 01:38:24.709406------------------------------------------------------ Precision@1: 65.81% 

[65.7, 65.64, 65.79, 65.73, 65.84, 65.81, 65.76, 65.81, 65.74, 65.61, 65.89, 65.81]

Epoch: 12
2024-03-10 01:38:24.982687 epoch: 12 step: 0 cls_loss= 0.30396 (110332 samples/sec)
2024-03-10 01:38:34.387750 epoch: 12 step: 100 cls_loss= 0.31566 (3190 samples/sec)
saving....
2024-03-10 01:38:44.468885------------------------------------------------------ Precision@1: 65.79% 

[65.7, 65.64, 65.79, 65.73, 65.84, 65.81, 65.76, 65.81, 65.74, 65.61, 65.89, 65.81, 65.79]

Epoch: 13
2024-03-10 01:38:44.731715 epoch: 13 step: 0 cls_loss= 0.30465 (114822 samples/sec)
2024-03-10 01:38:54.107694 epoch: 13 step: 100 cls_loss= 0.33276 (3200 samples/sec)
saving....
2024-03-10 01:39:04.125573------------------------------------------------------ Precision@1: 65.81% 

[65.7, 65.64, 65.79, 65.73, 65.84, 65.81, 65.76, 65.81, 65.74, 65.61, 65.89, 65.81, 65.79, 65.81]

Epoch: 14
2024-03-10 01:39:04.388906 epoch: 14 step: 0 cls_loss= 0.33683 (114627 samples/sec)
2024-03-10 01:39:13.791993 epoch: 14 step: 100 cls_loss= 0.35377 (3191 samples/sec)
saving....
2024-03-10 01:39:23.804642------------------------------------------------------ Precision@1: 65.87% 

[65.7, 65.64, 65.79, 65.73, 65.84, 65.81, 65.76, 65.81, 65.74, 65.61, 65.89, 65.81, 65.79, 65.81, 65.87]

Epoch: 15
2024-03-10 01:39:24.070630 epoch: 15 step: 0 cls_loss= 0.32431 (113412 samples/sec)
2024-03-10 01:39:33.487233 epoch: 15 step: 100 cls_loss= 0.30566 (3186 samples/sec)
saving....
2024-03-10 01:39:43.406640------------------------------------------------------ Precision@1: 65.74% 

[65.7, 65.64, 65.79, 65.73, 65.84, 65.81, 65.76, 65.81, 65.74, 65.61, 65.89, 65.81, 65.79, 65.81, 65.87, 65.74]

Epoch: 16
2024-03-10 01:39:43.668432 epoch: 16 step: 0 cls_loss= 0.29967 (115208 samples/sec)
2024-03-10 01:39:53.070096 epoch: 16 step: 100 cls_loss= 0.24455 (3191 samples/sec)
saving....
2024-03-10 01:40:03.047353------------------------------------------------------ Precision@1: 65.73% 

[65.7, 65.64, 65.79, 65.73, 65.84, 65.81, 65.76, 65.81, 65.74, 65.61, 65.89, 65.81, 65.79, 65.81, 65.87, 65.74, 65.73]

Epoch: 17
2024-03-10 01:40:03.322342 epoch: 17 step: 0 cls_loss= 0.27201 (109764 samples/sec)
2024-03-10 01:40:12.704664 epoch: 17 step: 100 cls_loss= 0.32716 (3198 samples/sec)
saving....
2024-03-10 01:40:22.676137------------------------------------------------------ Precision@1: 65.79% 

[65.7, 65.64, 65.79, 65.73, 65.84, 65.81, 65.76, 65.81, 65.74, 65.61, 65.89, 65.81, 65.79, 65.81, 65.87, 65.74, 65.73, 65.79]

Epoch: 18
2024-03-10 01:40:22.926164 epoch: 18 step: 0 cls_loss= 0.29816 (120617 samples/sec)
2024-03-10 01:40:32.279187 epoch: 18 step: 100 cls_loss= 0.35107 (3208 samples/sec)
saving....
2024-03-10 01:40:42.200851------------------------------------------------------ Precision@1: 66.01% 

[65.7, 65.64, 65.79, 65.73, 65.84, 65.81, 65.76, 65.81, 65.74, 65.61, 65.89, 65.81, 65.79, 65.81, 65.87, 65.74, 65.73, 65.79, 66.01]

Epoch: 19
2024-03-10 01:40:42.458545 epoch: 19 step: 0 cls_loss= 0.35634 (117072 samples/sec)
2024-03-10 01:40:51.856869 epoch: 19 step: 100 cls_loss= 0.38929 (3192 samples/sec)
saving....
2024-03-10 01:41:01.835909------------------------------------------------------ Precision@1: 65.71% 

[65.7, 65.64, 65.79, 65.73, 65.84, 65.81, 65.76, 65.81, 65.74, 65.61, 65.89, 65.81, 65.79, 65.81, 65.87, 65.74, 65.73, 65.79, 66.01, 65.71]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 01:41:04.688004 epoch: 0 step: 0 cls_loss= 0.35964 (41658 samples/sec)
2024-03-10 01:41:14.031651 epoch: 0 step: 100 cls_loss= 0.38623 (3210 samples/sec)
saving....
2024-03-10 01:41:24.189358------------------------------------------------------ Precision@1: 65.86% 

[65.86]

Epoch: 1
2024-03-10 01:41:24.459072 epoch: 1 step: 0 cls_loss= 0.31377 (111801 samples/sec)
2024-03-10 01:41:33.873604 epoch: 1 step: 100 cls_loss= 0.30901 (3187 samples/sec)
saving....
2024-03-10 01:41:43.767776------------------------------------------------------ Precision@1: 65.92% 

[65.86, 65.92]

Epoch: 2
2024-03-10 01:41:44.038151 epoch: 2 step: 0 cls_loss= 0.33520 (111595 samples/sec)
2024-03-10 01:41:53.409824 epoch: 2 step: 100 cls_loss= 0.30137 (3201 samples/sec)
saving....
2024-03-10 01:42:03.334940------------------------------------------------------ Precision@1: 65.90% 

[65.86, 65.92, 65.9]

Epoch: 3
2024-03-10 01:42:03.613878 epoch: 3 step: 0 cls_loss= 0.31621 (108123 samples/sec)
2024-03-10 01:42:12.984189 epoch: 3 step: 100 cls_loss= 0.31239 (3202 samples/sec)
saving....
2024-03-10 01:42:22.931660------------------------------------------------------ Precision@1: 65.80% 

[65.86, 65.92, 65.9, 65.8]

Epoch: 4
2024-03-10 01:42:23.180162 epoch: 4 step: 0 cls_loss= 0.31278 (121486 samples/sec)
2024-03-10 01:42:32.560992 epoch: 4 step: 100 cls_loss= 0.26520 (3198 samples/sec)
saving....
2024-03-10 01:42:42.596796------------------------------------------------------ Precision@1: 65.90% 

[65.86, 65.92, 65.9, 65.8, 65.9]

Epoch: 5
2024-03-10 01:42:42.860370 epoch: 5 step: 0 cls_loss= 0.31774 (114498 samples/sec)
2024-03-10 01:42:52.204099 epoch: 5 step: 100 cls_loss= 0.25433 (3211 samples/sec)
saving....
2024-03-10 01:43:02.154776------------------------------------------------------ Precision@1: 65.72% 

[65.86, 65.92, 65.9, 65.8, 65.9, 65.72]

Epoch: 6
2024-03-10 01:43:02.405082 epoch: 6 step: 0 cls_loss= 0.32003 (120582 samples/sec)
2024-03-10 01:43:11.781470 epoch: 6 step: 100 cls_loss= 0.39699 (3200 samples/sec)
saving....
2024-03-10 01:43:21.710563------------------------------------------------------ Precision@1: 65.63% 

[65.86, 65.92, 65.9, 65.8, 65.9, 65.72, 65.63]

Epoch: 7
2024-03-10 01:43:21.969199 epoch: 7 step: 0 cls_loss= 0.27731 (116739 samples/sec)
2024-03-10 01:43:31.340562 epoch: 7 step: 100 cls_loss= 0.31257 (3201 samples/sec)
saving....
2024-03-10 01:43:41.268568------------------------------------------------------ Precision@1: 65.61% 

[65.86, 65.92, 65.9, 65.8, 65.9, 65.72, 65.63, 65.61]

Epoch: 8
2024-03-10 01:43:41.531921 epoch: 8 step: 0 cls_loss= 0.32961 (114641 samples/sec)
2024-03-10 01:43:50.952953 epoch: 8 step: 100 cls_loss= 0.32362 (3185 samples/sec)
saving....
2024-03-10 01:44:00.898212------------------------------------------------------ Precision@1: 65.80% 

[65.86, 65.92, 65.9, 65.8, 65.9, 65.72, 65.63, 65.61, 65.8]

Epoch: 9
2024-03-10 01:44:01.169340 epoch: 9 step: 0 cls_loss= 0.31753 (111242 samples/sec)
2024-03-10 01:44:10.535241 epoch: 9 step: 100 cls_loss= 0.39597 (3203 samples/sec)
saving....
2024-03-10 01:44:20.452139------------------------------------------------------ Precision@1: 65.62% 

[65.86, 65.92, 65.9, 65.8, 65.9, 65.72, 65.63, 65.61, 65.8, 65.62]

Epoch: 10
2024-03-10 01:44:20.717173 epoch: 10 step: 0 cls_loss= 0.36507 (113894 samples/sec)
2024-03-10 01:44:30.085835 epoch: 10 step: 100 cls_loss= 0.33789 (3202 samples/sec)
saving....
2024-03-10 01:44:39.994722------------------------------------------------------ Precision@1: 65.68% 

[65.86, 65.92, 65.9, 65.8, 65.9, 65.72, 65.63, 65.61, 65.8, 65.62, 65.68]

Epoch: 11
2024-03-10 01:44:40.260623 epoch: 11 step: 0 cls_loss= 0.26911 (113413 samples/sec)
2024-03-10 01:44:49.634291 epoch: 11 step: 100 cls_loss= 0.27911 (3201 samples/sec)
saving....
2024-03-10 01:44:59.545445------------------------------------------------------ Precision@1: 65.79% 

[65.86, 65.92, 65.9, 65.8, 65.9, 65.72, 65.63, 65.61, 65.8, 65.62, 65.68, 65.79]

Epoch: 12
2024-03-10 01:44:59.806649 epoch: 12 step: 0 cls_loss= 0.37145 (115426 samples/sec)
2024-03-10 01:45:09.170800 epoch: 12 step: 100 cls_loss= 0.33315 (3204 samples/sec)
saving....
2024-03-10 01:45:19.070767------------------------------------------------------ Precision@1: 65.67% 

[65.86, 65.92, 65.9, 65.8, 65.9, 65.72, 65.63, 65.61, 65.8, 65.62, 65.68, 65.79, 65.67]

Epoch: 13
2024-03-10 01:45:19.331686 epoch: 13 step: 0 cls_loss= 0.33611 (115658 samples/sec)
2024-03-10 01:45:28.689273 epoch: 13 step: 100 cls_loss= 0.31477 (3206 samples/sec)
saving....
2024-03-10 01:45:38.596327------------------------------------------------------ Precision@1: 65.87% 

[65.86, 65.92, 65.9, 65.8, 65.9, 65.72, 65.63, 65.61, 65.8, 65.62, 65.68, 65.79, 65.67, 65.87]

Epoch: 14
2024-03-10 01:45:38.858626 epoch: 14 step: 0 cls_loss= 0.36508 (114969 samples/sec)
2024-03-10 01:45:48.214525 epoch: 14 step: 100 cls_loss= 0.32225 (3207 samples/sec)
saving....
2024-03-10 01:45:58.128375------------------------------------------------------ Precision@1: 65.77% 

[65.86, 65.92, 65.9, 65.8, 65.9, 65.72, 65.63, 65.61, 65.8, 65.62, 65.68, 65.79, 65.67, 65.87, 65.77]

Epoch: 15
2024-03-10 01:45:58.396557 epoch: 15 step: 0 cls_loss= 0.36895 (112525 samples/sec)
2024-03-10 01:46:07.792166 epoch: 15 step: 100 cls_loss= 0.29704 (3193 samples/sec)
saving....
2024-03-10 01:46:17.730434------------------------------------------------------ Precision@1: 65.73% 

[65.86, 65.92, 65.9, 65.8, 65.9, 65.72, 65.63, 65.61, 65.8, 65.62, 65.68, 65.79, 65.67, 65.87, 65.77, 65.73]

Epoch: 16
2024-03-10 01:46:17.999495 epoch: 16 step: 0 cls_loss= 0.35379 (112170 samples/sec)
2024-03-10 01:46:27.358840 epoch: 16 step: 100 cls_loss= 0.26581 (3206 samples/sec)
saving....
2024-03-10 01:46:37.362463------------------------------------------------------ Precision@1: 65.75% 

[65.86, 65.92, 65.9, 65.8, 65.9, 65.72, 65.63, 65.61, 65.8, 65.62, 65.68, 65.79, 65.67, 65.87, 65.77, 65.73, 65.75]

Epoch: 17
2024-03-10 01:46:37.623307 epoch: 17 step: 0 cls_loss= 0.36083 (115738 samples/sec)
2024-03-10 01:46:46.997080 epoch: 17 step: 100 cls_loss= 0.36303 (3201 samples/sec)
saving....
2024-03-10 01:46:56.892908------------------------------------------------------ Precision@1: 65.51% 

[65.86, 65.92, 65.9, 65.8, 65.9, 65.72, 65.63, 65.61, 65.8, 65.62, 65.68, 65.79, 65.67, 65.87, 65.77, 65.73, 65.75, 65.51]

Epoch: 18
2024-03-10 01:46:57.153333 epoch: 18 step: 0 cls_loss= 0.31941 (115935 samples/sec)
2024-03-10 01:47:06.519836 epoch: 18 step: 100 cls_loss= 0.23484 (3203 samples/sec)
saving....
2024-03-10 01:47:16.422685------------------------------------------------------ Precision@1: 65.81% 

[65.86, 65.92, 65.9, 65.8, 65.9, 65.72, 65.63, 65.61, 65.8, 65.62, 65.68, 65.79, 65.67, 65.87, 65.77, 65.73, 65.75, 65.51, 65.81]

Epoch: 19
2024-03-10 01:47:16.699060 epoch: 19 step: 0 cls_loss= 0.33567 (109123 samples/sec)
2024-03-10 01:47:26.071173 epoch: 19 step: 100 cls_loss= 0.34958 (3201 samples/sec)
saving....
2024-03-10 01:47:36.043589------------------------------------------------------ Precision@1: 65.76% 

[65.86, 65.92, 65.9, 65.8, 65.9, 65.72, 65.63, 65.61, 65.8, 65.62, 65.68, 65.79, 65.67, 65.87, 65.77, 65.73, 65.75, 65.51, 65.81, 65.76]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 01:47:38.948664 epoch: 0 step: 0 cls_loss= 0.32055 (40446 samples/sec)
2024-03-10 01:47:48.253561 epoch: 0 step: 100 cls_loss= 0.34230 (3224 samples/sec)
saving....
2024-03-10 01:47:58.361556------------------------------------------------------ Precision@1: 65.80% 

[65.8]

Epoch: 1
2024-03-10 01:47:58.615626 epoch: 1 step: 0 cls_loss= 0.33361 (118735 samples/sec)
2024-03-10 01:48:08.010437 epoch: 1 step: 100 cls_loss= 0.32973 (3196 samples/sec)
saving....
2024-03-10 01:48:18.063137------------------------------------------------------ Precision@1: 65.72% 

[65.8, 65.72]

Epoch: 2
2024-03-10 01:48:18.329660 epoch: 2 step: 0 cls_loss= 0.33738 (113248 samples/sec)
2024-03-10 01:48:27.671548 epoch: 2 step: 100 cls_loss= 0.31121 (3214 samples/sec)
saving....
2024-03-10 01:48:37.561879------------------------------------------------------ Precision@1: 65.64% 

[65.8, 65.72, 65.64]

Epoch: 3
2024-03-10 01:48:37.803643 epoch: 3 step: 0 cls_loss= 0.34292 (124885 samples/sec)
2024-03-10 01:48:47.148821 epoch: 3 step: 100 cls_loss= 0.33827 (3213 samples/sec)
saving....
2024-03-10 01:48:57.170756------------------------------------------------------ Precision@1: 65.88% 

[65.8, 65.72, 65.64, 65.88]

Epoch: 4
2024-03-10 01:48:57.440493 epoch: 4 step: 0 cls_loss= 0.37346 (111810 samples/sec)
2024-03-10 01:49:06.781893 epoch: 4 step: 100 cls_loss= 0.38800 (3212 samples/sec)
saving....
2024-03-10 01:49:16.713481------------------------------------------------------ Precision@1: 65.68% 

[65.8, 65.72, 65.64, 65.88, 65.68]

Epoch: 5
2024-03-10 01:49:16.998402 epoch: 5 step: 0 cls_loss= 0.30166 (105834 samples/sec)
2024-03-10 01:49:26.343067 epoch: 5 step: 100 cls_loss= 0.31691 (3212 samples/sec)
saving....
2024-03-10 01:49:36.239191------------------------------------------------------ Precision@1: 65.71% 

[65.8, 65.72, 65.64, 65.88, 65.68, 65.71]

Epoch: 6
2024-03-10 01:49:36.475065 epoch: 6 step: 0 cls_loss= 0.35912 (128005 samples/sec)
2024-03-10 01:49:45.801921 epoch: 6 step: 100 cls_loss= 0.31994 (3219 samples/sec)
saving....
2024-03-10 01:49:55.765700------------------------------------------------------ Precision@1: 65.68% 

[65.8, 65.72, 65.64, 65.88, 65.68, 65.71, 65.68]

Epoch: 7
2024-03-10 01:49:56.023772 epoch: 7 step: 0 cls_loss= 0.37464 (116962 samples/sec)
2024-03-10 01:50:05.349971 epoch: 7 step: 100 cls_loss= 0.31446 (3217 samples/sec)
saving....
2024-03-10 01:50:15.246159------------------------------------------------------ Precision@1: 65.77% 

[65.8, 65.72, 65.64, 65.88, 65.68, 65.71, 65.68, 65.77]

Epoch: 8
2024-03-10 01:50:15.493939 epoch: 8 step: 0 cls_loss= 0.38512 (121828 samples/sec)
2024-03-10 01:50:24.831699 epoch: 8 step: 100 cls_loss= 0.32104 (3214 samples/sec)
saving....
2024-03-10 01:50:34.791694------------------------------------------------------ Precision@1: 65.82% 

[65.8, 65.72, 65.64, 65.88, 65.68, 65.71, 65.68, 65.77, 65.82]

Epoch: 9
2024-03-10 01:50:35.053122 epoch: 9 step: 0 cls_loss= 0.31796 (115414 samples/sec)
2024-03-10 01:50:44.371751 epoch: 9 step: 100 cls_loss= 0.33997 (3220 samples/sec)
saving....
2024-03-10 01:50:54.432868------------------------------------------------------ Precision@1: 65.77% 

[65.8, 65.72, 65.64, 65.88, 65.68, 65.71, 65.68, 65.77, 65.82, 65.77]

Epoch: 10
2024-03-10 01:50:54.704817 epoch: 10 step: 0 cls_loss= 0.27620 (110892 samples/sec)
2024-03-10 01:51:04.034058 epoch: 10 step: 100 cls_loss= 0.30453 (3218 samples/sec)
saving....
2024-03-10 01:51:13.927886------------------------------------------------------ Precision@1: 65.73% 

[65.8, 65.72, 65.64, 65.88, 65.68, 65.71, 65.68, 65.77, 65.82, 65.77, 65.73]

Epoch: 11
2024-03-10 01:51:14.200161 epoch: 11 step: 0 cls_loss= 0.33576 (110711 samples/sec)
2024-03-10 01:51:23.529982 epoch: 11 step: 100 cls_loss= 0.35252 (3218 samples/sec)
saving....
2024-03-10 01:51:33.535901------------------------------------------------------ Precision@1: 65.66% 

[65.8, 65.72, 65.64, 65.88, 65.68, 65.71, 65.68, 65.77, 65.82, 65.77, 65.73, 65.66]

Epoch: 12
2024-03-10 01:51:33.792977 epoch: 12 step: 0 cls_loss= 0.23189 (117316 samples/sec)
2024-03-10 01:51:43.172797 epoch: 12 step: 100 cls_loss= 0.29562 (3201 samples/sec)
saving....
2024-03-10 01:51:53.085093------------------------------------------------------ Precision@1: 65.80% 

[65.8, 65.72, 65.64, 65.88, 65.68, 65.71, 65.68, 65.77, 65.82, 65.77, 65.73, 65.66, 65.8]

Epoch: 13
2024-03-10 01:51:53.337640 epoch: 13 step: 0 cls_loss= 0.36335 (119469 samples/sec)
2024-03-10 01:52:02.681161 epoch: 13 step: 100 cls_loss= 0.31600 (3213 samples/sec)
saving....
2024-03-10 01:52:12.676273------------------------------------------------------ Precision@1: 65.71% 

[65.8, 65.72, 65.64, 65.88, 65.68, 65.71, 65.68, 65.77, 65.82, 65.77, 65.73, 65.66, 65.8, 65.71]

Epoch: 14
2024-03-10 01:52:12.936608 epoch: 14 step: 0 cls_loss= 0.33123 (115942 samples/sec)
2024-03-10 01:52:22.265803 epoch: 14 step: 100 cls_loss= 0.25923 (3218 samples/sec)
saving....
2024-03-10 01:52:32.196115------------------------------------------------------ Precision@1: 65.66% 

[65.8, 65.72, 65.64, 65.88, 65.68, 65.71, 65.68, 65.77, 65.82, 65.77, 65.73, 65.66, 65.8, 65.71, 65.66]

Epoch: 15
2024-03-10 01:52:32.472733 epoch: 15 step: 0 cls_loss= 0.33735 (109118 samples/sec)
2024-03-10 01:52:41.930529 epoch: 15 step: 100 cls_loss= 0.28251 (3175 samples/sec)
saving....
2024-03-10 01:52:51.938372------------------------------------------------------ Precision@1: 65.69% 

[65.8, 65.72, 65.64, 65.88, 65.68, 65.71, 65.68, 65.77, 65.82, 65.77, 65.73, 65.66, 65.8, 65.71, 65.66, 65.69]

Epoch: 16
2024-03-10 01:52:52.213909 epoch: 16 step: 0 cls_loss= 0.38700 (109487 samples/sec)
2024-03-10 01:53:01.527681 epoch: 16 step: 100 cls_loss= 0.28765 (3222 samples/sec)
saving....
2024-03-10 01:53:11.441608------------------------------------------------------ Precision@1: 65.75% 

[65.8, 65.72, 65.64, 65.88, 65.68, 65.71, 65.68, 65.77, 65.82, 65.77, 65.73, 65.66, 65.8, 65.71, 65.66, 65.69, 65.75]

Epoch: 17
2024-03-10 01:53:11.698432 epoch: 17 step: 0 cls_loss= 0.35791 (117416 samples/sec)
2024-03-10 01:53:21.033026 epoch: 17 step: 100 cls_loss= 0.28214 (3217 samples/sec)
saving....
2024-03-10 01:53:30.959853------------------------------------------------------ Precision@1: 65.74% 

[65.8, 65.72, 65.64, 65.88, 65.68, 65.71, 65.68, 65.77, 65.82, 65.77, 65.73, 65.66, 65.8, 65.71, 65.66, 65.69, 65.75, 65.74]

Epoch: 18
2024-03-10 01:53:31.213172 epoch: 18 step: 0 cls_loss= 0.34580 (119094 samples/sec)
2024-03-10 01:53:40.555226 epoch: 18 step: 100 cls_loss= 0.39761 (3212 samples/sec)
saving....
2024-03-10 01:53:50.590154------------------------------------------------------ Precision@1: 65.55% 

[65.8, 65.72, 65.64, 65.88, 65.68, 65.71, 65.68, 65.77, 65.82, 65.77, 65.73, 65.66, 65.8, 65.71, 65.66, 65.69, 65.75, 65.74, 65.55]

Epoch: 19
2024-03-10 01:53:50.845892 epoch: 19 step: 0 cls_loss= 0.34067 (117909 samples/sec)
2024-03-10 01:54:00.154804 epoch: 19 step: 100 cls_loss= 0.29788 (3225 samples/sec)
saving....
2024-03-10 01:54:10.051622------------------------------------------------------ Precision@1: 65.62% 

[65.8, 65.72, 65.64, 65.88, 65.68, 65.71, 65.68, 65.77, 65.82, 65.77, 65.73, 65.66, 65.8, 65.71, 65.66, 65.69, 65.75, 65.74, 65.55, 65.62]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 01:54:12.935097 epoch: 0 step: 0 cls_loss= 0.39372 (40206 samples/sec)
2024-03-10 01:54:22.238702 epoch: 0 step: 100 cls_loss= 0.32319 (3224 samples/sec)
saving....
2024-03-10 01:54:32.383010------------------------------------------------------ Precision@1: 66.05% 

[66.05]

Epoch: 1
2024-03-10 01:54:32.634330 epoch: 1 step: 0 cls_loss= 0.28595 (120072 samples/sec)
2024-03-10 01:54:41.949035 epoch: 1 step: 100 cls_loss= 0.29697 (3223 samples/sec)
saving....
2024-03-10 01:54:51.850339------------------------------------------------------ Precision@1: 65.62% 

[66.05, 65.62]

Epoch: 2
2024-03-10 01:54:52.093328 epoch: 2 step: 0 cls_loss= 0.27271 (124263 samples/sec)
2024-03-10 01:55:01.439150 epoch: 2 step: 100 cls_loss= 0.31775 (3213 samples/sec)
saving....
2024-03-10 01:55:11.557327------------------------------------------------------ Precision@1: 65.67% 

[66.05, 65.62, 65.67]

Epoch: 3
2024-03-10 01:55:11.816818 epoch: 3 step: 0 cls_loss= 0.39835 (116328 samples/sec)
2024-03-10 01:55:21.211013 epoch: 3 step: 100 cls_loss= 0.34268 (3196 samples/sec)
saving....
2024-03-10 01:55:31.142978------------------------------------------------------ Precision@1: 65.71% 

[66.05, 65.62, 65.67, 65.71]

Epoch: 4
2024-03-10 01:55:31.380016 epoch: 4 step: 0 cls_loss= 0.38448 (127290 samples/sec)
2024-03-10 01:55:40.698282 epoch: 4 step: 100 cls_loss= 0.29359 (3222 samples/sec)
saving....
2024-03-10 01:55:50.751767------------------------------------------------------ Precision@1: 65.62% 

[66.05, 65.62, 65.67, 65.71, 65.62]

Epoch: 5
2024-03-10 01:55:51.034296 epoch: 5 step: 0 cls_loss= 0.35845 (106657 samples/sec)
2024-03-10 01:56:00.352971 epoch: 5 step: 100 cls_loss= 0.35834 (3220 samples/sec)
saving....
2024-03-10 01:56:10.285761------------------------------------------------------ Precision@1: 65.63% 

[66.05, 65.62, 65.67, 65.71, 65.62, 65.63]

Epoch: 6
2024-03-10 01:56:10.530427 epoch: 6 step: 0 cls_loss= 0.28420 (123425 samples/sec)
2024-03-10 01:56:19.832216 epoch: 6 step: 100 cls_loss= 0.30156 (3228 samples/sec)
saving....
2024-03-10 01:56:29.728341------------------------------------------------------ Precision@1: 65.80% 

[66.05, 65.62, 65.67, 65.71, 65.62, 65.63, 65.8]

Epoch: 7
2024-03-10 01:56:29.985478 epoch: 7 step: 0 cls_loss= 0.30376 (117387 samples/sec)
2024-03-10 01:56:39.312546 epoch: 7 step: 100 cls_loss= 0.35606 (3219 samples/sec)
saving....
2024-03-10 01:56:49.327710------------------------------------------------------ Precision@1: 65.90% 

[66.05, 65.62, 65.67, 65.71, 65.62, 65.63, 65.8, 65.9]

Epoch: 8
2024-03-10 01:56:49.586833 epoch: 8 step: 0 cls_loss= 0.35670 (116414 samples/sec)
2024-03-10 01:56:58.916700 epoch: 8 step: 100 cls_loss= 0.34604 (3218 samples/sec)
saving....
2024-03-10 01:57:08.827012------------------------------------------------------ Precision@1: 65.83% 

[66.05, 65.62, 65.67, 65.71, 65.62, 65.63, 65.8, 65.9, 65.83]

Epoch: 9
2024-03-10 01:57:09.095784 epoch: 9 step: 0 cls_loss= 0.35045 (112204 samples/sec)
2024-03-10 01:57:18.434602 epoch: 9 step: 100 cls_loss= 0.26481 (3215 samples/sec)
saving....
2024-03-10 01:57:28.346065------------------------------------------------------ Precision@1: 65.43% 

[66.05, 65.62, 65.67, 65.71, 65.62, 65.63, 65.8, 65.9, 65.83, 65.43]

Epoch: 10
2024-03-10 01:57:28.609692 epoch: 10 step: 0 cls_loss= 0.33411 (114443 samples/sec)
2024-03-10 01:57:37.961283 epoch: 10 step: 100 cls_loss= 0.30617 (3211 samples/sec)
saving....
2024-03-10 01:57:47.910751------------------------------------------------------ Precision@1: 65.72% 

[66.05, 65.62, 65.67, 65.71, 65.62, 65.63, 65.8, 65.9, 65.83, 65.43, 65.72]

Epoch: 11
2024-03-10 01:57:48.168327 epoch: 11 step: 0 cls_loss= 0.28784 (117251 samples/sec)
2024-03-10 01:57:57.478638 epoch: 11 step: 100 cls_loss= 0.30267 (3225 samples/sec)
saving....
2024-03-10 01:58:07.438067------------------------------------------------------ Precision@1: 65.75% 

[66.05, 65.62, 65.67, 65.71, 65.62, 65.63, 65.8, 65.9, 65.83, 65.43, 65.72, 65.75]

Epoch: 12
2024-03-10 01:58:07.697056 epoch: 12 step: 0 cls_loss= 0.29200 (116438 samples/sec)
2024-03-10 01:58:17.115184 epoch: 12 step: 100 cls_loss= 0.31190 (3188 samples/sec)
saving....
2024-03-10 01:58:27.014240------------------------------------------------------ Precision@1: 65.78% 

[66.05, 65.62, 65.67, 65.71, 65.62, 65.63, 65.8, 65.9, 65.83, 65.43, 65.72, 65.75, 65.78]

Epoch: 13
2024-03-10 01:58:27.296819 epoch: 13 step: 0 cls_loss= 0.28342 (106697 samples/sec)
2024-03-10 01:58:36.606478 epoch: 13 step: 100 cls_loss= 0.34289 (3225 samples/sec)
saving....
2024-03-10 01:58:46.501456------------------------------------------------------ Precision@1: 65.79% 

[66.05, 65.62, 65.67, 65.71, 65.62, 65.63, 65.8, 65.9, 65.83, 65.43, 65.72, 65.75, 65.78, 65.79]

Epoch: 14
2024-03-10 01:58:46.764928 epoch: 14 step: 0 cls_loss= 0.27276 (114594 samples/sec)
2024-03-10 01:58:56.085723 epoch: 14 step: 100 cls_loss= 0.39919 (3221 samples/sec)
saving....
2024-03-10 01:59:06.033506------------------------------------------------------ Precision@1: 65.76% 

[66.05, 65.62, 65.67, 65.71, 65.62, 65.63, 65.8, 65.9, 65.83, 65.43, 65.72, 65.75, 65.78, 65.79, 65.76]

Epoch: 15
2024-03-10 01:59:06.288985 epoch: 15 step: 0 cls_loss= 0.29685 (118211 samples/sec)
2024-03-10 01:59:15.628007 epoch: 15 step: 100 cls_loss= 0.30305 (3215 samples/sec)
saving....
2024-03-10 01:59:25.553899------------------------------------------------------ Precision@1: 65.65% 

[66.05, 65.62, 65.67, 65.71, 65.62, 65.63, 65.8, 65.9, 65.83, 65.43, 65.72, 65.75, 65.78, 65.79, 65.76, 65.65]

Epoch: 16
2024-03-10 01:59:25.800541 epoch: 16 step: 0 cls_loss= 0.36967 (122319 samples/sec)
2024-03-10 01:59:35.108313 epoch: 16 step: 100 cls_loss= 0.36641 (3226 samples/sec)
saving....
2024-03-10 01:59:45.008805------------------------------------------------------ Precision@1: 65.72% 

[66.05, 65.62, 65.67, 65.71, 65.62, 65.63, 65.8, 65.9, 65.83, 65.43, 65.72, 65.75, 65.78, 65.79, 65.76, 65.65, 65.72]

Epoch: 17
2024-03-10 01:59:45.260791 epoch: 17 step: 0 cls_loss= 0.34279 (119824 samples/sec)
2024-03-10 01:59:54.638166 epoch: 17 step: 100 cls_loss= 0.36992 (3202 samples/sec)
saving....
2024-03-10 02:00:04.584176------------------------------------------------------ Precision@1: 65.65% 

[66.05, 65.62, 65.67, 65.71, 65.62, 65.63, 65.8, 65.9, 65.83, 65.43, 65.72, 65.75, 65.78, 65.79, 65.76, 65.65, 65.72, 65.65]

Epoch: 18
2024-03-10 02:00:04.844725 epoch: 18 step: 0 cls_loss= 0.29626 (115828 samples/sec)
2024-03-10 02:00:14.204811 epoch: 18 step: 100 cls_loss= 0.36514 (3208 samples/sec)
saving....
2024-03-10 02:00:24.155530------------------------------------------------------ Precision@1: 65.66% 

[66.05, 65.62, 65.67, 65.71, 65.62, 65.63, 65.8, 65.9, 65.83, 65.43, 65.72, 65.75, 65.78, 65.79, 65.76, 65.65, 65.72, 65.65, 65.66]

Epoch: 19
2024-03-10 02:00:24.409961 epoch: 19 step: 0 cls_loss= 0.34066 (118703 samples/sec)
2024-03-10 02:00:33.729670 epoch: 19 step: 100 cls_loss= 0.33143 (3222 samples/sec)
saving....
2024-03-10 02:00:43.799568------------------------------------------------------ Precision@1: 65.71% 

[66.05, 65.62, 65.67, 65.71, 65.62, 65.63, 65.8, 65.9, 65.83, 65.43, 65.72, 65.75, 65.78, 65.79, 65.76, 65.65, 65.72, 65.65, 65.66, 65.71]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 02:00:46.695440 epoch: 0 step: 0 cls_loss= 0.29995 (40398 samples/sec)
2024-03-10 02:00:56.030632 epoch: 0 step: 100 cls_loss= 0.27730 (3213 samples/sec)
saving....
2024-03-10 02:01:06.199189------------------------------------------------------ Precision@1: 65.73% 

[65.73]

Epoch: 1
2024-03-10 02:01:06.457910 epoch: 1 step: 0 cls_loss= 0.29306 (116597 samples/sec)
2024-03-10 02:01:15.833244 epoch: 1 step: 100 cls_loss= 0.31220 (3203 samples/sec)
saving....
2024-03-10 02:01:25.800199------------------------------------------------------ Precision@1: 65.66% 

[65.73, 65.66]

Epoch: 2
2024-03-10 02:01:26.057484 epoch: 2 step: 0 cls_loss= 0.36165 (117302 samples/sec)
2024-03-10 02:01:35.422816 epoch: 2 step: 100 cls_loss= 0.39180 (3206 samples/sec)
saving....
2024-03-10 02:01:45.331664------------------------------------------------------ Precision@1: 65.82% 

[65.73, 65.66, 65.82]

Epoch: 3
2024-03-10 02:01:45.587130 epoch: 3 step: 0 cls_loss= 0.36907 (118140 samples/sec)
2024-03-10 02:01:54.922394 epoch: 3 step: 100 cls_loss= 0.33527 (3216 samples/sec)
saving....
2024-03-10 02:02:04.839666------------------------------------------------------ Precision@1: 65.77% 

[65.73, 65.66, 65.82, 65.77]

Epoch: 4
2024-03-10 02:02:05.132802 epoch: 4 step: 0 cls_loss= 0.33448 (102879 samples/sec)
2024-03-10 02:02:14.484410 epoch: 4 step: 100 cls_loss= 0.35836 (3209 samples/sec)
saving....
2024-03-10 02:02:24.501816------------------------------------------------------ Precision@1: 65.44% 

[65.73, 65.66, 65.82, 65.77, 65.44]

Epoch: 5
2024-03-10 02:02:24.758921 epoch: 5 step: 0 cls_loss= 0.36489 (117376 samples/sec)
2024-03-10 02:02:34.079753 epoch: 5 step: 100 cls_loss= 0.29092 (3221 samples/sec)
saving....
2024-03-10 02:02:43.963279------------------------------------------------------ Precision@1: 65.78% 

[65.73, 65.66, 65.82, 65.77, 65.44, 65.78]

Epoch: 6
2024-03-10 02:02:44.217764 epoch: 6 step: 0 cls_loss= 0.38310 (118599 samples/sec)
2024-03-10 02:02:53.539938 epoch: 6 step: 100 cls_loss= 0.36339 (3221 samples/sec)
saving....
2024-03-10 02:03:03.543095------------------------------------------------------ Precision@1: 65.48% 

[65.73, 65.66, 65.82, 65.77, 65.44, 65.78, 65.48]

Epoch: 7
2024-03-10 02:03:03.810128 epoch: 7 step: 0 cls_loss= 0.29576 (113027 samples/sec)
2024-03-10 02:03:13.141925 epoch: 7 step: 100 cls_loss= 0.29603 (3218 samples/sec)
saving....
2024-03-10 02:03:23.041198------------------------------------------------------ Precision@1: 65.76% 

[65.73, 65.66, 65.82, 65.77, 65.44, 65.78, 65.48, 65.76]

Epoch: 8
2024-03-10 02:03:23.301848 epoch: 8 step: 0 cls_loss= 0.26636 (115813 samples/sec)
2024-03-10 02:03:32.701278 epoch: 8 step: 100 cls_loss= 0.29987 (3194 samples/sec)
saving....
2024-03-10 02:03:42.716948------------------------------------------------------ Precision@1: 65.83% 

[65.73, 65.66, 65.82, 65.77, 65.44, 65.78, 65.48, 65.76, 65.83]

Epoch: 9
2024-03-10 02:03:42.969050 epoch: 9 step: 0 cls_loss= 0.33709 (119711 samples/sec)
2024-03-10 02:03:52.332548 epoch: 9 step: 100 cls_loss= 0.41433 (3207 samples/sec)
saving....
2024-03-10 02:04:02.242074------------------------------------------------------ Precision@1: 65.52% 

[65.73, 65.66, 65.82, 65.77, 65.44, 65.78, 65.48, 65.76, 65.83, 65.52]

Epoch: 10
2024-03-10 02:04:02.514597 epoch: 10 step: 0 cls_loss= 0.37664 (110651 samples/sec)
2024-03-10 02:04:11.840780 epoch: 10 step: 100 cls_loss= 0.36721 (3219 samples/sec)
saving....
2024-03-10 02:04:21.776871------------------------------------------------------ Precision@1: 65.68% 

[65.73, 65.66, 65.82, 65.77, 65.44, 65.78, 65.48, 65.76, 65.83, 65.52, 65.68]

Epoch: 11
2024-03-10 02:04:22.040144 epoch: 11 step: 0 cls_loss= 0.36818 (114685 samples/sec)
2024-03-10 02:04:31.380592 epoch: 11 step: 100 cls_loss= 0.27934 (3215 samples/sec)
saving....
2024-03-10 02:04:41.315776------------------------------------------------------ Precision@1: 65.74% 

[65.73, 65.66, 65.82, 65.77, 65.44, 65.78, 65.48, 65.76, 65.83, 65.52, 65.68, 65.74]

Epoch: 12
2024-03-10 02:04:41.576932 epoch: 12 step: 0 cls_loss= 0.31063 (115553 samples/sec)
2024-03-10 02:04:50.895294 epoch: 12 step: 100 cls_loss= 0.39354 (3222 samples/sec)
saving....
2024-03-10 02:05:00.876865------------------------------------------------------ Precision@1: 65.55% 

[65.73, 65.66, 65.82, 65.77, 65.44, 65.78, 65.48, 65.76, 65.83, 65.52, 65.68, 65.74, 65.55]

Epoch: 13
2024-03-10 02:05:01.140639 epoch: 13 step: 0 cls_loss= 0.32255 (114427 samples/sec)
2024-03-10 02:05:10.547539 epoch: 13 step: 100 cls_loss= 0.26765 (3192 samples/sec)
saving....
2024-03-10 02:05:20.634682------------------------------------------------------ Precision@1: 65.62% 

[65.73, 65.66, 65.82, 65.77, 65.44, 65.78, 65.48, 65.76, 65.83, 65.52, 65.68, 65.74, 65.55, 65.62]

Epoch: 14
2024-03-10 02:05:20.885649 epoch: 14 step: 0 cls_loss= 0.28173 (120285 samples/sec)
2024-03-10 02:05:30.269026 epoch: 14 step: 100 cls_loss= 0.30459 (3200 samples/sec)
saving....
2024-03-10 02:05:40.266673------------------------------------------------------ Precision@1: 65.68% 

[65.73, 65.66, 65.82, 65.77, 65.44, 65.78, 65.48, 65.76, 65.83, 65.52, 65.68, 65.74, 65.55, 65.62, 65.68]

Epoch: 15
2024-03-10 02:05:40.512323 epoch: 15 step: 0 cls_loss= 0.33163 (122873 samples/sec)
2024-03-10 02:05:49.878258 epoch: 15 step: 100 cls_loss= 0.32005 (3206 samples/sec)
saving....
2024-03-10 02:05:59.818344------------------------------------------------------ Precision@1: 65.78% 

[65.73, 65.66, 65.82, 65.77, 65.44, 65.78, 65.48, 65.76, 65.83, 65.52, 65.68, 65.74, 65.55, 65.62, 65.68, 65.78]

Epoch: 16
2024-03-10 02:06:00.074879 epoch: 16 step: 0 cls_loss= 0.29930 (117690 samples/sec)
2024-03-10 02:06:09.418353 epoch: 16 step: 100 cls_loss= 0.30289 (3213 samples/sec)
saving....
2024-03-10 02:06:19.362107------------------------------------------------------ Precision@1: 65.83% 

[65.73, 65.66, 65.82, 65.77, 65.44, 65.78, 65.48, 65.76, 65.83, 65.52, 65.68, 65.74, 65.55, 65.62, 65.68, 65.78, 65.83]

Epoch: 17
2024-03-10 02:06:19.620290 epoch: 17 step: 0 cls_loss= 0.30594 (116778 samples/sec)
2024-03-10 02:06:28.946556 epoch: 17 step: 100 cls_loss= 0.34905 (3219 samples/sec)
saving....
2024-03-10 02:06:38.985274------------------------------------------------------ Precision@1: 65.91% 

[65.73, 65.66, 65.82, 65.77, 65.44, 65.78, 65.48, 65.76, 65.83, 65.52, 65.68, 65.74, 65.55, 65.62, 65.68, 65.78, 65.83, 65.91]

Epoch: 18
2024-03-10 02:06:39.243733 epoch: 18 step: 0 cls_loss= 0.31687 (116781 samples/sec)
2024-03-10 02:06:48.625190 epoch: 18 step: 100 cls_loss= 0.30144 (3199 samples/sec)
saving....
2024-03-10 02:06:58.746888------------------------------------------------------ Precision@1: 65.69% 

[65.73, 65.66, 65.82, 65.77, 65.44, 65.78, 65.48, 65.76, 65.83, 65.52, 65.68, 65.74, 65.55, 65.62, 65.68, 65.78, 65.83, 65.91, 65.69]

Epoch: 19
2024-03-10 02:06:59.013767 epoch: 19 step: 0 cls_loss= 0.28117 (113025 samples/sec)
2024-03-10 02:07:08.377066 epoch: 19 step: 100 cls_loss= 0.28291 (3207 samples/sec)
saving....
2024-03-10 02:07:18.438053------------------------------------------------------ Precision@1: 65.70% 

[65.73, 65.66, 65.82, 65.77, 65.44, 65.78, 65.48, 65.76, 65.83, 65.52, 65.68, 65.74, 65.55, 65.62, 65.68, 65.78, 65.83, 65.91, 65.69, 65.7]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 02:07:21.308349 epoch: 0 step: 0 cls_loss= 0.33195 (40364 samples/sec)
2024-03-10 02:07:30.611148 epoch: 0 step: 100 cls_loss= 0.28008 (3224 samples/sec)
saving....
2024-03-10 02:07:40.790291------------------------------------------------------ Precision@1: 65.75% 

[65.75]

Epoch: 1
2024-03-10 02:07:41.038575 epoch: 1 step: 0 cls_loss= 0.26593 (121543 samples/sec)
2024-03-10 02:07:50.346876 epoch: 1 step: 100 cls_loss= 0.30219 (3226 samples/sec)
saving....
2024-03-10 02:08:00.262935------------------------------------------------------ Precision@1: 65.65% 

[65.75, 65.65]

Epoch: 2
2024-03-10 02:08:00.528124 epoch: 2 step: 0 cls_loss= 0.34450 (113748 samples/sec)
2024-03-10 02:08:09.855071 epoch: 2 step: 100 cls_loss= 0.28432 (3219 samples/sec)
saving....
2024-03-10 02:08:19.879153------------------------------------------------------ Precision@1: 65.76% 

[65.75, 65.65, 65.76]

Epoch: 3
2024-03-10 02:08:20.150846 epoch: 3 step: 0 cls_loss= 0.29667 (111065 samples/sec)
2024-03-10 02:08:29.482750 epoch: 3 step: 100 cls_loss= 0.34342 (3217 samples/sec)
saving....
2024-03-10 02:08:39.514814------------------------------------------------------ Precision@1: 65.61% 

[65.75, 65.65, 65.76, 65.61]

Epoch: 4
2024-03-10 02:08:39.757742 epoch: 4 step: 0 cls_loss= 0.33335 (124251 samples/sec)
2024-03-10 02:08:49.059649 epoch: 4 step: 100 cls_loss= 0.28080 (3228 samples/sec)
saving....
2024-03-10 02:08:58.961103------------------------------------------------------ Precision@1: 65.74% 

[65.75, 65.65, 65.76, 65.61, 65.74]

Epoch: 5
2024-03-10 02:08:59.227319 epoch: 5 step: 0 cls_loss= 0.30440 (113170 samples/sec)
2024-03-10 02:09:08.643308 epoch: 5 step: 100 cls_loss= 0.35953 (3189 samples/sec)
saving....
2024-03-10 02:09:18.607446------------------------------------------------------ Precision@1: 65.76% 

[65.75, 65.65, 65.76, 65.61, 65.74, 65.76]

Epoch: 6
2024-03-10 02:09:18.866918 epoch: 6 step: 0 cls_loss= 0.28029 (116307 samples/sec)
2024-03-10 02:09:28.192838 epoch: 6 step: 100 cls_loss= 0.31984 (3220 samples/sec)
saving....
2024-03-10 02:09:38.097248------------------------------------------------------ Precision@1: 65.70% 

[65.75, 65.65, 65.76, 65.61, 65.74, 65.76, 65.7]

Epoch: 7
2024-03-10 02:09:38.360524 epoch: 7 step: 0 cls_loss= 0.32396 (114663 samples/sec)
2024-03-10 02:09:47.713195 epoch: 7 step: 100 cls_loss= 0.33939 (3210 samples/sec)
saving....
2024-03-10 02:09:57.670084------------------------------------------------------ Precision@1: 65.64% 

[65.75, 65.65, 65.76, 65.61, 65.74, 65.76, 65.7, 65.64]

Epoch: 8
2024-03-10 02:09:57.912863 epoch: 8 step: 0 cls_loss= 0.36346 (124368 samples/sec)
2024-03-10 02:10:07.252596 epoch: 8 step: 100 cls_loss= 0.30650 (3215 samples/sec)
saving....
2024-03-10 02:10:17.261760------------------------------------------------------ Precision@1: 65.69% 

[65.75, 65.65, 65.76, 65.61, 65.74, 65.76, 65.7, 65.64, 65.69]

Epoch: 9
2024-03-10 02:10:17.517191 epoch: 9 step: 0 cls_loss= 0.32293 (118169 samples/sec)
2024-03-10 02:10:26.840696 epoch: 9 step: 100 cls_loss= 0.31497 (3220 samples/sec)
saving....
2024-03-10 02:10:36.766277------------------------------------------------------ Precision@1: 65.75% 

[65.75, 65.65, 65.76, 65.61, 65.74, 65.76, 65.7, 65.64, 65.69, 65.75]

Epoch: 10
2024-03-10 02:10:37.031669 epoch: 10 step: 0 cls_loss= 0.35334 (113547 samples/sec)
2024-03-10 02:10:46.369543 epoch: 10 step: 100 cls_loss= 0.32828 (3215 samples/sec)
saving....
2024-03-10 02:10:56.441260------------------------------------------------------ Precision@1: 65.72% 

[65.75, 65.65, 65.76, 65.61, 65.74, 65.76, 65.7, 65.64, 65.69, 65.75, 65.72]

Epoch: 11
2024-03-10 02:10:56.698557 epoch: 11 step: 0 cls_loss= 0.33803 (117243 samples/sec)
2024-03-10 02:11:06.049171 epoch: 11 step: 100 cls_loss= 0.38476 (3211 samples/sec)
saving....
2024-03-10 02:11:15.991608------------------------------------------------------ Precision@1: 65.65% 

[65.75, 65.65, 65.76, 65.61, 65.74, 65.76, 65.7, 65.64, 65.69, 65.75, 65.72, 65.65]

Epoch: 12
2024-03-10 02:11:16.260755 epoch: 12 step: 0 cls_loss= 0.31865 (112168 samples/sec)
2024-03-10 02:11:25.590902 epoch: 12 step: 100 cls_loss= 0.37132 (3218 samples/sec)
saving....
2024-03-10 02:11:35.594066------------------------------------------------------ Precision@1: 65.90% 

[65.75, 65.65, 65.76, 65.61, 65.74, 65.76, 65.7, 65.64, 65.69, 65.75, 65.72, 65.65, 65.9]

Epoch: 13
2024-03-10 02:11:35.857192 epoch: 13 step: 0 cls_loss= 0.28464 (114698 samples/sec)
2024-03-10 02:11:45.198440 epoch: 13 step: 100 cls_loss= 0.32345 (3212 samples/sec)
saving....
2024-03-10 02:11:55.180729------------------------------------------------------ Precision@1: 65.84% 

[65.75, 65.65, 65.76, 65.61, 65.74, 65.76, 65.7, 65.64, 65.69, 65.75, 65.72, 65.65, 65.9, 65.84]

Epoch: 14
2024-03-10 02:11:55.448379 epoch: 14 step: 0 cls_loss= 0.30288 (112590 samples/sec)
2024-03-10 02:12:04.780556 epoch: 14 step: 100 cls_loss= 0.31905 (3217 samples/sec)
saving....
2024-03-10 02:12:14.749285------------------------------------------------------ Precision@1: 65.67% 

[65.75, 65.65, 65.76, 65.61, 65.74, 65.76, 65.7, 65.64, 65.69, 65.75, 65.72, 65.65, 65.9, 65.84, 65.67]

Epoch: 15
2024-03-10 02:12:14.987484 epoch: 15 step: 0 cls_loss= 0.28051 (126743 samples/sec)
2024-03-10 02:12:24.361450 epoch: 15 step: 100 cls_loss= 0.32991 (3203 samples/sec)
saving....
2024-03-10 02:12:34.290706------------------------------------------------------ Precision@1: 65.61% 

[65.75, 65.65, 65.76, 65.61, 65.74, 65.76, 65.7, 65.64, 65.69, 65.75, 65.72, 65.65, 65.9, 65.84, 65.67, 65.61]

Epoch: 16
2024-03-10 02:12:34.552070 epoch: 16 step: 0 cls_loss= 0.29912 (115421 samples/sec)
2024-03-10 02:12:43.900004 epoch: 16 step: 100 cls_loss= 0.29167 (3211 samples/sec)
saving....
2024-03-10 02:12:53.816153------------------------------------------------------ Precision@1: 65.72% 

[65.75, 65.65, 65.76, 65.61, 65.74, 65.76, 65.7, 65.64, 65.69, 65.75, 65.72, 65.65, 65.9, 65.84, 65.67, 65.61, 65.72]

Epoch: 17
2024-03-10 02:12:54.077702 epoch: 17 step: 0 cls_loss= 0.35642 (115330 samples/sec)
2024-03-10 02:13:03.426233 epoch: 17 step: 100 cls_loss= 0.31999 (3212 samples/sec)
saving....
2024-03-10 02:13:13.388370------------------------------------------------------ Precision@1: 65.75% 

[65.75, 65.65, 65.76, 65.61, 65.74, 65.76, 65.7, 65.64, 65.69, 65.75, 65.72, 65.65, 65.9, 65.84, 65.67, 65.61, 65.72, 65.75]

Epoch: 18
2024-03-10 02:13:13.652605 epoch: 18 step: 0 cls_loss= 0.32962 (114178 samples/sec)
2024-03-10 02:13:22.960065 epoch: 18 step: 100 cls_loss= 0.34602 (3224 samples/sec)
saving....
2024-03-10 02:13:32.934430------------------------------------------------------ Precision@1: 65.85% 

[65.75, 65.65, 65.76, 65.61, 65.74, 65.76, 65.7, 65.64, 65.69, 65.75, 65.72, 65.65, 65.9, 65.84, 65.67, 65.61, 65.72, 65.75, 65.85]

Epoch: 19
2024-03-10 02:13:33.188052 epoch: 19 step: 0 cls_loss= 0.33229 (119048 samples/sec)
2024-03-10 02:13:42.534521 epoch: 19 step: 100 cls_loss= 0.32738 (3211 samples/sec)
saving....
2024-03-10 02:13:52.462887------------------------------------------------------ Precision@1: 65.74% 

[65.75, 65.65, 65.76, 65.61, 65.74, 65.76, 65.7, 65.64, 65.69, 65.75, 65.72, 65.65, 65.9, 65.84, 65.67, 65.61, 65.72, 65.75, 65.85, 65.74]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 02:13:55.329216 epoch: 0 step: 0 cls_loss= 0.30881 (41260 samples/sec)
2024-03-10 02:14:04.673388 epoch: 0 step: 100 cls_loss= 0.35972 (3210 samples/sec)
saving....
2024-03-10 02:14:14.890652------------------------------------------------------ Precision@1: 65.81% 

[65.81]

Epoch: 1
2024-03-10 02:14:15.143691 epoch: 1 step: 0 cls_loss= 0.36695 (119292 samples/sec)
2024-03-10 02:14:24.495158 epoch: 1 step: 100 cls_loss= 0.27026 (3211 samples/sec)
saving....
2024-03-10 02:14:34.393546------------------------------------------------------ Precision@1: 65.66% 

[65.81, 65.66]

Epoch: 2
2024-03-10 02:14:34.648328 epoch: 2 step: 0 cls_loss= 0.33638 (118448 samples/sec)
2024-03-10 02:14:43.965410 epoch: 2 step: 100 cls_loss= 0.32848 (3223 samples/sec)
saving....
2024-03-10 02:14:53.868439------------------------------------------------------ Precision@1: 65.69% 

[65.81, 65.66, 65.69]

Epoch: 3
2024-03-10 02:14:54.121697 epoch: 3 step: 0 cls_loss= 0.26109 (119207 samples/sec)
2024-03-10 02:15:03.420652 epoch: 3 step: 100 cls_loss= 0.28683 (3227 samples/sec)
saving....
2024-03-10 02:15:13.405564------------------------------------------------------ Precision@1: 65.71% 

[65.81, 65.66, 65.69, 65.71]

Epoch: 4
2024-03-10 02:15:13.671963 epoch: 4 step: 0 cls_loss= 0.24884 (113257 samples/sec)
2024-03-10 02:15:23.031092 epoch: 4 step: 100 cls_loss= 0.28790 (3208 samples/sec)
saving....
2024-03-10 02:15:32.949284------------------------------------------------------ Precision@1: 65.70% 

[65.81, 65.66, 65.69, 65.71, 65.7]

Epoch: 5
2024-03-10 02:15:33.202349 epoch: 5 step: 0 cls_loss= 0.28556 (119254 samples/sec)
2024-03-10 02:15:42.525867 epoch: 5 step: 100 cls_loss= 0.33704 (3220 samples/sec)
saving....
2024-03-10 02:15:52.432507------------------------------------------------------ Precision@1: 65.77% 

[65.81, 65.66, 65.69, 65.71, 65.7, 65.77]

Epoch: 6
2024-03-10 02:15:52.680493 epoch: 6 step: 0 cls_loss= 0.36575 (121717 samples/sec)
2024-03-10 02:16:02.023583 epoch: 6 step: 100 cls_loss= 0.29665 (3214 samples/sec)
saving....
2024-03-10 02:16:11.920172------------------------------------------------------ Precision@1: 65.67% 

[65.81, 65.66, 65.69, 65.71, 65.7, 65.77, 65.67]

Epoch: 7
2024-03-10 02:16:12.180495 epoch: 7 step: 0 cls_loss= 0.25896 (115815 samples/sec)
2024-03-10 02:16:21.496299 epoch: 7 step: 100 cls_loss= 0.28384 (3223 samples/sec)
saving....
2024-03-10 02:16:31.481864------------------------------------------------------ Precision@1: 65.60% 

[65.81, 65.66, 65.69, 65.71, 65.7, 65.77, 65.67, 65.6]

Epoch: 8
2024-03-10 02:16:31.731144 epoch: 8 step: 0 cls_loss= 0.34487 (121063 samples/sec)
2024-03-10 02:16:41.111564 epoch: 8 step: 100 cls_loss= 0.25599 (3201 samples/sec)
saving....
2024-03-10 02:16:51.087629------------------------------------------------------ Precision@1: 65.62% 

[65.81, 65.66, 65.69, 65.71, 65.7, 65.77, 65.67, 65.6, 65.62]

Epoch: 9
2024-03-10 02:16:51.349691 epoch: 9 step: 0 cls_loss= 0.31306 (115158 samples/sec)
2024-03-10 02:17:00.721385 epoch: 9 step: 100 cls_loss= 0.31911 (3202 samples/sec)
saving....
2024-03-10 02:17:10.638382------------------------------------------------------ Precision@1: 65.81% 

[65.81, 65.66, 65.69, 65.71, 65.7, 65.77, 65.67, 65.6, 65.62, 65.81]

Epoch: 10
2024-03-10 02:17:10.886330 epoch: 10 step: 0 cls_loss= 0.38977 (121746 samples/sec)
2024-03-10 02:17:20.220259 epoch: 10 step: 100 cls_loss= 0.31198 (3217 samples/sec)
saving....
2024-03-10 02:17:30.137393------------------------------------------------------ Precision@1: 65.72% 

[65.81, 65.66, 65.69, 65.71, 65.7, 65.77, 65.67, 65.6, 65.62, 65.81, 65.72]

Epoch: 11
2024-03-10 02:17:30.385236 epoch: 11 step: 0 cls_loss= 0.34307 (121823 samples/sec)
2024-03-10 02:17:39.760578 epoch: 11 step: 100 cls_loss= 0.30367 (3203 samples/sec)
saving....
2024-03-10 02:17:49.667520------------------------------------------------------ Precision@1: 65.74% 

[65.81, 65.66, 65.69, 65.71, 65.7, 65.77, 65.67, 65.6, 65.62, 65.81, 65.72, 65.74]

Epoch: 12
2024-03-10 02:17:49.931334 epoch: 12 step: 0 cls_loss= 0.31384 (114390 samples/sec)
2024-03-10 02:17:59.262867 epoch: 12 step: 100 cls_loss= 0.30480 (3218 samples/sec)
saving....
2024-03-10 02:18:09.159346------------------------------------------------------ Precision@1: 65.77% 

[65.81, 65.66, 65.69, 65.71, 65.7, 65.77, 65.67, 65.6, 65.62, 65.81, 65.72, 65.74, 65.77]

Epoch: 13
2024-03-10 02:18:09.408100 epoch: 13 step: 0 cls_loss= 0.34922 (121339 samples/sec)
2024-03-10 02:18:18.743562 epoch: 13 step: 100 cls_loss= 0.29942 (3216 samples/sec)
saving....
2024-03-10 02:18:28.605386------------------------------------------------------ Precision@1: 65.50% 

[65.81, 65.66, 65.69, 65.71, 65.7, 65.77, 65.67, 65.6, 65.62, 65.81, 65.72, 65.74, 65.77, 65.5]

Epoch: 14
2024-03-10 02:18:28.857260 epoch: 14 step: 0 cls_loss= 0.33061 (119916 samples/sec)
2024-03-10 02:18:38.194929 epoch: 14 step: 100 cls_loss= 0.33719 (3215 samples/sec)
saving....
2024-03-10 02:18:48.082996------------------------------------------------------ Precision@1: 66.07% 

[65.81, 65.66, 65.69, 65.71, 65.7, 65.77, 65.67, 65.6, 65.62, 65.81, 65.72, 65.74, 65.77, 65.5, 66.07]

Epoch: 15
2024-03-10 02:18:48.334467 epoch: 15 step: 0 cls_loss= 0.38107 (119982 samples/sec)
2024-03-10 02:18:57.678807 epoch: 15 step: 100 cls_loss= 0.33847 (3213 samples/sec)
saving....
2024-03-10 02:19:07.557977------------------------------------------------------ Precision@1: 65.70% 

[65.81, 65.66, 65.69, 65.71, 65.7, 65.77, 65.67, 65.6, 65.62, 65.81, 65.72, 65.74, 65.77, 65.5, 66.07, 65.7]

Epoch: 16
2024-03-10 02:19:07.822221 epoch: 16 step: 0 cls_loss= 0.32561 (114266 samples/sec)
2024-03-10 02:19:17.111929 epoch: 16 step: 100 cls_loss= 0.35168 (3232 samples/sec)
saving....
2024-03-10 02:19:26.935552------------------------------------------------------ Precision@1: 65.72% 

[65.81, 65.66, 65.69, 65.71, 65.7, 65.77, 65.67, 65.6, 65.62, 65.81, 65.72, 65.74, 65.77, 65.5, 66.07, 65.7, 65.72]

Epoch: 17
2024-03-10 02:19:27.196484 epoch: 17 step: 0 cls_loss= 0.28695 (115678 samples/sec)
2024-03-10 02:19:36.501153 epoch: 17 step: 100 cls_loss= 0.31501 (3227 samples/sec)
saving....
2024-03-10 02:19:46.397194------------------------------------------------------ Precision@1: 65.72% 

[65.81, 65.66, 65.69, 65.71, 65.7, 65.77, 65.67, 65.6, 65.62, 65.81, 65.72, 65.74, 65.77, 65.5, 66.07, 65.7, 65.72, 65.72]

Epoch: 18
2024-03-10 02:19:46.636937 epoch: 18 step: 0 cls_loss= 0.30096 (125907 samples/sec)
2024-03-10 02:19:55.939549 epoch: 18 step: 100 cls_loss= 0.27184 (3228 samples/sec)
saving....
2024-03-10 02:20:05.827220------------------------------------------------------ Precision@1: 65.66% 

[65.81, 65.66, 65.69, 65.71, 65.7, 65.77, 65.67, 65.6, 65.62, 65.81, 65.72, 65.74, 65.77, 65.5, 66.07, 65.7, 65.72, 65.72, 65.66]

Epoch: 19
2024-03-10 02:20:06.082862 epoch: 19 step: 0 cls_loss= 0.27301 (118056 samples/sec)
2024-03-10 02:20:15.431616 epoch: 19 step: 100 cls_loss= 0.33093 (3210 samples/sec)
saving....
2024-03-10 02:20:25.375862------------------------------------------------------ Precision@1: 65.94% 

[65.81, 65.66, 65.69, 65.71, 65.7, 65.77, 65.67, 65.6, 65.62, 65.81, 65.72, 65.74, 65.77, 65.5, 66.07, 65.7, 65.72, 65.72, 65.66, 65.94]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 02:20:28.219930 epoch: 0 step: 0 cls_loss= 0.32740 (41619 samples/sec)
2024-03-10 02:20:37.534499 epoch: 0 step: 100 cls_loss= 0.31563 (3220 samples/sec)
saving....
2024-03-10 02:20:47.742095------------------------------------------------------ Precision@1: 65.57% 

[65.57]

Epoch: 1
2024-03-10 02:20:47.991945 epoch: 1 step: 0 cls_loss= 0.30688 (120840 samples/sec)
2024-03-10 02:20:57.329842 epoch: 1 step: 100 cls_loss= 0.35484 (3215 samples/sec)
saving....
2024-03-10 02:21:07.279431------------------------------------------------------ Precision@1: 65.87% 

[65.57, 65.87]

Epoch: 2
2024-03-10 02:21:07.533130 epoch: 2 step: 0 cls_loss= 0.30554 (118905 samples/sec)
2024-03-10 02:21:16.852067 epoch: 2 step: 100 cls_loss= 0.39353 (3220 samples/sec)
saving....
2024-03-10 02:21:26.886073------------------------------------------------------ Precision@1: 65.67% 

[65.57, 65.87, 65.67]

Epoch: 3
2024-03-10 02:21:27.128481 epoch: 3 step: 0 cls_loss= 0.34789 (124532 samples/sec)
2024-03-10 02:21:36.455118 epoch: 3 step: 100 cls_loss= 0.28891 (3218 samples/sec)
saving....
2024-03-10 02:21:46.426803------------------------------------------------------ Precision@1: 65.75% 

[65.57, 65.87, 65.67, 65.75]

Epoch: 4
2024-03-10 02:21:46.697050 epoch: 4 step: 0 cls_loss= 0.31925 (111567 samples/sec)
2024-03-10 02:21:56.028672 epoch: 4 step: 100 cls_loss= 0.35104 (3218 samples/sec)
saving....
2024-03-10 02:22:05.990434------------------------------------------------------ Precision@1: 65.69% 

[65.57, 65.87, 65.67, 65.75, 65.69]

Epoch: 5
2024-03-10 02:22:06.242973 epoch: 5 step: 0 cls_loss= 0.27425 (119478 samples/sec)
2024-03-10 02:22:16.272265 epoch: 5 step: 100 cls_loss= 0.33851 (2994 samples/sec)
saving....
2024-03-10 02:22:26.182526------------------------------------------------------ Precision@1: 65.72% 

[65.57, 65.87, 65.67, 65.75, 65.69, 65.72]

Epoch: 6
2024-03-10 02:22:26.443189 epoch: 6 step: 0 cls_loss= 0.34016 (115731 samples/sec)
2024-03-10 02:22:35.815334 epoch: 6 step: 100 cls_loss= 0.34890 (3202 samples/sec)
saving....
2024-03-10 02:22:45.749286------------------------------------------------------ Precision@1: 66.03% 

[65.57, 65.87, 65.67, 65.75, 65.69, 65.72, 66.03]

Epoch: 7
2024-03-10 02:22:46.001224 epoch: 7 step: 0 cls_loss= 0.33462 (119847 samples/sec)
2024-03-10 02:22:55.353337 epoch: 7 step: 100 cls_loss= 0.30005 (3211 samples/sec)
saving....
2024-03-10 02:23:05.239097------------------------------------------------------ Precision@1: 65.60% 

[65.57, 65.87, 65.67, 65.75, 65.69, 65.72, 66.03, 65.6]

Epoch: 8
2024-03-10 02:23:05.506026 epoch: 8 step: 0 cls_loss= 0.29484 (113049 samples/sec)
2024-03-10 02:23:14.860129 epoch: 8 step: 100 cls_loss= 0.28991 (3210 samples/sec)
saving....
2024-03-10 02:23:24.818208------------------------------------------------------ Precision@1: 65.51% 

[65.57, 65.87, 65.67, 65.75, 65.69, 65.72, 66.03, 65.6, 65.51]

Epoch: 9
2024-03-10 02:23:25.089734 epoch: 9 step: 0 cls_loss= 0.36395 (111013 samples/sec)
2024-03-10 02:23:34.420216 epoch: 9 step: 100 cls_loss= 0.21808 (3218 samples/sec)
saving....
2024-03-10 02:23:44.387295------------------------------------------------------ Precision@1: 65.75% 

[65.57, 65.87, 65.67, 65.75, 65.69, 65.72, 66.03, 65.6, 65.51, 65.75]

Epoch: 10
2024-03-10 02:23:44.646351 epoch: 10 step: 0 cls_loss= 0.26398 (116486 samples/sec)
2024-03-10 02:23:54.021028 epoch: 10 step: 100 cls_loss= 0.33140 (3201 samples/sec)
saving....
2024-03-10 02:24:03.997598------------------------------------------------------ Precision@1: 65.82% 

[65.57, 65.87, 65.67, 65.75, 65.69, 65.72, 66.03, 65.6, 65.51, 65.75, 65.82]

Epoch: 11
2024-03-10 02:24:04.279399 epoch: 11 step: 0 cls_loss= 0.35194 (106965 samples/sec)
2024-03-10 02:24:13.610479 epoch: 11 step: 100 cls_loss= 0.30106 (3218 samples/sec)
saving....
2024-03-10 02:24:23.653063------------------------------------------------------ Precision@1: 65.89% 

[65.57, 65.87, 65.67, 65.75, 65.69, 65.72, 66.03, 65.6, 65.51, 65.75, 65.82, 65.89]

Epoch: 12
2024-03-10 02:24:23.913841 epoch: 12 step: 0 cls_loss= 0.31973 (115770 samples/sec)
2024-03-10 02:24:33.222057 epoch: 12 step: 100 cls_loss= 0.35234 (3226 samples/sec)
saving....
2024-03-10 02:24:43.095210------------------------------------------------------ Precision@1: 65.86% 

[65.57, 65.87, 65.67, 65.75, 65.69, 65.72, 66.03, 65.6, 65.51, 65.75, 65.82, 65.89, 65.86]

Epoch: 13
2024-03-10 02:24:43.354770 epoch: 13 step: 0 cls_loss= 0.32443 (116230 samples/sec)
2024-03-10 02:24:52.691623 epoch: 13 step: 100 cls_loss= 0.36616 (3216 samples/sec)
saving....
2024-03-10 02:25:02.630231------------------------------------------------------ Precision@1: 65.77% 

[65.57, 65.87, 65.67, 65.75, 65.69, 65.72, 66.03, 65.6, 65.51, 65.75, 65.82, 65.89, 65.86, 65.77]

Epoch: 14
2024-03-10 02:25:02.892935 epoch: 14 step: 0 cls_loss= 0.31971 (114856 samples/sec)
2024-03-10 02:25:12.237451 epoch: 14 step: 100 cls_loss= 0.31848 (3211 samples/sec)
saving....
2024-03-10 02:25:22.202583------------------------------------------------------ Precision@1: 65.76% 

[65.57, 65.87, 65.67, 65.75, 65.69, 65.72, 66.03, 65.6, 65.51, 65.75, 65.82, 65.89, 65.86, 65.77, 65.76]

Epoch: 15
2024-03-10 02:25:22.453973 epoch: 15 step: 0 cls_loss= 0.27279 (120099 samples/sec)
2024-03-10 02:25:31.780997 epoch: 15 step: 100 cls_loss= 0.31010 (3219 samples/sec)
saving....
2024-03-10 02:25:41.717178------------------------------------------------------ Precision@1: 65.67% 

[65.57, 65.87, 65.67, 65.75, 65.69, 65.72, 66.03, 65.6, 65.51, 65.75, 65.82, 65.89, 65.86, 65.77, 65.76, 65.67]

Epoch: 16
2024-03-10 02:25:41.976080 epoch: 16 step: 0 cls_loss= 0.30241 (116390 samples/sec)
2024-03-10 02:25:51.313224 epoch: 16 step: 100 cls_loss= 0.32806 (3214 samples/sec)
saving....
2024-03-10 02:26:01.382842------------------------------------------------------ Precision@1: 65.76% 

[65.57, 65.87, 65.67, 65.75, 65.69, 65.72, 66.03, 65.6, 65.51, 65.75, 65.82, 65.89, 65.86, 65.77, 65.76, 65.67, 65.76]

Epoch: 17
2024-03-10 02:26:01.658295 epoch: 17 step: 0 cls_loss= 0.33011 (109535 samples/sec)
2024-03-10 02:26:10.999345 epoch: 17 step: 100 cls_loss= 0.31934 (3214 samples/sec)
saving....
2024-03-10 02:26:20.941148------------------------------------------------------ Precision@1: 65.91% 

[65.57, 65.87, 65.67, 65.75, 65.69, 65.72, 66.03, 65.6, 65.51, 65.75, 65.82, 65.89, 65.86, 65.77, 65.76, 65.67, 65.76, 65.91]

Epoch: 18
2024-03-10 02:26:21.205907 epoch: 18 step: 0 cls_loss= 0.37175 (113970 samples/sec)
2024-03-10 02:26:30.552790 epoch: 18 step: 100 cls_loss= 0.32007 (3212 samples/sec)
saving....
2024-03-10 02:26:40.481143------------------------------------------------------ Precision@1: 65.76% 

[65.57, 65.87, 65.67, 65.75, 65.69, 65.72, 66.03, 65.6, 65.51, 65.75, 65.82, 65.89, 65.86, 65.77, 65.76, 65.67, 65.76, 65.91, 65.76]

Epoch: 19
2024-03-10 02:26:40.754612 epoch: 19 step: 0 cls_loss= 0.32454 (110149 samples/sec)
2024-03-10 02:26:50.150238 epoch: 19 step: 100 cls_loss= 0.35556 (3194 samples/sec)
saving....
2024-03-10 02:27:00.071918------------------------------------------------------ Precision@1: 65.76% 

[65.57, 65.87, 65.67, 65.75, 65.69, 65.72, 66.03, 65.6, 65.51, 65.75, 65.82, 65.89, 65.86, 65.77, 65.76, 65.67, 65.76, 65.91, 65.76, 65.76]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 02:27:02.974151 epoch: 0 step: 0 cls_loss= 0.43081 (40147 samples/sec)
2024-03-10 02:27:12.280811 epoch: 0 step: 100 cls_loss= 0.36567 (3223 samples/sec)
saving....
2024-03-10 02:27:22.390128------------------------------------------------------ Precision@1: 65.81% 

[65.81]

Epoch: 1
2024-03-10 02:27:22.641989 epoch: 1 step: 0 cls_loss= 0.32951 (119782 samples/sec)
2024-03-10 02:27:31.937774 epoch: 1 step: 100 cls_loss= 0.29235 (3228 samples/sec)
saving....
2024-03-10 02:27:41.853756------------------------------------------------------ Precision@1: 65.72% 

[65.81, 65.72]

Epoch: 2
2024-03-10 02:27:42.127633 epoch: 2 step: 0 cls_loss= 0.35633 (110154 samples/sec)
2024-03-10 02:27:51.482825 epoch: 2 step: 100 cls_loss= 0.27813 (3206 samples/sec)
saving....
2024-03-10 02:28:01.440783------------------------------------------------------ Precision@1: 65.81% 

[65.81, 65.72, 65.81]

Epoch: 3
2024-03-10 02:28:01.705370 epoch: 3 step: 0 cls_loss= 0.37731 (114044 samples/sec)
2024-03-10 02:28:11.001469 epoch: 3 step: 100 cls_loss= 0.32189 (3230 samples/sec)
saving....
2024-03-10 02:28:20.943524------------------------------------------------------ Precision@1: 65.77% 

[65.81, 65.72, 65.81, 65.77]

Epoch: 4
2024-03-10 02:28:21.199664 epoch: 4 step: 0 cls_loss= 0.31220 (117867 samples/sec)
2024-03-10 02:28:30.562181 epoch: 4 step: 100 cls_loss= 0.31571 (3205 samples/sec)
saving....
2024-03-10 02:28:40.544008------------------------------------------------------ Precision@1: 65.67% 

[65.81, 65.72, 65.81, 65.77, 65.67]

Epoch: 5
2024-03-10 02:28:40.801317 epoch: 5 step: 0 cls_loss= 0.37316 (117237 samples/sec)
2024-03-10 02:28:50.129153 epoch: 5 step: 100 cls_loss= 0.30329 (3219 samples/sec)
saving....
2024-03-10 02:29:00.014339------------------------------------------------------ Precision@1: 66.01% 

[65.81, 65.72, 65.81, 65.77, 65.67, 66.01]

Epoch: 6
2024-03-10 02:29:00.265223 epoch: 6 step: 0 cls_loss= 0.27312 (120331 samples/sec)
2024-03-10 02:29:09.593807 epoch: 6 step: 100 cls_loss= 0.31037 (3217 samples/sec)
saving....
2024-03-10 02:29:19.535679------------------------------------------------------ Precision@1: 65.94% 

[65.81, 65.72, 65.81, 65.77, 65.67, 66.01, 65.94]

Epoch: 7
2024-03-10 02:29:19.822722 epoch: 7 step: 0 cls_loss= 0.28002 (104987 samples/sec)
2024-03-10 02:29:29.150030 epoch: 7 step: 100 cls_loss= 0.32640 (3217 samples/sec)
saving....
2024-03-10 02:29:39.046848------------------------------------------------------ Precision@1: 65.67% 

[65.81, 65.72, 65.81, 65.77, 65.67, 66.01, 65.94, 65.67]

Epoch: 8
2024-03-10 02:29:39.306987 epoch: 8 step: 0 cls_loss= 0.33464 (115967 samples/sec)
2024-03-10 02:29:48.635057 epoch: 8 step: 100 cls_loss= 0.41600 (3219 samples/sec)
saving....
2024-03-10 02:29:58.513812------------------------------------------------------ Precision@1: 65.77% 

[65.81, 65.72, 65.81, 65.77, 65.67, 66.01, 65.94, 65.67, 65.77]

Epoch: 9
2024-03-10 02:29:58.765670 epoch: 9 step: 0 cls_loss= 0.32588 (119874 samples/sec)
2024-03-10 02:30:08.063498 epoch: 9 step: 100 cls_loss= 0.36659 (3229 samples/sec)
saving....
2024-03-10 02:30:17.954943------------------------------------------------------ Precision@1: 65.66% 

[65.81, 65.72, 65.81, 65.77, 65.67, 66.01, 65.94, 65.67, 65.77, 65.66]

Epoch: 10
2024-03-10 02:30:18.212652 epoch: 10 step: 0 cls_loss= 0.31967 (117151 samples/sec)
2024-03-10 02:30:27.516886 epoch: 10 step: 100 cls_loss= 0.36063 (3227 samples/sec)
saving....
2024-03-10 02:30:37.364284------------------------------------------------------ Precision@1: 65.76% 

[65.81, 65.72, 65.81, 65.77, 65.67, 66.01, 65.94, 65.67, 65.77, 65.66, 65.76]

Epoch: 11
2024-03-10 02:30:37.609840 epoch: 11 step: 0 cls_loss= 0.26246 (122895 samples/sec)
2024-03-10 02:30:46.932966 epoch: 11 step: 100 cls_loss= 0.34051 (3221 samples/sec)
saving....
2024-03-10 02:30:56.777415------------------------------------------------------ Precision@1: 66.00% 

[65.81, 65.72, 65.81, 65.77, 65.67, 66.01, 65.94, 65.67, 65.77, 65.66, 65.76, 66.0]

Epoch: 12
2024-03-10 02:30:57.029717 epoch: 12 step: 0 cls_loss= 0.28303 (119689 samples/sec)
2024-03-10 02:31:06.338534 epoch: 12 step: 100 cls_loss= 0.31026 (3225 samples/sec)
saving....
2024-03-10 02:31:16.212757------------------------------------------------------ Precision@1: 65.88% 

[65.81, 65.72, 65.81, 65.77, 65.67, 66.01, 65.94, 65.67, 65.77, 65.66, 65.76, 66.0, 65.88]

Epoch: 13
2024-03-10 02:31:16.479085 epoch: 13 step: 0 cls_loss= 0.30896 (113214 samples/sec)
2024-03-10 02:31:25.773264 epoch: 13 step: 100 cls_loss= 0.28702 (3229 samples/sec)
saving....
2024-03-10 02:31:35.643712------------------------------------------------------ Precision@1: 65.98% 

[65.81, 65.72, 65.81, 65.77, 65.67, 66.01, 65.94, 65.67, 65.77, 65.66, 65.76, 66.0, 65.88, 65.98]

Epoch: 14
2024-03-10 02:31:35.905708 epoch: 14 step: 0 cls_loss= 0.40510 (115100 samples/sec)
2024-03-10 02:31:45.230707 epoch: 14 step: 100 cls_loss= 0.30123 (3220 samples/sec)
saving....
2024-03-10 02:31:55.147456------------------------------------------------------ Precision@1: 65.86% 

[65.81, 65.72, 65.81, 65.77, 65.67, 66.01, 65.94, 65.67, 65.77, 65.66, 65.76, 66.0, 65.88, 65.98, 65.86]

Epoch: 15
2024-03-10 02:31:55.409433 epoch: 15 step: 0 cls_loss= 0.29488 (115141 samples/sec)
2024-03-10 02:32:04.742051 epoch: 15 step: 100 cls_loss= 0.32129 (3217 samples/sec)
saving....
2024-03-10 02:32:14.636228------------------------------------------------------ Precision@1: 65.65% 

[65.81, 65.72, 65.81, 65.77, 65.67, 66.01, 65.94, 65.67, 65.77, 65.66, 65.76, 66.0, 65.88, 65.98, 65.86, 65.65]

Epoch: 16
2024-03-10 02:32:14.904339 epoch: 16 step: 0 cls_loss= 0.37304 (112586 samples/sec)
2024-03-10 02:32:24.209634 epoch: 16 step: 100 cls_loss= 0.32221 (3227 samples/sec)
saving....
2024-03-10 02:32:34.068094------------------------------------------------------ Precision@1: 65.70% 

[65.81, 65.72, 65.81, 65.77, 65.67, 66.01, 65.94, 65.67, 65.77, 65.66, 65.76, 66.0, 65.88, 65.98, 65.86, 65.65, 65.7]

Epoch: 17
2024-03-10 02:32:34.330460 epoch: 17 step: 0 cls_loss= 0.32378 (114946 samples/sec)
2024-03-10 02:32:43.658935 epoch: 17 step: 100 cls_loss= 0.31580 (3219 samples/sec)
saving....
2024-03-10 02:32:53.585829------------------------------------------------------ Precision@1: 65.73% 

[65.81, 65.72, 65.81, 65.77, 65.67, 66.01, 65.94, 65.67, 65.77, 65.66, 65.76, 66.0, 65.88, 65.98, 65.86, 65.65, 65.7, 65.73]

Epoch: 18
2024-03-10 02:32:53.840693 epoch: 18 step: 0 cls_loss= 0.34728 (118450 samples/sec)
2024-03-10 02:33:03.206065 epoch: 18 step: 100 cls_loss= 0.33922 (3206 samples/sec)
saving....
2024-03-10 02:33:13.136551------------------------------------------------------ Precision@1: 65.72% 

[65.81, 65.72, 65.81, 65.77, 65.67, 66.01, 65.94, 65.67, 65.77, 65.66, 65.76, 66.0, 65.88, 65.98, 65.86, 65.65, 65.7, 65.73, 65.72]

Epoch: 19
2024-03-10 02:33:13.386298 epoch: 19 step: 0 cls_loss= 0.33204 (120909 samples/sec)
2024-03-10 02:33:22.707699 epoch: 19 step: 100 cls_loss= 0.30671 (3221 samples/sec)
saving....
2024-03-10 02:33:32.627464------------------------------------------------------ Precision@1: 65.88% 

[65.81, 65.72, 65.81, 65.77, 65.67, 66.01, 65.94, 65.67, 65.77, 65.66, 65.76, 66.0, 65.88, 65.98, 65.86, 65.65, 65.7, 65.73, 65.72, 65.88]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 02:33:35.536435 epoch: 0 step: 0 cls_loss= 0.27888 (38964 samples/sec)
2024-03-10 02:33:44.848566 epoch: 0 step: 100 cls_loss= 0.29124 (3221 samples/sec)
saving....
2024-03-10 02:33:55.057596------------------------------------------------------ Precision@1: 65.78% 

[65.78]

Epoch: 1
2024-03-10 02:33:55.312562 epoch: 1 step: 0 cls_loss= 0.32765 (118371 samples/sec)
2024-03-10 02:34:04.639993 epoch: 1 step: 100 cls_loss= 0.38586 (3217 samples/sec)
saving....
2024-03-10 02:34:14.584713------------------------------------------------------ Precision@1: 65.65% 

[65.78, 65.65]

Epoch: 2
2024-03-10 02:34:14.845943 epoch: 2 step: 0 cls_loss= 0.29814 (115578 samples/sec)
2024-03-10 02:34:24.195765 epoch: 2 step: 100 cls_loss= 0.31025 (3211 samples/sec)
saving....
2024-03-10 02:34:34.147006------------------------------------------------------ Precision@1: 65.62% 

[65.78, 65.65, 65.62]

Epoch: 3
2024-03-10 02:34:34.404324 epoch: 3 step: 0 cls_loss= 0.31076 (117277 samples/sec)
2024-03-10 02:34:43.728902 epoch: 3 step: 100 cls_loss= 0.28260 (3220 samples/sec)
saving....
2024-03-10 02:34:53.749864------------------------------------------------------ Precision@1: 65.64% 

[65.78, 65.65, 65.62, 65.64]

Epoch: 4
2024-03-10 02:34:54.012724 epoch: 4 step: 0 cls_loss= 0.28510 (114706 samples/sec)
2024-03-10 02:35:03.356840 epoch: 4 step: 100 cls_loss= 0.40261 (3213 samples/sec)
saving....
2024-03-10 02:35:13.289452------------------------------------------------------ Precision@1: 65.59% 

[65.78, 65.65, 65.62, 65.64, 65.59]

Epoch: 5
2024-03-10 02:35:13.541323 epoch: 5 step: 0 cls_loss= 0.33164 (119875 samples/sec)
2024-03-10 02:35:22.860384 epoch: 5 step: 100 cls_loss= 0.30965 (3222 samples/sec)
saving....
2024-03-10 02:35:32.741946------------------------------------------------------ Precision@1: 65.60% 

[65.78, 65.65, 65.62, 65.64, 65.59, 65.6]

Epoch: 6
2024-03-10 02:35:32.988332 epoch: 6 step: 0 cls_loss= 0.27958 (122543 samples/sec)
2024-03-10 02:35:42.323172 epoch: 6 step: 100 cls_loss= 0.31038 (3216 samples/sec)
saving....
2024-03-10 02:35:52.232581------------------------------------------------------ Precision@1: 65.91% 

[65.78, 65.65, 65.62, 65.64, 65.59, 65.6, 65.91]

Epoch: 7
2024-03-10 02:35:52.478705 epoch: 7 step: 0 cls_loss= 0.28246 (122668 samples/sec)
2024-03-10 02:36:01.816358 epoch: 7 step: 100 cls_loss= 0.34301 (3215 samples/sec)
saving....
2024-03-10 02:36:11.723174------------------------------------------------------ Precision@1: 65.91% 

[65.78, 65.65, 65.62, 65.64, 65.59, 65.6, 65.91, 65.91]

Epoch: 8
2024-03-10 02:36:11.994126 epoch: 8 step: 0 cls_loss= 0.30062 (111177 samples/sec)
2024-03-10 02:36:21.355411 epoch: 8 step: 100 cls_loss= 0.28103 (3205 samples/sec)
saving....
2024-03-10 02:36:31.243069------------------------------------------------------ Precision@1: 65.75% 

[65.78, 65.65, 65.62, 65.64, 65.59, 65.6, 65.91, 65.91, 65.75]

Epoch: 9
2024-03-10 02:36:31.501528 epoch: 9 step: 0 cls_loss= 0.31275 (116698 samples/sec)
2024-03-10 02:36:40.841133 epoch: 9 step: 100 cls_loss= 0.33856 (3215 samples/sec)
saving....
2024-03-10 02:36:50.781853------------------------------------------------------ Precision@1: 65.74% 

[65.78, 65.65, 65.62, 65.64, 65.59, 65.6, 65.91, 65.91, 65.75, 65.74]

Epoch: 10
2024-03-10 02:36:51.040167 epoch: 10 step: 0 cls_loss= 0.36755 (116896 samples/sec)
2024-03-10 02:37:00.465335 epoch: 10 step: 100 cls_loss= 0.37786 (3186 samples/sec)
saving....
2024-03-10 02:37:10.467985------------------------------------------------------ Precision@1: 65.56% 

[65.78, 65.65, 65.62, 65.64, 65.59, 65.6, 65.91, 65.91, 65.75, 65.74, 65.56]

Epoch: 11
2024-03-10 02:37:10.736909 epoch: 11 step: 0 cls_loss= 0.32994 (112133 samples/sec)
2024-03-10 02:37:20.088172 epoch: 11 step: 100 cls_loss= 0.30230 (3209 samples/sec)
saving....
2024-03-10 02:37:30.030375------------------------------------------------------ Precision@1: 65.61% 

[65.78, 65.65, 65.62, 65.64, 65.59, 65.6, 65.91, 65.91, 65.75, 65.74, 65.56, 65.61]

Epoch: 12
2024-03-10 02:37:30.289553 epoch: 12 step: 0 cls_loss= 0.32188 (116476 samples/sec)
2024-03-10 02:37:39.613041 epoch: 12 step: 100 cls_loss= 0.26111 (3220 samples/sec)
saving....
2024-03-10 02:37:49.561134------------------------------------------------------ Precision@1: 65.70% 

[65.78, 65.65, 65.62, 65.64, 65.59, 65.6, 65.91, 65.91, 65.75, 65.74, 65.56, 65.61, 65.7]

Epoch: 13
2024-03-10 02:37:49.825873 epoch: 13 step: 0 cls_loss= 0.31585 (113991 samples/sec)
2024-03-10 02:37:59.172152 epoch: 13 step: 100 cls_loss= 0.35666 (3213 samples/sec)
saving....
2024-03-10 02:38:09.059890------------------------------------------------------ Precision@1: 65.85% 

[65.78, 65.65, 65.62, 65.64, 65.59, 65.6, 65.91, 65.91, 65.75, 65.74, 65.56, 65.61, 65.7, 65.85]

Epoch: 14
2024-03-10 02:38:09.316936 epoch: 14 step: 0 cls_loss= 0.30818 (117416 samples/sec)
2024-03-10 02:38:18.669169 epoch: 14 step: 100 cls_loss= 0.32443 (3210 samples/sec)
saving....
2024-03-10 02:38:28.565075------------------------------------------------------ Precision@1: 65.80% 

[65.78, 65.65, 65.62, 65.64, 65.59, 65.6, 65.91, 65.91, 65.75, 65.74, 65.56, 65.61, 65.7, 65.85, 65.8]

Epoch: 15
2024-03-10 02:38:28.830564 epoch: 15 step: 0 cls_loss= 0.32062 (113565 samples/sec)
2024-03-10 02:38:38.160044 epoch: 15 step: 100 cls_loss= 0.29774 (3216 samples/sec)
saving....
2024-03-10 02:38:48.101457------------------------------------------------------ Precision@1: 65.79% 

[65.78, 65.65, 65.62, 65.64, 65.59, 65.6, 65.91, 65.91, 65.75, 65.74, 65.56, 65.61, 65.7, 65.85, 65.8, 65.79]

Epoch: 16
2024-03-10 02:38:48.357186 epoch: 16 step: 0 cls_loss= 0.31657 (118096 samples/sec)
2024-03-10 02:38:57.661525 epoch: 16 step: 100 cls_loss= 0.31372 (3225 samples/sec)
saving....
2024-03-10 02:39:07.617427------------------------------------------------------ Precision@1: 65.77% 

[65.78, 65.65, 65.62, 65.64, 65.59, 65.6, 65.91, 65.91, 65.75, 65.74, 65.56, 65.61, 65.7, 65.85, 65.8, 65.79, 65.77]

Epoch: 17
2024-03-10 02:39:07.872268 epoch: 17 step: 0 cls_loss= 0.28386 (118526 samples/sec)
2024-03-10 02:39:17.247460 epoch: 17 step: 100 cls_loss= 0.27060 (3200 samples/sec)
saving....
2024-03-10 02:39:27.164265------------------------------------------------------ Precision@1: 65.74% 

[65.78, 65.65, 65.62, 65.64, 65.59, 65.6, 65.91, 65.91, 65.75, 65.74, 65.56, 65.61, 65.7, 65.85, 65.8, 65.79, 65.77, 65.74]

Epoch: 18
2024-03-10 02:39:27.409436 epoch: 18 step: 0 cls_loss= 0.32098 (123235 samples/sec)
2024-03-10 02:39:36.737262 epoch: 18 step: 100 cls_loss= 0.35057 (3219 samples/sec)
saving....
2024-03-10 02:39:46.681050------------------------------------------------------ Precision@1: 65.60% 

[65.78, 65.65, 65.62, 65.64, 65.59, 65.6, 65.91, 65.91, 65.75, 65.74, 65.56, 65.61, 65.7, 65.85, 65.8, 65.79, 65.77, 65.74, 65.6]

Epoch: 19
2024-03-10 02:39:46.937179 epoch: 19 step: 0 cls_loss= 0.34908 (117835 samples/sec)
2024-03-10 02:39:56.380015 epoch: 19 step: 100 cls_loss= 0.35466 (3180 samples/sec)
saving....
2024-03-10 02:40:06.298543------------------------------------------------------ Precision@1: 65.83% 

[65.78, 65.65, 65.62, 65.64, 65.59, 65.6, 65.91, 65.91, 65.75, 65.74, 65.56, 65.61, 65.7, 65.85, 65.8, 65.79, 65.77, 65.74, 65.6, 65.83]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 02:40:09.170388 epoch: 0 step: 0 cls_loss= 0.28678 (42339 samples/sec)
2024-03-10 02:40:18.479498 epoch: 0 step: 100 cls_loss= 0.27248 (3222 samples/sec)
saving....
2024-03-10 02:40:28.665993------------------------------------------------------ Precision@1: 65.82% 

[65.82]

Epoch: 1
2024-03-10 02:40:28.912594 epoch: 1 step: 0 cls_loss= 0.33478 (122365 samples/sec)
2024-03-10 02:40:38.210316 epoch: 1 step: 100 cls_loss= 0.31914 (3229 samples/sec)
saving....
2024-03-10 02:40:48.074568------------------------------------------------------ Precision@1: 65.68% 

[65.82, 65.68]

Epoch: 2
2024-03-10 02:40:48.321446 epoch: 2 step: 0 cls_loss= 0.36891 (122248 samples/sec)
2024-03-10 02:40:57.667429 epoch: 2 step: 100 cls_loss= 0.34514 (3213 samples/sec)
saving....
2024-03-10 02:41:07.607383------------------------------------------------------ Precision@1: 65.77% 

[65.82, 65.68, 65.77]

Epoch: 3
2024-03-10 02:41:07.873776 epoch: 3 step: 0 cls_loss= 0.31577 (113278 samples/sec)
2024-03-10 02:41:17.180060 epoch: 3 step: 100 cls_loss= 0.40216 (3226 samples/sec)
saving....
2024-03-10 02:41:27.348860------------------------------------------------------ Precision@1: 65.67% 

[65.82, 65.68, 65.77, 65.67]

Epoch: 4
2024-03-10 02:41:27.588040 epoch: 4 step: 0 cls_loss= 0.30643 (126309 samples/sec)
2024-03-10 02:41:36.895688 epoch: 4 step: 100 cls_loss= 0.32357 (3226 samples/sec)
saving....
2024-03-10 02:41:46.802626------------------------------------------------------ Precision@1: 65.60% 

[65.82, 65.68, 65.77, 65.67, 65.6]

Epoch: 5
2024-03-10 02:41:47.047613 epoch: 5 step: 0 cls_loss= 0.25371 (123210 samples/sec)
2024-03-10 02:41:56.378888 epoch: 5 step: 100 cls_loss= 0.29121 (3218 samples/sec)
saving....
2024-03-10 02:42:06.306331------------------------------------------------------ Precision@1: 65.70% 

[65.82, 65.68, 65.77, 65.67, 65.6, 65.7]

Epoch: 6
2024-03-10 02:42:06.576028 epoch: 6 step: 0 cls_loss= 0.35302 (111866 samples/sec)
2024-03-10 02:42:15.871182 epoch: 6 step: 100 cls_loss= 0.27762 (3229 samples/sec)
saving....
2024-03-10 02:42:25.762902------------------------------------------------------ Precision@1: 65.89% 

[65.82, 65.68, 65.77, 65.67, 65.6, 65.7, 65.89]

Epoch: 7
2024-03-10 02:42:26.022734 epoch: 7 step: 0 cls_loss= 0.36779 (116200 samples/sec)
2024-03-10 02:42:35.324769 epoch: 7 step: 100 cls_loss= 0.32486 (3228 samples/sec)
saving....
2024-03-10 02:42:45.209174------------------------------------------------------ Precision@1: 65.93% 

[65.82, 65.68, 65.77, 65.67, 65.6, 65.7, 65.89, 65.93]

Epoch: 8
2024-03-10 02:42:45.470885 epoch: 8 step: 0 cls_loss= 0.26729 (115179 samples/sec)
2024-03-10 02:42:54.801825 epoch: 8 step: 100 cls_loss= 0.37968 (3218 samples/sec)
saving....
2024-03-10 02:43:04.710557------------------------------------------------------ Precision@1: 65.86% 

[65.82, 65.68, 65.77, 65.67, 65.6, 65.7, 65.89, 65.93, 65.86]

Epoch: 9
2024-03-10 02:43:04.950385 epoch: 9 step: 0 cls_loss= 0.31404 (125906 samples/sec)
2024-03-10 02:43:14.268868 epoch: 9 step: 100 cls_loss= 0.35850 (3222 samples/sec)
saving....
2024-03-10 02:43:24.271197------------------------------------------------------ Precision@1: 65.67% 

[65.82, 65.68, 65.77, 65.67, 65.6, 65.7, 65.89, 65.93, 65.86, 65.67]

Epoch: 10
2024-03-10 02:43:24.527489 epoch: 10 step: 0 cls_loss= 0.31588 (117790 samples/sec)
2024-03-10 02:43:33.843979 epoch: 10 step: 100 cls_loss= 0.29843 (3223 samples/sec)
saving....
2024-03-10 02:43:43.733000------------------------------------------------------ Precision@1: 65.59% 

[65.82, 65.68, 65.77, 65.67, 65.6, 65.7, 65.89, 65.93, 65.86, 65.67, 65.59]

Epoch: 11
2024-03-10 02:43:43.984086 epoch: 11 step: 0 cls_loss= 0.25506 (120200 samples/sec)
2024-03-10 02:43:53.326172 epoch: 11 step: 100 cls_loss= 0.26950 (3214 samples/sec)
saving....
2024-03-10 02:44:03.224127------------------------------------------------------ Precision@1: 65.78% 

[65.82, 65.68, 65.77, 65.67, 65.6, 65.7, 65.89, 65.93, 65.86, 65.67, 65.59, 65.78]

Epoch: 12
2024-03-10 02:44:03.515050 epoch: 12 step: 0 cls_loss= 0.28889 (103573 samples/sec)
2024-03-10 02:44:12.858592 epoch: 12 step: 100 cls_loss= 0.30404 (3211 samples/sec)
saving....
2024-03-10 02:44:22.755818------------------------------------------------------ Precision@1: 65.67% 

[65.82, 65.68, 65.77, 65.67, 65.6, 65.7, 65.89, 65.93, 65.86, 65.67, 65.59, 65.78, 65.67]

Epoch: 13
2024-03-10 02:44:23.018508 epoch: 13 step: 0 cls_loss= 0.29545 (114868 samples/sec)
2024-03-10 02:44:32.343969 epoch: 13 step: 100 cls_loss= 0.28629 (3220 samples/sec)
saving....
2024-03-10 02:44:42.274389------------------------------------------------------ Precision@1: 65.75% 

[65.82, 65.68, 65.77, 65.67, 65.6, 65.7, 65.89, 65.93, 65.86, 65.67, 65.59, 65.78, 65.67, 65.75]

Epoch: 14
2024-03-10 02:44:42.519156 epoch: 14 step: 0 cls_loss= 0.34928 (123251 samples/sec)
2024-03-10 02:44:51.814796 epoch: 14 step: 100 cls_loss= 0.25544 (3230 samples/sec)
saving....
2024-03-10 02:45:01.836065------------------------------------------------------ Precision@1: 65.85% 

[65.82, 65.68, 65.77, 65.67, 65.6, 65.7, 65.89, 65.93, 65.86, 65.67, 65.59, 65.78, 65.67, 65.75, 65.85]

Epoch: 15
2024-03-10 02:45:02.090320 epoch: 15 step: 0 cls_loss= 0.31001 (118701 samples/sec)
2024-03-10 02:45:11.439510 epoch: 15 step: 100 cls_loss= 0.31889 (3212 samples/sec)
saving....
2024-03-10 02:45:21.336726------------------------------------------------------ Precision@1: 65.70% 

[65.82, 65.68, 65.77, 65.67, 65.6, 65.7, 65.89, 65.93, 65.86, 65.67, 65.59, 65.78, 65.67, 65.75, 65.85, 65.7]

Epoch: 16
2024-03-10 02:45:21.619980 epoch: 16 step: 0 cls_loss= 0.34405 (106488 samples/sec)
2024-03-10 02:45:30.948245 epoch: 16 step: 100 cls_loss= 0.28306 (3217 samples/sec)
saving....
2024-03-10 02:45:40.839885------------------------------------------------------ Precision@1: 65.54% 

[65.82, 65.68, 65.77, 65.67, 65.6, 65.7, 65.89, 65.93, 65.86, 65.67, 65.59, 65.78, 65.67, 65.75, 65.85, 65.7, 65.54]

Epoch: 17
2024-03-10 02:45:41.104345 epoch: 17 step: 0 cls_loss= 0.27607 (114019 samples/sec)
2024-03-10 02:45:50.471039 epoch: 17 step: 100 cls_loss= 0.33293 (3203 samples/sec)
saving....
2024-03-10 02:46:00.388847------------------------------------------------------ Precision@1: 65.81% 

[65.82, 65.68, 65.77, 65.67, 65.6, 65.7, 65.89, 65.93, 65.86, 65.67, 65.59, 65.78, 65.67, 65.75, 65.85, 65.7, 65.54, 65.81]

Epoch: 18
2024-03-10 02:46:00.639050 epoch: 18 step: 0 cls_loss= 0.28720 (120544 samples/sec)
2024-03-10 02:46:09.946827 epoch: 18 step: 100 cls_loss= 0.37340 (3226 samples/sec)
saving....
2024-03-10 02:46:19.826809------------------------------------------------------ Precision@1: 65.77% 

[65.82, 65.68, 65.77, 65.67, 65.6, 65.7, 65.89, 65.93, 65.86, 65.67, 65.59, 65.78, 65.67, 65.75, 65.85, 65.7, 65.54, 65.81, 65.77]

Epoch: 19
2024-03-10 02:46:20.078105 epoch: 19 step: 0 cls_loss= 0.34123 (120114 samples/sec)
2024-03-10 02:46:29.401812 epoch: 19 step: 100 cls_loss= 0.31014 (3220 samples/sec)
saving....
2024-03-10 02:46:39.257696------------------------------------------------------ Precision@1: 65.64% 

[65.82, 65.68, 65.77, 65.67, 65.6, 65.7, 65.89, 65.93, 65.86, 65.67, 65.59, 65.78, 65.67, 65.75, 65.85, 65.7, 65.54, 65.81, 65.77, 65.64]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 02:46:42.112765 epoch: 0 step: 0 cls_loss= 0.35219 (41782 samples/sec)
2024-03-10 02:46:51.430628 epoch: 0 step: 100 cls_loss= 0.30602 (3219 samples/sec)
saving....
2024-03-10 02:47:01.571234------------------------------------------------------ Precision@1: 65.65% 

[65.65]

Epoch: 1
2024-03-10 02:47:01.832411 epoch: 1 step: 0 cls_loss= 0.37772 (115620 samples/sec)
2024-03-10 02:47:11.168892 epoch: 1 step: 100 cls_loss= 0.32748 (3216 samples/sec)
saving....
2024-03-10 02:47:21.080806------------------------------------------------------ Precision@1: 65.79% 

[65.65, 65.79]

Epoch: 2
2024-03-10 02:47:21.332971 epoch: 2 step: 0 cls_loss= 0.26664 (119618 samples/sec)
2024-03-10 02:47:30.656456 epoch: 2 step: 100 cls_loss= 0.34399 (3220 samples/sec)
saving....
2024-03-10 02:47:40.599376------------------------------------------------------ Precision@1: 65.79% 

[65.65, 65.79, 65.79]

Epoch: 3
2024-03-10 02:47:40.853053 epoch: 3 step: 0 cls_loss= 0.33513 (118971 samples/sec)
2024-03-10 02:47:50.210976 epoch: 3 step: 100 cls_loss= 0.31173 (3209 samples/sec)
saving....
2024-03-10 02:48:00.101739------------------------------------------------------ Precision@1: 65.61% 

[65.65, 65.79, 65.79, 65.61]

Epoch: 4
2024-03-10 02:48:00.351809 epoch: 4 step: 0 cls_loss= 0.29026 (120626 samples/sec)
2024-03-10 02:48:09.692863 epoch: 4 step: 100 cls_loss= 0.29437 (3212 samples/sec)
saving....
2024-03-10 02:48:19.591554------------------------------------------------------ Precision@1: 65.90% 

[65.65, 65.79, 65.79, 65.61, 65.9]

Epoch: 5
2024-03-10 02:48:19.860656 epoch: 5 step: 0 cls_loss= 0.35029 (111985 samples/sec)
2024-03-10 02:48:29.160507 epoch: 5 step: 100 cls_loss= 0.28296 (3227 samples/sec)
saving....
2024-03-10 02:48:39.038136------------------------------------------------------ Precision@1: 65.69% 

[65.65, 65.79, 65.79, 65.61, 65.9, 65.69]

Epoch: 6
2024-03-10 02:48:39.287490 epoch: 6 step: 0 cls_loss= 0.27923 (121107 samples/sec)
2024-03-10 02:48:48.661257 epoch: 6 step: 100 cls_loss= 0.34780 (3203 samples/sec)
saving....
2024-03-10 02:48:58.611571------------------------------------------------------ Precision@1: 65.66% 

[65.65, 65.79, 65.79, 65.61, 65.9, 65.69, 65.66]

Epoch: 7
2024-03-10 02:48:58.876420 epoch: 7 step: 0 cls_loss= 0.25135 (113822 samples/sec)
2024-03-10 02:49:08.237910 epoch: 7 step: 100 cls_loss= 0.33490 (3205 samples/sec)
saving....
2024-03-10 02:49:18.177528------------------------------------------------------ Precision@1: 65.70% 

[65.65, 65.79, 65.79, 65.61, 65.9, 65.69, 65.66, 65.7]

Epoch: 8
2024-03-10 02:49:18.444483 epoch: 8 step: 0 cls_loss= 0.31828 (112947 samples/sec)
2024-03-10 02:49:27.790183 epoch: 8 step: 100 cls_loss= 0.29856 (3213 samples/sec)
saving....
2024-03-10 02:49:37.690581------------------------------------------------------ Precision@1: 65.88% 

[65.65, 65.79, 65.79, 65.61, 65.9, 65.69, 65.66, 65.7, 65.88]

Epoch: 9
2024-03-10 02:49:37.947193 epoch: 9 step: 0 cls_loss= 0.31546 (117588 samples/sec)
2024-03-10 02:49:47.262556 epoch: 9 step: 100 cls_loss= 0.38265 (3223 samples/sec)
saving....
2024-03-10 02:49:57.177473------------------------------------------------------ Precision@1: 65.56% 

[65.65, 65.79, 65.79, 65.61, 65.9, 65.69, 65.66, 65.7, 65.88, 65.56]

Epoch: 10
2024-03-10 02:49:57.421722 epoch: 10 step: 0 cls_loss= 0.30193 (123591 samples/sec)
2024-03-10 02:50:06.732671 epoch: 10 step: 100 cls_loss= 0.27161 (3225 samples/sec)
saving....
2024-03-10 02:50:16.658325------------------------------------------------------ Precision@1: 65.91% 

[65.65, 65.79, 65.79, 65.61, 65.9, 65.69, 65.66, 65.7, 65.88, 65.56, 65.91]

Epoch: 11
2024-03-10 02:50:16.929433 epoch: 11 step: 0 cls_loss= 0.33593 (111191 samples/sec)
2024-03-10 02:50:26.257807 epoch: 11 step: 100 cls_loss= 0.31015 (3219 samples/sec)
saving....
2024-03-10 02:50:36.151136------------------------------------------------------ Precision@1: 65.52% 

[65.65, 65.79, 65.79, 65.61, 65.9, 65.69, 65.66, 65.7, 65.88, 65.56, 65.91, 65.52]

Epoch: 12
2024-03-10 02:50:36.415667 epoch: 12 step: 0 cls_loss= 0.31279 (114026 samples/sec)
2024-03-10 02:50:45.741560 epoch: 12 step: 100 cls_loss= 0.33994 (3220 samples/sec)
saving....
2024-03-10 02:50:55.592093------------------------------------------------------ Precision@1: 65.72% 

[65.65, 65.79, 65.79, 65.61, 65.9, 65.69, 65.66, 65.7, 65.88, 65.56, 65.91, 65.52, 65.72]

Epoch: 13
2024-03-10 02:50:55.845170 epoch: 13 step: 0 cls_loss= 0.33888 (119131 samples/sec)
2024-03-10 02:51:05.205078 epoch: 13 step: 100 cls_loss= 0.36813 (3208 samples/sec)
saving....
2024-03-10 02:51:15.112570------------------------------------------------------ Precision@1: 65.60% 

[65.65, 65.79, 65.79, 65.61, 65.9, 65.69, 65.66, 65.7, 65.88, 65.56, 65.91, 65.52, 65.72, 65.6]

Epoch: 14
2024-03-10 02:51:15.379165 epoch: 14 step: 0 cls_loss= 0.31163 (113253 samples/sec)
2024-03-10 02:51:24.710692 epoch: 14 step: 100 cls_loss= 0.32957 (3218 samples/sec)
saving....
2024-03-10 02:51:34.765870------------------------------------------------------ Precision@1: 65.59% 

[65.65, 65.79, 65.79, 65.61, 65.9, 65.69, 65.66, 65.7, 65.88, 65.56, 65.91, 65.52, 65.72, 65.6, 65.59]

Epoch: 15
2024-03-10 02:51:35.027581 epoch: 15 step: 0 cls_loss= 0.31134 (115354 samples/sec)
2024-03-10 02:51:44.369683 epoch: 15 step: 100 cls_loss= 0.34139 (3212 samples/sec)
saving....
2024-03-10 02:51:54.310889------------------------------------------------------ Precision@1: 65.74% 

[65.65, 65.79, 65.79, 65.61, 65.9, 65.69, 65.66, 65.7, 65.88, 65.56, 65.91, 65.52, 65.72, 65.6, 65.59, 65.74]

Epoch: 16
2024-03-10 02:51:54.573431 epoch: 16 step: 0 cls_loss= 0.27465 (114964 samples/sec)
2024-03-10 02:52:03.885297 epoch: 16 step: 100 cls_loss= 0.28510 (3224 samples/sec)
saving....
2024-03-10 02:52:13.773599------------------------------------------------------ Precision@1: 65.60% 

[65.65, 65.79, 65.79, 65.61, 65.9, 65.69, 65.66, 65.7, 65.88, 65.56, 65.91, 65.52, 65.72, 65.6, 65.59, 65.74, 65.6]

Epoch: 17
2024-03-10 02:52:14.006653 epoch: 17 step: 0 cls_loss= 0.37150 (129589 samples/sec)
2024-03-10 02:52:23.330417 epoch: 17 step: 100 cls_loss= 0.29067 (3220 samples/sec)
saving....
2024-03-10 02:52:33.232435------------------------------------------------------ Precision@1: 65.62% 

[65.65, 65.79, 65.79, 65.61, 65.9, 65.69, 65.66, 65.7, 65.88, 65.56, 65.91, 65.52, 65.72, 65.6, 65.59, 65.74, 65.6, 65.62]

Epoch: 18
2024-03-10 02:52:33.474549 epoch: 18 step: 0 cls_loss= 0.35295 (124713 samples/sec)
2024-03-10 02:52:42.787764 epoch: 18 step: 100 cls_loss= 0.31275 (3224 samples/sec)
saving....
2024-03-10 02:52:52.683334------------------------------------------------------ Precision@1: 65.66% 

[65.65, 65.79, 65.79, 65.61, 65.9, 65.69, 65.66, 65.7, 65.88, 65.56, 65.91, 65.52, 65.72, 65.6, 65.59, 65.74, 65.6, 65.62, 65.66]

Epoch: 19
2024-03-10 02:52:52.946256 epoch: 19 step: 0 cls_loss= 0.31772 (114766 samples/sec)
2024-03-10 02:53:02.261290 epoch: 19 step: 100 cls_loss= 0.32185 (3223 samples/sec)
saving....
2024-03-10 02:53:12.182930------------------------------------------------------ Precision@1: 65.75% 

[65.65, 65.79, 65.79, 65.61, 65.9, 65.69, 65.66, 65.7, 65.88, 65.56, 65.91, 65.52, 65.72, 65.6, 65.59, 65.74, 65.6, 65.62, 65.66, 65.75]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 02:53:15.042096 epoch: 0 step: 0 cls_loss= 0.26256 (42128 samples/sec)
2024-03-10 02:53:24.519081 epoch: 0 step: 100 cls_loss= 0.31487 (3165 samples/sec)
saving....
2024-03-10 02:53:34.807777------------------------------------------------------ Precision@1: 65.01% 

[65.01]

Epoch: 1
2024-03-10 02:53:35.074011 epoch: 1 step: 0 cls_loss= 0.42288 (113324 samples/sec)
2024-03-10 02:53:44.561468 epoch: 1 step: 100 cls_loss= 0.35743 (3162 samples/sec)
saving....
2024-03-10 02:53:54.571272------------------------------------------------------ Precision@1: 64.75% 

[65.01, 64.75]

Epoch: 2
2024-03-10 02:53:54.838270 epoch: 2 step: 0 cls_loss= 0.37927 (113005 samples/sec)
2024-03-10 02:54:04.317185 epoch: 2 step: 100 cls_loss= 0.33748 (3165 samples/sec)
saving....
2024-03-10 02:54:14.288314------------------------------------------------------ Precision@1: 64.49% 

[65.01, 64.75, 64.49]

Epoch: 3
2024-03-10 02:54:14.559912 epoch: 3 step: 0 cls_loss= 0.31173 (110906 samples/sec)
2024-03-10 02:54:24.015202 epoch: 3 step: 100 cls_loss= 0.30771 (3172 samples/sec)
saving....
2024-03-10 02:54:34.026293------------------------------------------------------ Precision@1: 64.65% 

[65.01, 64.75, 64.49, 64.65]

Epoch: 4
2024-03-10 02:54:34.294343 epoch: 4 step: 0 cls_loss= 0.32568 (112493 samples/sec)
2024-03-10 02:54:43.766437 epoch: 4 step: 100 cls_loss= 0.34391 (3167 samples/sec)
saving....
2024-03-10 02:54:53.771826------------------------------------------------------ Precision@1: 64.47% 

[65.01, 64.75, 64.49, 64.65, 64.47]

Epoch: 5
2024-03-10 02:54:54.016833 epoch: 5 step: 0 cls_loss= 0.30578 (123251 samples/sec)
2024-03-10 02:55:03.502759 epoch: 5 step: 100 cls_loss= 0.34303 (3162 samples/sec)
saving....
2024-03-10 02:55:13.572399------------------------------------------------------ Precision@1: 64.50% 

[65.01, 64.75, 64.49, 64.65, 64.47, 64.5]

Epoch: 6
2024-03-10 02:55:13.847207 epoch: 6 step: 0 cls_loss= 0.39945 (109728 samples/sec)
2024-03-10 02:55:23.337883 epoch: 6 step: 100 cls_loss= 0.23229 (3161 samples/sec)
saving....
2024-03-10 02:55:33.368312------------------------------------------------------ Precision@1: 65.30% 

[65.01, 64.75, 64.49, 64.65, 64.47, 64.5, 65.3]

Epoch: 7
2024-03-10 02:55:33.626326 epoch: 7 step: 0 cls_loss= 0.27446 (116976 samples/sec)
2024-03-10 02:55:43.090834 epoch: 7 step: 100 cls_loss= 0.26758 (3169 samples/sec)
saving....
2024-03-10 02:55:53.093988------------------------------------------------------ Precision@1: 64.55% 

[65.01, 64.75, 64.49, 64.65, 64.47, 64.5, 65.3, 64.55]

Epoch: 8
2024-03-10 02:55:53.352118 epoch: 8 step: 0 cls_loss= 0.26181 (116788 samples/sec)
2024-03-10 02:56:02.820044 epoch: 8 step: 100 cls_loss= 0.26325 (3168 samples/sec)
saving....
2024-03-10 02:56:12.799004------------------------------------------------------ Precision@1: 64.48% 

[65.01, 64.75, 64.49, 64.65, 64.47, 64.5, 65.3, 64.55, 64.48]

Epoch: 9
2024-03-10 02:56:13.075462 epoch: 9 step: 0 cls_loss= 0.26146 (109028 samples/sec)
2024-03-10 02:56:22.656517 epoch: 9 step: 100 cls_loss= 0.24759 (3131 samples/sec)
saving....
2024-03-10 02:56:32.703866------------------------------------------------------ Precision@1: 64.39% 

[65.01, 64.75, 64.49, 64.65, 64.47, 64.5, 65.3, 64.55, 64.48, 64.39]

Epoch: 10
2024-03-10 02:56:32.983351 epoch: 10 step: 0 cls_loss= 0.28153 (107922 samples/sec)
2024-03-10 02:56:42.483602 epoch: 10 step: 100 cls_loss= 0.27042 (3158 samples/sec)
saving....
2024-03-10 02:56:52.485943------------------------------------------------------ Precision@1: 64.14% 

[65.01, 64.75, 64.49, 64.65, 64.47, 64.5, 65.3, 64.55, 64.48, 64.39, 64.14]

Epoch: 11
2024-03-10 02:56:52.741493 epoch: 11 step: 0 cls_loss= 0.27883 (118115 samples/sec)
2024-03-10 02:57:02.244983 epoch: 11 step: 100 cls_loss= 0.30781 (3156 samples/sec)
saving....
2024-03-10 02:57:12.247975------------------------------------------------------ Precision@1: 64.22% 

[65.01, 64.75, 64.49, 64.65, 64.47, 64.5, 65.3, 64.55, 64.48, 64.39, 64.14, 64.22]

Epoch: 12
2024-03-10 02:57:12.515075 epoch: 12 step: 0 cls_loss= 0.23516 (112984 samples/sec)
2024-03-10 02:57:22.016865 epoch: 12 step: 100 cls_loss= 0.28101 (3157 samples/sec)
saving....
2024-03-10 02:57:32.038473------------------------------------------------------ Precision@1: 64.51% 

[65.01, 64.75, 64.49, 64.65, 64.47, 64.5, 65.3, 64.55, 64.48, 64.39, 64.14, 64.22, 64.51]

Epoch: 13
2024-03-10 02:57:32.297960 epoch: 13 step: 0 cls_loss= 0.19632 (116176 samples/sec)
2024-03-10 02:57:41.882953 epoch: 13 step: 100 cls_loss= 0.25533 (3130 samples/sec)
saving....
2024-03-10 02:57:51.925273------------------------------------------------------ Precision@1: 64.59% 

[65.01, 64.75, 64.49, 64.65, 64.47, 64.5, 65.3, 64.55, 64.48, 64.39, 64.14, 64.22, 64.51, 64.59]

Epoch: 14
2024-03-10 02:57:52.189702 epoch: 14 step: 0 cls_loss= 0.21710 (114023 samples/sec)
2024-03-10 02:58:01.682466 epoch: 14 step: 100 cls_loss= 0.25601 (3160 samples/sec)
saving....
2024-03-10 02:58:11.684055------------------------------------------------------ Precision@1: 64.00% 

[65.01, 64.75, 64.49, 64.65, 64.47, 64.5, 65.3, 64.55, 64.48, 64.39, 64.14, 64.22, 64.51, 64.59, 64.0]

Epoch: 15
2024-03-10 02:58:11.948027 epoch: 15 step: 0 cls_loss= 0.18024 (114248 samples/sec)
2024-03-10 02:58:21.437235 epoch: 15 step: 100 cls_loss= 0.29468 (3161 samples/sec)
saving....
2024-03-10 02:58:31.458541------------------------------------------------------ Precision@1: 64.46% 

[65.01, 64.75, 64.49, 64.65, 64.47, 64.5, 65.3, 64.55, 64.48, 64.39, 64.14, 64.22, 64.51, 64.59, 64.0, 64.46]

Epoch: 16
2024-03-10 02:58:31.704456 epoch: 16 step: 0 cls_loss= 0.22754 (122671 samples/sec)
2024-03-10 02:58:41.192180 epoch: 16 step: 100 cls_loss= 0.30940 (3162 samples/sec)
saving....
2024-03-10 02:58:51.184355------------------------------------------------------ Precision@1: 64.16% 

[65.01, 64.75, 64.49, 64.65, 64.47, 64.5, 65.3, 64.55, 64.48, 64.39, 64.14, 64.22, 64.51, 64.59, 64.0, 64.46, 64.16]

Epoch: 17
2024-03-10 02:58:51.460479 epoch: 17 step: 0 cls_loss= 0.15476 (109197 samples/sec)
2024-03-10 02:59:01.024852 epoch: 17 step: 100 cls_loss= 0.27102 (3136 samples/sec)
saving....
2024-03-10 02:59:11.037526------------------------------------------------------ Precision@1: 64.55% 

[65.01, 64.75, 64.49, 64.65, 64.47, 64.5, 65.3, 64.55, 64.48, 64.39, 64.14, 64.22, 64.51, 64.59, 64.0, 64.46, 64.16, 64.55]

Epoch: 18
2024-03-10 02:59:11.318045 epoch: 18 step: 0 cls_loss= 0.16582 (107568 samples/sec)
2024-03-10 02:59:20.779467 epoch: 18 step: 100 cls_loss= 0.26193 (3170 samples/sec)
saving....
2024-03-10 02:59:30.822157------------------------------------------------------ Precision@1: 64.21% 

[65.01, 64.75, 64.49, 64.65, 64.47, 64.5, 65.3, 64.55, 64.48, 64.39, 64.14, 64.22, 64.51, 64.59, 64.0, 64.46, 64.16, 64.55, 64.21]

Epoch: 19
2024-03-10 02:59:31.086927 epoch: 19 step: 0 cls_loss= 0.18847 (113927 samples/sec)
2024-03-10 02:59:40.547406 epoch: 19 step: 100 cls_loss= 0.16943 (3171 samples/sec)
saving....
2024-03-10 02:59:50.548011------------------------------------------------------ Precision@1: 64.13% 

[65.01, 64.75, 64.49, 64.65, 64.47, 64.5, 65.3, 64.55, 64.48, 64.39, 64.14, 64.22, 64.51, 64.59, 64.0, 64.46, 64.16, 64.55, 64.21, 64.13]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 02:59:53.448255 epoch: 0 step: 0 cls_loss= 0.31158 (39945 samples/sec)
2024-03-10 03:00:02.883414 epoch: 0 step: 100 cls_loss= 0.39850 (3179 samples/sec)
saving....
2024-03-10 03:00:13.200545------------------------------------------------------ Precision@1: 65.18% 

[65.18]

Epoch: 1
2024-03-10 03:00:13.474741 epoch: 1 step: 0 cls_loss= 0.39360 (109912 samples/sec)
2024-03-10 03:00:22.963997 epoch: 1 step: 100 cls_loss= 0.41905 (3161 samples/sec)
saving....
2024-03-10 03:00:33.036886------------------------------------------------------ Precision@1: 64.62% 

[65.18, 64.62]

Epoch: 2
2024-03-10 03:00:33.295761 epoch: 2 step: 0 cls_loss= 0.34140 (116631 samples/sec)
2024-03-10 03:00:42.762960 epoch: 2 step: 100 cls_loss= 0.40327 (3169 samples/sec)
saving....
2024-03-10 03:00:52.815388------------------------------------------------------ Precision@1: 64.59% 

[65.18, 64.62, 64.59]

Epoch: 3
2024-03-10 03:00:53.072413 epoch: 3 step: 0 cls_loss= 0.35078 (117468 samples/sec)
2024-03-10 03:01:02.527300 epoch: 3 step: 100 cls_loss= 0.34834 (3173 samples/sec)
saving....
2024-03-10 03:01:12.529996------------------------------------------------------ Precision@1: 64.63% 

[65.18, 64.62, 64.59, 64.63]

Epoch: 4
2024-03-10 03:01:12.794667 epoch: 4 step: 0 cls_loss= 0.31164 (114019 samples/sec)
2024-03-10 03:01:22.253221 epoch: 4 step: 100 cls_loss= 0.31200 (3171 samples/sec)
saving....
2024-03-10 03:01:32.219905------------------------------------------------------ Precision@1: 64.40% 

[65.18, 64.62, 64.59, 64.63, 64.4]

Epoch: 5
2024-03-10 03:01:32.485094 epoch: 5 step: 0 cls_loss= 0.30780 (113720 samples/sec)
2024-03-10 03:01:41.941776 epoch: 5 step: 100 cls_loss= 0.34360 (3172 samples/sec)
saving....
2024-03-10 03:01:51.920070------------------------------------------------------ Precision@1: 64.79% 

[65.18, 64.62, 64.59, 64.63, 64.4, 64.79]

Epoch: 6
2024-03-10 03:01:52.196446 epoch: 6 step: 0 cls_loss= 0.33267 (109105 samples/sec)
2024-03-10 03:02:01.676964 epoch: 6 step: 100 cls_loss= 0.28359 (3164 samples/sec)
saving....
2024-03-10 03:02:11.677444------------------------------------------------------ Precision@1: 64.65% 

[65.18, 64.62, 64.59, 64.63, 64.4, 64.79, 64.65]

Epoch: 7
2024-03-10 03:02:11.935458 epoch: 7 step: 0 cls_loss= 0.36441 (116954 samples/sec)
2024-03-10 03:02:21.430765 epoch: 7 step: 100 cls_loss= 0.33586 (3159 samples/sec)
saving....
2024-03-10 03:02:31.388768------------------------------------------------------ Precision@1: 64.90% 

[65.18, 64.62, 64.59, 64.63, 64.4, 64.79, 64.65, 64.9]

Epoch: 8
2024-03-10 03:02:31.648859 epoch: 8 step: 0 cls_loss= 0.29322 (116019 samples/sec)
2024-03-10 03:02:41.074055 epoch: 8 step: 100 cls_loss= 0.23966 (3183 samples/sec)
saving....
2024-03-10 03:02:51.029886------------------------------------------------------ Precision@1: 64.85% 

[65.18, 64.62, 64.59, 64.63, 64.4, 64.79, 64.65, 64.9, 64.85]

Epoch: 9
2024-03-10 03:02:51.291253 epoch: 9 step: 0 cls_loss= 0.25697 (115527 samples/sec)
2024-03-10 03:03:00.764567 epoch: 9 step: 100 cls_loss= 0.27240 (3166 samples/sec)
saving....
2024-03-10 03:03:10.700010------------------------------------------------------ Precision@1: 64.69% 

[65.18, 64.62, 64.59, 64.63, 64.4, 64.79, 64.65, 64.9, 64.85, 64.69]

Epoch: 10
2024-03-10 03:03:10.968039 epoch: 10 step: 0 cls_loss= 0.24705 (112570 samples/sec)
2024-03-10 03:03:20.440461 epoch: 10 step: 100 cls_loss= 0.26610 (3167 samples/sec)
saving....
2024-03-10 03:03:30.375434------------------------------------------------------ Precision@1: 64.56% 

[65.18, 64.62, 64.59, 64.63, 64.4, 64.79, 64.65, 64.9, 64.85, 64.69, 64.56]

Epoch: 11
2024-03-10 03:03:30.657443 epoch: 11 step: 0 cls_loss= 0.26328 (106930 samples/sec)
2024-03-10 03:03:40.082629 epoch: 11 step: 100 cls_loss= 0.32675 (3183 samples/sec)
saving....
2024-03-10 03:03:50.014443------------------------------------------------------ Precision@1: 64.40% 

[65.18, 64.62, 64.59, 64.63, 64.4, 64.79, 64.65, 64.9, 64.85, 64.69, 64.56, 64.4]

Epoch: 12
2024-03-10 03:03:50.278389 epoch: 12 step: 0 cls_loss= 0.22455 (114332 samples/sec)
2024-03-10 03:03:59.743565 epoch: 12 step: 100 cls_loss= 0.28890 (3169 samples/sec)
saving....
2024-03-10 03:04:09.860853------------------------------------------------------ Precision@1: 64.57% 

[65.18, 64.62, 64.59, 64.63, 64.4, 64.79, 64.65, 64.9, 64.85, 64.69, 64.56, 64.4, 64.57]

Epoch: 13
2024-03-10 03:04:10.131870 epoch: 13 step: 0 cls_loss= 0.24684 (111382 samples/sec)
2024-03-10 03:04:19.558330 epoch: 13 step: 100 cls_loss= 0.29602 (3182 samples/sec)
saving....
2024-03-10 03:04:29.549029------------------------------------------------------ Precision@1: 64.42% 

[65.18, 64.62, 64.59, 64.63, 64.4, 64.79, 64.65, 64.9, 64.85, 64.69, 64.56, 64.4, 64.57, 64.42]

Epoch: 14
2024-03-10 03:04:29.831818 epoch: 14 step: 0 cls_loss= 0.20185 (106576 samples/sec)
2024-03-10 03:04:39.360563 epoch: 14 step: 100 cls_loss= 0.25922 (3148 samples/sec)
saving....
2024-03-10 03:04:49.360471------------------------------------------------------ Precision@1: 64.11% 

[65.18, 64.62, 64.59, 64.63, 64.4, 64.79, 64.65, 64.9, 64.85, 64.69, 64.56, 64.4, 64.57, 64.42, 64.11]

Epoch: 15
2024-03-10 03:04:49.621635 epoch: 15 step: 0 cls_loss= 0.24353 (115496 samples/sec)
2024-03-10 03:04:59.061055 epoch: 15 step: 100 cls_loss= 0.21544 (3178 samples/sec)
saving....
2024-03-10 03:05:09.126281------------------------------------------------------ Precision@1: 64.51% 

[65.18, 64.62, 64.59, 64.63, 64.4, 64.79, 64.65, 64.9, 64.85, 64.69, 64.56, 64.4, 64.57, 64.42, 64.11, 64.51]

Epoch: 16
2024-03-10 03:05:09.391093 epoch: 16 step: 0 cls_loss= 0.17686 (113756 samples/sec)
2024-03-10 03:05:18.823013 epoch: 16 step: 100 cls_loss= 0.25163 (3180 samples/sec)
saving....
2024-03-10 03:05:28.756490------------------------------------------------------ Precision@1: 64.71% 

[65.18, 64.62, 64.59, 64.63, 64.4, 64.79, 64.65, 64.9, 64.85, 64.69, 64.56, 64.4, 64.57, 64.42, 64.11, 64.51, 64.71]

Epoch: 17
2024-03-10 03:05:29.033815 epoch: 17 step: 0 cls_loss= 0.19215 (108702 samples/sec)
2024-03-10 03:05:38.484941 epoch: 17 step: 100 cls_loss= 0.22708 (3174 samples/sec)
saving....
2024-03-10 03:05:48.440152------------------------------------------------------ Precision@1: 64.65% 

[65.18, 64.62, 64.59, 64.63, 64.4, 64.79, 64.65, 64.9, 64.85, 64.69, 64.56, 64.4, 64.57, 64.42, 64.11, 64.51, 64.71, 64.65]

Epoch: 18
2024-03-10 03:05:48.718309 epoch: 18 step: 0 cls_loss= 0.24007 (108436 samples/sec)
2024-03-10 03:05:58.210794 epoch: 18 step: 100 cls_loss= 0.21598 (3160 samples/sec)
saving....
2024-03-10 03:06:08.265325------------------------------------------------------ Precision@1: 64.59% 

[65.18, 64.62, 64.59, 64.63, 64.4, 64.79, 64.65, 64.9, 64.85, 64.69, 64.56, 64.4, 64.57, 64.42, 64.11, 64.51, 64.71, 64.65, 64.59]

Epoch: 19
2024-03-10 03:06:08.539903 epoch: 19 step: 0 cls_loss= 0.22468 (109765 samples/sec)
2024-03-10 03:06:18.056208 epoch: 19 step: 100 cls_loss= 0.20325 (3152 samples/sec)
saving....
2024-03-10 03:06:28.160962------------------------------------------------------ Precision@1: 64.39% 

[65.18, 64.62, 64.59, 64.63, 64.4, 64.79, 64.65, 64.9, 64.85, 64.69, 64.56, 64.4, 64.57, 64.42, 64.11, 64.51, 64.71, 64.65, 64.59, 64.39]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 03:06:31.034478 epoch: 0 step: 0 cls_loss= 0.35211 (40926 samples/sec)
2024-03-10 03:06:40.501158 epoch: 0 step: 100 cls_loss= 0.34377 (3169 samples/sec)
saving....
2024-03-10 03:06:50.755033------------------------------------------------------ Precision@1: 65.11% 

[65.11]

Epoch: 1
2024-03-10 03:06:51.014072 epoch: 1 step: 0 cls_loss= 0.34573 (116515 samples/sec)
2024-03-10 03:07:00.484943 epoch: 1 step: 100 cls_loss= 0.35300 (3167 samples/sec)
saving....
2024-03-10 03:07:10.463700------------------------------------------------------ Precision@1: 64.86% 

[65.11, 64.86]

Epoch: 2
2024-03-10 03:07:10.730744 epoch: 2 step: 0 cls_loss= 0.25190 (112897 samples/sec)
2024-03-10 03:07:20.211280 epoch: 2 step: 100 cls_loss= 0.39808 (3164 samples/sec)
saving....
2024-03-10 03:07:30.232358------------------------------------------------------ Precision@1: 64.44% 

[65.11, 64.86, 64.44]

Epoch: 3
2024-03-10 03:07:30.494033 epoch: 3 step: 0 cls_loss= 0.34130 (115340 samples/sec)
2024-03-10 03:07:39.984905 epoch: 3 step: 100 cls_loss= 0.28564 (3161 samples/sec)
saving....
2024-03-10 03:07:50.039472------------------------------------------------------ Precision@1: 64.57% 

[65.11, 64.86, 64.44, 64.57]

Epoch: 4
2024-03-10 03:07:50.303525 epoch: 4 step: 0 cls_loss= 0.34774 (114194 samples/sec)
2024-03-10 03:07:59.765946 epoch: 4 step: 100 cls_loss= 0.33961 (3170 samples/sec)
saving....
2024-03-10 03:08:09.764178------------------------------------------------------ Precision@1: 65.04% 

[65.11, 64.86, 64.44, 64.57, 65.04]

Epoch: 5
2024-03-10 03:08:10.041516 epoch: 5 step: 0 cls_loss= 0.24856 (108818 samples/sec)
2024-03-10 03:08:19.500054 epoch: 5 step: 100 cls_loss= 0.34307 (3171 samples/sec)
saving....
2024-03-10 03:08:29.511713------------------------------------------------------ Precision@1: 64.52% 

[65.11, 64.86, 64.44, 64.57, 65.04, 64.52]

Epoch: 6
2024-03-10 03:08:29.765720 epoch: 6 step: 0 cls_loss= 0.34103 (118840 samples/sec)
2024-03-10 03:08:39.235198 epoch: 6 step: 100 cls_loss= 0.28261 (3168 samples/sec)
saving....
2024-03-10 03:08:49.330623------------------------------------------------------ Precision@1: 64.85% 

[65.11, 64.86, 64.44, 64.57, 65.04, 64.52, 64.85]

Epoch: 7
2024-03-10 03:08:49.599788 epoch: 7 step: 0 cls_loss= 0.29073 (112058 samples/sec)
2024-03-10 03:08:59.090499 epoch: 7 step: 100 cls_loss= 0.38145 (3161 samples/sec)
saving....
2024-03-10 03:09:09.053651------------------------------------------------------ Precision@1: 64.50% 

[65.11, 64.86, 64.44, 64.57, 65.04, 64.52, 64.85, 64.5]

Epoch: 8
2024-03-10 03:09:09.334143 epoch: 8 step: 0 cls_loss= 0.27317 (107459 samples/sec)
2024-03-10 03:09:18.809302 epoch: 8 step: 100 cls_loss= 0.31863 (3166 samples/sec)
saving....
2024-03-10 03:09:28.785591------------------------------------------------------ Precision@1: 64.58% 

[65.11, 64.86, 64.44, 64.57, 65.04, 64.52, 64.85, 64.5, 64.58]

Epoch: 9
2024-03-10 03:09:29.053327 epoch: 9 step: 0 cls_loss= 0.30241 (112648 samples/sec)
2024-03-10 03:09:38.566973 epoch: 9 step: 100 cls_loss= 0.32843 (3153 samples/sec)
saving....
2024-03-10 03:09:48.573642------------------------------------------------------ Precision@1: 64.03% 

[65.11, 64.86, 64.44, 64.57, 65.04, 64.52, 64.85, 64.5, 64.58, 64.03]

Epoch: 10
2024-03-10 03:09:48.847389 epoch: 10 step: 0 cls_loss= 0.21092 (110180 samples/sec)
2024-03-10 03:09:58.313149 epoch: 10 step: 100 cls_loss= 0.35645 (3169 samples/sec)
saving....
2024-03-10 03:10:08.338543------------------------------------------------------ Precision@1: 64.42% 

[65.11, 64.86, 64.44, 64.57, 65.04, 64.52, 64.85, 64.5, 64.58, 64.03, 64.42]

Epoch: 11
2024-03-10 03:10:08.590758 epoch: 11 step: 0 cls_loss= 0.21581 (119593 samples/sec)
2024-03-10 03:10:18.065735 epoch: 11 step: 100 cls_loss= 0.26643 (3166 samples/sec)
saving....
2024-03-10 03:10:28.157627------------------------------------------------------ Precision@1: 64.20% 

[65.11, 64.86, 64.44, 64.57, 65.04, 64.52, 64.85, 64.5, 64.58, 64.03, 64.42, 64.2]

Epoch: 12
2024-03-10 03:10:28.434755 epoch: 12 step: 0 cls_loss= 0.26483 (108806 samples/sec)
2024-03-10 03:10:37.937273 epoch: 12 step: 100 cls_loss= 0.29570 (3157 samples/sec)
saving....
2024-03-10 03:10:48.029712------------------------------------------------------ Precision@1: 64.66% 

[65.11, 64.86, 64.44, 64.57, 65.04, 64.52, 64.85, 64.5, 64.58, 64.03, 64.42, 64.2, 64.66]

Epoch: 13
2024-03-10 03:10:48.309363 epoch: 13 step: 0 cls_loss= 0.22607 (107880 samples/sec)
2024-03-10 03:10:57.816614 epoch: 13 step: 100 cls_loss= 0.29259 (3155 samples/sec)
saving....
2024-03-10 03:11:07.847680------------------------------------------------------ Precision@1: 64.67% 

[65.11, 64.86, 64.44, 64.57, 65.04, 64.52, 64.85, 64.5, 64.58, 64.03, 64.42, 64.2, 64.66, 64.67]

Epoch: 14
2024-03-10 03:11:08.102011 epoch: 14 step: 0 cls_loss= 0.17317 (118680 samples/sec)
2024-03-10 03:11:17.582693 epoch: 14 step: 100 cls_loss= 0.21218 (3164 samples/sec)
saving....
2024-03-10 03:11:27.660819------------------------------------------------------ Precision@1: 64.35% 

[65.11, 64.86, 64.44, 64.57, 65.04, 64.52, 64.85, 64.5, 64.58, 64.03, 64.42, 64.2, 64.66, 64.67, 64.35]

Epoch: 15
2024-03-10 03:11:27.914829 epoch: 15 step: 0 cls_loss= 0.21815 (118803 samples/sec)
2024-03-10 03:11:37.380707 epoch: 15 step: 100 cls_loss= 0.21224 (3169 samples/sec)
saving....
2024-03-10 03:11:47.389973------------------------------------------------------ Precision@1: 64.32% 

[65.11, 64.86, 64.44, 64.57, 65.04, 64.52, 64.85, 64.5, 64.58, 64.03, 64.42, 64.2, 64.66, 64.67, 64.35, 64.32]

Epoch: 16
2024-03-10 03:11:47.642728 epoch: 16 step: 0 cls_loss= 0.22344 (119459 samples/sec)
2024-03-10 03:11:57.124139 epoch: 16 step: 100 cls_loss= 0.19958 (3164 samples/sec)
saving....
2024-03-10 03:12:07.143276------------------------------------------------------ Precision@1: 64.20% 

[65.11, 64.86, 64.44, 64.57, 65.04, 64.52, 64.85, 64.5, 64.58, 64.03, 64.42, 64.2, 64.66, 64.67, 64.35, 64.32, 64.2]

Epoch: 17
2024-03-10 03:12:07.402273 epoch: 17 step: 0 cls_loss= 0.26245 (116496 samples/sec)
2024-03-10 03:12:16.875051 epoch: 17 step: 100 cls_loss= 0.25750 (3167 samples/sec)
saving....
2024-03-10 03:12:26.876590------------------------------------------------------ Precision@1: 64.17% 

[65.11, 64.86, 64.44, 64.57, 65.04, 64.52, 64.85, 64.5, 64.58, 64.03, 64.42, 64.2, 64.66, 64.67, 64.35, 64.32, 64.2, 64.17]

Epoch: 18
2024-03-10 03:12:27.139529 epoch: 18 step: 0 cls_loss= 0.20936 (114764 samples/sec)
2024-03-10 03:12:36.624265 epoch: 18 step: 100 cls_loss= 0.22689 (3163 samples/sec)
saving....
2024-03-10 03:12:46.639258------------------------------------------------------ Precision@1: 64.46% 

[65.11, 64.86, 64.44, 64.57, 65.04, 64.52, 64.85, 64.5, 64.58, 64.03, 64.42, 64.2, 64.66, 64.67, 64.35, 64.32, 64.2, 64.17, 64.46]

Epoch: 19
2024-03-10 03:12:46.901657 epoch: 19 step: 0 cls_loss= 0.22874 (115051 samples/sec)
2024-03-10 03:12:56.378727 epoch: 19 step: 100 cls_loss= 0.20939 (3165 samples/sec)
saving....
2024-03-10 03:13:06.378290------------------------------------------------------ Precision@1: 64.27% 

[65.11, 64.86, 64.44, 64.57, 65.04, 64.52, 64.85, 64.5, 64.58, 64.03, 64.42, 64.2, 64.66, 64.67, 64.35, 64.32, 64.2, 64.17, 64.46, 64.27]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 03:13:09.277166 epoch: 0 step: 0 cls_loss= 0.28135 (39855 samples/sec)
2024-03-10 03:13:18.750045 epoch: 0 step: 100 cls_loss= 0.35548 (3166 samples/sec)
saving....
2024-03-10 03:13:29.073195------------------------------------------------------ Precision@1: 64.92% 

[64.92]

Epoch: 1
2024-03-10 03:13:29.335137 epoch: 1 step: 0 cls_loss= 0.35953 (115217 samples/sec)
2024-03-10 03:13:38.833084 epoch: 1 step: 100 cls_loss= 0.36810 (3158 samples/sec)
saving....
2024-03-10 03:13:48.947634------------------------------------------------------ Precision@1: 64.96% 

[64.92, 64.96]

Epoch: 2
2024-03-10 03:13:49.201222 epoch: 2 step: 0 cls_loss= 0.32808 (119074 samples/sec)
2024-03-10 03:13:58.721481 epoch: 2 step: 100 cls_loss= 0.36560 (3151 samples/sec)
saving....
2024-03-10 03:14:08.825995------------------------------------------------------ Precision@1: 64.69% 

[64.92, 64.96, 64.69]

Epoch: 3
2024-03-10 03:14:09.088717 epoch: 3 step: 0 cls_loss= 0.31978 (114870 samples/sec)
2024-03-10 03:14:18.612764 epoch: 3 step: 100 cls_loss= 0.32025 (3150 samples/sec)
saving....
2024-03-10 03:14:28.753557------------------------------------------------------ Precision@1: 64.96% 

[64.92, 64.96, 64.69, 64.96]

Epoch: 4
2024-03-10 03:14:29.019114 epoch: 4 step: 0 cls_loss= 0.30111 (113635 samples/sec)
2024-03-10 03:14:38.518947 epoch: 4 step: 100 cls_loss= 0.35429 (3157 samples/sec)
saving....
2024-03-10 03:14:48.605457------------------------------------------------------ Precision@1: 64.10% 

[64.92, 64.96, 64.69, 64.96, 64.1]

Epoch: 5
2024-03-10 03:14:48.867641 epoch: 5 step: 0 cls_loss= 0.35871 (115095 samples/sec)
2024-03-10 03:14:58.380813 epoch: 5 step: 100 cls_loss= 0.33679 (3153 samples/sec)
saving....
2024-03-10 03:15:08.402590------------------------------------------------------ Precision@1: 63.88% 

[64.92, 64.96, 64.69, 64.96, 64.1, 63.88]

Epoch: 6
2024-03-10 03:15:08.675277 epoch: 6 step: 0 cls_loss= 0.28631 (110560 samples/sec)
2024-03-10 03:15:18.271193 epoch: 6 step: 100 cls_loss= 0.31861 (3126 samples/sec)
saving....
2024-03-10 03:15:28.379592------------------------------------------------------ Precision@1: 64.53% 

[64.92, 64.96, 64.69, 64.96, 64.1, 63.88, 64.53]

Epoch: 7
2024-03-10 03:15:28.659891 epoch: 7 step: 0 cls_loss= 0.27747 (107588 samples/sec)
2024-03-10 03:15:38.148152 epoch: 7 step: 100 cls_loss= 0.27264 (3161 samples/sec)
saving....
2024-03-10 03:15:48.227394------------------------------------------------------ Precision@1: 64.88% 

[64.92, 64.96, 64.69, 64.96, 64.1, 63.88, 64.53, 64.88]

Epoch: 8
2024-03-10 03:15:48.489451 epoch: 8 step: 0 cls_loss= 0.27326 (115196 samples/sec)
2024-03-10 03:15:57.969779 epoch: 8 step: 100 cls_loss= 0.27446 (3164 samples/sec)
saving....
2024-03-10 03:16:08.069192------------------------------------------------------ Precision@1: 64.62% 

[64.92, 64.96, 64.69, 64.96, 64.1, 63.88, 64.53, 64.88, 64.62]

Epoch: 9
2024-03-10 03:16:08.319813 epoch: 9 step: 0 cls_loss= 0.25416 (120407 samples/sec)
2024-03-10 03:16:17.822999 epoch: 9 step: 100 cls_loss= 0.26381 (3157 samples/sec)
saving....
2024-03-10 03:16:28.118883------------------------------------------------------ Precision@1: 64.11% 

[64.92, 64.96, 64.69, 64.96, 64.1, 63.88, 64.53, 64.88, 64.62, 64.11]

Epoch: 10
2024-03-10 03:16:28.377124 epoch: 10 step: 0 cls_loss= 0.21663 (116901 samples/sec)
2024-03-10 03:16:37.904466 epoch: 10 step: 100 cls_loss= 0.28000 (3149 samples/sec)
saving....
2024-03-10 03:16:47.925744------------------------------------------------------ Precision@1: 64.34% 

[64.92, 64.96, 64.69, 64.96, 64.1, 63.88, 64.53, 64.88, 64.62, 64.11, 64.34]

Epoch: 11
2024-03-10 03:16:48.190830 epoch: 11 step: 0 cls_loss= 0.28616 (113882 samples/sec)
2024-03-10 03:16:57.668520 epoch: 11 step: 100 cls_loss= 0.32080 (3165 samples/sec)
saving....
2024-03-10 03:17:07.703889------------------------------------------------------ Precision@1: 64.77% 

[64.92, 64.96, 64.69, 64.96, 64.1, 63.88, 64.53, 64.88, 64.62, 64.11, 64.34, 64.77]

Epoch: 12
2024-03-10 03:17:07.954086 epoch: 12 step: 0 cls_loss= 0.22959 (120654 samples/sec)
2024-03-10 03:17:17.412916 epoch: 12 step: 100 cls_loss= 0.21331 (3171 samples/sec)
saving....
2024-03-10 03:17:27.401019------------------------------------------------------ Precision@1: 64.10% 

[64.92, 64.96, 64.69, 64.96, 64.1, 63.88, 64.53, 64.88, 64.62, 64.11, 64.34, 64.77, 64.1]

Epoch: 13
2024-03-10 03:17:27.666681 epoch: 13 step: 0 cls_loss= 0.27231 (113572 samples/sec)
2024-03-10 03:17:37.161684 epoch: 13 step: 100 cls_loss= 0.28501 (3159 samples/sec)
saving....
2024-03-10 03:17:47.178111------------------------------------------------------ Precision@1: 64.02% 

[64.92, 64.96, 64.69, 64.96, 64.1, 63.88, 64.53, 64.88, 64.62, 64.11, 64.34, 64.77, 64.1, 64.02]

Epoch: 14
2024-03-10 03:17:47.452068 epoch: 14 step: 0 cls_loss= 0.27466 (110139 samples/sec)
2024-03-10 03:17:56.995178 epoch: 14 step: 100 cls_loss= 0.25683 (3143 samples/sec)
saving....
2024-03-10 03:18:07.072772------------------------------------------------------ Precision@1: 64.39% 

[64.92, 64.96, 64.69, 64.96, 64.1, 63.88, 64.53, 64.88, 64.62, 64.11, 64.34, 64.77, 64.1, 64.02, 64.39]

Epoch: 15
2024-03-10 03:18:07.350589 epoch: 15 step: 0 cls_loss= 0.21818 (108581 samples/sec)
2024-03-10 03:18:16.791885 epoch: 15 step: 100 cls_loss= 0.28588 (3177 samples/sec)
saving....
2024-03-10 03:18:26.771121------------------------------------------------------ Precision@1: 64.28% 

[64.92, 64.96, 64.69, 64.96, 64.1, 63.88, 64.53, 64.88, 64.62, 64.11, 64.34, 64.77, 64.1, 64.02, 64.39, 64.28]

Epoch: 16
2024-03-10 03:18:27.053620 epoch: 16 step: 0 cls_loss= 0.19361 (106659 samples/sec)
2024-03-10 03:18:36.516300 epoch: 16 step: 100 cls_loss= 0.23394 (3170 samples/sec)
saving....
2024-03-10 03:18:46.514267------------------------------------------------------ Precision@1: 64.22% 

[64.92, 64.96, 64.69, 64.96, 64.1, 63.88, 64.53, 64.88, 64.62, 64.11, 64.34, 64.77, 64.1, 64.02, 64.39, 64.28, 64.22]

Epoch: 17
2024-03-10 03:18:46.790895 epoch: 17 step: 0 cls_loss= 0.20098 (108983 samples/sec)
2024-03-10 03:18:56.248841 epoch: 17 step: 100 cls_loss= 0.17097 (3172 samples/sec)
saving....
2024-03-10 03:19:06.292842------------------------------------------------------ Precision@1: 64.24% 

[64.92, 64.96, 64.69, 64.96, 64.1, 63.88, 64.53, 64.88, 64.62, 64.11, 64.34, 64.77, 64.1, 64.02, 64.39, 64.28, 64.22, 64.24]

Epoch: 18
2024-03-10 03:19:06.566070 epoch: 18 step: 0 cls_loss= 0.19716 (110458 samples/sec)
2024-03-10 03:19:16.101190 epoch: 18 step: 100 cls_loss= 0.18802 (3146 samples/sec)
saving....
2024-03-10 03:19:26.152214------------------------------------------------------ Precision@1: 64.26% 

[64.92, 64.96, 64.69, 64.96, 64.1, 63.88, 64.53, 64.88, 64.62, 64.11, 64.34, 64.77, 64.1, 64.02, 64.39, 64.28, 64.22, 64.24, 64.26]

Epoch: 19
2024-03-10 03:19:26.417824 epoch: 19 step: 0 cls_loss= 0.21709 (113598 samples/sec)
2024-03-10 03:19:35.964601 epoch: 19 step: 100 cls_loss= 0.19115 (3142 samples/sec)
saving....
2024-03-10 03:19:46.105860------------------------------------------------------ Precision@1: 64.79% 

[64.92, 64.96, 64.69, 64.96, 64.1, 63.88, 64.53, 64.88, 64.62, 64.11, 64.34, 64.77, 64.1, 64.02, 64.39, 64.28, 64.22, 64.24, 64.26, 64.79]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 03:19:49.051397 epoch: 0 step: 0 cls_loss= 0.32609 (38931 samples/sec)
2024-03-10 03:19:58.530295 epoch: 0 step: 100 cls_loss= 0.51421 (3164 samples/sec)
saving....
2024-03-10 03:20:08.798559------------------------------------------------------ Precision@1: 64.89% 

[64.89]

Epoch: 1
2024-03-10 03:20:09.051541 epoch: 1 step: 0 cls_loss= 0.31018 (119311 samples/sec)
2024-03-10 03:20:18.524560 epoch: 1 step: 100 cls_loss= 0.32679 (3167 samples/sec)
saving....
2024-03-10 03:20:28.498401------------------------------------------------------ Precision@1: 64.92% 

[64.89, 64.92]

Epoch: 2
2024-03-10 03:20:28.767718 epoch: 2 step: 0 cls_loss= 0.27981 (111933 samples/sec)
2024-03-10 03:20:38.291733 epoch: 2 step: 100 cls_loss= 0.31401 (3150 samples/sec)
saving....
2024-03-10 03:20:48.265984------------------------------------------------------ Precision@1: 64.64% 

[64.89, 64.92, 64.64]

Epoch: 3
2024-03-10 03:20:48.526152 epoch: 3 step: 0 cls_loss= 0.37214 (115951 samples/sec)
2024-03-10 03:20:58.102445 epoch: 3 step: 100 cls_loss= 0.38643 (3132 samples/sec)
saving....
2024-03-10 03:21:08.179851------------------------------------------------------ Precision@1: 64.81% 

[64.89, 64.92, 64.64, 64.81]

Epoch: 4
2024-03-10 03:21:08.435267 epoch: 4 step: 0 cls_loss= 0.33924 (118201 samples/sec)
2024-03-10 03:21:17.898781 epoch: 4 step: 100 cls_loss= 0.26800 (3170 samples/sec)
saving....
2024-03-10 03:21:27.914776------------------------------------------------------ Precision@1: 64.72% 

[64.89, 64.92, 64.64, 64.81, 64.72]

Epoch: 5
2024-03-10 03:21:28.163916 epoch: 5 step: 0 cls_loss= 0.31873 (121120 samples/sec)
2024-03-10 03:21:37.632496 epoch: 5 step: 100 cls_loss= 0.30541 (3168 samples/sec)
saving....
2024-03-10 03:21:47.624009------------------------------------------------------ Precision@1: 64.51% 

[64.89, 64.92, 64.64, 64.81, 64.72, 64.51]

Epoch: 6
2024-03-10 03:21:47.899831 epoch: 6 step: 0 cls_loss= 0.29076 (109276 samples/sec)
2024-03-10 03:21:57.387098 epoch: 6 step: 100 cls_loss= 0.23237 (3162 samples/sec)
saving....
2024-03-10 03:22:07.427044------------------------------------------------------ Precision@1: 64.68% 

[64.89, 64.92, 64.64, 64.81, 64.72, 64.51, 64.68]

Epoch: 7
2024-03-10 03:22:07.701620 epoch: 7 step: 0 cls_loss= 0.31436 (109876 samples/sec)
2024-03-10 03:22:17.203686 epoch: 7 step: 100 cls_loss= 0.32769 (3157 samples/sec)
saving....
2024-03-10 03:22:27.331719------------------------------------------------------ Precision@1: 64.45% 

[64.89, 64.92, 64.64, 64.81, 64.72, 64.51, 64.68, 64.45]

Epoch: 8
2024-03-10 03:22:27.618673 epoch: 8 step: 0 cls_loss= 0.31002 (105059 samples/sec)
2024-03-10 03:22:37.068695 epoch: 8 step: 100 cls_loss= 0.30753 (3174 samples/sec)
saving....
2024-03-10 03:22:47.093574------------------------------------------------------ Precision@1: 64.86% 

[64.89, 64.92, 64.64, 64.81, 64.72, 64.51, 64.68, 64.45, 64.86]

Epoch: 9
2024-03-10 03:22:47.358717 epoch: 9 step: 0 cls_loss= 0.29022 (113823 samples/sec)
2024-03-10 03:22:56.880154 epoch: 9 step: 100 cls_loss= 0.27539 (3150 samples/sec)
saving....
2024-03-10 03:23:06.934480------------------------------------------------------ Precision@1: 64.47% 

[64.89, 64.92, 64.64, 64.81, 64.72, 64.51, 64.68, 64.45, 64.86, 64.47]

Epoch: 10
2024-03-10 03:23:07.191154 epoch: 10 step: 0 cls_loss= 0.24158 (117585 samples/sec)
2024-03-10 03:23:16.669642 epoch: 10 step: 100 cls_loss= 0.27245 (3165 samples/sec)
saving....
2024-03-10 03:23:26.814088------------------------------------------------------ Precision@1: 64.82% 

[64.89, 64.92, 64.64, 64.81, 64.72, 64.51, 64.68, 64.45, 64.86, 64.47, 64.82]

Epoch: 11
2024-03-10 03:23:27.089737 epoch: 11 step: 0 cls_loss= 0.23075 (109401 samples/sec)
2024-03-10 03:23:36.619934 epoch: 11 step: 100 cls_loss= 0.26195 (3148 samples/sec)
saving....
2024-03-10 03:23:46.711526------------------------------------------------------ Precision@1: 64.30% 

[64.89, 64.92, 64.64, 64.81, 64.72, 64.51, 64.68, 64.45, 64.86, 64.47, 64.82, 64.3]

Epoch: 12
2024-03-10 03:23:46.969862 epoch: 12 step: 0 cls_loss= 0.26414 (116835 samples/sec)
2024-03-10 03:23:56.428740 epoch: 12 step: 100 cls_loss= 0.30379 (3171 samples/sec)
saving....
2024-03-10 03:24:06.436692------------------------------------------------------ Precision@1: 64.54% 

[64.89, 64.92, 64.64, 64.81, 64.72, 64.51, 64.68, 64.45, 64.86, 64.47, 64.82, 64.3, 64.54]

Epoch: 13
2024-03-10 03:24:06.703450 epoch: 13 step: 0 cls_loss= 0.24646 (113132 samples/sec)
2024-03-10 03:24:16.172507 epoch: 13 step: 100 cls_loss= 0.25434 (3168 samples/sec)
saving....
2024-03-10 03:24:26.129435------------------------------------------------------ Precision@1: 64.19% 

[64.89, 64.92, 64.64, 64.81, 64.72, 64.51, 64.68, 64.45, 64.86, 64.47, 64.82, 64.3, 64.54, 64.19]

Epoch: 14
2024-03-10 03:24:26.379452 epoch: 14 step: 0 cls_loss= 0.27590 (120702 samples/sec)
2024-03-10 03:24:35.857041 epoch: 14 step: 100 cls_loss= 0.20322 (3165 samples/sec)
saving....
2024-03-10 03:24:45.835429------------------------------------------------------ Precision@1: 64.18% 

[64.89, 64.92, 64.64, 64.81, 64.72, 64.51, 64.68, 64.45, 64.86, 64.47, 64.82, 64.3, 64.54, 64.19, 64.18]

Epoch: 15
2024-03-10 03:24:46.101583 epoch: 15 step: 0 cls_loss= 0.23193 (113277 samples/sec)
2024-03-10 03:24:55.580127 epoch: 15 step: 100 cls_loss= 0.29333 (3165 samples/sec)
saving....
2024-03-10 03:25:05.597834------------------------------------------------------ Precision@1: 64.07% 

[64.89, 64.92, 64.64, 64.81, 64.72, 64.51, 64.68, 64.45, 64.86, 64.47, 64.82, 64.3, 64.54, 64.19, 64.18, 64.07]

Epoch: 16
2024-03-10 03:25:05.864928 epoch: 16 step: 0 cls_loss= 0.24550 (112885 samples/sec)
2024-03-10 03:25:15.337115 epoch: 16 step: 100 cls_loss= 0.17841 (3167 samples/sec)
saving....
2024-03-10 03:25:25.450051------------------------------------------------------ Precision@1: 63.97% 

[64.89, 64.92, 64.64, 64.81, 64.72, 64.51, 64.68, 64.45, 64.86, 64.47, 64.82, 64.3, 64.54, 64.19, 64.18, 64.07, 63.97]

Epoch: 17
2024-03-10 03:25:25.725640 epoch: 17 step: 0 cls_loss= 0.21096 (109371 samples/sec)
2024-03-10 03:25:35.361457 epoch: 17 step: 100 cls_loss= 0.25505 (3113 samples/sec)
saving....
2024-03-10 03:25:45.459464------------------------------------------------------ Precision@1: 64.42% 

[64.89, 64.92, 64.64, 64.81, 64.72, 64.51, 64.68, 64.45, 64.86, 64.47, 64.82, 64.3, 64.54, 64.19, 64.18, 64.07, 63.97, 64.42]

Epoch: 18
2024-03-10 03:25:45.705856 epoch: 18 step: 0 cls_loss= 0.20749 (122509 samples/sec)
2024-03-10 03:25:55.177458 epoch: 18 step: 100 cls_loss= 0.15882 (3167 samples/sec)
saving....
2024-03-10 03:26:05.194301------------------------------------------------------ Precision@1: 64.52% 

[64.89, 64.92, 64.64, 64.81, 64.72, 64.51, 64.68, 64.45, 64.86, 64.47, 64.82, 64.3, 64.54, 64.19, 64.18, 64.07, 63.97, 64.42, 64.52]

Epoch: 19
2024-03-10 03:26:05.465433 epoch: 19 step: 0 cls_loss= 0.20179 (111296 samples/sec)
2024-03-10 03:26:15.037668 epoch: 19 step: 100 cls_loss= 0.19965 (3134 samples/sec)
saving....
2024-03-10 03:26:25.047245------------------------------------------------------ Precision@1: 63.92% 

[64.89, 64.92, 64.64, 64.81, 64.72, 64.51, 64.68, 64.45, 64.86, 64.47, 64.82, 64.3, 64.54, 64.19, 64.18, 64.07, 63.97, 64.42, 64.52, 63.92]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 03:26:27.932260 epoch: 0 step: 0 cls_loss= 0.29261 (41006 samples/sec)
2024-03-10 03:26:37.411776 epoch: 0 step: 100 cls_loss= 0.39961 (3164 samples/sec)
saving....
2024-03-10 03:26:47.716133------------------------------------------------------ Precision@1: 65.00% 

[65.0]

Epoch: 1
2024-03-10 03:26:47.986880 epoch: 1 step: 0 cls_loss= 0.34082 (111462 samples/sec)
2024-03-10 03:26:57.523590 epoch: 1 step: 100 cls_loss= 0.42197 (3145 samples/sec)
saving....
2024-03-10 03:27:07.585253------------------------------------------------------ Precision@1: 64.89% 

[65.0, 64.89]

Epoch: 2
2024-03-10 03:27:07.863357 epoch: 2 step: 0 cls_loss= 0.32594 (108464 samples/sec)
2024-03-10 03:27:17.330892 epoch: 2 step: 100 cls_loss= 0.28758 (3168 samples/sec)
saving....
2024-03-10 03:27:27.381766------------------------------------------------------ Precision@1: 65.16% 

[65.0, 64.89, 65.16]

Epoch: 3
2024-03-10 03:27:27.653632 epoch: 3 step: 0 cls_loss= 0.28495 (110938 samples/sec)
2024-03-10 03:27:37.219388 epoch: 3 step: 100 cls_loss= 0.31421 (3136 samples/sec)
saving....
2024-03-10 03:27:47.296900------------------------------------------------------ Precision@1: 64.81% 

[65.0, 64.89, 65.16, 64.81]

Epoch: 4
2024-03-10 03:27:47.575025 epoch: 4 step: 0 cls_loss= 0.28124 (108430 samples/sec)
2024-03-10 03:27:57.064437 epoch: 4 step: 100 cls_loss= 0.40833 (3161 samples/sec)
saving....
2024-03-10 03:28:07.129886------------------------------------------------------ Precision@1: 64.63% 

[65.0, 64.89, 65.16, 64.81, 64.63]

Epoch: 5
2024-03-10 03:28:07.402983 epoch: 5 step: 0 cls_loss= 0.35640 (110481 samples/sec)
2024-03-10 03:28:16.879088 epoch: 5 step: 100 cls_loss= 0.30537 (3166 samples/sec)
saving....
2024-03-10 03:28:26.911120------------------------------------------------------ Precision@1: 64.63% 

[65.0, 64.89, 65.16, 64.81, 64.63, 64.63]

Epoch: 6
2024-03-10 03:28:27.180755 epoch: 6 step: 0 cls_loss= 0.28752 (111835 samples/sec)
2024-03-10 03:28:36.658800 epoch: 6 step: 100 cls_loss= 0.30820 (3165 samples/sec)
saving....
2024-03-10 03:28:46.706357------------------------------------------------------ Precision@1: 64.60% 

[65.0, 64.89, 65.16, 64.81, 64.63, 64.63, 64.6]

Epoch: 7
2024-03-10 03:28:46.987046 epoch: 7 step: 0 cls_loss= 0.36782 (107421 samples/sec)
2024-03-10 03:28:56.568970 epoch: 7 step: 100 cls_loss= 0.31006 (3131 samples/sec)
saving....
2024-03-10 03:29:06.669418------------------------------------------------------ Precision@1: 64.43% 

[65.0, 64.89, 65.16, 64.81, 64.63, 64.63, 64.6, 64.43]

Epoch: 8
2024-03-10 03:29:06.928312 epoch: 8 step: 0 cls_loss= 0.28134 (116572 samples/sec)
2024-03-10 03:29:16.407608 epoch: 8 step: 100 cls_loss= 0.32509 (3164 samples/sec)
saving....
2024-03-10 03:29:26.539250------------------------------------------------------ Precision@1: 64.37% 

[65.0, 64.89, 65.16, 64.81, 64.63, 64.63, 64.6, 64.43, 64.37]

Epoch: 9
2024-03-10 03:29:26.801879 epoch: 9 step: 0 cls_loss= 0.25327 (114861 samples/sec)
2024-03-10 03:29:36.291253 epoch: 9 step: 100 cls_loss= 0.33695 (3161 samples/sec)
saving....
2024-03-10 03:29:46.473273------------------------------------------------------ Precision@1: 64.44% 

[65.0, 64.89, 65.16, 64.81, 64.63, 64.63, 64.6, 64.43, 64.37, 64.44]

Epoch: 10
2024-03-10 03:29:46.727401 epoch: 10 step: 0 cls_loss= 0.25165 (118839 samples/sec)
2024-03-10 03:29:56.261977 epoch: 10 step: 100 cls_loss= 0.28228 (3146 samples/sec)
saving....
2024-03-10 03:30:06.295351------------------------------------------------------ Precision@1: 64.38% 

[65.0, 64.89, 65.16, 64.81, 64.63, 64.63, 64.6, 64.43, 64.37, 64.44, 64.38]

Epoch: 11
2024-03-10 03:30:06.562285 epoch: 11 step: 0 cls_loss= 0.24852 (113012 samples/sec)
2024-03-10 03:30:16.037449 epoch: 11 step: 100 cls_loss= 0.24635 (3166 samples/sec)
saving....
2024-03-10 03:30:26.118587------------------------------------------------------ Precision@1: 64.39% 

[65.0, 64.89, 65.16, 64.81, 64.63, 64.63, 64.6, 64.43, 64.37, 64.44, 64.38, 64.39]

Epoch: 12
2024-03-10 03:30:26.384279 epoch: 12 step: 0 cls_loss= 0.27665 (113468 samples/sec)
2024-03-10 03:30:35.936121 epoch: 12 step: 100 cls_loss= 0.24402 (3140 samples/sec)
saving....
2024-03-10 03:30:46.017513------------------------------------------------------ Precision@1: 63.94% 

[65.0, 64.89, 65.16, 64.81, 64.63, 64.63, 64.6, 64.43, 64.37, 64.44, 64.38, 64.39, 63.94]

Epoch: 13
2024-03-10 03:30:46.284582 epoch: 13 step: 0 cls_loss= 0.22860 (112941 samples/sec)
2024-03-10 03:30:55.750305 epoch: 13 step: 100 cls_loss= 0.18609 (3169 samples/sec)
saving....
2024-03-10 03:31:05.833274------------------------------------------------------ Precision@1: 64.09% 

[65.0, 64.89, 65.16, 64.81, 64.63, 64.63, 64.6, 64.43, 64.37, 64.44, 64.38, 64.39, 63.94, 64.09]

Epoch: 14
2024-03-10 03:31:06.093563 epoch: 14 step: 0 cls_loss= 0.20830 (115948 samples/sec)
2024-03-10 03:31:15.572640 epoch: 14 step: 100 cls_loss= 0.20601 (3165 samples/sec)
saving....
2024-03-10 03:31:25.675314------------------------------------------------------ Precision@1: 64.39% 

[65.0, 64.89, 65.16, 64.81, 64.63, 64.63, 64.6, 64.43, 64.37, 64.44, 64.38, 64.39, 63.94, 64.09, 64.39]

Epoch: 15
2024-03-10 03:31:25.949058 epoch: 15 step: 0 cls_loss= 0.21561 (110176 samples/sec)
2024-03-10 03:31:35.488315 epoch: 15 step: 100 cls_loss= 0.24119 (3145 samples/sec)
saving....
2024-03-10 03:31:45.621239------------------------------------------------------ Precision@1: 63.83% 

[65.0, 64.89, 65.16, 64.81, 64.63, 64.63, 64.6, 64.43, 64.37, 64.44, 64.38, 64.39, 63.94, 64.09, 64.39, 63.83]

Epoch: 16
2024-03-10 03:31:45.896257 epoch: 16 step: 0 cls_loss= 0.23688 (109693 samples/sec)
2024-03-10 03:31:55.395462 epoch: 16 step: 100 cls_loss= 0.24639 (3158 samples/sec)
saving....
2024-03-10 03:32:05.429703------------------------------------------------------ Precision@1: 64.04% 

[65.0, 64.89, 65.16, 64.81, 64.63, 64.63, 64.6, 64.43, 64.37, 64.44, 64.38, 64.39, 63.94, 64.09, 64.39, 63.83, 64.04]

Epoch: 17
2024-03-10 03:32:05.695160 epoch: 17 step: 0 cls_loss= 0.16522 (113663 samples/sec)
2024-03-10 03:32:15.366267 epoch: 17 step: 100 cls_loss= 0.22913 (3102 samples/sec)
saving....
2024-03-10 03:32:25.517899------------------------------------------------------ Precision@1: 64.37% 

[65.0, 64.89, 65.16, 64.81, 64.63, 64.63, 64.6, 64.43, 64.37, 64.44, 64.38, 64.39, 63.94, 64.09, 64.39, 63.83, 64.04, 64.37]

Epoch: 18
2024-03-10 03:32:25.783810 epoch: 18 step: 0 cls_loss= 0.19667 (113422 samples/sec)
2024-03-10 03:32:35.319831 epoch: 18 step: 100 cls_loss= 0.19859 (3146 samples/sec)
saving....
2024-03-10 03:32:45.469916------------------------------------------------------ Precision@1: 64.40% 

[65.0, 64.89, 65.16, 64.81, 64.63, 64.63, 64.6, 64.43, 64.37, 64.44, 64.38, 64.39, 63.94, 64.09, 64.39, 63.83, 64.04, 64.37, 64.4]

Epoch: 19
2024-03-10 03:32:45.722841 epoch: 19 step: 0 cls_loss= 0.25428 (119381 samples/sec)
2024-03-10 03:32:55.249378 epoch: 19 step: 100 cls_loss= 0.20293 (3149 samples/sec)
saving....
2024-03-10 03:33:05.340640------------------------------------------------------ Precision@1: 64.07% 

[65.0, 64.89, 65.16, 64.81, 64.63, 64.63, 64.6, 64.43, 64.37, 64.44, 64.38, 64.39, 63.94, 64.09, 64.39, 63.83, 64.04, 64.37, 64.4, 64.07]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 03:33:08.230381 epoch: 0 step: 0 cls_loss= 0.30634 (39354 samples/sec)
2024-03-10 03:33:17.713185 epoch: 0 step: 100 cls_loss= 0.37753 (3163 samples/sec)
saving....
2024-03-10 03:33:27.995031------------------------------------------------------ Precision@1: 65.38% 

[65.38]

Epoch: 1
2024-03-10 03:33:28.259625 epoch: 1 step: 0 cls_loss= 0.39116 (114028 samples/sec)
2024-03-10 03:33:37.682632 epoch: 1 step: 100 cls_loss= 0.40924 (3183 samples/sec)
saving....
2024-03-10 03:33:47.680026------------------------------------------------------ Precision@1: 65.00% 

[65.38, 65.0]

Epoch: 2
2024-03-10 03:33:47.942262 epoch: 2 step: 0 cls_loss= 0.39647 (115122 samples/sec)
2024-03-10 03:33:57.408816 epoch: 2 step: 100 cls_loss= 0.42258 (3169 samples/sec)
saving....
2024-03-10 03:34:07.412909------------------------------------------------------ Precision@1: 64.58% 

[65.38, 65.0, 64.58]

Epoch: 3
2024-03-10 03:34:07.675322 epoch: 3 step: 0 cls_loss= 0.26842 (114930 samples/sec)
2024-03-10 03:34:17.228683 epoch: 3 step: 100 cls_loss= 0.36239 (3140 samples/sec)
saving....
2024-03-10 03:34:27.253163------------------------------------------------------ Precision@1: 64.42% 

[65.38, 65.0, 64.58, 64.42]

Epoch: 4
2024-03-10 03:34:27.527514 epoch: 4 step: 0 cls_loss= 0.30446 (109896 samples/sec)
2024-03-10 03:34:37.068891 epoch: 4 step: 100 cls_loss= 0.35815 (3144 samples/sec)
saving....
2024-03-10 03:34:47.040885------------------------------------------------------ Precision@1: 64.66% 

[65.38, 65.0, 64.58, 64.42, 64.66]

Epoch: 5
2024-03-10 03:34:47.308720 epoch: 5 step: 0 cls_loss= 0.35134 (112662 samples/sec)
2024-03-10 03:34:56.759100 epoch: 5 step: 100 cls_loss= 0.37332 (3174 samples/sec)
saving....
2024-03-10 03:35:06.718287------------------------------------------------------ Precision@1: 64.87% 

[65.38, 65.0, 64.58, 64.42, 64.66, 64.87]

Epoch: 6
2024-03-10 03:35:06.986575 epoch: 6 step: 0 cls_loss= 0.27668 (112460 samples/sec)
2024-03-10 03:35:16.431018 epoch: 6 step: 100 cls_loss= 0.34864 (3176 samples/sec)
saving....
2024-03-10 03:35:26.417498------------------------------------------------------ Precision@1: 64.54% 

[65.38, 65.0, 64.58, 64.42, 64.66, 64.87, 64.54]

Epoch: 7
2024-03-10 03:35:26.681094 epoch: 7 step: 0 cls_loss= 0.27915 (114442 samples/sec)
2024-03-10 03:35:36.190916 epoch: 7 step: 100 cls_loss= 0.27723 (3154 samples/sec)
saving....
2024-03-10 03:35:46.241248------------------------------------------------------ Precision@1: 64.44% 

[65.38, 65.0, 64.58, 64.42, 64.66, 64.87, 64.54, 64.44]

Epoch: 8
2024-03-10 03:35:46.503428 epoch: 8 step: 0 cls_loss= 0.27293 (115105 samples/sec)
2024-03-10 03:35:55.967074 epoch: 8 step: 100 cls_loss= 0.31344 (3170 samples/sec)
saving....
2024-03-10 03:36:05.959121------------------------------------------------------ Precision@1: 64.32% 

[65.38, 65.0, 64.58, 64.42, 64.66, 64.87, 64.54, 64.44, 64.32]

Epoch: 9
2024-03-10 03:36:06.225927 epoch: 9 step: 0 cls_loss= 0.25162 (113093 samples/sec)
2024-03-10 03:36:15.739466 epoch: 9 step: 100 cls_loss= 0.24378 (3153 samples/sec)
saving....
2024-03-10 03:36:25.729609------------------------------------------------------ Precision@1: 64.16% 

[65.38, 65.0, 64.58, 64.42, 64.66, 64.87, 64.54, 64.44, 64.32, 64.16]

Epoch: 10
2024-03-10 03:36:25.991293 epoch: 10 step: 0 cls_loss= 0.18038 (115356 samples/sec)
2024-03-10 03:36:35.487685 epoch: 10 step: 100 cls_loss= 0.31399 (3159 samples/sec)
saving....
2024-03-10 03:36:45.462989------------------------------------------------------ Precision@1: 64.41% 

[65.38, 65.0, 64.58, 64.42, 64.66, 64.87, 64.54, 64.44, 64.32, 64.16, 64.41]

Epoch: 11
2024-03-10 03:36:45.716492 epoch: 11 step: 0 cls_loss= 0.23328 (119115 samples/sec)
2024-03-10 03:36:55.179112 epoch: 11 step: 100 cls_loss= 0.27618 (3170 samples/sec)
saving....
2024-03-10 03:37:05.196201------------------------------------------------------ Precision@1: 64.72% 

[65.38, 65.0, 64.58, 64.42, 64.66, 64.87, 64.54, 64.44, 64.32, 64.16, 64.41, 64.72]

Epoch: 12
2024-03-10 03:37:05.461322 epoch: 12 step: 0 cls_loss= 0.30675 (113782 samples/sec)
2024-03-10 03:37:14.948758 epoch: 12 step: 100 cls_loss= 0.26800 (3162 samples/sec)
saving....
2024-03-10 03:37:24.994886------------------------------------------------------ Precision@1: 64.29% 

[65.38, 65.0, 64.58, 64.42, 64.66, 64.87, 64.54, 64.44, 64.32, 64.16, 64.41, 64.72, 64.29]

Epoch: 13
2024-03-10 03:37:25.254890 epoch: 13 step: 0 cls_loss= 0.25227 (116031 samples/sec)
2024-03-10 03:37:34.687348 epoch: 13 step: 100 cls_loss= 0.25069 (3180 samples/sec)
saving....
2024-03-10 03:37:44.667469------------------------------------------------------ Precision@1: 63.95% 

[65.38, 65.0, 64.58, 64.42, 64.66, 64.87, 64.54, 64.44, 64.32, 64.16, 64.41, 64.72, 64.29, 63.95]

Epoch: 14
2024-03-10 03:37:44.942953 epoch: 14 step: 0 cls_loss= 0.30099 (109568 samples/sec)
2024-03-10 03:37:54.450067 epoch: 14 step: 100 cls_loss= 0.27236 (3155 samples/sec)
saving....
2024-03-10 03:38:04.477309------------------------------------------------------ Precision@1: 64.16% 

[65.38, 65.0, 64.58, 64.42, 64.66, 64.87, 64.54, 64.44, 64.32, 64.16, 64.41, 64.72, 64.29, 63.95, 64.16]

Epoch: 15
2024-03-10 03:38:04.769821 epoch: 15 step: 0 cls_loss= 0.27352 (103223 samples/sec)
2024-03-10 03:38:14.293396 epoch: 15 step: 100 cls_loss= 0.21793 (3150 samples/sec)
saving....
2024-03-10 03:38:24.288570------------------------------------------------------ Precision@1: 64.07% 

[65.38, 65.0, 64.58, 64.42, 64.66, 64.87, 64.54, 64.44, 64.32, 64.16, 64.41, 64.72, 64.29, 63.95, 64.16, 64.07]

Epoch: 16
2024-03-10 03:38:24.550497 epoch: 16 step: 0 cls_loss= 0.26495 (115210 samples/sec)
2024-03-10 03:38:34.017848 epoch: 16 step: 100 cls_loss= 0.17339 (3168 samples/sec)
saving....
2024-03-10 03:38:44.049680------------------------------------------------------ Precision@1: 64.03% 

[65.38, 65.0, 64.58, 64.42, 64.66, 64.87, 64.54, 64.44, 64.32, 64.16, 64.41, 64.72, 64.29, 63.95, 64.16, 64.07, 64.03]

Epoch: 17
2024-03-10 03:38:44.328307 epoch: 17 step: 0 cls_loss= 0.19163 (108129 samples/sec)
2024-03-10 03:38:53.817091 epoch: 17 step: 100 cls_loss= 0.22787 (3161 samples/sec)
saving....
2024-03-10 03:39:03.892260------------------------------------------------------ Precision@1: 64.06% 

[65.38, 65.0, 64.58, 64.42, 64.66, 64.87, 64.54, 64.44, 64.32, 64.16, 64.41, 64.72, 64.29, 63.95, 64.16, 64.07, 64.03, 64.06]

Epoch: 18
2024-03-10 03:39:04.154355 epoch: 18 step: 0 cls_loss= 0.19832 (115223 samples/sec)
2024-03-10 03:39:13.638101 epoch: 18 step: 100 cls_loss= 0.21146 (3163 samples/sec)
saving....
2024-03-10 03:39:23.729086------------------------------------------------------ Precision@1: 64.25% 

[65.38, 65.0, 64.58, 64.42, 64.66, 64.87, 64.54, 64.44, 64.32, 64.16, 64.41, 64.72, 64.29, 63.95, 64.16, 64.07, 64.03, 64.06, 64.25]

Epoch: 19
2024-03-10 03:39:23.992885 epoch: 19 step: 0 cls_loss= 0.17562 (114485 samples/sec)
2024-03-10 03:39:33.443408 epoch: 19 step: 100 cls_loss= 0.18762 (3174 samples/sec)
saving....
2024-03-10 03:39:43.430754------------------------------------------------------ Precision@1: 64.39% 

[65.38, 65.0, 64.58, 64.42, 64.66, 64.87, 64.54, 64.44, 64.32, 64.16, 64.41, 64.72, 64.29, 63.95, 64.16, 64.07, 64.03, 64.06, 64.25, 64.39]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 03:39:46.327309 epoch: 0 step: 0 cls_loss= 0.31218 (40151 samples/sec)
2024-03-10 03:39:55.788312 epoch: 0 step: 100 cls_loss= 0.47542 (3170 samples/sec)
saving....
2024-03-10 03:40:06.067982------------------------------------------------------ Precision@1: 65.05% 

[65.05]

Epoch: 1
2024-03-10 03:40:06.334277 epoch: 1 step: 0 cls_loss= 0.32340 (113339 samples/sec)
2024-03-10 03:40:15.842608 epoch: 1 step: 100 cls_loss= 0.36714 (3155 samples/sec)
saving....
2024-03-10 03:40:25.881671------------------------------------------------------ Precision@1: 64.91% 

[65.05, 64.91]

Epoch: 2
2024-03-10 03:40:26.158127 epoch: 2 step: 0 cls_loss= 0.29827 (109071 samples/sec)
2024-03-10 03:40:35.702025 epoch: 2 step: 100 cls_loss= 0.37479 (3143 samples/sec)
saving....
2024-03-10 03:40:45.770049------------------------------------------------------ Precision@1: 64.73% 

[65.05, 64.91, 64.73]

Epoch: 3
2024-03-10 03:40:46.037637 epoch: 3 step: 0 cls_loss= 0.33150 (112607 samples/sec)
2024-03-10 03:40:55.480512 epoch: 3 step: 100 cls_loss= 0.40360 (3177 samples/sec)
saving....
2024-03-10 03:41:05.500473------------------------------------------------------ Precision@1: 65.04% 

[65.05, 64.91, 64.73, 65.04]

Epoch: 4
2024-03-10 03:41:05.765467 epoch: 4 step: 0 cls_loss= 0.34208 (113887 samples/sec)
2024-03-10 03:41:15.273517 epoch: 4 step: 100 cls_loss= 0.34359 (3155 samples/sec)
saving....
2024-03-10 03:41:25.354901------------------------------------------------------ Precision@1: 65.27% 

[65.05, 64.91, 64.73, 65.04, 65.27]

Epoch: 5
2024-03-10 03:41:25.620426 epoch: 5 step: 0 cls_loss= 0.30190 (113569 samples/sec)
2024-03-10 03:41:35.085521 epoch: 5 step: 100 cls_loss= 0.33144 (3169 samples/sec)
saving....
2024-03-10 03:41:45.123453------------------------------------------------------ Precision@1: 64.54% 

[65.05, 64.91, 64.73, 65.04, 65.27, 64.54]

Epoch: 6
2024-03-10 03:41:45.378576 epoch: 6 step: 0 cls_loss= 0.30069 (118183 samples/sec)
2024-03-10 03:41:54.859393 epoch: 6 step: 100 cls_loss= 0.30961 (3164 samples/sec)
saving....
2024-03-10 03:42:04.944202------------------------------------------------------ Precision@1: 64.85% 

[65.05, 64.91, 64.73, 65.04, 65.27, 64.54, 64.85]

Epoch: 7
2024-03-10 03:42:05.212728 epoch: 7 step: 0 cls_loss= 0.34730 (112371 samples/sec)
2024-03-10 03:42:14.712262 epoch: 7 step: 100 cls_loss= 0.27266 (3158 samples/sec)
saving....
2024-03-10 03:42:24.752588------------------------------------------------------ Precision@1: 64.63% 

[65.05, 64.91, 64.73, 65.04, 65.27, 64.54, 64.85, 64.63]

Epoch: 8
2024-03-10 03:42:25.020585 epoch: 8 step: 0 cls_loss= 0.30521 (112596 samples/sec)
2024-03-10 03:42:34.528447 epoch: 8 step: 100 cls_loss= 0.24674 (3155 samples/sec)
saving....
2024-03-10 03:42:44.544929------------------------------------------------------ Precision@1: 64.56% 

[65.05, 64.91, 64.73, 65.04, 65.27, 64.54, 64.85, 64.63, 64.56]

Epoch: 9
2024-03-10 03:42:44.824819 epoch: 9 step: 0 cls_loss= 0.32765 (107706 samples/sec)
2024-03-10 03:42:54.301121 epoch: 9 step: 100 cls_loss= 0.34405 (3165 samples/sec)
saving....
2024-03-10 03:43:04.323779------------------------------------------------------ Precision@1: 64.45% 

[65.05, 64.91, 64.73, 65.04, 65.27, 64.54, 64.85, 64.63, 64.56, 64.45]

Epoch: 10
2024-03-10 03:43:04.582674 epoch: 10 step: 0 cls_loss= 0.32101 (116576 samples/sec)
2024-03-10 03:43:14.106171 epoch: 10 step: 100 cls_loss= 0.26305 (3150 samples/sec)
saving....
2024-03-10 03:43:24.149473------------------------------------------------------ Precision@1: 64.81% 

[65.05, 64.91, 64.73, 65.04, 65.27, 64.54, 64.85, 64.63, 64.56, 64.45, 64.81]

Epoch: 11
2024-03-10 03:43:24.422540 epoch: 11 step: 0 cls_loss= 0.19821 (110501 samples/sec)
2024-03-10 03:43:33.913225 epoch: 11 step: 100 cls_loss= 0.32334 (3161 samples/sec)
saving....
2024-03-10 03:43:43.965946------------------------------------------------------ Precision@1: 64.45% 

[65.05, 64.91, 64.73, 65.04, 65.27, 64.54, 64.85, 64.63, 64.56, 64.45, 64.81, 64.45]

Epoch: 12
2024-03-10 03:43:44.238785 epoch: 12 step: 0 cls_loss= 0.23050 (110441 samples/sec)
2024-03-10 03:43:53.757284 epoch: 12 step: 100 cls_loss= 0.28474 (3151 samples/sec)
saving....
2024-03-10 03:44:03.803135------------------------------------------------------ Precision@1: 64.28% 

[65.05, 64.91, 64.73, 65.04, 65.27, 64.54, 64.85, 64.63, 64.56, 64.45, 64.81, 64.45, 64.28]

Epoch: 13
2024-03-10 03:44:04.081044 epoch: 13 step: 0 cls_loss= 0.21683 (108567 samples/sec)
2024-03-10 03:44:13.594179 epoch: 13 step: 100 cls_loss= 0.29836 (3153 samples/sec)
saving....
2024-03-10 03:44:23.610344------------------------------------------------------ Precision@1: 64.48% 

[65.05, 64.91, 64.73, 65.04, 65.27, 64.54, 64.85, 64.63, 64.56, 64.45, 64.81, 64.45, 64.28, 64.48]

Epoch: 14
2024-03-10 03:44:23.886556 epoch: 14 step: 0 cls_loss= 0.24613 (109208 samples/sec)
2024-03-10 03:44:33.379900 epoch: 14 step: 100 cls_loss= 0.26672 (3160 samples/sec)
saving....
2024-03-10 03:44:43.437432------------------------------------------------------ Precision@1: 64.52% 

[65.05, 64.91, 64.73, 65.04, 65.27, 64.54, 64.85, 64.63, 64.56, 64.45, 64.81, 64.45, 64.28, 64.48, 64.52]

Epoch: 15
2024-03-10 03:44:43.711950 epoch: 15 step: 0 cls_loss= 0.27749 (109943 samples/sec)
2024-03-10 03:44:53.217113 epoch: 15 step: 100 cls_loss= 0.25922 (3156 samples/sec)
saving....
2024-03-10 03:45:03.252382------------------------------------------------------ Precision@1: 64.79% 

[65.05, 64.91, 64.73, 65.04, 65.27, 64.54, 64.85, 64.63, 64.56, 64.45, 64.81, 64.45, 64.28, 64.48, 64.52, 64.79]

Epoch: 16
2024-03-10 03:45:03.513471 epoch: 16 step: 0 cls_loss= 0.21823 (115589 samples/sec)
2024-03-10 03:45:13.025494 epoch: 16 step: 100 cls_loss= 0.25347 (3154 samples/sec)
saving....
2024-03-10 03:45:23.082767------------------------------------------------------ Precision@1: 64.13% 

[65.05, 64.91, 64.73, 65.04, 65.27, 64.54, 64.85, 64.63, 64.56, 64.45, 64.81, 64.45, 64.28, 64.48, 64.52, 64.79, 64.13]

Epoch: 17
2024-03-10 03:45:23.338173 epoch: 17 step: 0 cls_loss= 0.20797 (118181 samples/sec)
2024-03-10 03:45:32.793421 epoch: 17 step: 100 cls_loss= 0.22549 (3173 samples/sec)
saving....
2024-03-10 03:45:42.868755------------------------------------------------------ Precision@1: 64.83% 

[65.05, 64.91, 64.73, 65.04, 65.27, 64.54, 64.85, 64.63, 64.56, 64.45, 64.81, 64.45, 64.28, 64.48, 64.52, 64.79, 64.13, 64.83]

Epoch: 18
2024-03-10 03:45:43.130638 epoch: 18 step: 0 cls_loss= 0.21885 (115245 samples/sec)
2024-03-10 03:45:52.620234 epoch: 18 step: 100 cls_loss= 0.27022 (3161 samples/sec)
saving....
2024-03-10 03:46:02.667165------------------------------------------------------ Precision@1: 64.32% 

[65.05, 64.91, 64.73, 65.04, 65.27, 64.54, 64.85, 64.63, 64.56, 64.45, 64.81, 64.45, 64.28, 64.48, 64.52, 64.79, 64.13, 64.83, 64.32]

Epoch: 19
2024-03-10 03:46:02.933427 epoch: 19 step: 0 cls_loss= 0.20488 (113333 samples/sec)
2024-03-10 03:46:12.452716 epoch: 19 step: 100 cls_loss= 0.21711 (3151 samples/sec)
saving....
2024-03-10 03:46:22.437814------------------------------------------------------ Precision@1: 64.55% 

[65.05, 64.91, 64.73, 65.04, 65.27, 64.54, 64.85, 64.63, 64.56, 64.45, 64.81, 64.45, 64.28, 64.48, 64.52, 64.79, 64.13, 64.83, 64.32, 64.55]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 03:46:25.335565 epoch: 0 step: 0 cls_loss= 0.28303 (40412 samples/sec)
2024-03-10 03:46:34.822945 epoch: 0 step: 100 cls_loss= 0.41853 (3162 samples/sec)
saving....
2024-03-10 03:46:45.090984------------------------------------------------------ Precision@1: 64.64% 

[64.64]

Epoch: 1
2024-03-10 03:46:45.354724 epoch: 1 step: 0 cls_loss= 0.32184 (114416 samples/sec)
2024-03-10 03:46:54.863275 epoch: 1 step: 100 cls_loss= 0.44720 (3155 samples/sec)
saving....
2024-03-10 03:47:04.963162------------------------------------------------------ Precision@1: 64.77% 

[64.64, 64.77]

Epoch: 2
2024-03-10 03:47:05.233437 epoch: 2 step: 0 cls_loss= 0.30401 (111572 samples/sec)
2024-03-10 03:47:14.721726 epoch: 2 step: 100 cls_loss= 0.32578 (3161 samples/sec)
saving....
2024-03-10 03:47:24.995143------------------------------------------------------ Precision@1: 64.63% 

[64.64, 64.77, 64.63]

Epoch: 3
2024-03-10 03:47:25.261085 epoch: 3 step: 0 cls_loss= 0.38156 (113382 samples/sec)
2024-03-10 03:47:34.789880 epoch: 3 step: 100 cls_loss= 0.40649 (3148 samples/sec)
saving....
2024-03-10 03:47:44.854966------------------------------------------------------ Precision@1: 64.57% 

[64.64, 64.77, 64.63, 64.57]

Epoch: 4
2024-03-10 03:47:45.138078 epoch: 4 step: 0 cls_loss= 0.28518 (106557 samples/sec)
2024-03-10 03:47:54.618009 epoch: 4 step: 100 cls_loss= 0.29369 (3164 samples/sec)
saving....
2024-03-10 03:48:04.636887------------------------------------------------------ Precision@1: 64.77% 

[64.64, 64.77, 64.63, 64.57, 64.77]

Epoch: 5
2024-03-10 03:48:04.893309 epoch: 5 step: 0 cls_loss= 0.31756 (117687 samples/sec)
2024-03-10 03:48:14.419894 epoch: 5 step: 100 cls_loss= 0.30507 (3149 samples/sec)
saving....
2024-03-10 03:48:24.469925------------------------------------------------------ Precision@1: 64.63% 

[64.64, 64.77, 64.63, 64.57, 64.77, 64.63]

Epoch: 6
2024-03-10 03:48:24.746199 epoch: 6 step: 0 cls_loss= 0.24539 (109120 samples/sec)
2024-03-10 03:48:34.277717 epoch: 6 step: 100 cls_loss= 0.32302 (3147 samples/sec)
saving....
2024-03-10 03:48:44.375225------------------------------------------------------ Precision@1: 64.34% 

[64.64, 64.77, 64.63, 64.57, 64.77, 64.63, 64.34]

Epoch: 7
2024-03-10 03:48:44.649430 epoch: 7 step: 0 cls_loss= 0.30318 (109976 samples/sec)
2024-03-10 03:48:54.179186 epoch: 7 step: 100 cls_loss= 0.27490 (3148 samples/sec)
saving....
2024-03-10 03:49:04.258390------------------------------------------------------ Precision@1: 64.35% 

[64.64, 64.77, 64.63, 64.57, 64.77, 64.63, 64.34, 64.35]

Epoch: 8
2024-03-10 03:49:04.528589 epoch: 8 step: 0 cls_loss= 0.21583 (111651 samples/sec)
2024-03-10 03:49:14.029387 epoch: 8 step: 100 cls_loss= 0.36373 (3157 samples/sec)
saving....
2024-03-10 03:49:24.102687------------------------------------------------------ Precision@1: 64.43% 

[64.64, 64.77, 64.63, 64.57, 64.77, 64.63, 64.34, 64.35, 64.43]

Epoch: 9
2024-03-10 03:49:24.377107 epoch: 9 step: 0 cls_loss= 0.24248 (109935 samples/sec)
2024-03-10 03:49:33.874829 epoch: 9 step: 100 cls_loss= 0.25145 (3158 samples/sec)
saving....
2024-03-10 03:49:43.877572------------------------------------------------------ Precision@1: 64.42% 

[64.64, 64.77, 64.63, 64.57, 64.77, 64.63, 64.34, 64.35, 64.43, 64.42]

Epoch: 10
2024-03-10 03:49:44.127691 epoch: 10 step: 0 cls_loss= 0.21447 (120680 samples/sec)
2024-03-10 03:49:53.640247 epoch: 10 step: 100 cls_loss= 0.31522 (3153 samples/sec)
saving....
2024-03-10 03:50:03.699656------------------------------------------------------ Precision@1: 64.16% 

[64.64, 64.77, 64.63, 64.57, 64.77, 64.63, 64.34, 64.35, 64.43, 64.42, 64.16]

Epoch: 11
2024-03-10 03:50:03.982202 epoch: 11 step: 0 cls_loss= 0.26642 (106756 samples/sec)
2024-03-10 03:50:13.513423 epoch: 11 step: 100 cls_loss= 0.30623 (3147 samples/sec)
saving....
2024-03-10 03:50:23.628246------------------------------------------------------ Precision@1: 64.93% 

[64.64, 64.77, 64.63, 64.57, 64.77, 64.63, 64.34, 64.35, 64.43, 64.42, 64.16, 64.93]

Epoch: 12
2024-03-10 03:50:23.894952 epoch: 12 step: 0 cls_loss= 0.25495 (113073 samples/sec)
2024-03-10 03:50:33.424239 epoch: 12 step: 100 cls_loss= 0.23629 (3148 samples/sec)
saving....
2024-03-10 03:50:43.519505------------------------------------------------------ Precision@1: 64.43% 

[64.64, 64.77, 64.63, 64.57, 64.77, 64.63, 64.34, 64.35, 64.43, 64.42, 64.16, 64.93, 64.43]

Epoch: 13
2024-03-10 03:50:43.796557 epoch: 13 step: 0 cls_loss= 0.23626 (108869 samples/sec)
2024-03-10 03:50:53.358612 epoch: 13 step: 100 cls_loss= 0.35064 (3137 samples/sec)
saving....
2024-03-10 03:51:03.480601------------------------------------------------------ Precision@1: 64.61% 

[64.64, 64.77, 64.63, 64.57, 64.77, 64.63, 64.34, 64.35, 64.43, 64.42, 64.16, 64.93, 64.43, 64.61]

Epoch: 14
2024-03-10 03:51:03.724830 epoch: 14 step: 0 cls_loss= 0.20994 (123626 samples/sec)
2024-03-10 03:51:13.219537 epoch: 14 step: 100 cls_loss= 0.27793 (3159 samples/sec)
saving....
2024-03-10 03:51:23.274713------------------------------------------------------ Precision@1: 64.24% 

[64.64, 64.77, 64.63, 64.57, 64.77, 64.63, 64.34, 64.35, 64.43, 64.42, 64.16, 64.93, 64.43, 64.61, 64.24]

Epoch: 15
2024-03-10 03:51:23.534185 epoch: 15 step: 0 cls_loss= 0.21771 (116329 samples/sec)
2024-03-10 03:51:33.026782 epoch: 15 step: 100 cls_loss= 0.27335 (3160 samples/sec)
saving....
2024-03-10 03:51:43.100908------------------------------------------------------ Precision@1: 64.15% 

[64.64, 64.77, 64.63, 64.57, 64.77, 64.63, 64.34, 64.35, 64.43, 64.42, 64.16, 64.93, 64.43, 64.61, 64.24, 64.15]

Epoch: 16
2024-03-10 03:51:43.360631 epoch: 16 step: 0 cls_loss= 0.24822 (116200 samples/sec)
2024-03-10 03:51:52.834654 epoch: 16 step: 100 cls_loss= 0.21125 (3166 samples/sec)
saving....
2024-03-10 03:52:02.879601------------------------------------------------------ Precision@1: 64.41% 

[64.64, 64.77, 64.63, 64.57, 64.77, 64.63, 64.34, 64.35, 64.43, 64.42, 64.16, 64.93, 64.43, 64.61, 64.24, 64.15, 64.41]

Epoch: 17
2024-03-10 03:52:03.133032 epoch: 17 step: 0 cls_loss= 0.21177 (119167 samples/sec)
2024-03-10 03:52:12.690368 epoch: 17 step: 100 cls_loss= 0.26604 (3139 samples/sec)
saving....
2024-03-10 03:52:22.785622------------------------------------------------------ Precision@1: 64.30% 

[64.64, 64.77, 64.63, 64.57, 64.77, 64.63, 64.34, 64.35, 64.43, 64.42, 64.16, 64.93, 64.43, 64.61, 64.24, 64.15, 64.41, 64.3]

Epoch: 18
2024-03-10 03:52:23.051158 epoch: 18 step: 0 cls_loss= 0.20363 (113661 samples/sec)
2024-03-10 03:52:32.527490 epoch: 18 step: 100 cls_loss= 0.27275 (3165 samples/sec)
saving....
2024-03-10 03:52:42.600746------------------------------------------------------ Precision@1: 63.79% 

[64.64, 64.77, 64.63, 64.57, 64.77, 64.63, 64.34, 64.35, 64.43, 64.42, 64.16, 64.93, 64.43, 64.61, 64.24, 64.15, 64.41, 64.3, 63.79]

Epoch: 19
2024-03-10 03:52:42.869974 epoch: 19 step: 0 cls_loss= 0.20815 (112092 samples/sec)
2024-03-10 03:52:52.371064 epoch: 19 step: 100 cls_loss= 0.24321 (3157 samples/sec)
saving....
2024-03-10 03:53:02.407067------------------------------------------------------ Precision@1: 64.20% 

[64.64, 64.77, 64.63, 64.57, 64.77, 64.63, 64.34, 64.35, 64.43, 64.42, 64.16, 64.93, 64.43, 64.61, 64.24, 64.15, 64.41, 64.3, 63.79, 64.2]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 03:53:05.339106 epoch: 0 step: 0 cls_loss= 0.24377 (38452 samples/sec)
2024-03-10 03:53:14.873622 epoch: 0 step: 100 cls_loss= 0.41710 (3146 samples/sec)
saving....
2024-03-10 03:53:25.224963------------------------------------------------------ Precision@1: 65.05% 

[65.05]

Epoch: 1
2024-03-10 03:53:25.492095 epoch: 1 step: 0 cls_loss= 0.30031 (113014 samples/sec)
2024-03-10 03:53:35.004096 epoch: 1 step: 100 cls_loss= 0.41953 (3154 samples/sec)
saving....
2024-03-10 03:53:45.020543------------------------------------------------------ Precision@1: 64.91% 

[65.05, 64.91]

Epoch: 2
2024-03-10 03:53:45.294573 epoch: 2 step: 0 cls_loss= 0.27631 (110121 samples/sec)
2024-03-10 03:53:54.825489 epoch: 2 step: 100 cls_loss= 0.34743 (3147 samples/sec)
saving....
2024-03-10 03:54:04.895727------------------------------------------------------ Precision@1: 64.80% 

[65.05, 64.91, 64.8]

Epoch: 3
2024-03-10 03:54:05.158890 epoch: 3 step: 0 cls_loss= 0.29573 (114651 samples/sec)
2024-03-10 03:54:14.648450 epoch: 3 step: 100 cls_loss= 0.35506 (3161 samples/sec)
saving....
2024-03-10 03:54:24.662749------------------------------------------------------ Precision@1: 64.80% 

[65.05, 64.91, 64.8, 64.8]

Epoch: 4
2024-03-10 03:54:24.933170 epoch: 4 step: 0 cls_loss= 0.26282 (111595 samples/sec)
2024-03-10 03:54:34.450200 epoch: 4 step: 100 cls_loss= 0.36199 (3152 samples/sec)
saving....
2024-03-10 03:54:44.517093------------------------------------------------------ Precision@1: 64.61% 

[65.05, 64.91, 64.8, 64.8, 64.61]

Epoch: 5
2024-03-10 03:54:44.795363 epoch: 5 step: 0 cls_loss= 0.30869 (108447 samples/sec)
2024-03-10 03:54:54.268893 epoch: 5 step: 100 cls_loss= 0.34583 (3166 samples/sec)
saving....
2024-03-10 03:55:04.309759------------------------------------------------------ Precision@1: 64.70% 

[65.05, 64.91, 64.8, 64.8, 64.61, 64.7]

Epoch: 6
2024-03-10 03:55:04.577740 epoch: 6 step: 0 cls_loss= 0.28720 (112510 samples/sec)
2024-03-10 03:55:14.095512 epoch: 6 step: 100 cls_loss= 0.28757 (3152 samples/sec)
saving....
2024-03-10 03:55:24.163435------------------------------------------------------ Precision@1: 64.48% 

[65.05, 64.91, 64.8, 64.8, 64.61, 64.7, 64.48]

Epoch: 7
2024-03-10 03:55:24.429819 epoch: 7 step: 0 cls_loss= 0.25384 (113272 samples/sec)
2024-03-10 03:55:33.936435 epoch: 7 step: 100 cls_loss= 0.29922 (3155 samples/sec)
saving....
2024-03-10 03:55:43.992578------------------------------------------------------ Precision@1: 64.87% 

[65.05, 64.91, 64.8, 64.8, 64.61, 64.7, 64.48, 64.87]

Epoch: 8
2024-03-10 03:55:44.280517 epoch: 8 step: 0 cls_loss= 0.30199 (104704 samples/sec)
2024-03-10 03:55:53.799192 epoch: 8 step: 100 cls_loss= 0.32249 (3151 samples/sec)
saving....
2024-03-10 03:56:03.846931------------------------------------------------------ Precision@1: 64.74% 

[65.05, 64.91, 64.8, 64.8, 64.61, 64.7, 64.48, 64.87, 64.74]

Epoch: 9
2024-03-10 03:56:04.120946 epoch: 9 step: 0 cls_loss= 0.22496 (110034 samples/sec)
2024-03-10 03:56:13.642923 epoch: 9 step: 100 cls_loss= 0.28139 (3150 samples/sec)
saving....
2024-03-10 03:56:23.725052------------------------------------------------------ Precision@1: 64.34% 

[65.05, 64.91, 64.8, 64.8, 64.61, 64.7, 64.48, 64.87, 64.74, 64.34]

Epoch: 10
2024-03-10 03:56:23.997750 epoch: 10 step: 0 cls_loss= 0.23460 (110639 samples/sec)
2024-03-10 03:56:33.510414 epoch: 10 step: 100 cls_loss= 0.30312 (3153 samples/sec)
saving....
2024-03-10 03:56:43.606414------------------------------------------------------ Precision@1: 64.07% 

[65.05, 64.91, 64.8, 64.8, 64.61, 64.7, 64.48, 64.87, 64.74, 64.34, 64.07]

Epoch: 11
2024-03-10 03:56:43.889174 epoch: 11 step: 0 cls_loss= 0.29354 (106666 samples/sec)
2024-03-10 03:56:53.403597 epoch: 11 step: 100 cls_loss= 0.25054 (3153 samples/sec)
saving....
2024-03-10 03:57:03.456676------------------------------------------------------ Precision@1: 64.56% 

[65.05, 64.91, 64.8, 64.8, 64.61, 64.7, 64.48, 64.87, 64.74, 64.34, 64.07, 64.56]

Epoch: 12
2024-03-10 03:57:03.716075 epoch: 12 step: 0 cls_loss= 0.19417 (116360 samples/sec)
2024-03-10 03:57:13.276322 epoch: 12 step: 100 cls_loss= 0.27290 (3138 samples/sec)
saving....
2024-03-10 03:57:23.364935------------------------------------------------------ Precision@1: 64.47% 

[65.05, 64.91, 64.8, 64.8, 64.61, 64.7, 64.48, 64.87, 64.74, 64.34, 64.07, 64.56, 64.47]

Epoch: 13
2024-03-10 03:57:23.652019 epoch: 13 step: 0 cls_loss= 0.28724 (105037 samples/sec)
2024-03-10 03:57:33.175153 epoch: 13 step: 100 cls_loss= 0.23847 (3150 samples/sec)
saving....
2024-03-10 03:57:43.201061------------------------------------------------------ Precision@1: 64.35% 

[65.05, 64.91, 64.8, 64.8, 64.61, 64.7, 64.48, 64.87, 64.74, 64.34, 64.07, 64.56, 64.47, 64.35]

Epoch: 14
2024-03-10 03:57:43.475474 epoch: 14 step: 0 cls_loss= 0.22650 (109817 samples/sec)
2024-03-10 03:57:53.069873 epoch: 14 step: 100 cls_loss= 0.22028 (3126 samples/sec)
saving....
2024-03-10 03:58:03.107554------------------------------------------------------ Precision@1: 64.08% 

[65.05, 64.91, 64.8, 64.8, 64.61, 64.7, 64.48, 64.87, 64.74, 64.34, 64.07, 64.56, 64.47, 64.35, 64.08]

Epoch: 15
2024-03-10 03:58:03.385513 epoch: 15 step: 0 cls_loss= 0.24231 (108446 samples/sec)
2024-03-10 03:58:12.861703 epoch: 15 step: 100 cls_loss= 0.25120 (3166 samples/sec)
saving....
2024-03-10 03:58:22.870343------------------------------------------------------ Precision@1: 64.09% 

[65.05, 64.91, 64.8, 64.8, 64.61, 64.7, 64.48, 64.87, 64.74, 64.34, 64.07, 64.56, 64.47, 64.35, 64.08, 64.09]

Epoch: 16
2024-03-10 03:58:23.146911 epoch: 16 step: 0 cls_loss= 0.19700 (109041 samples/sec)
2024-03-10 03:58:32.685168 epoch: 16 step: 100 cls_loss= 0.25406 (3145 samples/sec)
saving....
2024-03-10 03:58:42.721772------------------------------------------------------ Precision@1: 64.32% 

[65.05, 64.91, 64.8, 64.8, 64.61, 64.7, 64.48, 64.87, 64.74, 64.34, 64.07, 64.56, 64.47, 64.35, 64.08, 64.09, 64.32]

Epoch: 17
2024-03-10 03:58:42.985280 epoch: 17 step: 0 cls_loss= 0.14360 (114523 samples/sec)
2024-03-10 03:58:52.448476 epoch: 17 step: 100 cls_loss= 0.25891 (3170 samples/sec)
saving....
2024-03-10 03:59:02.569382------------------------------------------------------ Precision@1: 64.60% 

[65.05, 64.91, 64.8, 64.8, 64.61, 64.7, 64.48, 64.87, 64.74, 64.34, 64.07, 64.56, 64.47, 64.35, 64.08, 64.09, 64.32, 64.6]

Epoch: 18
2024-03-10 03:59:02.839907 epoch: 18 step: 0 cls_loss= 0.17911 (111534 samples/sec)
2024-03-10 03:59:12.334576 epoch: 18 step: 100 cls_loss= 0.28220 (3159 samples/sec)
saving....
2024-03-10 03:59:22.360497------------------------------------------------------ Precision@1: 64.19% 

[65.05, 64.91, 64.8, 64.8, 64.61, 64.7, 64.48, 64.87, 64.74, 64.34, 64.07, 64.56, 64.47, 64.35, 64.08, 64.09, 64.32, 64.6, 64.19]

Epoch: 19
2024-03-10 03:59:22.624168 epoch: 19 step: 0 cls_loss= 0.20709 (114385 samples/sec)
2024-03-10 03:59:32.112321 epoch: 19 step: 100 cls_loss= 0.21185 (3162 samples/sec)
saving....
2024-03-10 03:59:42.141330------------------------------------------------------ Precision@1: 64.60% 

[65.05, 64.91, 64.8, 64.8, 64.61, 64.7, 64.48, 64.87, 64.74, 64.34, 64.07, 64.56, 64.47, 64.35, 64.08, 64.09, 64.32, 64.6, 64.19, 64.6]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 03:59:45.054402 epoch: 0 step: 0 cls_loss= 0.28521 (38756 samples/sec)
2024-03-10 03:59:54.427142 epoch: 0 step: 100 cls_loss= 0.40501 (3200 samples/sec)
saving....
2024-03-10 04:00:04.712466------------------------------------------------------ Precision@1: 65.77% 

[65.77]

Epoch: 1
2024-03-10 04:00:04.976915 epoch: 1 step: 0 cls_loss= 0.35406 (114093 samples/sec)
2024-03-10 04:00:14.369634 epoch: 1 step: 100 cls_loss= 0.28126 (3194 samples/sec)
saving....
2024-03-10 04:00:24.337361------------------------------------------------------ Precision@1: 65.76% 

[65.77, 65.76]

Epoch: 2
2024-03-10 04:00:24.624818 epoch: 2 step: 0 cls_loss= 0.36153 (104858 samples/sec)
2024-03-10 04:00:34.017974 epoch: 2 step: 100 cls_loss= 0.36270 (3194 samples/sec)
saving....
2024-03-10 04:00:44.014572------------------------------------------------------ Precision@1: 65.68% 

[65.77, 65.76, 65.68]

Epoch: 3
2024-03-10 04:00:44.277132 epoch: 3 step: 0 cls_loss= 0.30494 (114943 samples/sec)
2024-03-10 04:00:53.670688 epoch: 3 step: 100 cls_loss= 0.33324 (3194 samples/sec)
saving....
2024-03-10 04:01:03.716027------------------------------------------------------ Precision@1: 65.80% 

[65.77, 65.76, 65.68, 65.8]

Epoch: 4
2024-03-10 04:01:03.979763 epoch: 4 step: 0 cls_loss= 0.34083 (114410 samples/sec)
2024-03-10 04:01:13.354059 epoch: 4 step: 100 cls_loss= 0.28505 (3200 samples/sec)
saving....
2024-03-10 04:01:23.320069------------------------------------------------------ Precision@1: 65.76% 

[65.77, 65.76, 65.68, 65.8, 65.76]

Epoch: 5
2024-03-10 04:01:23.585756 epoch: 5 step: 0 cls_loss= 0.30313 (113574 samples/sec)
2024-03-10 04:01:32.971865 epoch: 5 step: 100 cls_loss= 0.29313 (3196 samples/sec)
saving....
2024-03-10 04:01:42.939296------------------------------------------------------ Precision@1: 65.81% 

[65.77, 65.76, 65.68, 65.8, 65.76, 65.81]

Epoch: 6
2024-03-10 04:01:43.210998 epoch: 6 step: 0 cls_loss= 0.36017 (111040 samples/sec)
2024-03-10 04:01:52.618971 epoch: 6 step: 100 cls_loss= 0.32943 (3189 samples/sec)
saving....
2024-03-10 04:02:02.605328------------------------------------------------------ Precision@1: 65.80% 

[65.77, 65.76, 65.68, 65.8, 65.76, 65.81, 65.8]

Epoch: 7
2024-03-10 04:02:02.871342 epoch: 7 step: 0 cls_loss= 0.30747 (113392 samples/sec)
2024-03-10 04:02:12.235270 epoch: 7 step: 100 cls_loss= 0.30090 (3204 samples/sec)
saving....
2024-03-10 04:02:22.281612------------------------------------------------------ Precision@1: 65.61% 

[65.77, 65.76, 65.68, 65.8, 65.76, 65.81, 65.8, 65.61]

Epoch: 8
2024-03-10 04:02:22.538663 epoch: 8 step: 0 cls_loss= 0.26868 (117448 samples/sec)
2024-03-10 04:02:31.904683 epoch: 8 step: 100 cls_loss= 0.27983 (3203 samples/sec)
saving....
2024-03-10 04:02:41.833535------------------------------------------------------ Precision@1: 65.80% 

[65.77, 65.76, 65.68, 65.8, 65.76, 65.81, 65.8, 65.61, 65.8]

Epoch: 9
2024-03-10 04:02:42.112672 epoch: 9 step: 0 cls_loss= 0.33105 (108047 samples/sec)
2024-03-10 04:02:51.478367 epoch: 9 step: 100 cls_loss= 0.32219 (3203 samples/sec)
saving....
2024-03-10 04:03:01.461393------------------------------------------------------ Precision@1: 65.73% 

[65.77, 65.76, 65.68, 65.8, 65.76, 65.81, 65.8, 65.61, 65.8, 65.73]

Epoch: 10
2024-03-10 04:03:01.726586 epoch: 10 step: 0 cls_loss= 0.35207 (113785 samples/sec)
2024-03-10 04:03:11.085981 epoch: 10 step: 100 cls_loss= 0.27405 (3205 samples/sec)
saving....
2024-03-10 04:03:21.008071------------------------------------------------------ Precision@1: 65.79% 

[65.77, 65.76, 65.68, 65.8, 65.76, 65.81, 65.8, 65.61, 65.8, 65.73, 65.79]

Epoch: 11
2024-03-10 04:03:21.257716 epoch: 11 step: 0 cls_loss= 0.34180 (120882 samples/sec)
2024-03-10 04:03:30.605932 epoch: 11 step: 100 cls_loss= 0.30625 (3209 samples/sec)
saving....
2024-03-10 04:03:40.539797------------------------------------------------------ Precision@1: 65.76% 

[65.77, 65.76, 65.68, 65.8, 65.76, 65.81, 65.8, 65.61, 65.8, 65.73, 65.79, 65.76]

Epoch: 12
2024-03-10 04:03:40.802211 epoch: 12 step: 0 cls_loss= 0.33737 (115032 samples/sec)
2024-03-10 04:03:50.186621 epoch: 12 step: 100 cls_loss= 0.38734 (3197 samples/sec)
saving....
2024-03-10 04:04:00.109818------------------------------------------------------ Precision@1: 65.70% 

[65.77, 65.76, 65.68, 65.8, 65.76, 65.81, 65.8, 65.61, 65.8, 65.73, 65.79, 65.76, 65.7]

Epoch: 13
2024-03-10 04:04:00.370763 epoch: 13 step: 0 cls_loss= 0.35050 (115690 samples/sec)
2024-03-10 04:04:09.749941 epoch: 13 step: 100 cls_loss= 0.28939 (3199 samples/sec)
saving....
2024-03-10 04:04:19.698351------------------------------------------------------ Precision@1: 65.77% 

[65.77, 65.76, 65.68, 65.8, 65.76, 65.81, 65.8, 65.61, 65.8, 65.73, 65.79, 65.76, 65.7, 65.77]

Epoch: 14
2024-03-10 04:04:19.958466 epoch: 14 step: 0 cls_loss= 0.27840 (116019 samples/sec)
2024-03-10 04:04:29.338167 epoch: 14 step: 100 cls_loss= 0.33388 (3199 samples/sec)
saving....
2024-03-10 04:04:39.303788------------------------------------------------------ Precision@1: 65.82% 

[65.77, 65.76, 65.68, 65.8, 65.76, 65.81, 65.8, 65.61, 65.8, 65.73, 65.79, 65.76, 65.7, 65.77, 65.82]

Epoch: 15
2024-03-10 04:04:39.570850 epoch: 15 step: 0 cls_loss= 0.34169 (112993 samples/sec)
2024-03-10 04:04:48.972593 epoch: 15 step: 100 cls_loss= 0.35382 (3191 samples/sec)
saving....
2024-03-10 04:04:58.942116------------------------------------------------------ Precision@1: 65.71% 

[65.77, 65.76, 65.68, 65.8, 65.76, 65.81, 65.8, 65.61, 65.8, 65.73, 65.79, 65.76, 65.7, 65.77, 65.82, 65.71]

Epoch: 16
2024-03-10 04:04:59.213753 epoch: 16 step: 0 cls_loss= 0.33767 (110984 samples/sec)
2024-03-10 04:05:08.626778 epoch: 16 step: 100 cls_loss= 0.35292 (3187 samples/sec)
saving....
2024-03-10 04:05:18.634391------------------------------------------------------ Precision@1: 65.61% 

[65.77, 65.76, 65.68, 65.8, 65.76, 65.81, 65.8, 65.61, 65.8, 65.73, 65.79, 65.76, 65.7, 65.77, 65.82, 65.71, 65.61]

Epoch: 17
2024-03-10 04:05:18.926714 epoch: 17 step: 0 cls_loss= 0.37432 (103022 samples/sec)
2024-03-10 04:05:28.374503 epoch: 17 step: 100 cls_loss= 0.34180 (3175 samples/sec)
saving....
2024-03-10 04:05:38.389189------------------------------------------------------ Precision@1: 65.76% 

[65.77, 65.76, 65.68, 65.8, 65.76, 65.81, 65.8, 65.61, 65.8, 65.73, 65.79, 65.76, 65.7, 65.77, 65.82, 65.71, 65.61, 65.76]

Epoch: 18
2024-03-10 04:05:38.654230 epoch: 18 step: 0 cls_loss= 0.32475 (113825 samples/sec)
2024-03-10 04:05:48.008112 epoch: 18 step: 100 cls_loss= 0.34233 (3207 samples/sec)
saving....
2024-03-10 04:05:57.988477------------------------------------------------------ Precision@1: 65.73% 

[65.77, 65.76, 65.68, 65.8, 65.76, 65.81, 65.8, 65.61, 65.8, 65.73, 65.79, 65.76, 65.7, 65.77, 65.82, 65.71, 65.61, 65.76, 65.73]

Epoch: 19
2024-03-10 04:05:58.278201 epoch: 19 step: 0 cls_loss= 0.31639 (104111 samples/sec)
2024-03-10 04:06:07.736697 epoch: 19 step: 100 cls_loss= 0.31190 (3172 samples/sec)
saving....
2024-03-10 04:06:17.728100------------------------------------------------------ Precision@1: 65.62% 

[65.77, 65.76, 65.68, 65.8, 65.76, 65.81, 65.8, 65.61, 65.8, 65.73, 65.79, 65.76, 65.7, 65.77, 65.82, 65.71, 65.61, 65.76, 65.73, 65.62]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 04:06:20.596313 epoch: 0 step: 0 cls_loss= 0.31049 (40372 samples/sec)
2024-03-10 04:06:29.958898 epoch: 0 step: 100 cls_loss= 0.30541 (3204 samples/sec)
saving....
2024-03-10 04:06:40.109875------------------------------------------------------ Precision@1: 65.78% 

[65.78]

Epoch: 1
2024-03-10 04:06:40.382790 epoch: 1 step: 0 cls_loss= 0.25473 (110571 samples/sec)
2024-03-10 04:06:49.727359 epoch: 1 step: 100 cls_loss= 0.32821 (3211 samples/sec)
saving....
2024-03-10 04:06:59.642687------------------------------------------------------ Precision@1: 65.76% 

[65.78, 65.76]

Epoch: 2
2024-03-10 04:06:59.902422 epoch: 2 step: 0 cls_loss= 0.39519 (116199 samples/sec)
2024-03-10 04:07:09.285276 epoch: 2 step: 100 cls_loss= 0.30497 (3197 samples/sec)
saving....
2024-03-10 04:07:19.199537------------------------------------------------------ Precision@1: 65.71% 

[65.78, 65.76, 65.71]

Epoch: 3
2024-03-10 04:07:19.468297 epoch: 3 step: 0 cls_loss= 0.28992 (112238 samples/sec)
2024-03-10 04:07:28.836257 epoch: 3 step: 100 cls_loss= 0.29183 (3203 samples/sec)
saving....
2024-03-10 04:07:38.713624------------------------------------------------------ Precision@1: 65.77% 

[65.78, 65.76, 65.71, 65.77]

Epoch: 4
2024-03-10 04:07:38.975590 epoch: 4 step: 0 cls_loss= 0.33434 (115202 samples/sec)
2024-03-10 04:07:48.299704 epoch: 4 step: 100 cls_loss= 0.35566 (3218 samples/sec)
saving....
2024-03-10 04:07:58.158500------------------------------------------------------ Precision@1: 65.78% 

[65.78, 65.76, 65.71, 65.77, 65.78]

Epoch: 5
2024-03-10 04:07:58.402004 epoch: 5 step: 0 cls_loss= 0.32016 (123963 samples/sec)
2024-03-10 04:08:07.736057 epoch: 5 step: 100 cls_loss= 0.34463 (3214 samples/sec)
saving....
2024-03-10 04:08:17.596701------------------------------------------------------ Precision@1: 65.87% 

[65.78, 65.76, 65.71, 65.77, 65.78, 65.87]

Epoch: 6
2024-03-10 04:08:17.870150 epoch: 6 step: 0 cls_loss= 0.34850 (110335 samples/sec)
2024-03-10 04:08:27.189884 epoch: 6 step: 100 cls_loss= 0.26164 (3219 samples/sec)
saving....
2024-03-10 04:08:37.057518------------------------------------------------------ Precision@1: 65.78% 

[65.78, 65.76, 65.71, 65.77, 65.78, 65.87, 65.78]

Epoch: 7
2024-03-10 04:08:37.307397 epoch: 7 step: 0 cls_loss= 0.32315 (120838 samples/sec)
2024-03-10 04:08:46.692590 epoch: 7 step: 100 cls_loss= 0.24661 (3197 samples/sec)
saving....
2024-03-10 04:08:56.621232------------------------------------------------------ Precision@1: 65.80% 

[65.78, 65.76, 65.71, 65.77, 65.78, 65.87, 65.78, 65.8]

Epoch: 8
2024-03-10 04:08:56.878759 epoch: 8 step: 0 cls_loss= 0.30558 (117295 samples/sec)
2024-03-10 04:09:06.221436 epoch: 8 step: 100 cls_loss= 0.25266 (3211 samples/sec)
saving....
2024-03-10 04:09:16.116788------------------------------------------------------ Precision@1: 65.83% 

[65.78, 65.76, 65.71, 65.77, 65.78, 65.87, 65.78, 65.8, 65.83]

Epoch: 9
2024-03-10 04:09:16.385971 epoch: 9 step: 0 cls_loss= 0.36507 (112111 samples/sec)
2024-03-10 04:09:25.750426 epoch: 9 step: 100 cls_loss= 0.37373 (3204 samples/sec)
saving....
2024-03-10 04:09:35.651342------------------------------------------------------ Precision@1: 65.97% 

[65.78, 65.76, 65.71, 65.77, 65.78, 65.87, 65.78, 65.8, 65.83, 65.97]

Epoch: 10
2024-03-10 04:09:35.911862 epoch: 10 step: 0 cls_loss= 0.32709 (115869 samples/sec)
2024-03-10 04:09:45.287633 epoch: 10 step: 100 cls_loss= 0.34204 (3200 samples/sec)
saving....
2024-03-10 04:09:55.218248------------------------------------------------------ Precision@1: 65.69% 

[65.78, 65.76, 65.71, 65.77, 65.78, 65.87, 65.78, 65.8, 65.83, 65.97, 65.69]

Epoch: 11
2024-03-10 04:09:55.484740 epoch: 11 step: 0 cls_loss= 0.31043 (113202 samples/sec)
2024-03-10 04:10:04.892701 epoch: 11 step: 100 cls_loss= 0.30686 (3189 samples/sec)
saving....
2024-03-10 04:10:14.843193------------------------------------------------------ Precision@1: 65.93% 

[65.78, 65.76, 65.71, 65.77, 65.78, 65.87, 65.78, 65.8, 65.83, 65.97, 65.69, 65.93]

Epoch: 12
2024-03-10 04:10:15.121354 epoch: 12 step: 0 cls_loss= 0.32596 (108505 samples/sec)
2024-03-10 04:10:24.541760 epoch: 12 step: 100 cls_loss= 0.39286 (3185 samples/sec)
saving....
2024-03-10 04:10:34.469015------------------------------------------------------ Precision@1: 65.90% 

[65.78, 65.76, 65.71, 65.77, 65.78, 65.87, 65.78, 65.8, 65.83, 65.97, 65.69, 65.93, 65.9]

Epoch: 13
2024-03-10 04:10:34.742756 epoch: 13 step: 0 cls_loss= 0.27914 (110111 samples/sec)
2024-03-10 04:10:44.087490 epoch: 13 step: 100 cls_loss= 0.37089 (3211 samples/sec)
saving....
2024-03-10 04:10:53.971601------------------------------------------------------ Precision@1: 65.71% 

[65.78, 65.76, 65.71, 65.77, 65.78, 65.87, 65.78, 65.8, 65.83, 65.97, 65.69, 65.93, 65.9, 65.71]

Epoch: 14
2024-03-10 04:10:54.238651 epoch: 14 step: 0 cls_loss= 0.27025 (112894 samples/sec)
2024-03-10 04:11:03.613237 epoch: 14 step: 100 cls_loss= 0.39489 (3200 samples/sec)
saving....
2024-03-10 04:11:13.581520------------------------------------------------------ Precision@1: 65.81% 

[65.78, 65.76, 65.71, 65.77, 65.78, 65.87, 65.78, 65.8, 65.83, 65.97, 65.69, 65.93, 65.9, 65.71, 65.81]

Epoch: 15
2024-03-10 04:11:13.845775 epoch: 15 step: 0 cls_loss= 0.28603 (114185 samples/sec)
2024-03-10 04:11:23.221494 epoch: 15 step: 100 cls_loss= 0.27374 (3200 samples/sec)
saving....
2024-03-10 04:11:33.165906------------------------------------------------------ Precision@1: 65.66% 

[65.78, 65.76, 65.71, 65.77, 65.78, 65.87, 65.78, 65.8, 65.83, 65.97, 65.69, 65.93, 65.9, 65.71, 65.81, 65.66]

Epoch: 16
2024-03-10 04:11:33.440011 epoch: 16 step: 0 cls_loss= 0.27977 (110088 samples/sec)
2024-03-10 04:11:42.827362 epoch: 16 step: 100 cls_loss= 0.34809 (3196 samples/sec)
saving....
2024-03-10 04:11:52.717266------------------------------------------------------ Precision@1: 65.77% 

[65.78, 65.76, 65.71, 65.77, 65.78, 65.87, 65.78, 65.8, 65.83, 65.97, 65.69, 65.93, 65.9, 65.71, 65.81, 65.66, 65.77]

Epoch: 17
2024-03-10 04:11:52.981402 epoch: 17 step: 0 cls_loss= 0.31951 (114096 samples/sec)
2024-03-10 04:12:02.324187 epoch: 17 step: 100 cls_loss= 0.26884 (3211 samples/sec)
saving....
2024-03-10 04:12:12.320199------------------------------------------------------ Precision@1: 65.72% 

[65.78, 65.76, 65.71, 65.77, 65.78, 65.87, 65.78, 65.8, 65.83, 65.97, 65.69, 65.93, 65.9, 65.71, 65.81, 65.66, 65.77, 65.72]

Epoch: 18
2024-03-10 04:12:12.577922 epoch: 18 step: 0 cls_loss= 0.31902 (117140 samples/sec)
2024-03-10 04:12:21.970746 epoch: 18 step: 100 cls_loss= 0.32482 (3194 samples/sec)
saving....
2024-03-10 04:12:31.899678------------------------------------------------------ Precision@1: 65.81% 

[65.78, 65.76, 65.71, 65.77, 65.78, 65.87, 65.78, 65.8, 65.83, 65.97, 65.69, 65.93, 65.9, 65.71, 65.81, 65.66, 65.77, 65.72, 65.81]

Epoch: 19
2024-03-10 04:12:32.184447 epoch: 19 step: 0 cls_loss= 0.27250 (105931 samples/sec)
2024-03-10 04:12:41.562178 epoch: 19 step: 100 cls_loss= 0.30006 (3199 samples/sec)
saving....
2024-03-10 04:12:51.570144------------------------------------------------------ Precision@1: 65.84% 

[65.78, 65.76, 65.71, 65.77, 65.78, 65.87, 65.78, 65.8, 65.83, 65.97, 65.69, 65.93, 65.9, 65.71, 65.81, 65.66, 65.77, 65.72, 65.81, 65.84]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 04:12:54.446222 epoch: 0 step: 0 cls_loss= 0.36444 (40653 samples/sec)
2024-03-10 04:13:03.812658 epoch: 0 step: 100 cls_loss= 0.31445 (3202 samples/sec)
saving....
2024-03-10 04:13:14.009894------------------------------------------------------ Precision@1: 65.65% 

[65.65]

Epoch: 1
2024-03-10 04:13:14.265996 epoch: 1 step: 0 cls_loss= 0.28983 (117830 samples/sec)
2024-03-10 04:13:23.628618 epoch: 1 step: 100 cls_loss= 0.36980 (3204 samples/sec)
saving....
2024-03-10 04:13:33.541637------------------------------------------------------ Precision@1: 65.64% 

[65.65, 65.64]

Epoch: 2
2024-03-10 04:13:33.816794 epoch: 2 step: 0 cls_loss= 0.33747 (109640 samples/sec)
2024-03-10 04:13:43.237374 epoch: 2 step: 100 cls_loss= 0.34358 (3185 samples/sec)
saving....
2024-03-10 04:13:53.197054------------------------------------------------------ Precision@1: 65.90% 

[65.65, 65.64, 65.9]

Epoch: 3
2024-03-10 04:13:53.456917 epoch: 3 step: 0 cls_loss= 0.35888 (116116 samples/sec)
2024-03-10 04:14:02.829488 epoch: 3 step: 100 cls_loss= 0.27531 (3201 samples/sec)
saving....
2024-03-10 04:14:12.779964------------------------------------------------------ Precision@1: 65.69% 

[65.65, 65.64, 65.9, 65.69]

Epoch: 4
2024-03-10 04:14:13.036598 epoch: 4 step: 0 cls_loss= 0.28115 (117614 samples/sec)
2024-03-10 04:14:22.392517 epoch: 4 step: 100 cls_loss= 0.40779 (3207 samples/sec)
saving....
2024-03-10 04:14:32.280879------------------------------------------------------ Precision@1: 65.81% 

[65.65, 65.64, 65.9, 65.69, 65.81]

Epoch: 5
2024-03-10 04:14:32.565945 epoch: 5 step: 0 cls_loss= 0.34674 (105690 samples/sec)
2024-03-10 04:14:42.035155 epoch: 5 step: 100 cls_loss= 0.32658 (3168 samples/sec)
saving....
2024-03-10 04:14:52.123857------------------------------------------------------ Precision@1: 65.75% 

[65.65, 65.64, 65.9, 65.69, 65.81, 65.75]

Epoch: 6
2024-03-10 04:14:52.377899 epoch: 6 step: 0 cls_loss= 0.35420 (118819 samples/sec)
2024-03-10 04:15:01.731673 epoch: 6 step: 100 cls_loss= 0.28841 (3207 samples/sec)
saving....
2024-03-10 04:15:11.756614------------------------------------------------------ Precision@1: 65.60% 

[65.65, 65.64, 65.9, 65.69, 65.81, 65.75, 65.6]

Epoch: 7
2024-03-10 04:15:12.013943 epoch: 7 step: 0 cls_loss= 0.34588 (117294 samples/sec)
2024-03-10 04:15:21.393688 epoch: 7 step: 100 cls_loss= 0.35294 (3199 samples/sec)
saving....
2024-03-10 04:15:31.337618------------------------------------------------------ Precision@1: 65.79% 

[65.65, 65.64, 65.9, 65.69, 65.81, 65.75, 65.6, 65.79]

Epoch: 8
2024-03-10 04:15:31.599558 epoch: 8 step: 0 cls_loss= 0.32292 (115224 samples/sec)
2024-03-10 04:15:41.001889 epoch: 8 step: 100 cls_loss= 0.29219 (3191 samples/sec)
saving....
2024-03-10 04:15:50.927600------------------------------------------------------ Precision@1: 65.72% 

[65.65, 65.64, 65.9, 65.69, 65.81, 65.75, 65.6, 65.79, 65.72]

Epoch: 9
2024-03-10 04:15:51.203836 epoch: 9 step: 0 cls_loss= 0.29784 (109040 samples/sec)
2024-03-10 04:16:00.574084 epoch: 9 step: 100 cls_loss= 0.39476 (3202 samples/sec)
saving....
2024-03-10 04:16:10.504943------------------------------------------------------ Precision@1: 65.78% 

[65.65, 65.64, 65.9, 65.69, 65.81, 65.75, 65.6, 65.79, 65.72, 65.78]

Epoch: 10
2024-03-10 04:16:10.764104 epoch: 10 step: 0 cls_loss= 0.33010 (116378 samples/sec)
2024-03-10 04:16:20.144340 epoch: 10 step: 100 cls_loss= 0.36048 (3198 samples/sec)
saving....
2024-03-10 04:16:30.144242------------------------------------------------------ Precision@1: 65.63% 

[65.65, 65.64, 65.9, 65.69, 65.81, 65.75, 65.6, 65.79, 65.72, 65.78, 65.63]

Epoch: 11
2024-03-10 04:16:30.414130 epoch: 11 step: 0 cls_loss= 0.33542 (111767 samples/sec)
2024-03-10 04:16:39.795501 epoch: 11 step: 100 cls_loss= 0.33603 (3198 samples/sec)
saving....
2024-03-10 04:16:49.692824------------------------------------------------------ Precision@1: 65.67% 

[65.65, 65.64, 65.9, 65.69, 65.81, 65.75, 65.6, 65.79, 65.72, 65.78, 65.63, 65.67]

Epoch: 12
2024-03-10 04:16:49.954662 epoch: 12 step: 0 cls_loss= 0.29312 (115225 samples/sec)
2024-03-10 04:16:59.375592 epoch: 12 step: 100 cls_loss= 0.35525 (3185 samples/sec)
saving....
2024-03-10 04:17:09.353704------------------------------------------------------ Precision@1: 65.56% 

[65.65, 65.64, 65.9, 65.69, 65.81, 65.75, 65.6, 65.79, 65.72, 65.78, 65.63, 65.67, 65.56]

Epoch: 13
2024-03-10 04:17:09.615797 epoch: 13 step: 0 cls_loss= 0.33541 (115192 samples/sec)
2024-03-10 04:17:19.008456 epoch: 13 step: 100 cls_loss= 0.36098 (3194 samples/sec)
saving....
2024-03-10 04:17:28.951402------------------------------------------------------ Precision@1: 65.68% 

[65.65, 65.64, 65.9, 65.69, 65.81, 65.75, 65.6, 65.79, 65.72, 65.78, 65.63, 65.67, 65.56, 65.68]

Epoch: 14
2024-03-10 04:17:29.235295 epoch: 14 step: 0 cls_loss= 0.37894 (106129 samples/sec)
2024-03-10 04:17:38.603455 epoch: 14 step: 100 cls_loss= 0.27306 (3203 samples/sec)
saving....
2024-03-10 04:17:48.547356------------------------------------------------------ Precision@1: 65.77% 

[65.65, 65.64, 65.9, 65.69, 65.81, 65.75, 65.6, 65.79, 65.72, 65.78, 65.63, 65.67, 65.56, 65.68, 65.77]

Epoch: 15
2024-03-10 04:17:48.838230 epoch: 15 step: 0 cls_loss= 0.29226 (103684 samples/sec)
2024-03-10 04:17:58.269532 epoch: 15 step: 100 cls_loss= 0.37282 (3181 samples/sec)
saving....
2024-03-10 04:18:08.231181------------------------------------------------------ Precision@1: 65.81% 

[65.65, 65.64, 65.9, 65.69, 65.81, 65.75, 65.6, 65.79, 65.72, 65.78, 65.63, 65.67, 65.56, 65.68, 65.77, 65.81]

Epoch: 16
2024-03-10 04:18:08.480628 epoch: 16 step: 0 cls_loss= 0.33224 (120984 samples/sec)
2024-03-10 04:18:17.857840 epoch: 16 step: 100 cls_loss= 0.31636 (3199 samples/sec)
saving....
2024-03-10 04:18:27.851205------------------------------------------------------ Precision@1: 65.66% 

[65.65, 65.64, 65.9, 65.69, 65.81, 65.75, 65.6, 65.79, 65.72, 65.78, 65.63, 65.67, 65.56, 65.68, 65.77, 65.81, 65.66]

Epoch: 17
2024-03-10 04:18:28.119790 epoch: 17 step: 0 cls_loss= 0.29513 (112361 samples/sec)
2024-03-10 04:18:37.483465 epoch: 17 step: 100 cls_loss= 0.30018 (3204 samples/sec)
saving....
2024-03-10 04:18:47.430223------------------------------------------------------ Precision@1: 65.77% 

[65.65, 65.64, 65.9, 65.69, 65.81, 65.75, 65.6, 65.79, 65.72, 65.78, 65.63, 65.67, 65.56, 65.68, 65.77, 65.81, 65.66, 65.77]

Epoch: 18
2024-03-10 04:18:47.684747 epoch: 18 step: 0 cls_loss= 0.36334 (118487 samples/sec)
2024-03-10 04:18:57.022594 epoch: 18 step: 100 cls_loss= 0.31099 (3213 samples/sec)
saving....
2024-03-10 04:19:06.928316------------------------------------------------------ Precision@1: 65.79% 

[65.65, 65.64, 65.9, 65.69, 65.81, 65.75, 65.6, 65.79, 65.72, 65.78, 65.63, 65.67, 65.56, 65.68, 65.77, 65.81, 65.66, 65.77, 65.79]

Epoch: 19
2024-03-10 04:19:07.203990 epoch: 19 step: 0 cls_loss= 0.33229 (109393 samples/sec)
2024-03-10 04:19:16.563230 epoch: 19 step: 100 cls_loss= 0.26393 (3206 samples/sec)
saving....
2024-03-10 04:19:26.489662------------------------------------------------------ Precision@1: 65.68% 

[65.65, 65.64, 65.9, 65.69, 65.81, 65.75, 65.6, 65.79, 65.72, 65.78, 65.63, 65.67, 65.56, 65.68, 65.77, 65.81, 65.66, 65.77, 65.79, 65.68]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 04:19:29.348103 epoch: 0 step: 0 cls_loss= 0.29420 (41560 samples/sec)
2024-03-10 04:19:38.688988 epoch: 0 step: 100 cls_loss= 0.30642 (3211 samples/sec)
saving....
2024-03-10 04:19:48.849653------------------------------------------------------ Precision@1: 65.74% 

[65.74]

Epoch: 1
2024-03-10 04:19:49.113040 epoch: 1 step: 0 cls_loss= 0.32214 (114751 samples/sec)
2024-03-10 04:19:58.471517 epoch: 1 step: 100 cls_loss= 0.36528 (3206 samples/sec)
saving....
2024-03-10 04:20:08.442944------------------------------------------------------ Precision@1: 65.77% 

[65.74, 65.77]

Epoch: 2
2024-03-10 04:20:08.689047 epoch: 2 step: 0 cls_loss= 0.31134 (122700 samples/sec)
2024-03-10 04:20:18.058286 epoch: 2 step: 100 cls_loss= 0.39788 (3202 samples/sec)
saving....
2024-03-10 04:20:27.975892------------------------------------------------------ Precision@1: 65.66% 

[65.74, 65.77, 65.66]

Epoch: 3
2024-03-10 04:20:28.242187 epoch: 3 step: 0 cls_loss= 0.26879 (113318 samples/sec)
2024-03-10 04:20:37.594719 epoch: 3 step: 100 cls_loss= 0.31997 (3208 samples/sec)
saving....
2024-03-10 04:20:47.499879------------------------------------------------------ Precision@1: 65.83% 

[65.74, 65.77, 65.66, 65.83]

Epoch: 4
2024-03-10 04:20:47.760123 epoch: 4 step: 0 cls_loss= 0.28211 (115901 samples/sec)
2024-03-10 04:20:57.160459 epoch: 4 step: 100 cls_loss= 0.39208 (3192 samples/sec)
saving....
2024-03-10 04:21:07.097637------------------------------------------------------ Precision@1: 65.68% 

[65.74, 65.77, 65.66, 65.83, 65.68]

Epoch: 5
2024-03-10 04:21:07.368116 epoch: 5 step: 0 cls_loss= 0.31566 (111552 samples/sec)
2024-03-10 04:21:16.725569 epoch: 5 step: 100 cls_loss= 0.34648 (3206 samples/sec)
saving....
2024-03-10 04:21:26.732918------------------------------------------------------ Precision@1: 65.82% 

[65.74, 65.77, 65.66, 65.83, 65.68, 65.82]

Epoch: 6
2024-03-10 04:21:27.002750 epoch: 6 step: 0 cls_loss= 0.27090 (111874 samples/sec)
2024-03-10 04:21:36.390209 epoch: 6 step: 100 cls_loss= 0.30824 (3196 samples/sec)
saving....
2024-03-10 04:21:46.309280------------------------------------------------------ Precision@1: 65.82% 

[65.74, 65.77, 65.66, 65.83, 65.68, 65.82, 65.82]

Epoch: 7
2024-03-10 04:21:46.588925 epoch: 7 step: 0 cls_loss= 0.34948 (107857 samples/sec)
2024-03-10 04:21:56.017849 epoch: 7 step: 100 cls_loss= 0.35649 (3182 samples/sec)
saving....
2024-03-10 04:22:05.986954------------------------------------------------------ Precision@1: 65.72% 

[65.74, 65.77, 65.66, 65.83, 65.68, 65.82, 65.82, 65.72]

Epoch: 8
2024-03-10 04:22:06.269038 epoch: 8 step: 0 cls_loss= 0.34398 (106869 samples/sec)
2024-03-10 04:22:15.625380 epoch: 8 step: 100 cls_loss= 0.23570 (3207 samples/sec)
saving....
2024-03-10 04:22:25.648431------------------------------------------------------ Precision@1: 65.83% 

[65.74, 65.77, 65.66, 65.83, 65.68, 65.82, 65.82, 65.72, 65.83]

Epoch: 9
2024-03-10 04:22:25.894237 epoch: 9 step: 0 cls_loss= 0.28091 (122835 samples/sec)
2024-03-10 04:22:35.237741 epoch: 9 step: 100 cls_loss= 0.35683 (3211 samples/sec)
saving....
2024-03-10 04:22:45.160232------------------------------------------------------ Precision@1: 65.77% 

[65.74, 65.77, 65.66, 65.83, 65.68, 65.82, 65.82, 65.72, 65.83, 65.77]

Epoch: 10
2024-03-10 04:22:45.416053 epoch: 10 step: 0 cls_loss= 0.33009 (117999 samples/sec)
2024-03-10 04:22:54.825403 epoch: 10 step: 100 cls_loss= 0.28420 (3188 samples/sec)
saving....
2024-03-10 04:23:04.805179------------------------------------------------------ Precision@1: 65.64% 

[65.74, 65.77, 65.66, 65.83, 65.68, 65.82, 65.82, 65.72, 65.83, 65.77, 65.64]

Epoch: 11
2024-03-10 04:23:05.072516 epoch: 11 step: 0 cls_loss= 0.28557 (112895 samples/sec)
2024-03-10 04:23:14.454804 epoch: 11 step: 100 cls_loss= 0.35691 (3198 samples/sec)
saving....
2024-03-10 04:23:24.455173------------------------------------------------------ Precision@1: 65.63% 

[65.74, 65.77, 65.66, 65.83, 65.68, 65.82, 65.82, 65.72, 65.83, 65.77, 65.64, 65.63]

Epoch: 12
2024-03-10 04:23:24.720419 epoch: 12 step: 0 cls_loss= 0.29335 (113816 samples/sec)
2024-03-10 04:23:34.092339 epoch: 12 step: 100 cls_loss= 0.37979 (3201 samples/sec)
saving....
2024-03-10 04:23:44.086019------------------------------------------------------ Precision@1: 65.79% 

[65.74, 65.77, 65.66, 65.83, 65.68, 65.82, 65.82, 65.72, 65.83, 65.77, 65.64, 65.63, 65.79]

Epoch: 13
2024-03-10 04:23:44.348191 epoch: 13 step: 0 cls_loss= 0.29928 (115106 samples/sec)
2024-03-10 04:23:53.702731 epoch: 13 step: 100 cls_loss= 0.32026 (3207 samples/sec)
saving....
2024-03-10 04:24:03.617436------------------------------------------------------ Precision@1: 65.77% 

[65.74, 65.77, 65.66, 65.83, 65.68, 65.82, 65.82, 65.72, 65.83, 65.77, 65.64, 65.63, 65.79, 65.77]

Epoch: 14
2024-03-10 04:24:03.893469 epoch: 14 step: 0 cls_loss= 0.33333 (109277 samples/sec)
2024-03-10 04:24:13.226195 epoch: 14 step: 100 cls_loss= 0.41252 (3215 samples/sec)
saving....
2024-03-10 04:24:23.208872------------------------------------------------------ Precision@1: 65.62% 

[65.74, 65.77, 65.66, 65.83, 65.68, 65.82, 65.82, 65.72, 65.83, 65.77, 65.64, 65.63, 65.79, 65.77, 65.62]

Epoch: 15
2024-03-10 04:24:23.468994 epoch: 15 step: 0 cls_loss= 0.33149 (116070 samples/sec)
2024-03-10 04:24:32.809094 epoch: 15 step: 100 cls_loss= 0.37759 (3212 samples/sec)
saving....
2024-03-10 04:24:42.688284------------------------------------------------------ Precision@1: 65.77% 

[65.74, 65.77, 65.66, 65.83, 65.68, 65.82, 65.82, 65.72, 65.83, 65.77, 65.64, 65.63, 65.79, 65.77, 65.62, 65.77]

Epoch: 16
2024-03-10 04:24:42.966762 epoch: 16 step: 0 cls_loss= 0.36891 (108208 samples/sec)
2024-03-10 04:24:52.321051 epoch: 16 step: 100 cls_loss= 0.32599 (3207 samples/sec)
saving....
2024-03-10 04:25:02.272025------------------------------------------------------ Precision@1: 65.72% 

[65.74, 65.77, 65.66, 65.83, 65.68, 65.82, 65.82, 65.72, 65.83, 65.77, 65.64, 65.63, 65.79, 65.77, 65.62, 65.77, 65.72]

Epoch: 17
2024-03-10 04:25:02.536523 epoch: 17 step: 0 cls_loss= 0.32313 (114056 samples/sec)
2024-03-10 04:25:11.890988 epoch: 17 step: 100 cls_loss= 0.36002 (3207 samples/sec)
saving....
2024-03-10 04:25:21.778637------------------------------------------------------ Precision@1: 66.00% 

[65.74, 65.77, 65.66, 65.83, 65.68, 65.82, 65.82, 65.72, 65.83, 65.77, 65.64, 65.63, 65.79, 65.77, 65.62, 65.77, 65.72, 66.0]

Epoch: 18
2024-03-10 04:25:22.024101 epoch: 18 step: 0 cls_loss= 0.30580 (123009 samples/sec)
2024-03-10 04:25:31.365940 epoch: 18 step: 100 cls_loss= 0.37891 (3212 samples/sec)
saving....
2024-03-10 04:25:41.226814------------------------------------------------------ Precision@1: 65.89% 

[65.74, 65.77, 65.66, 65.83, 65.68, 65.82, 65.82, 65.72, 65.83, 65.77, 65.64, 65.63, 65.79, 65.77, 65.62, 65.77, 65.72, 66.0, 65.89]

Epoch: 19
2024-03-10 04:25:41.488996 epoch: 19 step: 0 cls_loss= 0.32900 (115092 samples/sec)
2024-03-10 04:25:50.828883 epoch: 19 step: 100 cls_loss= 0.35584 (3212 samples/sec)
saving....
2024-03-10 04:26:00.763155------------------------------------------------------ Precision@1: 65.69% 

[65.74, 65.77, 65.66, 65.83, 65.68, 65.82, 65.82, 65.72, 65.83, 65.77, 65.64, 65.63, 65.79, 65.77, 65.62, 65.77, 65.72, 66.0, 65.89, 65.69]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 04:26:03.661160 epoch: 0 step: 0 cls_loss= 0.26270 (40288 samples/sec)
2024-03-10 04:26:13.023943 epoch: 0 step: 100 cls_loss= 0.30475 (3204 samples/sec)
saving....
2024-03-10 04:26:23.260676------------------------------------------------------ Precision@1: 65.86% 

[65.86]

Epoch: 1
2024-03-10 04:26:23.515407 epoch: 1 step: 0 cls_loss= 0.34630 (118477 samples/sec)
2024-03-10 04:26:32.867340 epoch: 1 step: 100 cls_loss= 0.26225 (3208 samples/sec)
saving....
2024-03-10 04:26:42.823962------------------------------------------------------ Precision@1: 65.55% 

[65.86, 65.55]

Epoch: 2
2024-03-10 04:26:43.110910 epoch: 2 step: 0 cls_loss= 0.27515 (105106 samples/sec)
2024-03-10 04:26:52.472049 epoch: 2 step: 100 cls_loss= 0.28224 (3205 samples/sec)
saving....
2024-03-10 04:27:02.421248------------------------------------------------------ Precision@1: 65.97% 

[65.86, 65.55, 65.97]

Epoch: 3
2024-03-10 04:27:02.714860 epoch: 3 step: 0 cls_loss= 0.31729 (102693 samples/sec)
2024-03-10 04:27:12.097016 epoch: 3 step: 100 cls_loss= 0.32769 (3198 samples/sec)
saving....
2024-03-10 04:27:22.022754------------------------------------------------------ Precision@1: 65.94% 

[65.86, 65.55, 65.97, 65.94]

Epoch: 4
2024-03-10 04:27:22.278747 epoch: 4 step: 0 cls_loss= 0.31429 (117914 samples/sec)
2024-03-10 04:27:31.657757 epoch: 4 step: 100 cls_loss= 0.35220 (3199 samples/sec)
saving....
2024-03-10 04:27:41.710623------------------------------------------------------ Precision@1: 65.62% 

[65.86, 65.55, 65.97, 65.94, 65.62]

Epoch: 5
2024-03-10 04:27:41.967999 epoch: 5 step: 0 cls_loss= 0.27335 (117277 samples/sec)
2024-03-10 04:27:51.347808 epoch: 5 step: 100 cls_loss= 0.24681 (3199 samples/sec)
saving....
2024-03-10 04:28:01.412087------------------------------------------------------ Precision@1: 65.71% 

[65.86, 65.55, 65.97, 65.94, 65.62, 65.71]

Epoch: 6
2024-03-10 04:28:01.674012 epoch: 6 step: 0 cls_loss= 0.28157 (115273 samples/sec)
2024-03-10 04:28:11.040995 epoch: 6 step: 100 cls_loss= 0.25233 (3203 samples/sec)
saving....
2024-03-10 04:28:21.028995------------------------------------------------------ Precision@1: 65.69% 

[65.86, 65.55, 65.97, 65.94, 65.62, 65.71, 65.69]

Epoch: 7
2024-03-10 04:28:21.313798 epoch: 7 step: 0 cls_loss= 0.35956 (105805 samples/sec)
2024-03-10 04:28:30.676651 epoch: 7 step: 100 cls_loss= 0.31335 (3204 samples/sec)
saving....
2024-03-10 04:28:40.635847------------------------------------------------------ Precision@1: 65.83% 

[65.86, 65.55, 65.97, 65.94, 65.62, 65.71, 65.69, 65.83]

Epoch: 8
2024-03-10 04:28:40.895339 epoch: 8 step: 0 cls_loss= 0.32627 (116340 samples/sec)
2024-03-10 04:28:50.263835 epoch: 8 step: 100 cls_loss= 0.31664 (3202 samples/sec)
saving....
2024-03-10 04:29:00.242959------------------------------------------------------ Precision@1: 65.90% 

[65.86, 65.55, 65.97, 65.94, 65.62, 65.71, 65.69, 65.83, 65.9]

Epoch: 9
2024-03-10 04:29:00.506177 epoch: 9 step: 0 cls_loss= 0.35507 (114581 samples/sec)
2024-03-10 04:29:09.894963 epoch: 9 step: 100 cls_loss= 0.29267 (3195 samples/sec)
saving....
2024-03-10 04:29:19.879235------------------------------------------------------ Precision@1: 65.84% 

[65.86, 65.55, 65.97, 65.94, 65.62, 65.71, 65.69, 65.83, 65.9, 65.84]

Epoch: 10
2024-03-10 04:29:20.141451 epoch: 10 step: 0 cls_loss= 0.34066 (115011 samples/sec)
2024-03-10 04:29:29.545647 epoch: 10 step: 100 cls_loss= 0.37299 (3190 samples/sec)
saving....
2024-03-10 04:29:39.552328------------------------------------------------------ Precision@1: 65.68% 

[65.86, 65.55, 65.97, 65.94, 65.62, 65.71, 65.69, 65.83, 65.9, 65.84, 65.68]

Epoch: 11
2024-03-10 04:29:39.818818 epoch: 11 step: 0 cls_loss= 0.33507 (113143 samples/sec)
2024-03-10 04:29:49.207671 epoch: 11 step: 100 cls_loss= 0.34639 (3195 samples/sec)
saving....
2024-03-10 04:29:59.161520------------------------------------------------------ Precision@1: 65.73% 

[65.86, 65.55, 65.97, 65.94, 65.62, 65.71, 65.69, 65.83, 65.9, 65.84, 65.68, 65.73]

Epoch: 12
2024-03-10 04:29:59.431100 epoch: 12 step: 0 cls_loss= 0.30702 (111881 samples/sec)
2024-03-10 04:30:08.823187 epoch: 12 step: 100 cls_loss= 0.30492 (3194 samples/sec)
saving....
2024-03-10 04:30:18.897390------------------------------------------------------ Precision@1: 65.73% 

[65.86, 65.55, 65.97, 65.94, 65.62, 65.71, 65.69, 65.83, 65.9, 65.84, 65.68, 65.73, 65.73]

Epoch: 13
2024-03-10 04:30:19.153645 epoch: 13 step: 0 cls_loss= 0.30100 (117696 samples/sec)
2024-03-10 04:30:28.525428 epoch: 13 step: 100 cls_loss= 0.30833 (3201 samples/sec)
saving....
2024-03-10 04:30:38.458549------------------------------------------------------ Precision@1: 65.84% 

[65.86, 65.55, 65.97, 65.94, 65.62, 65.71, 65.69, 65.83, 65.9, 65.84, 65.68, 65.73, 65.73, 65.84]

Epoch: 14
2024-03-10 04:30:38.729473 epoch: 14 step: 0 cls_loss= 0.34482 (111377 samples/sec)
2024-03-10 04:30:48.101077 epoch: 14 step: 100 cls_loss= 0.31374 (3201 samples/sec)
saving....
2024-03-10 04:30:58.057620------------------------------------------------------ Precision@1: 65.86% 

[65.86, 65.55, 65.97, 65.94, 65.62, 65.71, 65.69, 65.83, 65.9, 65.84, 65.68, 65.73, 65.73, 65.84, 65.86]

Epoch: 15
2024-03-10 04:30:58.332303 epoch: 15 step: 0 cls_loss= 0.27423 (109814 samples/sec)
2024-03-10 04:31:07.763859 epoch: 15 step: 100 cls_loss= 0.32016 (3181 samples/sec)
saving....
2024-03-10 04:31:17.741567------------------------------------------------------ Precision@1: 65.84% 

[65.86, 65.55, 65.97, 65.94, 65.62, 65.71, 65.69, 65.83, 65.9, 65.84, 65.68, 65.73, 65.73, 65.84, 65.86, 65.84]

Epoch: 16
2024-03-10 04:31:17.999434 epoch: 16 step: 0 cls_loss= 0.40989 (117082 samples/sec)
2024-03-10 04:31:27.404322 epoch: 16 step: 100 cls_loss= 0.27062 (3190 samples/sec)
saving....
2024-03-10 04:31:37.370842------------------------------------------------------ Precision@1: 65.88% 

[65.86, 65.55, 65.97, 65.94, 65.62, 65.71, 65.69, 65.83, 65.9, 65.84, 65.68, 65.73, 65.73, 65.84, 65.86, 65.84, 65.88]

Epoch: 17
2024-03-10 04:31:37.625466 epoch: 17 step: 0 cls_loss= 0.33620 (118537 samples/sec)
2024-03-10 04:31:47.027382 epoch: 17 step: 100 cls_loss= 0.37527 (3191 samples/sec)
saving....
2024-03-10 04:31:56.992418------------------------------------------------------ Precision@1: 65.81% 

[65.86, 65.55, 65.97, 65.94, 65.62, 65.71, 65.69, 65.83, 65.9, 65.84, 65.68, 65.73, 65.73, 65.84, 65.86, 65.84, 65.88, 65.81]

Epoch: 18
2024-03-10 04:31:57.282064 epoch: 18 step: 0 cls_loss= 0.35116 (103949 samples/sec)
2024-03-10 04:32:06.763686 epoch: 18 step: 100 cls_loss= 0.29583 (3164 samples/sec)
saving....
2024-03-10 04:32:16.784800------------------------------------------------------ Precision@1: 65.83% 

[65.86, 65.55, 65.97, 65.94, 65.62, 65.71, 65.69, 65.83, 65.9, 65.84, 65.68, 65.73, 65.73, 65.84, 65.86, 65.84, 65.88, 65.81, 65.83]

Epoch: 19
2024-03-10 04:32:17.065100 epoch: 19 step: 0 cls_loss= 0.30398 (107709 samples/sec)
2024-03-10 04:32:26.432906 epoch: 19 step: 100 cls_loss= 0.33421 (3203 samples/sec)
saving....
2024-03-10 04:32:36.385715------------------------------------------------------ Precision@1: 66.06% 

[65.86, 65.55, 65.97, 65.94, 65.62, 65.71, 65.69, 65.83, 65.9, 65.84, 65.68, 65.73, 65.73, 65.84, 65.86, 65.84, 65.88, 65.81, 65.83, 66.06]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 04:32:39.264256 epoch: 0 step: 0 cls_loss= 0.29863 (40835 samples/sec)
2024-03-10 04:32:48.626768 epoch: 0 step: 100 cls_loss= 0.30119 (3204 samples/sec)
saving....
2024-03-10 04:32:58.765276------------------------------------------------------ Precision@1: 65.74% 

[65.74]

Epoch: 1
2024-03-10 04:32:59.014229 epoch: 1 step: 0 cls_loss= 0.31740 (121290 samples/sec)
2024-03-10 04:33:08.387113 epoch: 1 step: 100 cls_loss= 0.31056 (3201 samples/sec)
saving....
2024-03-10 04:33:18.325367------------------------------------------------------ Precision@1: 65.81% 

[65.74, 65.81]

Epoch: 2
2024-03-10 04:33:18.570165 epoch: 2 step: 0 cls_loss= 0.36512 (123091 samples/sec)
2024-03-10 04:33:27.911124 epoch: 2 step: 100 cls_loss= 0.30286 (3212 samples/sec)
saving....
2024-03-10 04:33:37.933985------------------------------------------------------ Precision@1: 65.77% 

[65.74, 65.81, 65.77]

Epoch: 3
2024-03-10 04:33:38.191114 epoch: 3 step: 0 cls_loss= 0.30499 (117357 samples/sec)
2024-03-10 04:33:47.553488 epoch: 3 step: 100 cls_loss= 0.32417 (3204 samples/sec)
saving....
2024-03-10 04:33:57.446660------------------------------------------------------ Precision@1: 65.53% 

[65.74, 65.81, 65.77, 65.53]

Epoch: 4
2024-03-10 04:33:57.709825 epoch: 4 step: 0 cls_loss= 0.28680 (114714 samples/sec)
2024-03-10 04:34:07.138380 epoch: 4 step: 100 cls_loss= 0.28386 (3182 samples/sec)
saving....
2024-03-10 04:34:17.113659------------------------------------------------------ Precision@1: 65.81% 

[65.74, 65.81, 65.77, 65.53, 65.81]

Epoch: 5
2024-03-10 04:34:17.367608 epoch: 5 step: 0 cls_loss= 0.31176 (118899 samples/sec)
2024-03-10 04:34:26.734740 epoch: 5 step: 100 cls_loss= 0.30846 (3203 samples/sec)
saving....
2024-03-10 04:34:36.684313------------------------------------------------------ Precision@1: 65.70% 

[65.74, 65.81, 65.77, 65.53, 65.81, 65.7]

Epoch: 6
2024-03-10 04:34:36.944619 epoch: 6 step: 0 cls_loss= 0.33923 (115896 samples/sec)
2024-03-10 04:34:46.312336 epoch: 6 step: 100 cls_loss= 0.29373 (3203 samples/sec)
saving....
2024-03-10 04:34:56.257708------------------------------------------------------ Precision@1: 65.72% 

[65.74, 65.81, 65.77, 65.53, 65.81, 65.7, 65.72]

Epoch: 7
2024-03-10 04:34:56.531545 epoch: 7 step: 0 cls_loss= 0.30524 (110185 samples/sec)
2024-03-10 04:35:05.891268 epoch: 7 step: 100 cls_loss= 0.37328 (3205 samples/sec)
saving....
2024-03-10 04:35:15.818289------------------------------------------------------ Precision@1: 65.80% 

[65.74, 65.81, 65.77, 65.53, 65.81, 65.7, 65.72, 65.8]

Epoch: 8
2024-03-10 04:35:16.085305 epoch: 8 step: 0 cls_loss= 0.33545 (112953 samples/sec)
2024-03-10 04:35:25.428373 epoch: 8 step: 100 cls_loss= 0.32321 (3211 samples/sec)
saving....
2024-03-10 04:35:35.429603------------------------------------------------------ Precision@1: 65.85% 

[65.74, 65.81, 65.77, 65.53, 65.81, 65.7, 65.72, 65.8, 65.85]

Epoch: 9
2024-03-10 04:35:35.692591 epoch: 9 step: 0 cls_loss= 0.32921 (114745 samples/sec)
2024-03-10 04:35:45.050063 epoch: 9 step: 100 cls_loss= 0.43336 (3206 samples/sec)
saving....
2024-03-10 04:35:54.961563------------------------------------------------------ Precision@1: 65.90% 

[65.74, 65.81, 65.77, 65.53, 65.81, 65.7, 65.72, 65.8, 65.85, 65.9]

Epoch: 10
2024-03-10 04:35:55.233482 epoch: 10 step: 0 cls_loss= 0.29936 (110932 samples/sec)
2024-03-10 04:36:04.622348 epoch: 10 step: 100 cls_loss= 0.29030 (3195 samples/sec)
saving....
2024-03-10 04:36:14.594584------------------------------------------------------ Precision@1: 65.75% 

[65.74, 65.81, 65.77, 65.53, 65.81, 65.7, 65.72, 65.8, 65.85, 65.9, 65.75]

Epoch: 11
2024-03-10 04:36:14.857028 epoch: 11 step: 0 cls_loss= 0.32541 (114990 samples/sec)
2024-03-10 04:36:24.212909 epoch: 11 step: 100 cls_loss= 0.30279 (3207 samples/sec)
saving....
2024-03-10 04:36:34.083078------------------------------------------------------ Precision@1: 65.78% 

[65.74, 65.81, 65.77, 65.53, 65.81, 65.7, 65.72, 65.8, 65.85, 65.9, 65.75, 65.78]

Epoch: 12
2024-03-10 04:36:34.359312 epoch: 12 step: 0 cls_loss= 0.36714 (109166 samples/sec)
2024-03-10 04:36:43.721244 epoch: 12 step: 100 cls_loss= 0.29963 (3205 samples/sec)
saving....
2024-03-10 04:36:53.614562------------------------------------------------------ Precision@1: 65.74% 

[65.74, 65.81, 65.77, 65.53, 65.81, 65.7, 65.72, 65.8, 65.85, 65.9, 65.75, 65.78, 65.74]

Epoch: 13
2024-03-10 04:36:53.879321 epoch: 13 step: 0 cls_loss= 0.32652 (114010 samples/sec)
2024-03-10 04:37:03.251802 epoch: 13 step: 100 cls_loss= 0.34288 (3201 samples/sec)
saving....
2024-03-10 04:37:13.214207------------------------------------------------------ Precision@1: 65.80% 

[65.74, 65.81, 65.77, 65.53, 65.81, 65.7, 65.72, 65.8, 65.85, 65.9, 65.75, 65.78, 65.74, 65.8]

Epoch: 14
2024-03-10 04:37:13.483894 epoch: 14 step: 0 cls_loss= 0.30143 (111810 samples/sec)
2024-03-10 04:37:22.838573 epoch: 14 step: 100 cls_loss= 0.27458 (3207 samples/sec)
saving....
2024-03-10 04:37:32.731058------------------------------------------------------ Precision@1: 65.94% 

[65.74, 65.81, 65.77, 65.53, 65.81, 65.7, 65.72, 65.8, 65.85, 65.9, 65.75, 65.78, 65.74, 65.8, 65.94]

Epoch: 15
2024-03-10 04:37:32.979625 epoch: 15 step: 0 cls_loss= 0.32429 (121477 samples/sec)
2024-03-10 04:37:42.351009 epoch: 15 step: 100 cls_loss= 0.37817 (3201 samples/sec)
saving....
2024-03-10 04:37:52.287565------------------------------------------------------ Precision@1: 65.75% 

[65.74, 65.81, 65.77, 65.53, 65.81, 65.7, 65.72, 65.8, 65.85, 65.9, 65.75, 65.78, 65.74, 65.8, 65.94, 65.75]

Epoch: 16
2024-03-10 04:37:52.557083 epoch: 16 step: 0 cls_loss= 0.31079 (111940 samples/sec)
2024-03-10 04:38:01.939498 epoch: 16 step: 100 cls_loss= 0.31759 (3198 samples/sec)
saving....
2024-03-10 04:38:11.838150------------------------------------------------------ Precision@1: 65.73% 

[65.74, 65.81, 65.77, 65.53, 65.81, 65.7, 65.72, 65.8, 65.85, 65.9, 65.75, 65.78, 65.74, 65.8, 65.94, 65.75, 65.73]

Epoch: 17
2024-03-10 04:38:12.077278 epoch: 17 step: 0 cls_loss= 0.31912 (126344 samples/sec)
2024-03-10 04:38:21.416216 epoch: 17 step: 100 cls_loss= 0.31103 (3213 samples/sec)
saving....
2024-03-10 04:38:31.308816------------------------------------------------------ Precision@1: 65.72% 

[65.74, 65.81, 65.77, 65.53, 65.81, 65.7, 65.72, 65.8, 65.85, 65.9, 65.75, 65.78, 65.74, 65.8, 65.94, 65.75, 65.73, 65.72]

Epoch: 18
2024-03-10 04:38:31.596209 epoch: 18 step: 0 cls_loss= 0.34890 (104871 samples/sec)
2024-03-10 04:38:40.926447 epoch: 18 step: 100 cls_loss= 0.29470 (3216 samples/sec)
saving....
2024-03-10 04:38:50.805864------------------------------------------------------ Precision@1: 65.90% 

[65.74, 65.81, 65.77, 65.53, 65.81, 65.7, 65.72, 65.8, 65.85, 65.9, 65.75, 65.78, 65.74, 65.8, 65.94, 65.75, 65.73, 65.72, 65.9]

Epoch: 19
2024-03-10 04:38:51.060069 epoch: 19 step: 0 cls_loss= 0.24796 (118611 samples/sec)
2024-03-10 04:39:00.393685 epoch: 19 step: 100 cls_loss= 0.31031 (3214 samples/sec)
saving....
2024-03-10 04:39:10.331168------------------------------------------------------ Precision@1: 65.59% 

[65.74, 65.81, 65.77, 65.53, 65.81, 65.7, 65.72, 65.8, 65.85, 65.9, 65.75, 65.78, 65.74, 65.8, 65.94, 65.75, 65.73, 65.72, 65.9, 65.59]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 04:39:13.226378 epoch: 0 step: 0 cls_loss= 0.29351 (40403 samples/sec)
2024-03-10 04:39:22.586132 epoch: 0 step: 100 cls_loss= 0.29593 (3205 samples/sec)
saving....
2024-03-10 04:39:32.744628------------------------------------------------------ Precision@1: 65.76% 

[65.76]

Epoch: 1
2024-03-10 04:39:32.998655 epoch: 1 step: 0 cls_loss= 0.30105 (118825 samples/sec)
2024-03-10 04:39:42.345116 epoch: 1 step: 100 cls_loss= 0.35551 (3210 samples/sec)
saving....
2024-03-10 04:39:52.235146------------------------------------------------------ Precision@1: 65.75% 

[65.76, 65.75]

Epoch: 2
2024-03-10 04:39:52.520735 epoch: 2 step: 0 cls_loss= 0.39189 (105482 samples/sec)
2024-03-10 04:40:01.866602 epoch: 2 step: 100 cls_loss= 0.27574 (3210 samples/sec)
saving....
2024-03-10 04:40:11.814427------------------------------------------------------ Precision@1: 65.75% 

[65.76, 65.75, 65.75]

Epoch: 3
2024-03-10 04:40:12.081756 epoch: 3 step: 0 cls_loss= 0.30883 (112926 samples/sec)
2024-03-10 04:40:21.467445 epoch: 3 step: 100 cls_loss= 0.31051 (3197 samples/sec)
saving....
2024-03-10 04:40:31.466712------------------------------------------------------ Precision@1: 65.84% 

[65.76, 65.75, 65.75, 65.84]

Epoch: 4
2024-03-10 04:40:31.738714 epoch: 4 step: 0 cls_loss= 0.30291 (110916 samples/sec)
2024-03-10 04:40:41.100803 epoch: 4 step: 100 cls_loss= 0.30100 (3205 samples/sec)
saving....
2024-03-10 04:40:51.035671------------------------------------------------------ Precision@1: 65.82% 

[65.76, 65.75, 65.75, 65.84, 65.82]

Epoch: 5
2024-03-10 04:40:51.303707 epoch: 5 step: 0 cls_loss= 0.36254 (112568 samples/sec)
2024-03-10 04:41:00.703015 epoch: 5 step: 100 cls_loss= 0.29282 (3192 samples/sec)
saving....
2024-03-10 04:41:10.668195------------------------------------------------------ Precision@1: 65.78% 

[65.76, 65.75, 65.75, 65.84, 65.82, 65.78]

Epoch: 6
2024-03-10 04:41:10.931737 epoch: 6 step: 0 cls_loss= 0.31332 (114578 samples/sec)
2024-03-10 04:41:20.325679 epoch: 6 step: 100 cls_loss= 0.30115 (3194 samples/sec)
saving....
2024-03-10 04:41:30.309604------------------------------------------------------ Precision@1: 65.76% 

[65.76, 65.75, 65.75, 65.84, 65.82, 65.78, 65.76]

Epoch: 7
2024-03-10 04:41:30.586353 epoch: 7 step: 0 cls_loss= 0.28123 (109007 samples/sec)
2024-03-10 04:41:39.988979 epoch: 7 step: 100 cls_loss= 0.37388 (3191 samples/sec)
saving....
2024-03-10 04:41:50.044657------------------------------------------------------ Precision@1: 65.82% 

[65.76, 65.75, 65.75, 65.84, 65.82, 65.78, 65.76, 65.82]

Epoch: 8
2024-03-10 04:41:50.337697 epoch: 8 step: 0 cls_loss= 0.39501 (102881 samples/sec)
2024-03-10 04:41:59.759738 epoch: 8 step: 100 cls_loss= 0.40074 (3184 samples/sec)
saving....
2024-03-10 04:42:09.733246------------------------------------------------------ Precision@1: 65.78% 

[65.76, 65.75, 65.75, 65.84, 65.82, 65.78, 65.76, 65.82, 65.78]

Epoch: 9
2024-03-10 04:42:09.998872 epoch: 9 step: 0 cls_loss= 0.33643 (113541 samples/sec)
2024-03-10 04:42:19.391101 epoch: 9 step: 100 cls_loss= 0.35745 (3194 samples/sec)
saving....
2024-03-10 04:42:29.358634------------------------------------------------------ Precision@1: 65.84% 

[65.76, 65.75, 65.75, 65.84, 65.82, 65.78, 65.76, 65.82, 65.78, 65.84]

Epoch: 10
2024-03-10 04:42:29.620553 epoch: 10 step: 0 cls_loss= 0.36583 (115241 samples/sec)
2024-03-10 04:42:38.968041 epoch: 10 step: 100 cls_loss= 0.35641 (3210 samples/sec)
saving....
2024-03-10 04:42:48.930355------------------------------------------------------ Precision@1: 65.77% 

[65.76, 65.75, 65.75, 65.84, 65.82, 65.78, 65.76, 65.82, 65.78, 65.84, 65.77]

Epoch: 11
2024-03-10 04:42:49.190507 epoch: 11 step: 0 cls_loss= 0.31710 (116005 samples/sec)
2024-03-10 04:42:58.542268 epoch: 11 step: 100 cls_loss= 0.30380 (3208 samples/sec)
saving....
2024-03-10 04:43:08.568842------------------------------------------------------ Precision@1: 66.01% 

[65.76, 65.75, 65.75, 65.84, 65.82, 65.78, 65.76, 65.82, 65.78, 65.84, 65.77, 66.01]

Epoch: 12
2024-03-10 04:43:08.857773 epoch: 12 step: 0 cls_loss= 0.33370 (104390 samples/sec)
2024-03-10 04:43:18.203302 epoch: 12 step: 100 cls_loss= 0.32875 (3210 samples/sec)
saving....
2024-03-10 04:43:28.105905------------------------------------------------------ Precision@1: 65.98% 

[65.76, 65.75, 65.75, 65.84, 65.82, 65.78, 65.76, 65.82, 65.78, 65.84, 65.77, 66.01, 65.98]

Epoch: 13
2024-03-10 04:43:28.364859 epoch: 13 step: 0 cls_loss= 0.34653 (116577 samples/sec)
2024-03-10 04:43:37.744476 epoch: 13 step: 100 cls_loss= 0.36208 (3199 samples/sec)
saving....
2024-03-10 04:43:47.696856------------------------------------------------------ Precision@1: 65.72% 

[65.76, 65.75, 65.75, 65.84, 65.82, 65.78, 65.76, 65.82, 65.78, 65.84, 65.77, 66.01, 65.98, 65.72]

Epoch: 14
2024-03-10 04:43:47.975123 epoch: 14 step: 0 cls_loss= 0.27957 (108432 samples/sec)
2024-03-10 04:43:57.341687 epoch: 14 step: 100 cls_loss= 0.31878 (3203 samples/sec)
saving....
2024-03-10 04:44:07.301293------------------------------------------------------ Precision@1: 65.80% 

[65.76, 65.75, 65.75, 65.84, 65.82, 65.78, 65.76, 65.82, 65.78, 65.84, 65.77, 66.01, 65.98, 65.72, 65.8]

Epoch: 15
2024-03-10 04:44:07.560664 epoch: 15 step: 0 cls_loss= 0.29799 (116349 samples/sec)
2024-03-10 04:44:16.927490 epoch: 15 step: 100 cls_loss= 0.35582 (3203 samples/sec)
saving....
2024-03-10 04:44:26.896583------------------------------------------------------ Precision@1: 65.80% 

[65.76, 65.75, 65.75, 65.84, 65.82, 65.78, 65.76, 65.82, 65.78, 65.84, 65.77, 66.01, 65.98, 65.72, 65.8, 65.8]

Epoch: 16
2024-03-10 04:44:27.172869 epoch: 16 step: 0 cls_loss= 0.28808 (109087 samples/sec)
2024-03-10 04:44:36.548070 epoch: 16 step: 100 cls_loss= 0.32055 (3200 samples/sec)
saving....
2024-03-10 04:44:46.439186------------------------------------------------------ Precision@1: 65.77% 

[65.76, 65.75, 65.75, 65.84, 65.82, 65.78, 65.76, 65.82, 65.78, 65.84, 65.77, 66.01, 65.98, 65.72, 65.8, 65.8, 65.77]

Epoch: 17
2024-03-10 04:44:46.705383 epoch: 17 step: 0 cls_loss= 0.36856 (113362 samples/sec)
2024-03-10 04:44:56.147992 epoch: 17 step: 100 cls_loss= 0.28791 (3177 samples/sec)
saving....
2024-03-10 04:45:06.118846------------------------------------------------------ Precision@1: 65.77% 

[65.76, 65.75, 65.75, 65.84, 65.82, 65.78, 65.76, 65.82, 65.78, 65.84, 65.77, 66.01, 65.98, 65.72, 65.8, 65.8, 65.77, 65.77]

Epoch: 18
2024-03-10 04:45:06.391381 epoch: 18 step: 0 cls_loss= 0.33908 (110598 samples/sec)
2024-03-10 04:45:15.765591 epoch: 18 step: 100 cls_loss= 0.31919 (3200 samples/sec)
saving....
2024-03-10 04:45:25.789988------------------------------------------------------ Precision@1: 65.71% 

[65.76, 65.75, 65.75, 65.84, 65.82, 65.78, 65.76, 65.82, 65.78, 65.84, 65.77, 66.01, 65.98, 65.72, 65.8, 65.8, 65.77, 65.77, 65.71]

Epoch: 19
2024-03-10 04:45:26.059082 epoch: 19 step: 0 cls_loss= 0.26574 (112014 samples/sec)
2024-03-10 04:45:35.431887 epoch: 19 step: 100 cls_loss= 0.34234 (3201 samples/sec)
saving....
2024-03-10 04:45:45.381566------------------------------------------------------ Precision@1: 65.72% 

[65.76, 65.75, 65.75, 65.84, 65.82, 65.78, 65.76, 65.82, 65.78, 65.84, 65.77, 66.01, 65.98, 65.72, 65.8, 65.8, 65.77, 65.77, 65.71, 65.72]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 04:45:48.239607 epoch: 0 step: 0 cls_loss= 0.32932 (41651 samples/sec)
2024-03-10 04:45:57.632531 epoch: 0 step: 100 cls_loss= 0.32412 (3193 samples/sec)
saving....
2024-03-10 04:46:08.088519------------------------------------------------------ Precision@1: 65.90% 

[65.9]

Epoch: 1
2024-03-10 04:46:08.385139 epoch: 1 step: 0 cls_loss= 0.32957 (101686 samples/sec)
2024-03-10 04:46:17.728001 epoch: 1 step: 100 cls_loss= 0.31027 (3211 samples/sec)
saving....
2024-03-10 04:46:27.629976------------------------------------------------------ Precision@1: 65.88% 

[65.9, 65.88]

Epoch: 2
2024-03-10 04:46:27.910927 epoch: 2 step: 0 cls_loss= 0.30054 (107273 samples/sec)
2024-03-10 04:46:37.311484 epoch: 2 step: 100 cls_loss= 0.33578 (3191 samples/sec)
saving....
2024-03-10 04:46:47.273349------------------------------------------------------ Precision@1: 65.75% 

[65.9, 65.88, 65.75]

Epoch: 3
2024-03-10 04:46:47.547434 epoch: 3 step: 0 cls_loss= 0.28609 (110124 samples/sec)
2024-03-10 04:46:56.929085 epoch: 3 step: 100 cls_loss= 0.33153 (3198 samples/sec)
saving....
2024-03-10 04:47:06.889880------------------------------------------------------ Precision@1: 65.93% 

[65.9, 65.88, 65.75, 65.93]

Epoch: 4
2024-03-10 04:47:07.141275 epoch: 4 step: 0 cls_loss= 0.28744 (120068 samples/sec)
2024-03-10 04:47:16.513845 epoch: 4 step: 100 cls_loss= 0.33075 (3201 samples/sec)
saving....
2024-03-10 04:47:26.453990------------------------------------------------------ Precision@1: 65.70% 

[65.9, 65.88, 65.75, 65.93, 65.7]

Epoch: 5
2024-03-10 04:47:26.718323 epoch: 5 step: 0 cls_loss= 0.38321 (114142 samples/sec)
2024-03-10 04:47:36.088949 epoch: 5 step: 100 cls_loss= 0.30538 (3202 samples/sec)
saving....
2024-03-10 04:47:45.987065------------------------------------------------------ Precision@1: 65.75% 

[65.9, 65.88, 65.75, 65.93, 65.7, 65.75]

Epoch: 6
2024-03-10 04:47:46.276390 epoch: 6 step: 0 cls_loss= 0.29716 (104284 samples/sec)
2024-03-10 04:47:55.646869 epoch: 6 step: 100 cls_loss= 0.29701 (3202 samples/sec)
saving....
2024-03-10 04:48:05.571150------------------------------------------------------ Precision@1: 65.77% 

[65.9, 65.88, 65.75, 65.93, 65.7, 65.75, 65.77]

Epoch: 7
2024-03-10 04:48:05.837082 epoch: 7 step: 0 cls_loss= 0.32779 (113410 samples/sec)
2024-03-10 04:48:15.223700 epoch: 7 step: 100 cls_loss= 0.36424 (3196 samples/sec)
saving....
2024-03-10 04:48:25.177945------------------------------------------------------ Precision@1: 65.96% 

[65.9, 65.88, 65.75, 65.93, 65.7, 65.75, 65.77, 65.96]

Epoch: 8
2024-03-10 04:48:25.428843 epoch: 8 step: 0 cls_loss= 0.29416 (120334 samples/sec)
2024-03-10 04:48:34.783312 epoch: 8 step: 100 cls_loss= 0.32906 (3207 samples/sec)
saving....
2024-03-10 04:48:44.749923------------------------------------------------------ Precision@1: 65.80% 

[65.9, 65.88, 65.75, 65.93, 65.7, 65.75, 65.77, 65.96, 65.8]

Epoch: 9
2024-03-10 04:48:45.016373 epoch: 9 step: 0 cls_loss= 0.29012 (113258 samples/sec)
2024-03-10 04:48:54.393695 epoch: 9 step: 100 cls_loss= 0.30865 (3199 samples/sec)
saving....
2024-03-10 04:49:04.384537------------------------------------------------------ Precision@1: 65.69% 

[65.9, 65.88, 65.75, 65.93, 65.7, 65.75, 65.77, 65.96, 65.8, 65.69]

Epoch: 10
2024-03-10 04:49:04.648675 epoch: 10 step: 0 cls_loss= 0.28937 (114042 samples/sec)
2024-03-10 04:49:14.024870 epoch: 10 step: 100 cls_loss= 0.36632 (3200 samples/sec)
saving....
2024-03-10 04:49:23.986311------------------------------------------------------ Precision@1: 65.81% 

[65.9, 65.88, 65.75, 65.93, 65.7, 65.75, 65.77, 65.96, 65.8, 65.69, 65.81]

Epoch: 11
2024-03-10 04:49:24.263498 epoch: 11 step: 0 cls_loss= 0.28030 (108795 samples/sec)
2024-03-10 04:49:33.626269 epoch: 11 step: 100 cls_loss= 0.28530 (3204 samples/sec)
saving....
2024-03-10 04:49:43.627288------------------------------------------------------ Precision@1: 65.79% 

[65.9, 65.88, 65.75, 65.93, 65.7, 65.75, 65.77, 65.96, 65.8, 65.69, 65.81, 65.79]

Epoch: 12
2024-03-10 04:49:43.885571 epoch: 12 step: 0 cls_loss= 0.33004 (116778 samples/sec)
2024-03-10 04:49:53.258962 epoch: 12 step: 100 cls_loss= 0.33225 (3201 samples/sec)
saving....
2024-03-10 04:50:03.186030------------------------------------------------------ Precision@1: 65.78% 

[65.9, 65.88, 65.75, 65.93, 65.7, 65.75, 65.77, 65.96, 65.8, 65.69, 65.81, 65.79, 65.78]

Epoch: 13
2024-03-10 04:50:03.462000 epoch: 13 step: 0 cls_loss= 0.29584 (109245 samples/sec)
2024-03-10 04:50:12.841280 epoch: 13 step: 100 cls_loss= 0.39709 (3199 samples/sec)
saving....
2024-03-10 04:50:22.820835------------------------------------------------------ Precision@1: 65.72% 

[65.9, 65.88, 65.75, 65.93, 65.7, 65.75, 65.77, 65.96, 65.8, 65.69, 65.81, 65.79, 65.78, 65.72]

Epoch: 14
2024-03-10 04:50:23.084768 epoch: 14 step: 0 cls_loss= 0.39271 (114399 samples/sec)
2024-03-10 04:50:32.415786 epoch: 14 step: 100 cls_loss= 0.38243 (3215 samples/sec)
saving....
2024-03-10 04:50:42.357460------------------------------------------------------ Precision@1: 65.97% 

[65.9, 65.88, 65.75, 65.93, 65.7, 65.75, 65.77, 65.96, 65.8, 65.69, 65.81, 65.79, 65.78, 65.72, 65.97]

Epoch: 15
2024-03-10 04:50:42.619562 epoch: 15 step: 0 cls_loss= 0.31022 (115137 samples/sec)
2024-03-10 04:50:51.963282 epoch: 15 step: 100 cls_loss= 0.33096 (3211 samples/sec)
saving....
2024-03-10 04:51:01.935547------------------------------------------------------ Precision@1: 65.83% 

[65.9, 65.88, 65.75, 65.93, 65.7, 65.75, 65.77, 65.96, 65.8, 65.69, 65.81, 65.79, 65.78, 65.72, 65.97, 65.83]

Epoch: 16
2024-03-10 04:51:02.183741 epoch: 16 step: 0 cls_loss= 0.32865 (121640 samples/sec)
2024-03-10 04:51:11.581857 epoch: 16 step: 100 cls_loss= 0.32692 (3192 samples/sec)
saving....
2024-03-10 04:51:21.566392------------------------------------------------------ Precision@1: 65.77% 

[65.9, 65.88, 65.75, 65.93, 65.7, 65.75, 65.77, 65.96, 65.8, 65.69, 65.81, 65.79, 65.78, 65.72, 65.97, 65.83, 65.77]

Epoch: 17
2024-03-10 04:51:21.846530 epoch: 17 step: 0 cls_loss= 0.30350 (107620 samples/sec)
2024-03-10 04:51:31.210447 epoch: 17 step: 100 cls_loss= 0.40561 (3204 samples/sec)
saving....
2024-03-10 04:51:41.153588------------------------------------------------------ Precision@1: 65.80% 

[65.9, 65.88, 65.75, 65.93, 65.7, 65.75, 65.77, 65.96, 65.8, 65.69, 65.81, 65.79, 65.78, 65.72, 65.97, 65.83, 65.77, 65.8]

Epoch: 18
2024-03-10 04:51:41.421223 epoch: 18 step: 0 cls_loss= 0.35929 (112716 samples/sec)
2024-03-10 04:51:50.763182 epoch: 18 step: 100 cls_loss= 0.30796 (3211 samples/sec)
saving....
2024-03-10 04:52:00.696850------------------------------------------------------ Precision@1: 65.79% 

[65.9, 65.88, 65.75, 65.93, 65.7, 65.75, 65.77, 65.96, 65.8, 65.69, 65.81, 65.79, 65.78, 65.72, 65.97, 65.83, 65.77, 65.8, 65.79]

Epoch: 19
2024-03-10 04:52:00.965972 epoch: 19 step: 0 cls_loss= 0.34305 (112113 samples/sec)
2024-03-10 04:52:10.451809 epoch: 19 step: 100 cls_loss= 0.35350 (3163 samples/sec)
saving....
2024-03-10 04:52:20.473357------------------------------------------------------ Precision@1: 65.66% 

[65.9, 65.88, 65.75, 65.93, 65.7, 65.75, 65.77, 65.96, 65.8, 65.69, 65.81, 65.79, 65.78, 65.72, 65.97, 65.83, 65.77, 65.8, 65.79, 65.66]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 04:52:23.364537 epoch: 0 step: 0 cls_loss= 0.28860 (41020 samples/sec)
2024-03-10 04:52:32.739729 epoch: 0 step: 100 cls_loss= 0.42915 (3199 samples/sec)
saving....
2024-03-10 04:52:43.087709------------------------------------------------------ Precision@1: 65.85% 

[65.85]

Epoch: 1
2024-03-10 04:52:43.363308 epoch: 1 step: 0 cls_loss= 0.27248 (109373 samples/sec)
2024-03-10 04:52:52.726967 epoch: 1 step: 100 cls_loss= 0.30657 (3204 samples/sec)
saving....
2024-03-10 04:53:02.665763------------------------------------------------------ Precision@1: 65.60% 

[65.85, 65.6]

Epoch: 2
2024-03-10 04:53:02.930833 epoch: 2 step: 0 cls_loss= 0.37863 (113766 samples/sec)
2024-03-10 04:53:12.294724 epoch: 2 step: 100 cls_loss= 0.30074 (3204 samples/sec)
saving....
2024-03-10 04:53:22.221598------------------------------------------------------ Precision@1: 65.72% 

[65.85, 65.6, 65.72]

Epoch: 3
2024-03-10 04:53:22.474168 epoch: 3 step: 0 cls_loss= 0.34026 (119547 samples/sec)
2024-03-10 04:53:31.838824 epoch: 3 step: 100 cls_loss= 0.28640 (3204 samples/sec)
saving....
2024-03-10 04:53:41.770499------------------------------------------------------ Precision@1: 65.74% 

[65.85, 65.6, 65.72, 65.74]

Epoch: 4
2024-03-10 04:53:42.041754 epoch: 4 step: 0 cls_loss= 0.29950 (111187 samples/sec)
2024-03-10 04:53:51.453576 epoch: 4 step: 100 cls_loss= 0.30034 (3188 samples/sec)
saving....
2024-03-10 04:54:01.381916------------------------------------------------------ Precision@1: 65.75% 

[65.85, 65.6, 65.72, 65.74, 65.75]

Epoch: 5
2024-03-10 04:54:01.648546 epoch: 5 step: 0 cls_loss= 0.31826 (113240 samples/sec)
2024-03-10 04:54:11.019674 epoch: 5 step: 100 cls_loss= 0.41323 (3201 samples/sec)
saving....
2024-03-10 04:54:21.018387------------------------------------------------------ Precision@1: 65.82% 

[65.85, 65.6, 65.72, 65.74, 65.75, 65.82]

Epoch: 6
2024-03-10 04:54:21.304370 epoch: 6 step: 0 cls_loss= 0.33366 (105449 samples/sec)
2024-03-10 04:54:30.694440 epoch: 6 step: 100 cls_loss= 0.29407 (3195 samples/sec)
saving....
2024-03-10 04:54:40.672028------------------------------------------------------ Precision@1: 65.79% 

[65.85, 65.6, 65.72, 65.74, 65.75, 65.82, 65.79]

Epoch: 7
2024-03-10 04:54:40.936910 epoch: 7 step: 0 cls_loss= 0.30332 (113907 samples/sec)
2024-03-10 04:54:50.315159 epoch: 7 step: 100 cls_loss= 0.31867 (3199 samples/sec)
saving....
2024-03-10 04:55:00.221813------------------------------------------------------ Precision@1: 65.72% 

[65.85, 65.6, 65.72, 65.74, 65.75, 65.82, 65.79, 65.72]

Epoch: 8
2024-03-10 04:55:00.475778 epoch: 8 step: 0 cls_loss= 0.27630 (118911 samples/sec)
2024-03-10 04:55:09.867168 epoch: 8 step: 100 cls_loss= 0.32546 (3195 samples/sec)
saving....
2024-03-10 04:55:19.808018------------------------------------------------------ Precision@1: 65.80% 

[65.85, 65.6, 65.72, 65.74, 65.75, 65.82, 65.79, 65.72, 65.8]

Epoch: 9
2024-03-10 04:55:20.074337 epoch: 9 step: 0 cls_loss= 0.34453 (113365 samples/sec)
2024-03-10 04:55:29.525015 epoch: 9 step: 100 cls_loss= 0.30423 (3175 samples/sec)
saving....
2024-03-10 04:55:39.440541------------------------------------------------------ Precision@1: 65.67% 

[65.85, 65.6, 65.72, 65.74, 65.75, 65.82, 65.79, 65.72, 65.8, 65.67]

Epoch: 10
2024-03-10 04:55:39.713689 epoch: 10 step: 0 cls_loss= 0.21747 (110462 samples/sec)
2024-03-10 04:55:49.090698 epoch: 10 step: 100 cls_loss= 0.33096 (3199 samples/sec)
saving....
2024-03-10 04:55:59.030885------------------------------------------------------ Precision@1: 65.89% 

[65.85, 65.6, 65.72, 65.74, 65.75, 65.82, 65.79, 65.72, 65.8, 65.67, 65.89]

Epoch: 11
2024-03-10 04:55:59.295953 epoch: 11 step: 0 cls_loss= 0.39166 (113849 samples/sec)
2024-03-10 04:56:08.675029 epoch: 11 step: 100 cls_loss= 0.34112 (3199 samples/sec)
saving....
2024-03-10 04:56:18.640760------------------------------------------------------ Precision@1: 65.82% 

[65.85, 65.6, 65.72, 65.74, 65.75, 65.82, 65.79, 65.72, 65.8, 65.67, 65.89, 65.82]

Epoch: 12
2024-03-10 04:56:18.904942 epoch: 12 step: 0 cls_loss= 0.27332 (114249 samples/sec)
2024-03-10 04:56:28.304012 epoch: 12 step: 100 cls_loss= 0.36479 (3192 samples/sec)
saving....
2024-03-10 04:56:38.253937------------------------------------------------------ Precision@1: 65.85% 

[65.85, 65.6, 65.72, 65.74, 65.75, 65.82, 65.79, 65.72, 65.8, 65.67, 65.89, 65.82, 65.85]

Epoch: 13
2024-03-10 04:56:38.523951 epoch: 13 step: 0 cls_loss= 0.32626 (111768 samples/sec)
2024-03-10 04:56:47.885120 epoch: 13 step: 100 cls_loss= 0.35900 (3205 samples/sec)
saving....
2024-03-10 04:56:57.794298------------------------------------------------------ Precision@1: 65.72% 

[65.85, 65.6, 65.72, 65.74, 65.75, 65.82, 65.79, 65.72, 65.8, 65.67, 65.89, 65.82, 65.85, 65.72]

Epoch: 14
2024-03-10 04:56:58.054274 epoch: 14 step: 0 cls_loss= 0.26928 (116169 samples/sec)
2024-03-10 04:57:07.507695 epoch: 14 step: 100 cls_loss= 0.28361 (3174 samples/sec)
saving....
2024-03-10 04:57:17.473921------------------------------------------------------ Precision@1: 65.65% 

[65.85, 65.6, 65.72, 65.74, 65.75, 65.82, 65.79, 65.72, 65.8, 65.67, 65.89, 65.82, 65.85, 65.72, 65.65]

Epoch: 15
2024-03-10 04:57:17.749254 epoch: 15 step: 0 cls_loss= 0.32668 (109578 samples/sec)
2024-03-10 04:57:27.107973 epoch: 15 step: 100 cls_loss= 0.36027 (3206 samples/sec)
saving....
2024-03-10 04:57:37.029385------------------------------------------------------ Precision@1: 65.68% 

[65.85, 65.6, 65.72, 65.74, 65.75, 65.82, 65.79, 65.72, 65.8, 65.67, 65.89, 65.82, 65.85, 65.72, 65.65, 65.68]

Epoch: 16
2024-03-10 04:57:37.283778 epoch: 16 step: 0 cls_loss= 0.24365 (118560 samples/sec)
2024-03-10 04:57:46.630538 epoch: 16 step: 100 cls_loss= 0.35915 (3210 samples/sec)
saving....
2024-03-10 04:57:56.570511------------------------------------------------------ Precision@1: 65.82% 

[65.85, 65.6, 65.72, 65.74, 65.75, 65.82, 65.79, 65.72, 65.8, 65.67, 65.89, 65.82, 65.85, 65.72, 65.65, 65.68, 65.82]

Epoch: 17
2024-03-10 04:57:56.814573 epoch: 17 step: 0 cls_loss= 0.30931 (123739 samples/sec)
2024-03-10 04:58:06.207216 epoch: 17 step: 100 cls_loss= 0.41316 (3194 samples/sec)
saving....
2024-03-10 04:58:16.142689------------------------------------------------------ Precision@1: 65.87% 

[65.85, 65.6, 65.72, 65.74, 65.75, 65.82, 65.79, 65.72, 65.8, 65.67, 65.89, 65.82, 65.85, 65.72, 65.65, 65.68, 65.82, 65.87]

Epoch: 18
2024-03-10 04:58:16.420700 epoch: 18 step: 0 cls_loss= 0.31631 (108501 samples/sec)
2024-03-10 04:58:25.818075 epoch: 18 step: 100 cls_loss= 0.38146 (3193 samples/sec)
saving....
2024-03-10 04:58:35.740752------------------------------------------------------ Precision@1: 65.93% 

[65.85, 65.6, 65.72, 65.74, 65.75, 65.82, 65.79, 65.72, 65.8, 65.67, 65.89, 65.82, 65.85, 65.72, 65.65, 65.68, 65.82, 65.87, 65.93]

Epoch: 19
2024-03-10 04:58:36.010556 epoch: 19 step: 0 cls_loss= 0.37156 (111672 samples/sec)
2024-03-10 04:58:45.404747 epoch: 19 step: 100 cls_loss= 0.33458 (3194 samples/sec)
saving....
2024-03-10 04:58:55.345682------------------------------------------------------ Precision@1: 65.80% 

[65.85, 65.6, 65.72, 65.74, 65.75, 65.82, 65.79, 65.72, 65.8, 65.67, 65.89, 65.82, 65.85, 65.72, 65.65, 65.68, 65.82, 65.87, 65.93, 65.8]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => SSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:117: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 04:58:58.238867 epoch: 0 step: 0 cls_loss= 0.32830 (40397 samples/sec)
2024-03-10 04:59:07.621538 epoch: 0 step: 100 cls_loss= 0.33549 (3197 samples/sec)
saving....
2024-03-10 04:59:17.819602------------------------------------------------------ Precision@1: 65.71% 

[65.71]

Epoch: 1
2024-03-10 04:59:18.071341 epoch: 1 step: 0 cls_loss= 0.31895 (119897 samples/sec)
2024-03-10 04:59:27.457950 epoch: 1 step: 100 cls_loss= 0.33167 (3196 samples/sec)
saving....
2024-03-10 04:59:37.413229------------------------------------------------------ Precision@1: 65.74% 

[65.71, 65.74]

Epoch: 2
2024-03-10 04:59:37.672088 epoch: 2 step: 0 cls_loss= 0.28748 (116567 samples/sec)
2024-03-10 04:59:47.036779 epoch: 2 step: 100 cls_loss= 0.32795 (3204 samples/sec)
saving....
2024-03-10 04:59:56.973433------------------------------------------------------ Precision@1: 66.10% 

[65.71, 65.74, 66.1]

Epoch: 3
2024-03-10 04:59:57.237235 epoch: 3 step: 0 cls_loss= 0.32988 (114421 samples/sec)
2024-03-10 05:00:06.621011 epoch: 3 step: 100 cls_loss= 0.31015 (3197 samples/sec)
saving....
2024-03-10 05:00:16.535174------------------------------------------------------ Precision@1: 65.83% 

[65.71, 65.74, 66.1, 65.83]

Epoch: 4
2024-03-10 05:00:16.787313 epoch: 4 step: 0 cls_loss= 0.32107 (119728 samples/sec)
2024-03-10 05:00:26.153988 epoch: 4 step: 100 cls_loss= 0.29209 (3203 samples/sec)
saving....
2024-03-10 05:00:36.056642------------------------------------------------------ Precision@1: 65.66% 

[65.71, 65.74, 66.1, 65.83, 65.66]

Epoch: 5
2024-03-10 05:00:36.313999 epoch: 5 step: 0 cls_loss= 0.34266 (117295 samples/sec)
2024-03-10 05:00:45.677902 epoch: 5 step: 100 cls_loss= 0.38415 (3204 samples/sec)
saving....
2024-03-10 05:00:55.614140------------------------------------------------------ Precision@1: 65.66% 

[65.71, 65.74, 66.1, 65.83, 65.66, 65.66]

Epoch: 6
2024-03-10 05:00:55.872201 epoch: 6 step: 0 cls_loss= 0.29985 (116949 samples/sec)
2024-03-10 05:01:05.268890 epoch: 6 step: 100 cls_loss= 0.32933 (3193 samples/sec)
saving....
2024-03-10 05:01:15.203880------------------------------------------------------ Precision@1: 65.84% 

[65.71, 65.74, 66.1, 65.83, 65.66, 65.66, 65.84]

Epoch: 7
2024-03-10 05:01:15.465507 epoch: 7 step: 0 cls_loss= 0.28702 (115193 samples/sec)
2024-03-10 05:01:24.884953 epoch: 7 step: 100 cls_loss= 0.32500 (3185 samples/sec)
saving....
2024-03-10 05:01:34.994880------------------------------------------------------ Precision@1: 65.89% 

[65.71, 65.74, 66.1, 65.83, 65.66, 65.66, 65.84, 65.89]

Epoch: 8
2024-03-10 05:01:35.250933 epoch: 8 step: 0 cls_loss= 0.40658 (117840 samples/sec)
2024-03-10 05:01:44.655889 epoch: 8 step: 100 cls_loss= 0.29766 (3190 samples/sec)
saving....
2024-03-10 05:01:54.620486------------------------------------------------------ Precision@1: 65.87% 

[65.71, 65.74, 66.1, 65.83, 65.66, 65.66, 65.84, 65.89, 65.87]

Epoch: 9
2024-03-10 05:01:54.885182 epoch: 9 step: 0 cls_loss= 0.32961 (114008 samples/sec)
2024-03-10 05:02:04.271728 epoch: 9 step: 100 cls_loss= 0.26546 (3196 samples/sec)
saving....
2024-03-10 05:02:14.303508------------------------------------------------------ Precision@1: 65.86% 

[65.71, 65.74, 66.1, 65.83, 65.66, 65.66, 65.84, 65.89, 65.87, 65.86]

Epoch: 10
2024-03-10 05:02:14.555474 epoch: 10 step: 0 cls_loss= 0.30541 (119803 samples/sec)
2024-03-10 05:02:23.916356 epoch: 10 step: 100 cls_loss= 0.30699 (3205 samples/sec)
saving....
2024-03-10 05:02:33.875307------------------------------------------------------ Precision@1: 65.72% 

[65.71, 65.74, 66.1, 65.83, 65.66, 65.66, 65.84, 65.89, 65.87, 65.86, 65.72]

Epoch: 11
2024-03-10 05:02:34.120114 epoch: 11 step: 0 cls_loss= 0.32426 (123307 samples/sec)
2024-03-10 05:02:43.568593 epoch: 11 step: 100 cls_loss= 0.27906 (3175 samples/sec)
saving....
2024-03-10 05:02:53.568883------------------------------------------------------ Precision@1: 65.87% 

[65.71, 65.74, 66.1, 65.83, 65.66, 65.66, 65.84, 65.89, 65.87, 65.86, 65.72, 65.87]

Epoch: 12
2024-03-10 05:02:53.821499 epoch: 12 step: 0 cls_loss= 0.40190 (119518 samples/sec)
2024-03-10 05:03:03.187991 epoch: 12 step: 100 cls_loss= 0.28804 (3203 samples/sec)
saving....
2024-03-10 05:03:13.134004------------------------------------------------------ Precision@1: 65.57% 

[65.71, 65.74, 66.1, 65.83, 65.66, 65.66, 65.84, 65.89, 65.87, 65.86, 65.72, 65.87, 65.57]

Epoch: 13
2024-03-10 05:03:13.400081 epoch: 13 step: 0 cls_loss= 0.31001 (113367 samples/sec)
2024-03-10 05:03:22.802517 epoch: 13 step: 100 cls_loss= 0.31613 (3191 samples/sec)
saving....
2024-03-10 05:03:32.728312------------------------------------------------------ Precision@1: 65.91% 

[65.71, 65.74, 66.1, 65.83, 65.66, 65.66, 65.84, 65.89, 65.87, 65.86, 65.72, 65.87, 65.57, 65.91]

Epoch: 14
2024-03-10 05:03:32.982356 epoch: 14 step: 0 cls_loss= 0.32525 (118869 samples/sec)
2024-03-10 05:03:42.323498 epoch: 14 step: 100 cls_loss= 0.37457 (3212 samples/sec)
saving....
2024-03-10 05:03:52.263948------------------------------------------------------ Precision@1: 65.70% 

[65.71, 65.74, 66.1, 65.83, 65.66, 65.66, 65.84, 65.89, 65.87, 65.86, 65.72, 65.87, 65.57, 65.91, 65.7]

Epoch: 15
2024-03-10 05:03:52.532329 epoch: 15 step: 0 cls_loss= 0.35687 (112429 samples/sec)
2024-03-10 05:04:01.895781 epoch: 15 step: 100 cls_loss= 0.32120 (3204 samples/sec)
saving....
2024-03-10 05:04:11.809269------------------------------------------------------ Precision@1: 65.72% 

[65.71, 65.74, 66.1, 65.83, 65.66, 65.66, 65.84, 65.89, 65.87, 65.86, 65.72, 65.87, 65.57, 65.91, 65.7, 65.72]

Epoch: 16
2024-03-10 05:04:12.071125 epoch: 16 step: 0 cls_loss= 0.30280 (115317 samples/sec)
2024-03-10 05:04:21.611045 epoch: 16 step: 100 cls_loss= 0.33018 (3145 samples/sec)
saving....
2024-03-10 05:04:31.681378------------------------------------------------------ Precision@1: 65.75% 

[65.71, 65.74, 66.1, 65.83, 65.66, 65.66, 65.84, 65.89, 65.87, 65.86, 65.72, 65.87, 65.57, 65.91, 65.7, 65.72, 65.75]

Epoch: 17
2024-03-10 05:04:31.944476 epoch: 17 step: 0 cls_loss= 0.35178 (114705 samples/sec)
2024-03-10 05:04:41.348689 epoch: 17 step: 100 cls_loss= 0.33662 (3190 samples/sec)
saving....
2024-03-10 05:04:51.393511------------------------------------------------------ Precision@1: 65.80% 

[65.71, 65.74, 66.1, 65.83, 65.66, 65.66, 65.84, 65.89, 65.87, 65.86, 65.72, 65.87, 65.57, 65.91, 65.7, 65.72, 65.75, 65.8]

Epoch: 18
2024-03-10 05:04:51.666927 epoch: 18 step: 0 cls_loss= 0.27969 (110264 samples/sec)
2024-03-10 05:05:01.071938 epoch: 18 step: 100 cls_loss= 0.38958 (3190 samples/sec)
saving....
2024-03-10 05:05:11.078328------------------------------------------------------ Precision@1: 65.74% 

[65.71, 65.74, 66.1, 65.83, 65.66, 65.66, 65.84, 65.89, 65.87, 65.86, 65.72, 65.87, 65.57, 65.91, 65.7, 65.72, 65.75, 65.8, 65.74]

Epoch: 19
2024-03-10 05:05:11.336598 epoch: 19 step: 0 cls_loss= 0.32627 (116801 samples/sec)
2024-03-10 05:05:20.757661 epoch: 19 step: 100 cls_loss= 0.32421 (3185 samples/sec)
saving....
2024-03-10 05:05:30.772839------------------------------------------------------ Precision@1: 65.80% 

[65.71, 65.74, 66.1, 65.83, 65.66, 65.66, 65.84, 65.89, 65.87, 65.86, 65.72, 65.87, 65.57, 65.91, 65.7, 65.72, 65.75, 65.8, 65.74, 65.8]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 05:05:33.627462 epoch: 0 step: 0 cls_loss= 0.27643 (41399 samples/sec)
2024-03-10 05:05:42.955176 epoch: 0 step: 100 cls_loss= 0.28846 (3216 samples/sec)
saving....
2024-03-10 05:05:53.071739------------------------------------------------------ Precision@1: 65.48% 

[65.48]

Epoch: 1
2024-03-10 05:05:53.333897 epoch: 1 step: 0 cls_loss= 0.35812 (114949 samples/sec)
2024-03-10 05:06:02.667157 epoch: 1 step: 100 cls_loss= 0.34500 (3217 samples/sec)
saving....
2024-03-10 05:06:12.589270------------------------------------------------------ Precision@1: 65.88% 

[65.48, 65.88]

Epoch: 2
2024-03-10 05:06:12.847267 epoch: 2 step: 0 cls_loss= 0.29364 (116996 samples/sec)
2024-03-10 05:06:22.222396 epoch: 2 step: 100 cls_loss= 0.27948 (3203 samples/sec)
saving....
2024-03-10 05:06:32.171507------------------------------------------------------ Precision@1: 65.73% 

[65.48, 65.88, 65.73]

Epoch: 3
2024-03-10 05:06:32.427872 epoch: 3 step: 0 cls_loss= 0.34007 (117676 samples/sec)
2024-03-10 05:06:41.798494 epoch: 3 step: 100 cls_loss= 0.34842 (3204 samples/sec)
saving....
2024-03-10 05:06:51.767354------------------------------------------------------ Precision@1: 65.72% 

[65.48, 65.88, 65.73, 65.72]

Epoch: 4
2024-03-10 05:06:52.036326 epoch: 4 step: 0 cls_loss= 0.35833 (112237 samples/sec)
2024-03-10 05:07:01.373201 epoch: 4 step: 100 cls_loss= 0.34793 (3216 samples/sec)
saving....
2024-03-10 05:07:11.332209------------------------------------------------------ Precision@1: 65.72% 

[65.48, 65.88, 65.73, 65.72, 65.72]

Epoch: 5
2024-03-10 05:07:11.588592 epoch: 5 step: 0 cls_loss= 0.36320 (117680 samples/sec)
2024-03-10 05:07:20.935791 epoch: 5 step: 100 cls_loss= 0.32367 (3212 samples/sec)
saving....
2024-03-10 05:07:30.870445------------------------------------------------------ Precision@1: 65.51% 

[65.48, 65.88, 65.73, 65.72, 65.72, 65.51]

Epoch: 6
2024-03-10 05:07:31.134598 epoch: 6 step: 0 cls_loss= 0.35712 (114223 samples/sec)
2024-03-10 05:07:40.487712 epoch: 6 step: 100 cls_loss= 0.29114 (3210 samples/sec)
saving....
2024-03-10 05:07:50.456459------------------------------------------------------ Precision@1: 65.73% 

[65.48, 65.88, 65.73, 65.72, 65.72, 65.51, 65.73]

Epoch: 7
2024-03-10 05:07:50.716751 epoch: 7 step: 0 cls_loss= 0.33412 (115977 samples/sec)
2024-03-10 05:08:00.051569 epoch: 7 step: 100 cls_loss= 0.33526 (3214 samples/sec)
saving....
2024-03-10 05:08:10.032478------------------------------------------------------ Precision@1: 65.82% 

[65.48, 65.88, 65.73, 65.72, 65.72, 65.51, 65.73, 65.82]

Epoch: 8
2024-03-10 05:08:10.288839 epoch: 8 step: 0 cls_loss= 0.34229 (117722 samples/sec)
2024-03-10 05:08:19.635768 epoch: 8 step: 100 cls_loss= 0.35107 (3212 samples/sec)
saving....
2024-03-10 05:08:29.596466------------------------------------------------------ Precision@1: 65.53% 

[65.48, 65.88, 65.73, 65.72, 65.72, 65.51, 65.73, 65.82, 65.53]

Epoch: 9
2024-03-10 05:08:29.846388 epoch: 9 step: 0 cls_loss= 0.32251 (120761 samples/sec)
2024-03-10 05:08:39.169785 epoch: 9 step: 100 cls_loss= 0.32711 (3220 samples/sec)
saving....
2024-03-10 05:08:49.110693------------------------------------------------------ Precision@1: 65.72% 

[65.48, 65.88, 65.73, 65.72, 65.72, 65.51, 65.73, 65.82, 65.53, 65.72]

Epoch: 10
2024-03-10 05:08:49.369596 epoch: 10 step: 0 cls_loss= 0.32224 (116487 samples/sec)
2024-03-10 05:08:58.713738 epoch: 10 step: 100 cls_loss= 0.36015 (3211 samples/sec)
saving....
2024-03-10 05:09:08.663820------------------------------------------------------ Precision@1: 65.87% 

[65.48, 65.88, 65.73, 65.72, 65.72, 65.51, 65.73, 65.82, 65.53, 65.72, 65.87]

Epoch: 11
2024-03-10 05:09:08.930090 epoch: 11 step: 0 cls_loss= 0.27542 (113355 samples/sec)
2024-03-10 05:09:18.265636 epoch: 11 step: 100 cls_loss= 0.32266 (3215 samples/sec)
saving....
2024-03-10 05:09:28.208778------------------------------------------------------ Precision@1: 65.74% 

[65.48, 65.88, 65.73, 65.72, 65.72, 65.51, 65.73, 65.82, 65.53, 65.72, 65.87, 65.74]

Epoch: 12
2024-03-10 05:09:28.444181 epoch: 12 step: 0 cls_loss= 0.37391 (128324 samples/sec)
2024-03-10 05:09:37.846167 epoch: 12 step: 100 cls_loss= 0.34127 (3193 samples/sec)
saving....
2024-03-10 05:09:47.807056------------------------------------------------------ Precision@1: 65.76% 

[65.48, 65.88, 65.73, 65.72, 65.72, 65.51, 65.73, 65.82, 65.53, 65.72, 65.87, 65.74, 65.76]

Epoch: 13
2024-03-10 05:09:48.051942 epoch: 13 step: 0 cls_loss= 0.33005 (123233 samples/sec)
2024-03-10 05:09:57.470098 epoch: 13 step: 100 cls_loss= 0.28100 (3188 samples/sec)
saving....
2024-03-10 05:10:07.472767------------------------------------------------------ Precision@1: 65.77% 

[65.48, 65.88, 65.73, 65.72, 65.72, 65.51, 65.73, 65.82, 65.53, 65.72, 65.87, 65.74, 65.76, 65.77]

Epoch: 14
2024-03-10 05:10:07.714596 epoch: 14 step: 0 cls_loss= 0.30068 (124736 samples/sec)
2024-03-10 05:10:17.088364 epoch: 14 step: 100 cls_loss= 0.32938 (3201 samples/sec)
saving....
2024-03-10 05:10:27.073920------------------------------------------------------ Precision@1: 65.90% 

[65.48, 65.88, 65.73, 65.72, 65.72, 65.51, 65.73, 65.82, 65.53, 65.72, 65.87, 65.74, 65.76, 65.77, 65.9]

Epoch: 15
2024-03-10 05:10:27.324402 epoch: 15 step: 0 cls_loss= 0.29764 (120425 samples/sec)
2024-03-10 05:10:36.680052 epoch: 15 step: 100 cls_loss= 0.29821 (3209 samples/sec)
saving....
2024-03-10 05:10:46.744351------------------------------------------------------ Precision@1: 65.65% 

[65.48, 65.88, 65.73, 65.72, 65.72, 65.51, 65.73, 65.82, 65.53, 65.72, 65.87, 65.74, 65.76, 65.77, 65.9, 65.65]

Epoch: 16
2024-03-10 05:10:47.013301 epoch: 16 step: 0 cls_loss= 0.31158 (112190 samples/sec)
2024-03-10 05:10:56.369995 epoch: 16 step: 100 cls_loss= 0.31680 (3209 samples/sec)
saving....
2024-03-10 05:11:06.561419------------------------------------------------------ Precision@1: 65.77% 

[65.48, 65.88, 65.73, 65.72, 65.72, 65.51, 65.73, 65.82, 65.53, 65.72, 65.87, 65.74, 65.76, 65.77, 65.9, 65.65, 65.77]

Epoch: 17
2024-03-10 05:11:06.812613 epoch: 17 step: 0 cls_loss= 0.32178 (120156 samples/sec)
2024-03-10 05:11:16.178793 epoch: 17 step: 100 cls_loss= 0.32695 (3206 samples/sec)
saving....
2024-03-10 05:11:26.080636------------------------------------------------------ Precision@1: 65.79% 

[65.48, 65.88, 65.73, 65.72, 65.72, 65.51, 65.73, 65.82, 65.53, 65.72, 65.87, 65.74, 65.76, 65.77, 65.9, 65.65, 65.77, 65.79]

Epoch: 18
2024-03-10 05:11:26.352137 epoch: 18 step: 0 cls_loss= 0.29606 (111193 samples/sec)
2024-03-10 05:11:35.752485 epoch: 18 step: 100 cls_loss= 0.34095 (3194 samples/sec)
saving....
2024-03-10 05:11:45.681930------------------------------------------------------ Precision@1: 65.91% 

[65.48, 65.88, 65.73, 65.72, 65.72, 65.51, 65.73, 65.82, 65.53, 65.72, 65.87, 65.74, 65.76, 65.77, 65.9, 65.65, 65.77, 65.79, 65.91]

Epoch: 19
2024-03-10 05:11:45.949506 epoch: 19 step: 0 cls_loss= 0.35339 (112765 samples/sec)
2024-03-10 05:11:55.253733 epoch: 19 step: 100 cls_loss= 0.27224 (3225 samples/sec)
saving....
2024-03-10 05:12:05.132448------------------------------------------------------ Precision@1: 65.77% 

[65.48, 65.88, 65.73, 65.72, 65.72, 65.51, 65.73, 65.82, 65.53, 65.72, 65.87, 65.74, 65.76, 65.77, 65.9, 65.65, 65.77, 65.79, 65.91, 65.77]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 05:12:08.057594 epoch: 0 step: 0 cls_loss= 0.27989 (40330 samples/sec)
2024-03-10 05:12:17.371043 epoch: 0 step: 100 cls_loss= 0.29157 (3221 samples/sec)
saving....
2024-03-10 05:12:27.563683------------------------------------------------------ Precision@1: 65.72% 

[65.72]

Epoch: 1
2024-03-10 05:12:27.831252 epoch: 1 step: 0 cls_loss= 0.36949 (112749 samples/sec)
2024-03-10 05:12:37.171030 epoch: 1 step: 100 cls_loss= 0.33077 (3213 samples/sec)
saving....
2024-03-10 05:12:47.114729------------------------------------------------------ Precision@1: 65.67% 

[65.72, 65.67]

Epoch: 2
2024-03-10 05:12:47.363234 epoch: 2 step: 0 cls_loss= 0.33806 (121440 samples/sec)
2024-03-10 05:12:56.699667 epoch: 2 step: 100 cls_loss= 0.32440 (3216 samples/sec)
saving....
2024-03-10 05:13:06.591855------------------------------------------------------ Precision@1: 65.92% 

[65.72, 65.67, 65.92]

Epoch: 3
2024-03-10 05:13:06.838245 epoch: 3 step: 0 cls_loss= 0.30326 (122474 samples/sec)
2024-03-10 05:13:16.155295 epoch: 3 step: 100 cls_loss= 0.32363 (3223 samples/sec)
saving....
2024-03-10 05:13:26.097870------------------------------------------------------ Precision@1: 65.66% 

[65.72, 65.67, 65.92, 65.66]

Epoch: 4
2024-03-10 05:13:26.359576 epoch: 4 step: 0 cls_loss= 0.41353 (115245 samples/sec)
2024-03-10 05:13:35.723967 epoch: 4 step: 100 cls_loss= 0.27055 (3205 samples/sec)
saving....
2024-03-10 05:13:45.650122------------------------------------------------------ Precision@1: 65.82% 

[65.72, 65.67, 65.92, 65.66, 65.82]

Epoch: 5
2024-03-10 05:13:45.911545 epoch: 5 step: 0 cls_loss= 0.29011 (115427 samples/sec)
2024-03-10 05:13:55.269035 epoch: 5 step: 100 cls_loss= 0.33388 (3209 samples/sec)
saving....
2024-03-10 05:14:05.191946------------------------------------------------------ Precision@1: 65.58% 

[65.72, 65.67, 65.92, 65.66, 65.82, 65.58]

Epoch: 6
2024-03-10 05:14:05.454609 epoch: 6 step: 0 cls_loss= 0.35225 (114881 samples/sec)
2024-03-10 05:14:14.772471 epoch: 6 step: 100 cls_loss= 0.31811 (3222 samples/sec)
saving....
2024-03-10 05:14:24.692568------------------------------------------------------ Precision@1: 65.94% 

[65.72, 65.67, 65.92, 65.66, 65.82, 65.58, 65.94]

Epoch: 7
2024-03-10 05:14:24.944247 epoch: 7 step: 0 cls_loss= 0.32325 (119919 samples/sec)
2024-03-10 05:14:34.268829 epoch: 7 step: 100 cls_loss= 0.31930 (3220 samples/sec)
saving....
2024-03-10 05:14:44.169321------------------------------------------------------ Precision@1: 65.86% 

[65.72, 65.67, 65.92, 65.66, 65.82, 65.58, 65.94, 65.86]

Epoch: 8
2024-03-10 05:14:44.423186 epoch: 8 step: 0 cls_loss= 0.36822 (118918 samples/sec)
2024-03-10 05:14:53.767530 epoch: 8 step: 100 cls_loss= 0.30475 (3213 samples/sec)
saving....
2024-03-10 05:15:03.685607------------------------------------------------------ Precision@1: 65.88% 

[65.72, 65.67, 65.92, 65.66, 65.82, 65.58, 65.94, 65.86, 65.88]

Epoch: 9
2024-03-10 05:15:03.953106 epoch: 9 step: 0 cls_loss= 0.33563 (112757 samples/sec)
2024-03-10 05:15:13.256628 epoch: 9 step: 100 cls_loss= 0.36874 (3227 samples/sec)
saving....
2024-03-10 05:15:23.144538------------------------------------------------------ Precision@1: 65.96% 

[65.72, 65.67, 65.92, 65.66, 65.82, 65.58, 65.94, 65.86, 65.88, 65.96]

Epoch: 10
2024-03-10 05:15:23.391572 epoch: 10 step: 0 cls_loss= 0.31813 (122153 samples/sec)
2024-03-10 05:15:32.742115 epoch: 10 step: 100 cls_loss= 0.22884 (3211 samples/sec)
saving....
2024-03-10 05:15:42.663837------------------------------------------------------ Precision@1: 65.73% 

[65.72, 65.67, 65.92, 65.66, 65.82, 65.58, 65.94, 65.86, 65.88, 65.96, 65.73]

Epoch: 11
2024-03-10 05:15:42.925819 epoch: 11 step: 0 cls_loss= 0.37815 (115194 samples/sec)
2024-03-10 05:15:52.280865 epoch: 11 step: 100 cls_loss= 0.34763 (3210 samples/sec)
saving....
2024-03-10 05:16:02.198723------------------------------------------------------ Precision@1: 65.79% 

[65.72, 65.67, 65.92, 65.66, 65.82, 65.58, 65.94, 65.86, 65.88, 65.96, 65.73, 65.79]

Epoch: 12
2024-03-10 05:16:02.472590 epoch: 12 step: 0 cls_loss= 0.34607 (110127 samples/sec)
2024-03-10 05:16:11.843160 epoch: 12 step: 100 cls_loss= 0.32916 (3204 samples/sec)
saving....
2024-03-10 05:16:21.721029------------------------------------------------------ Precision@1: 65.65% 

[65.72, 65.67, 65.92, 65.66, 65.82, 65.58, 65.94, 65.86, 65.88, 65.96, 65.73, 65.79, 65.65]

Epoch: 13
2024-03-10 05:16:21.977633 epoch: 13 step: 0 cls_loss= 0.35918 (117598 samples/sec)
2024-03-10 05:16:31.271805 epoch: 13 step: 100 cls_loss= 0.33562 (3231 samples/sec)
saving....
2024-03-10 05:16:41.101750------------------------------------------------------ Precision@1: 65.78% 

[65.72, 65.67, 65.92, 65.66, 65.82, 65.58, 65.94, 65.86, 65.88, 65.96, 65.73, 65.79, 65.65, 65.78]

Epoch: 14
2024-03-10 05:16:41.354176 epoch: 14 step: 0 cls_loss= 0.27335 (119529 samples/sec)
2024-03-10 05:16:50.672356 epoch: 14 step: 100 cls_loss= 0.29901 (3222 samples/sec)
saving....
2024-03-10 05:17:00.590658------------------------------------------------------ Precision@1: 65.81% 

[65.72, 65.67, 65.92, 65.66, 65.82, 65.58, 65.94, 65.86, 65.88, 65.96, 65.73, 65.79, 65.65, 65.78, 65.81]

Epoch: 15
2024-03-10 05:17:00.843993 epoch: 15 step: 0 cls_loss= 0.37600 (119155 samples/sec)
2024-03-10 05:17:10.238886 epoch: 15 step: 100 cls_loss= 0.29967 (3196 samples/sec)
saving....
2024-03-10 05:17:20.169021------------------------------------------------------ Precision@1: 65.67% 

[65.72, 65.67, 65.92, 65.66, 65.82, 65.58, 65.94, 65.86, 65.88, 65.96, 65.73, 65.79, 65.65, 65.78, 65.81, 65.67]

Epoch: 16
2024-03-10 05:17:20.432525 epoch: 16 step: 0 cls_loss= 0.30452 (114478 samples/sec)
2024-03-10 05:17:29.771012 epoch: 16 step: 100 cls_loss= 0.32766 (3215 samples/sec)
saving....
2024-03-10 05:17:39.695247------------------------------------------------------ Precision@1: 65.67% 

[65.72, 65.67, 65.92, 65.66, 65.82, 65.58, 65.94, 65.86, 65.88, 65.96, 65.73, 65.79, 65.65, 65.78, 65.81, 65.67, 65.67]

Epoch: 17
2024-03-10 05:17:39.949022 epoch: 17 step: 0 cls_loss= 0.34180 (118929 samples/sec)
2024-03-10 05:17:49.306097 epoch: 17 step: 100 cls_loss= 0.35057 (3209 samples/sec)
saving....
2024-03-10 05:17:59.228565------------------------------------------------------ Precision@1: 65.86% 

[65.72, 65.67, 65.92, 65.66, 65.82, 65.58, 65.94, 65.86, 65.88, 65.96, 65.73, 65.79, 65.65, 65.78, 65.81, 65.67, 65.67, 65.86]

Epoch: 18
2024-03-10 05:17:59.487788 epoch: 18 step: 0 cls_loss= 0.34106 (116420 samples/sec)
2024-03-10 05:18:08.812245 epoch: 18 step: 100 cls_loss= 0.29469 (3218 samples/sec)
saving....
2024-03-10 05:18:18.700859------------------------------------------------------ Precision@1: 65.84% 

[65.72, 65.67, 65.92, 65.66, 65.82, 65.58, 65.94, 65.86, 65.88, 65.96, 65.73, 65.79, 65.65, 65.78, 65.81, 65.67, 65.67, 65.86, 65.84]

Epoch: 19
2024-03-10 05:18:18.964293 epoch: 19 step: 0 cls_loss= 0.32329 (114537 samples/sec)
2024-03-10 05:18:28.288770 epoch: 19 step: 100 cls_loss= 0.29638 (3220 samples/sec)
saving....
2024-03-10 05:18:38.174701------------------------------------------------------ Precision@1: 65.80% 

[65.72, 65.67, 65.92, 65.66, 65.82, 65.58, 65.94, 65.86, 65.88, 65.96, 65.73, 65.79, 65.65, 65.78, 65.81, 65.67, 65.67, 65.86, 65.84, 65.8]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 05:18:41.053207 epoch: 0 step: 0 cls_loss= 0.35187 (40228 samples/sec)
2024-03-10 05:18:50.403598 epoch: 0 step: 100 cls_loss= 0.27183 (3208 samples/sec)
saving....
2024-03-10 05:19:00.641520------------------------------------------------------ Precision@1: 65.72% 

[65.72]

Epoch: 1
2024-03-10 05:19:00.896440 epoch: 1 step: 0 cls_loss= 0.34919 (118384 samples/sec)
2024-03-10 05:19:10.232497 epoch: 1 step: 100 cls_loss= 0.30178 (3216 samples/sec)
saving....
2024-03-10 05:19:20.236238------------------------------------------------------ Precision@1: 65.81% 

[65.72, 65.81]

Epoch: 2
2024-03-10 05:19:20.492849 epoch: 2 step: 0 cls_loss= 0.30755 (117608 samples/sec)
2024-03-10 05:19:29.838179 epoch: 2 step: 100 cls_loss= 0.31611 (3211 samples/sec)
saving....
2024-03-10 05:19:39.762941------------------------------------------------------ Precision@1: 65.82% 

[65.72, 65.81, 65.82]

Epoch: 3
2024-03-10 05:19:40.021436 epoch: 3 step: 0 cls_loss= 0.34811 (116735 samples/sec)
2024-03-10 05:19:49.341029 epoch: 3 step: 100 cls_loss= 0.35585 (3222 samples/sec)
saving....
2024-03-10 05:19:59.221892------------------------------------------------------ Precision@1: 65.72% 

[65.72, 65.81, 65.82, 65.72]

Epoch: 4
2024-03-10 05:19:59.463643 epoch: 4 step: 0 cls_loss= 0.34924 (124888 samples/sec)
2024-03-10 05:20:08.821506 epoch: 4 step: 100 cls_loss= 0.38523 (3209 samples/sec)
saving....
2024-03-10 05:20:18.802526------------------------------------------------------ Precision@1: 65.73% 

[65.72, 65.81, 65.82, 65.72, 65.73]

Epoch: 5
2024-03-10 05:20:19.080320 epoch: 5 step: 0 cls_loss= 0.38979 (108495 samples/sec)
2024-03-10 05:20:28.425016 epoch: 5 step: 100 cls_loss= 0.33822 (3211 samples/sec)
saving....
2024-03-10 05:20:38.440218------------------------------------------------------ Precision@1: 66.10% 

[65.72, 65.81, 65.82, 65.72, 65.73, 66.1]

Epoch: 6
2024-03-10 05:20:38.714773 epoch: 6 step: 0 cls_loss= 0.38356 (109746 samples/sec)
2024-03-10 05:20:48.048948 epoch: 6 step: 100 cls_loss= 0.39275 (3216 samples/sec)
saving....
2024-03-10 05:20:58.113862------------------------------------------------------ Precision@1: 65.79% 

[65.72, 65.81, 65.82, 65.72, 65.73, 66.1, 65.79]

Epoch: 7
2024-03-10 05:20:58.368017 epoch: 7 step: 0 cls_loss= 0.33327 (118785 samples/sec)
2024-03-10 05:21:07.729342 epoch: 7 step: 100 cls_loss= 0.25751 (3207 samples/sec)
saving....
2024-03-10 05:21:17.681734------------------------------------------------------ Precision@1: 65.74% 

[65.72, 65.81, 65.82, 65.72, 65.73, 66.1, 65.79, 65.74]

Epoch: 8
2024-03-10 05:21:17.937684 epoch: 8 step: 0 cls_loss= 0.32494 (117898 samples/sec)
2024-03-10 05:21:27.270019 epoch: 8 step: 100 cls_loss= 0.31974 (3217 samples/sec)
saving....
2024-03-10 05:21:37.160158------------------------------------------------------ Precision@1: 65.97% 

[65.72, 65.81, 65.82, 65.72, 65.73, 66.1, 65.79, 65.74, 65.97]

Epoch: 9
2024-03-10 05:21:37.413271 epoch: 9 step: 0 cls_loss= 0.32370 (119183 samples/sec)
2024-03-10 05:21:46.792135 epoch: 9 step: 100 cls_loss= 0.30872 (3201 samples/sec)
saving....
2024-03-10 05:21:56.761929------------------------------------------------------ Precision@1: 65.72% 

[65.72, 65.81, 65.82, 65.72, 65.73, 66.1, 65.79, 65.74, 65.97, 65.72]

Epoch: 10
2024-03-10 05:21:57.028987 epoch: 10 step: 0 cls_loss= 0.33461 (112976 samples/sec)
2024-03-10 05:22:06.435449 epoch: 10 step: 100 cls_loss= 0.33042 (3192 samples/sec)
saving....
2024-03-10 05:22:16.406001------------------------------------------------------ Precision@1: 65.65% 

[65.72, 65.81, 65.82, 65.72, 65.73, 66.1, 65.79, 65.74, 65.97, 65.72, 65.65]

Epoch: 11
2024-03-10 05:22:16.662053 epoch: 11 step: 0 cls_loss= 0.32632 (117784 samples/sec)
2024-03-10 05:22:25.982176 epoch: 11 step: 100 cls_loss= 0.30553 (3222 samples/sec)
saving....
2024-03-10 05:22:35.937283------------------------------------------------------ Precision@1: 65.58% 

[65.72, 65.81, 65.82, 65.72, 65.73, 66.1, 65.79, 65.74, 65.97, 65.72, 65.65, 65.58]

Epoch: 12
2024-03-10 05:22:36.206592 epoch: 12 step: 0 cls_loss= 0.28687 (111992 samples/sec)
2024-03-10 05:22:45.545692 epoch: 12 step: 100 cls_loss= 0.33691 (3215 samples/sec)
saving....
2024-03-10 05:22:55.417890------------------------------------------------------ Precision@1: 65.78% 

[65.72, 65.81, 65.82, 65.72, 65.73, 66.1, 65.79, 65.74, 65.97, 65.72, 65.65, 65.58, 65.78]

Epoch: 13
2024-03-10 05:22:55.663293 epoch: 13 step: 0 cls_loss= 0.39886 (122969 samples/sec)
2024-03-10 05:23:05.036920 epoch: 13 step: 100 cls_loss= 0.32412 (3203 samples/sec)
saving....
2024-03-10 05:23:15.044400------------------------------------------------------ Precision@1: 65.68% 

[65.72, 65.81, 65.82, 65.72, 65.73, 66.1, 65.79, 65.74, 65.97, 65.72, 65.65, 65.58, 65.78, 65.68]

Epoch: 14
2024-03-10 05:23:15.310835 epoch: 14 step: 0 cls_loss= 0.38233 (113245 samples/sec)
2024-03-10 05:23:24.647476 epoch: 14 step: 100 cls_loss= 0.32219 (3216 samples/sec)
saving....
2024-03-10 05:23:34.600803------------------------------------------------------ Precision@1: 65.58% 

[65.72, 65.81, 65.82, 65.72, 65.73, 66.1, 65.79, 65.74, 65.97, 65.72, 65.65, 65.58, 65.78, 65.68, 65.58]

Epoch: 15
2024-03-10 05:23:34.871666 epoch: 15 step: 0 cls_loss= 0.31656 (111391 samples/sec)
2024-03-10 05:23:44.207896 epoch: 15 step: 100 cls_loss= 0.32851 (3216 samples/sec)
saving....
2024-03-10 05:23:54.132699------------------------------------------------------ Precision@1: 65.85% 

[65.72, 65.81, 65.82, 65.72, 65.73, 66.1, 65.79, 65.74, 65.97, 65.72, 65.65, 65.58, 65.78, 65.68, 65.58, 65.85]

Epoch: 16
2024-03-10 05:23:54.405451 epoch: 16 step: 0 cls_loss= 0.33221 (110552 samples/sec)
2024-03-10 05:24:03.749263 epoch: 16 step: 100 cls_loss= 0.31710 (3213 samples/sec)
saving....
2024-03-10 05:24:13.690939------------------------------------------------------ Precision@1: 65.66% 

[65.72, 65.81, 65.82, 65.72, 65.73, 66.1, 65.79, 65.74, 65.97, 65.72, 65.65, 65.58, 65.78, 65.68, 65.58, 65.85, 65.66]

Epoch: 17
2024-03-10 05:24:13.943611 epoch: 17 step: 0 cls_loss= 0.36732 (119460 samples/sec)
2024-03-10 05:24:23.271905 epoch: 17 step: 100 cls_loss= 0.31294 (3219 samples/sec)
saving....
2024-03-10 05:24:33.327765------------------------------------------------------ Precision@1: 65.64% 

[65.72, 65.81, 65.82, 65.72, 65.73, 66.1, 65.79, 65.74, 65.97, 65.72, 65.65, 65.58, 65.78, 65.68, 65.58, 65.85, 65.66, 65.64]

Epoch: 18
2024-03-10 05:24:33.595342 epoch: 18 step: 0 cls_loss= 0.32695 (112759 samples/sec)
2024-03-10 05:24:42.941863 epoch: 18 step: 100 cls_loss= 0.35774 (3212 samples/sec)
saving....
2024-03-10 05:24:52.864730------------------------------------------------------ Precision@1: 65.54% 

[65.72, 65.81, 65.82, 65.72, 65.73, 66.1, 65.79, 65.74, 65.97, 65.72, 65.65, 65.58, 65.78, 65.68, 65.58, 65.85, 65.66, 65.64, 65.54]

Epoch: 19
2024-03-10 05:24:53.111100 epoch: 19 step: 0 cls_loss= 0.32395 (122534 samples/sec)
2024-03-10 05:25:02.430391 epoch: 19 step: 100 cls_loss= 0.33729 (3222 samples/sec)
saving....
2024-03-10 05:25:12.342488------------------------------------------------------ Precision@1: 65.84% 

[65.72, 65.81, 65.82, 65.72, 65.73, 66.1, 65.79, 65.74, 65.97, 65.72, 65.65, 65.58, 65.78, 65.68, 65.58, 65.85, 65.66, 65.64, 65.54, 65.84]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 05:25:15.202953 epoch: 0 step: 0 cls_loss= 0.32989 (41622 samples/sec)
2024-03-10 05:25:24.516546 epoch: 0 step: 100 cls_loss= 0.37127 (3221 samples/sec)
saving....
2024-03-10 05:25:34.672941------------------------------------------------------ Precision@1: 65.77% 

[65.77]

Epoch: 1
2024-03-10 05:25:34.937831 epoch: 1 step: 0 cls_loss= 0.23239 (113936 samples/sec)
2024-03-10 05:25:44.357661 epoch: 1 step: 100 cls_loss= 0.35321 (3187 samples/sec)
saving....
2024-03-10 05:25:54.361777------------------------------------------------------ Precision@1: 65.63% 

[65.77, 65.63]

Epoch: 2
2024-03-10 05:25:54.627000 epoch: 2 step: 0 cls_loss= 0.27868 (113709 samples/sec)
2024-03-10 05:26:03.944962 epoch: 2 step: 100 cls_loss= 0.36555 (3222 samples/sec)
saving....
2024-03-10 05:26:13.848805------------------------------------------------------ Precision@1: 65.86% 

[65.77, 65.63, 65.86]

Epoch: 3
2024-03-10 05:26:14.098509 epoch: 3 step: 0 cls_loss= 0.36104 (120907 samples/sec)
2024-03-10 05:26:23.456882 epoch: 3 step: 100 cls_loss= 0.34662 (3208 samples/sec)
saving....
2024-03-10 05:26:33.455168------------------------------------------------------ Precision@1: 65.61% 

[65.77, 65.63, 65.86, 65.61]

Epoch: 4
2024-03-10 05:26:33.703669 epoch: 4 step: 0 cls_loss= 0.24164 (121497 samples/sec)
2024-03-10 05:26:43.040210 epoch: 4 step: 100 cls_loss= 0.27873 (3214 samples/sec)
saving....
2024-03-10 05:26:52.976245------------------------------------------------------ Precision@1: 65.93% 

[65.77, 65.63, 65.86, 65.61, 65.93]

Epoch: 5
2024-03-10 05:26:53.235616 epoch: 5 step: 0 cls_loss= 0.36407 (116334 samples/sec)
2024-03-10 05:27:02.570928 epoch: 5 step: 100 cls_loss= 0.31934 (3216 samples/sec)
saving....
2024-03-10 05:27:12.502168------------------------------------------------------ Precision@1: 65.57% 

[65.77, 65.63, 65.86, 65.61, 65.93, 65.57]

Epoch: 6
2024-03-10 05:27:12.762781 epoch: 6 step: 0 cls_loss= 0.31774 (115743 samples/sec)
2024-03-10 05:27:22.106863 epoch: 6 step: 100 cls_loss= 0.30915 (3213 samples/sec)
saving....
2024-03-10 05:27:31.962016------------------------------------------------------ Precision@1: 65.72% 

[65.77, 65.63, 65.86, 65.61, 65.93, 65.57, 65.72]

Epoch: 7
2024-03-10 05:27:32.215966 epoch: 7 step: 0 cls_loss= 0.34543 (118887 samples/sec)
2024-03-10 05:27:41.554659 epoch: 7 step: 100 cls_loss= 0.30974 (3215 samples/sec)
saving....
2024-03-10 05:27:51.427829------------------------------------------------------ Precision@1: 65.79% 

[65.77, 65.63, 65.86, 65.61, 65.93, 65.57, 65.72, 65.79]

Epoch: 8
2024-03-10 05:27:51.662499 epoch: 8 step: 0 cls_loss= 0.29397 (128566 samples/sec)
2024-03-10 05:28:00.996254 epoch: 8 step: 100 cls_loss= 0.30572 (3217 samples/sec)
saving....
2024-03-10 05:28:10.900331------------------------------------------------------ Precision@1: 65.69% 

[65.77, 65.63, 65.86, 65.61, 65.93, 65.57, 65.72, 65.79, 65.69]

Epoch: 9
2024-03-10 05:28:11.158729 epoch: 9 step: 0 cls_loss= 0.34984 (116792 samples/sec)
2024-03-10 05:28:20.472023 epoch: 9 step: 100 cls_loss= 0.31733 (3224 samples/sec)
saving....
2024-03-10 05:28:30.369603------------------------------------------------------ Precision@1: 65.78% 

[65.77, 65.63, 65.86, 65.61, 65.93, 65.57, 65.72, 65.79, 65.69, 65.78]

Epoch: 10
2024-03-10 05:28:30.612786 epoch: 10 step: 0 cls_loss= 0.32690 (124125 samples/sec)
2024-03-10 05:28:39.956813 epoch: 10 step: 100 cls_loss= 0.31792 (3213 samples/sec)
saving....
2024-03-10 05:28:49.880641------------------------------------------------------ Precision@1: 65.78% 

[65.77, 65.63, 65.86, 65.61, 65.93, 65.57, 65.72, 65.79, 65.69, 65.78, 65.78]

Epoch: 11
2024-03-10 05:28:50.139640 epoch: 11 step: 0 cls_loss= 0.41547 (116423 samples/sec)
2024-03-10 05:28:59.479217 epoch: 11 step: 100 cls_loss= 0.32072 (3215 samples/sec)
saving....
2024-03-10 05:29:09.355731------------------------------------------------------ Precision@1: 65.87% 

[65.77, 65.63, 65.86, 65.61, 65.93, 65.57, 65.72, 65.79, 65.69, 65.78, 65.78, 65.87]

Epoch: 12
2024-03-10 05:29:09.609072 epoch: 12 step: 0 cls_loss= 0.32063 (119159 samples/sec)
2024-03-10 05:29:18.962840 epoch: 12 step: 100 cls_loss= 0.31686 (3208 samples/sec)
saving....
2024-03-10 05:29:28.846089------------------------------------------------------ Precision@1: 65.75% 

[65.77, 65.63, 65.86, 65.61, 65.93, 65.57, 65.72, 65.79, 65.69, 65.78, 65.78, 65.87, 65.75]

Epoch: 13
2024-03-10 05:29:29.102028 epoch: 13 step: 0 cls_loss= 0.36172 (117900 samples/sec)
2024-03-10 05:29:38.448989 epoch: 13 step: 100 cls_loss= 0.33316 (3212 samples/sec)
saving....
2024-03-10 05:29:48.385578------------------------------------------------------ Precision@1: 65.77% 

[65.77, 65.63, 65.86, 65.61, 65.93, 65.57, 65.72, 65.79, 65.69, 65.78, 65.78, 65.87, 65.75, 65.77]

Epoch: 14
2024-03-10 05:29:48.639276 epoch: 14 step: 0 cls_loss= 0.30211 (118986 samples/sec)
2024-03-10 05:29:57.969076 epoch: 14 step: 100 cls_loss= 0.36571 (3218 samples/sec)
saving....
2024-03-10 05:30:07.863265------------------------------------------------------ Precision@1: 65.77% 

[65.77, 65.63, 65.86, 65.61, 65.93, 65.57, 65.72, 65.79, 65.69, 65.78, 65.78, 65.87, 65.75, 65.77, 65.77]

Epoch: 15
2024-03-10 05:30:08.118391 epoch: 15 step: 0 cls_loss= 0.28942 (118320 samples/sec)
2024-03-10 05:30:17.466224 epoch: 15 step: 100 cls_loss= 0.30954 (3212 samples/sec)
saving....
2024-03-10 05:30:27.355620------------------------------------------------------ Precision@1: 65.85% 

[65.77, 65.63, 65.86, 65.61, 65.93, 65.57, 65.72, 65.79, 65.69, 65.78, 65.78, 65.87, 65.75, 65.77, 65.77, 65.85]

Epoch: 16
2024-03-10 05:30:27.610546 epoch: 16 step: 0 cls_loss= 0.34651 (118324 samples/sec)
2024-03-10 05:30:36.960237 epoch: 16 step: 100 cls_loss= 0.33611 (3211 samples/sec)
saving....
2024-03-10 05:30:46.863222------------------------------------------------------ Precision@1: 65.63% 

[65.77, 65.63, 65.86, 65.61, 65.93, 65.57, 65.72, 65.79, 65.69, 65.78, 65.78, 65.87, 65.75, 65.77, 65.77, 65.85, 65.63]

Epoch: 17
2024-03-10 05:30:47.115761 epoch: 17 step: 0 cls_loss= 0.31571 (119525 samples/sec)
2024-03-10 05:30:56.455729 epoch: 17 step: 100 cls_loss= 0.44945 (3215 samples/sec)
saving....
2024-03-10 05:31:06.352804------------------------------------------------------ Precision@1: 65.46% 

[65.77, 65.63, 65.86, 65.61, 65.93, 65.57, 65.72, 65.79, 65.69, 65.78, 65.78, 65.87, 65.75, 65.77, 65.77, 65.85, 65.63, 65.46]

Epoch: 18
2024-03-10 05:31:06.602026 epoch: 18 step: 0 cls_loss= 0.31891 (121179 samples/sec)
2024-03-10 05:31:15.959511 epoch: 18 step: 100 cls_loss= 0.34387 (3209 samples/sec)
saving....
2024-03-10 05:31:25.894312------------------------------------------------------ Precision@1: 65.79% 

[65.77, 65.63, 65.86, 65.61, 65.93, 65.57, 65.72, 65.79, 65.69, 65.78, 65.78, 65.87, 65.75, 65.77, 65.77, 65.85, 65.63, 65.46, 65.79]

Epoch: 19
2024-03-10 05:31:26.157365 epoch: 19 step: 0 cls_loss= 0.30894 (114599 samples/sec)
2024-03-10 05:31:35.479404 epoch: 19 step: 100 cls_loss= 0.31871 (3221 samples/sec)
saving....
2024-03-10 05:31:45.383967------------------------------------------------------ Precision@1: 65.64% 

[65.77, 65.63, 65.86, 65.61, 65.93, 65.57, 65.72, 65.79, 65.69, 65.78, 65.78, 65.87, 65.75, 65.77, 65.77, 65.85, 65.63, 65.46, 65.79, 65.64]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 05:31:48.259866 epoch: 0 step: 0 cls_loss= 0.33117 (40631 samples/sec)
2024-03-10 05:31:57.610724 epoch: 0 step: 100 cls_loss= 0.37655 (3208 samples/sec)
saving....
2024-03-10 05:32:07.981137------------------------------------------------------ Precision@1: 65.90% 

[65.9]

Epoch: 1
2024-03-10 05:32:08.242435 epoch: 1 step: 0 cls_loss= 0.33897 (115472 samples/sec)
2024-03-10 05:32:17.565075 epoch: 1 step: 100 cls_loss= 0.31360 (3221 samples/sec)
saving....
2024-03-10 05:32:27.513612------------------------------------------------------ Precision@1: 65.80% 

[65.9, 65.8]

Epoch: 2
2024-03-10 05:32:27.769681 epoch: 2 step: 0 cls_loss= 0.32638 (117855 samples/sec)
2024-03-10 05:32:37.134496 epoch: 2 step: 100 cls_loss= 0.29691 (3206 samples/sec)
saving....
2024-03-10 05:32:47.185915------------------------------------------------------ Precision@1: 65.61% 

[65.9, 65.8, 65.61]

Epoch: 3
2024-03-10 05:32:47.430556 epoch: 3 step: 0 cls_loss= 0.31762 (123406 samples/sec)
2024-03-10 05:32:56.750751 epoch: 3 step: 100 cls_loss= 0.34414 (3222 samples/sec)
saving....
2024-03-10 05:33:06.666877------------------------------------------------------ Precision@1: 65.81% 

[65.9, 65.8, 65.61, 65.81]

Epoch: 4
2024-03-10 05:33:06.913459 epoch: 4 step: 0 cls_loss= 0.33061 (122326 samples/sec)
2024-03-10 05:33:16.240937 epoch: 4 step: 100 cls_loss= 0.36003 (3219 samples/sec)
saving....
2024-03-10 05:33:26.146594------------------------------------------------------ Precision@1: 65.67% 

[65.9, 65.8, 65.61, 65.81, 65.67]

Epoch: 5
2024-03-10 05:33:26.406470 epoch: 5 step: 0 cls_loss= 0.33024 (115939 samples/sec)
2024-03-10 13:42:39.356799 epoch: 5 step: 100 cls_loss= 0.33699 (3209 samples/sec)
saving....
2024-03-10 13:42:49.239917------------------------------------------------------ Precision@1: 65.63% 

[65.9, 65.8, 65.61, 65.81, 65.67, 65.63]

Epoch: 6
2024-03-10 13:42:49.491798 epoch: 6 step: 0 cls_loss= 0.32161 (119666 samples/sec)
2024-03-10 13:42:58.866227 epoch: 6 step: 100 cls_loss= 0.36436 (3203 samples/sec)
saving....
2024-03-10 13:43:08.828365------------------------------------------------------ Precision@1: 65.79% 

[65.9, 65.8, 65.61, 65.81, 65.67, 65.63, 65.79]

Epoch: 7
2024-03-10 13:43:09.090972 epoch: 7 step: 0 cls_loss= 0.32938 (114820 samples/sec)
2024-03-10 13:43:18.416411 epoch: 7 step: 100 cls_loss= 0.33908 (3220 samples/sec)
saving....
2024-03-10 13:43:28.322449------------------------------------------------------ Precision@1: 65.62% 

[65.9, 65.8, 65.61, 65.81, 65.67, 65.63, 65.79, 65.62]

Epoch: 8
2024-03-10 13:43:28.571788 epoch: 8 step: 0 cls_loss= 0.32818 (121116 samples/sec)
2024-03-10 13:43:37.885015 epoch: 8 step: 100 cls_loss= 0.28511 (3224 samples/sec)
saving....
2024-03-10 13:43:47.788442------------------------------------------------------ Precision@1: 65.67% 

[65.9, 65.8, 65.61, 65.81, 65.67, 65.63, 65.79, 65.62, 65.67]

Epoch: 9
2024-03-10 13:43:48.041154 epoch: 9 step: 0 cls_loss= 0.29577 (119360 samples/sec)
2024-03-10 13:43:57.333275 epoch: 9 step: 100 cls_loss= 0.37853 (3230 samples/sec)
saving....
2024-03-10 13:44:07.227260------------------------------------------------------ Precision@1: 65.70% 

[65.9, 65.8, 65.61, 65.81, 65.67, 65.63, 65.79, 65.62, 65.67, 65.7]

Epoch: 10
2024-03-10 13:44:07.499474 epoch: 10 step: 0 cls_loss= 0.30652 (110853 samples/sec)
2024-03-10 13:44:16.810993 epoch: 10 step: 100 cls_loss= 0.36767 (3223 samples/sec)
saving....
2024-03-10 13:44:26.687876------------------------------------------------------ Precision@1: 65.81% 

[65.9, 65.8, 65.61, 65.81, 65.67, 65.63, 65.79, 65.62, 65.67, 65.7, 65.81]

Epoch: 11
2024-03-10 13:44:26.936046 epoch: 11 step: 0 cls_loss= 0.31018 (121718 samples/sec)
2024-03-10 13:44:36.287553 epoch: 11 step: 100 cls_loss= 0.31339 (3211 samples/sec)
saving....
2024-03-10 13:44:46.205670------------------------------------------------------ Precision@1: 65.82% 

[65.9, 65.8, 65.61, 65.81, 65.67, 65.63, 65.79, 65.62, 65.67, 65.7, 65.81, 65.82]

Epoch: 12
2024-03-10 13:44:46.450867 epoch: 12 step: 0 cls_loss= 0.34488 (123182 samples/sec)
2024-03-10 13:44:55.799911 epoch: 12 step: 100 cls_loss= 0.35771 (3212 samples/sec)
saving....
2024-03-10 13:45:05.716479------------------------------------------------------ Precision@1: 65.95% 

[65.9, 65.8, 65.61, 65.81, 65.67, 65.63, 65.79, 65.62, 65.67, 65.7, 65.81, 65.82, 65.95]

Epoch: 13
2024-03-10 13:45:05.963339 epoch: 13 step: 0 cls_loss= 0.34294 (122376 samples/sec)
2024-03-10 13:45:15.276574 epoch: 13 step: 100 cls_loss= 0.33375 (3224 samples/sec)
saving....
2024-03-10 13:45:25.209859------------------------------------------------------ Precision@1: 65.64% 

[65.9, 65.8, 65.61, 65.81, 65.67, 65.63, 65.79, 65.62, 65.67, 65.7, 65.81, 65.82, 65.95, 65.64]

Epoch: 14
2024-03-10 13:45:25.472035 epoch: 14 step: 0 cls_loss= 0.31495 (115172 samples/sec)
2024-03-10 13:45:34.806780 epoch: 14 step: 100 cls_loss= 0.32535 (3216 samples/sec)
saving....
2024-03-10 13:45:44.879882------------------------------------------------------ Precision@1: 65.74% 

[65.9, 65.8, 65.61, 65.81, 65.67, 65.63, 65.79, 65.62, 65.67, 65.7, 65.81, 65.82, 65.95, 65.64, 65.74]

Epoch: 15
2024-03-10 13:45:45.144824 epoch: 15 step: 0 cls_loss= 0.38083 (113713 samples/sec)
2024-03-10 13:45:54.498846 epoch: 15 step: 100 cls_loss= 0.32245 (3210 samples/sec)
saving....
2024-03-10 13:46:04.436008------------------------------------------------------ Precision@1: 65.56% 

[65.9, 65.8, 65.61, 65.81, 65.67, 65.63, 65.79, 65.62, 65.67, 65.7, 65.81, 65.82, 65.95, 65.64, 65.74, 65.56]

Epoch: 16
2024-03-10 13:46:04.696348 epoch: 16 step: 0 cls_loss= 0.34366 (115982 samples/sec)
2024-03-10 13:46:14.030601 epoch: 16 step: 100 cls_loss= 0.29778 (3217 samples/sec)
saving....
2024-03-10 13:46:23.957249------------------------------------------------------ Precision@1: 65.81% 

[65.9, 65.8, 65.61, 65.81, 65.67, 65.63, 65.79, 65.62, 65.67, 65.7, 65.81, 65.82, 65.95, 65.64, 65.74, 65.56, 65.81]

Epoch: 17
2024-03-10 13:46:24.215620 epoch: 17 step: 0 cls_loss= 0.32849 (116879 samples/sec)
2024-03-10 13:46:33.549417 epoch: 17 step: 100 cls_loss= 0.36467 (3217 samples/sec)
saving....
2024-03-10 13:46:43.503805------------------------------------------------------ Precision@1: 65.68% 

[65.9, 65.8, 65.61, 65.81, 65.67, 65.63, 65.79, 65.62, 65.67, 65.7, 65.81, 65.82, 65.95, 65.64, 65.74, 65.56, 65.81, 65.68]

Epoch: 18
2024-03-10 13:46:43.764082 epoch: 18 step: 0 cls_loss= 0.27977 (115863 samples/sec)
2024-03-10 13:46:53.096927 epoch: 18 step: 100 cls_loss= 0.27389 (3217 samples/sec)
saving....
2024-03-10 13:47:03.153575------------------------------------------------------ Precision@1: 65.60% 

[65.9, 65.8, 65.61, 65.81, 65.67, 65.63, 65.79, 65.62, 65.67, 65.7, 65.81, 65.82, 65.95, 65.64, 65.74, 65.56, 65.81, 65.68, 65.6]

Epoch: 19
2024-03-10 13:47:03.420800 epoch: 19 step: 0 cls_loss= 0.35112 (112832 samples/sec)
2024-03-10 13:47:12.738703 epoch: 19 step: 100 cls_loss= 0.30693 (3222 samples/sec)
saving....
2024-03-10 13:47:22.663661------------------------------------------------------ Precision@1: 65.75% 

[65.9, 65.8, 65.61, 65.81, 65.67, 65.63, 65.79, 65.62, 65.67, 65.7, 65.81, 65.82, 65.95, 65.64, 65.74, 65.56, 65.81, 65.68, 65.6, 65.75]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 13:47:25.545968 epoch: 0 step: 0 cls_loss= 0.36273 (41399 samples/sec)
2024-03-10 13:47:34.892089 epoch: 0 step: 100 cls_loss= 0.29878 (3209 samples/sec)
saving....
2024-03-10 13:47:45.138930------------------------------------------------------ Precision@1: 65.74% 

[65.74]

Epoch: 1
2024-03-10 13:47:45.391518 epoch: 1 step: 0 cls_loss= 0.35839 (119501 samples/sec)
2024-03-10 13:47:54.740877 epoch: 1 step: 100 cls_loss= 0.29654 (3211 samples/sec)
saving....
2024-03-10 13:48:04.744264------------------------------------------------------ Precision@1: 65.72% 

[65.74, 65.72]

Epoch: 2
2024-03-10 13:48:05.008884 epoch: 2 step: 0 cls_loss= 0.39748 (114038 samples/sec)
2024-03-10 13:48:14.354910 epoch: 2 step: 100 cls_loss= 0.31448 (3213 samples/sec)
saving....
2024-03-10 13:48:24.323831------------------------------------------------------ Precision@1: 65.81% 

[65.74, 65.72, 65.81]

Epoch: 3
2024-03-10 13:48:24.585622 epoch: 3 step: 0 cls_loss= 0.29226 (115177 samples/sec)
2024-03-10 13:48:33.975894 epoch: 3 step: 100 cls_loss= 0.31292 (3196 samples/sec)
saving....
2024-03-10 13:48:44.001989------------------------------------------------------ Precision@1: 65.60% 

[65.74, 65.72, 65.81, 65.6]

Epoch: 4
2024-03-10 13:48:44.261383 epoch: 4 step: 0 cls_loss= 0.34471 (116363 samples/sec)
2024-03-10 13:48:53.628567 epoch: 4 step: 100 cls_loss= 0.32918 (3205 samples/sec)
saving....
2024-03-10 13:49:03.662265------------------------------------------------------ Precision@1: 65.87% 

[65.74, 65.72, 65.81, 65.6, 65.87]

Epoch: 5
2024-03-10 13:49:03.932225 epoch: 5 step: 0 cls_loss= 0.26642 (111800 samples/sec)
2024-03-10 13:49:13.358663 epoch: 5 step: 100 cls_loss= 0.35417 (3185 samples/sec)
saving....
2024-03-10 13:49:23.343052------------------------------------------------------ Precision@1: 65.86% 

[65.74, 65.72, 65.81, 65.6, 65.87, 65.86]

Epoch: 6
2024-03-10 13:49:23.613715 epoch: 6 step: 0 cls_loss= 0.32758 (111444 samples/sec)
2024-03-10 13:49:32.947828 epoch: 6 step: 100 cls_loss= 0.32950 (3217 samples/sec)
saving....
2024-03-10 13:49:42.952336------------------------------------------------------ Precision@1: 65.84% 

[65.74, 65.72, 65.81, 65.6, 65.87, 65.86, 65.84]

Epoch: 7
2024-03-10 13:49:43.214828 epoch: 7 step: 0 cls_loss= 0.36670 (114999 samples/sec)
2024-03-10 13:49:52.607506 epoch: 7 step: 100 cls_loss= 0.33123 (3197 samples/sec)
saving....
2024-03-10 13:50:02.591199------------------------------------------------------ Precision@1: 65.96% 

[65.74, 65.72, 65.81, 65.6, 65.87, 65.86, 65.84, 65.96]

Epoch: 8
2024-03-10 13:50:02.846168 epoch: 8 step: 0 cls_loss= 0.35575 (118307 samples/sec)
2024-03-10 13:50:12.204353 epoch: 8 step: 100 cls_loss= 0.28286 (3208 samples/sec)
saving....
2024-03-10 13:50:22.149773------------------------------------------------------ Precision@1: 65.79% 

[65.74, 65.72, 65.81, 65.6, 65.87, 65.86, 65.84, 65.96, 65.79]

Epoch: 9
2024-03-10 13:50:22.418856 epoch: 9 step: 0 cls_loss= 0.31332 (112098 samples/sec)
2024-03-10 13:50:31.773700 epoch: 9 step: 100 cls_loss= 0.28249 (3210 samples/sec)
saving....
2024-03-10 13:50:41.693020------------------------------------------------------ Precision@1: 65.78% 

[65.74, 65.72, 65.81, 65.6, 65.87, 65.86, 65.84, 65.96, 65.79, 65.78]

Epoch: 10
2024-03-10 13:50:41.945376 epoch: 10 step: 0 cls_loss= 0.25623 (119587 samples/sec)
2024-03-10 13:50:51.333491 epoch: 10 step: 100 cls_loss= 0.28254 (3198 samples/sec)
saving....
2024-03-10 13:51:01.244773------------------------------------------------------ Precision@1: 65.95% 

[65.74, 65.72, 65.81, 65.6, 65.87, 65.86, 65.84, 65.96, 65.79, 65.78, 65.95]

Epoch: 11
2024-03-10 13:51:01.496125 epoch: 11 step: 0 cls_loss= 0.33187 (120058 samples/sec)
2024-03-10 13:51:10.862922 epoch: 11 step: 100 cls_loss= 0.27085 (3205 samples/sec)
saving....
2024-03-10 13:51:20.851396------------------------------------------------------ Precision@1: 65.72% 

[65.74, 65.72, 65.81, 65.6, 65.87, 65.86, 65.84, 65.96, 65.79, 65.78, 65.95, 65.72]

Epoch: 12
2024-03-10 13:51:21.106942 epoch: 12 step: 0 cls_loss= 0.38729 (118101 samples/sec)
2024-03-10 13:51:30.421214 epoch: 12 step: 100 cls_loss= 0.32942 (3224 samples/sec)
saving....
2024-03-10 13:51:40.365199------------------------------------------------------ Precision@1: 65.75% 

[65.74, 65.72, 65.81, 65.6, 65.87, 65.86, 65.84, 65.96, 65.79, 65.78, 65.95, 65.72, 65.75]

Epoch: 13
2024-03-10 13:51:40.637613 epoch: 13 step: 0 cls_loss= 0.29719 (110742 samples/sec)
2024-03-10 13:51:49.967974 epoch: 13 step: 100 cls_loss= 0.34756 (3216 samples/sec)
saving....
2024-03-10 13:51:59.895230------------------------------------------------------ Precision@1: 65.62% 

[65.74, 65.72, 65.81, 65.6, 65.87, 65.86, 65.84, 65.96, 65.79, 65.78, 65.95, 65.72, 65.75, 65.62]

Epoch: 14
2024-03-10 13:52:00.149743 epoch: 14 step: 0 cls_loss= 0.34087 (118591 samples/sec)
2024-03-10 13:52:09.471088 epoch: 14 step: 100 cls_loss= 0.31586 (3219 samples/sec)
saving....
2024-03-10 13:52:19.441871------------------------------------------------------ Precision@1: 65.45% 

[65.74, 65.72, 65.81, 65.6, 65.87, 65.86, 65.84, 65.96, 65.79, 65.78, 65.95, 65.72, 65.75, 65.62, 65.45]

Epoch: 15
2024-03-10 13:52:19.710510 epoch: 15 step: 0 cls_loss= 0.39074 (112327 samples/sec)
2024-03-10 13:52:29.086000 epoch: 15 step: 100 cls_loss= 0.31835 (3202 samples/sec)
saving....
2024-03-10 13:52:39.059760------------------------------------------------------ Precision@1: 65.90% 

[65.74, 65.72, 65.81, 65.6, 65.87, 65.86, 65.84, 65.96, 65.79, 65.78, 65.95, 65.72, 65.75, 65.62, 65.45, 65.9]

Epoch: 16
2024-03-10 13:52:39.326706 epoch: 16 step: 0 cls_loss= 0.41251 (113046 samples/sec)
2024-03-10 13:52:48.697421 epoch: 16 step: 100 cls_loss= 0.32244 (3204 samples/sec)
saving....
2024-03-10 13:52:58.659164------------------------------------------------------ Precision@1: 65.77% 

[65.74, 65.72, 65.81, 65.6, 65.87, 65.86, 65.84, 65.96, 65.79, 65.78, 65.95, 65.72, 65.75, 65.62, 65.45, 65.9, 65.77]

Epoch: 17
2024-03-10 13:52:58.910570 epoch: 17 step: 0 cls_loss= 0.35998 (120094 samples/sec)
2024-03-10 13:53:08.227380 epoch: 17 step: 100 cls_loss= 0.35306 (3223 samples/sec)
saving....
2024-03-10 13:53:18.235900------------------------------------------------------ Precision@1: 65.89% 

[65.74, 65.72, 65.81, 65.6, 65.87, 65.86, 65.84, 65.96, 65.79, 65.78, 65.95, 65.72, 65.75, 65.62, 65.45, 65.9, 65.77, 65.89]

Epoch: 18
2024-03-10 13:53:18.507746 epoch: 18 step: 0 cls_loss= 0.31455 (111031 samples/sec)
2024-03-10 13:53:27.858939 epoch: 18 step: 100 cls_loss= 0.39638 (3208 samples/sec)
saving....
2024-03-10 13:53:37.836297------------------------------------------------------ Precision@1: 65.84% 

[65.74, 65.72, 65.81, 65.6, 65.87, 65.86, 65.84, 65.96, 65.79, 65.78, 65.95, 65.72, 65.75, 65.62, 65.45, 65.9, 65.77, 65.89, 65.84]

Epoch: 19
2024-03-10 13:53:38.105889 epoch: 19 step: 0 cls_loss= 0.32939 (111831 samples/sec)
2024-03-10 13:53:47.580660 epoch: 19 step: 100 cls_loss= 0.37056 (3169 samples/sec)
saving....
2024-03-10 13:53:57.599503------------------------------------------------------ Precision@1: 65.75% 

[65.74, 65.72, 65.81, 65.6, 65.87, 65.86, 65.84, 65.96, 65.79, 65.78, 65.95, 65.72, 65.75, 65.62, 65.45, 65.9, 65.77, 65.89, 65.84, 65.75]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 13:54:00.462933 epoch: 0 step: 0 cls_loss= 0.31826 (41189 samples/sec)
2024-03-10 13:54:09.801459 epoch: 0 step: 100 cls_loss= 0.36290 (3212 samples/sec)
saving....
2024-03-10 13:54:19.945315------------------------------------------------------ Precision@1: 65.76% 

[65.76]

Epoch: 1
2024-03-10 13:54:20.195967 epoch: 1 step: 0 cls_loss= 0.29148 (120404 samples/sec)
2024-03-10 13:54:29.523511 epoch: 1 step: 100 cls_loss= 0.28439 (3219 samples/sec)
saving....
2024-03-10 13:54:39.494151------------------------------------------------------ Precision@1: 65.84% 

[65.76, 65.84]

Epoch: 2
2024-03-10 13:54:39.764469 epoch: 2 step: 0 cls_loss= 0.28523 (111853 samples/sec)
2024-03-10 13:54:49.121295 epoch: 2 step: 100 cls_loss= 0.30601 (3209 samples/sec)
saving....
2024-03-10 13:54:59.079560------------------------------------------------------ Precision@1: 65.61% 

[65.76, 65.84, 65.61]

Epoch: 3
2024-03-10 13:54:59.338259 epoch: 3 step: 0 cls_loss= 0.34682 (116686 samples/sec)
2024-03-10 13:55:08.708049 epoch: 3 step: 100 cls_loss= 0.37642 (3204 samples/sec)
saving....
2024-03-10 13:55:18.684418------------------------------------------------------ Precision@1: 65.67% 

[65.76, 65.84, 65.61, 65.67]

Epoch: 4
2024-03-10 13:55:18.928748 epoch: 4 step: 0 cls_loss= 0.28956 (123618 samples/sec)
2024-03-10 13:55:28.292855 epoch: 4 step: 100 cls_loss= 0.30404 (3206 samples/sec)
saving....
2024-03-10 13:55:38.235795------------------------------------------------------ Precision@1: 65.77% 

[65.76, 65.84, 65.61, 65.67, 65.77]

Epoch: 5
2024-03-10 13:55:38.497636 epoch: 5 step: 0 cls_loss= 0.38335 (115209 samples/sec)
2024-03-10 13:55:47.863070 epoch: 5 step: 100 cls_loss= 0.37441 (3204 samples/sec)
saving....
2024-03-10 13:55:57.871696------------------------------------------------------ Precision@1: 65.69% 

[65.76, 65.84, 65.61, 65.67, 65.77, 65.69]

Epoch: 6
2024-03-10 13:55:58.130449 epoch: 6 step: 0 cls_loss= 0.34481 (116447 samples/sec)
2024-03-10 13:56:07.519420 epoch: 6 step: 100 cls_loss= 0.33819 (3198 samples/sec)
saving....
2024-03-10 13:56:17.522299------------------------------------------------------ Precision@1: 65.63% 

[65.76, 65.84, 65.61, 65.67, 65.77, 65.69, 65.63]

Epoch: 7
2024-03-10 13:56:17.789588 epoch: 7 step: 0 cls_loss= 0.29539 (112892 samples/sec)
2024-03-10 13:56:27.228800 epoch: 7 step: 100 cls_loss= 0.33329 (3181 samples/sec)
saving....
2024-03-10 13:56:37.217868------------------------------------------------------ Precision@1: 65.74% 

[65.76, 65.84, 65.61, 65.67, 65.77, 65.69, 65.63, 65.74]

Epoch: 8
2024-03-10 13:56:37.480868 epoch: 8 step: 0 cls_loss= 0.34849 (114743 samples/sec)
2024-03-10 13:56:46.836935 epoch: 8 step: 100 cls_loss= 0.34616 (3207 samples/sec)
saving....
2024-03-10 13:56:56.876963------------------------------------------------------ Precision@1: 65.70% 

[65.76, 65.84, 65.61, 65.67, 65.77, 65.69, 65.63, 65.74, 65.7]

Epoch: 9
2024-03-10 13:56:57.145147 epoch: 9 step: 0 cls_loss= 0.28729 (112556 samples/sec)
2024-03-10 13:57:06.515828 epoch: 9 step: 100 cls_loss= 0.28358 (3204 samples/sec)
saving....
2024-03-10 13:57:16.511581------------------------------------------------------ Precision@1: 66.07% 

[65.76, 65.84, 65.61, 65.67, 65.77, 65.69, 65.63, 65.74, 65.7, 66.07]

Epoch: 10
2024-03-10 13:57:16.768768 epoch: 10 step: 0 cls_loss= 0.33011 (117232 samples/sec)
2024-03-10 13:57:26.156576 epoch: 10 step: 100 cls_loss= 0.37111 (3198 samples/sec)
saving....
2024-03-10 13:57:36.170816------------------------------------------------------ Precision@1: 65.74% 

[65.76, 65.84, 65.61, 65.67, 65.77, 65.69, 65.63, 65.74, 65.7, 66.07, 65.74]

Epoch: 11
2024-03-10 13:57:36.424217 epoch: 11 step: 0 cls_loss= 0.31628 (119110 samples/sec)
2024-03-10 13:57:45.815013 epoch: 11 step: 100 cls_loss= 0.24782 (3197 samples/sec)
saving....
2024-03-10 13:57:55.817814------------------------------------------------------ Precision@1: 65.68% 

[65.76, 65.84, 65.61, 65.67, 65.77, 65.69, 65.63, 65.74, 65.7, 66.07, 65.74, 65.68]

Epoch: 12
2024-03-10 13:57:56.070254 epoch: 12 step: 0 cls_loss= 0.31646 (119450 samples/sec)
2024-03-10 13:58:05.467858 epoch: 12 step: 100 cls_loss= 0.29263 (3195 samples/sec)
saving....
2024-03-10 13:58:15.457165------------------------------------------------------ Precision@1: 65.67% 

[65.76, 65.84, 65.61, 65.67, 65.77, 65.69, 65.63, 65.74, 65.7, 66.07, 65.74, 65.68, 65.67]

Epoch: 13
2024-03-10 13:58:15.712228 epoch: 13 step: 0 cls_loss= 0.30441 (118399 samples/sec)
2024-03-10 13:58:25.053620 epoch: 13 step: 100 cls_loss= 0.35416 (3214 samples/sec)
saving....
2024-03-10 13:58:35.030892------------------------------------------------------ Precision@1: 65.76% 

[65.76, 65.84, 65.61, 65.67, 65.77, 65.69, 65.63, 65.74, 65.7, 66.07, 65.74, 65.68, 65.67, 65.76]

Epoch: 14
2024-03-10 13:58:35.297230 epoch: 14 step: 0 cls_loss= 0.31022 (113281 samples/sec)
2024-03-10 13:58:44.654902 epoch: 14 step: 100 cls_loss= 0.27455 (3209 samples/sec)
saving....
2024-03-10 13:58:54.646069------------------------------------------------------ Precision@1: 65.82% 

[65.76, 65.84, 65.61, 65.67, 65.77, 65.69, 65.63, 65.74, 65.7, 66.07, 65.74, 65.68, 65.67, 65.76, 65.82]

Epoch: 15
2024-03-10 13:58:54.899025 epoch: 15 step: 0 cls_loss= 0.31953 (119325 samples/sec)
2024-03-10 13:59:04.229925 epoch: 15 step: 100 cls_loss= 0.35414 (3218 samples/sec)
saving....
2024-03-10 13:59:14.217415------------------------------------------------------ Precision@1: 65.76% 

[65.76, 65.84, 65.61, 65.67, 65.77, 65.69, 65.63, 65.74, 65.7, 66.07, 65.74, 65.68, 65.67, 65.76, 65.82, 65.76]

Epoch: 16
2024-03-10 13:59:14.474796 epoch: 16 step: 0 cls_loss= 0.35592 (117238 samples/sec)
2024-03-10 13:59:23.814222 epoch: 16 step: 100 cls_loss= 0.29716 (3215 samples/sec)
saving....
2024-03-10 13:59:33.808420------------------------------------------------------ Precision@1: 65.72% 

[65.76, 65.84, 65.61, 65.67, 65.77, 65.69, 65.63, 65.74, 65.7, 66.07, 65.74, 65.68, 65.67, 65.76, 65.82, 65.76, 65.72]

Epoch: 17
2024-03-10 13:59:34.058536 epoch: 17 step: 0 cls_loss= 0.30664 (120711 samples/sec)
2024-03-10 13:59:43.416602 epoch: 17 step: 100 cls_loss= 0.34422 (3208 samples/sec)
saving....
2024-03-10 13:59:53.552728------------------------------------------------------ Precision@1: 65.61% 

[65.76, 65.84, 65.61, 65.67, 65.77, 65.69, 65.63, 65.74, 65.7, 66.07, 65.74, 65.68, 65.67, 65.76, 65.82, 65.76, 65.72, 65.61]

Epoch: 18
2024-03-10 13:59:53.805758 epoch: 18 step: 0 cls_loss= 0.34492 (119213 samples/sec)
2024-03-10 14:00:03.123099 epoch: 18 step: 100 cls_loss= 0.36442 (3223 samples/sec)
saving....
2024-03-10 14:00:13.066805------------------------------------------------------ Precision@1: 65.69% 

[65.76, 65.84, 65.61, 65.67, 65.77, 65.69, 65.63, 65.74, 65.7, 66.07, 65.74, 65.68, 65.67, 65.76, 65.82, 65.76, 65.72, 65.61, 65.69]

Epoch: 19
2024-03-10 14:00:13.324983 epoch: 19 step: 0 cls_loss= 0.36128 (116904 samples/sec)
2024-03-10 14:00:22.672499 epoch: 19 step: 100 cls_loss= 0.38244 (3212 samples/sec)
saving....
2024-03-10 14:00:32.557904------------------------------------------------------ Precision@1: 66.07% 

[65.76, 65.84, 65.61, 65.67, 65.77, 65.69, 65.63, 65.74, 65.7, 66.07, 65.74, 65.68, 65.67, 65.76, 65.82, 65.76, 65.72, 65.61, 65.69, 66.07]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 14:00:35.471263 epoch: 0 step: 0 cls_loss= 0.34276 (40376 samples/sec)
2024-03-10 14:00:44.818980 epoch: 0 step: 100 cls_loss= 0.29584 (3209 samples/sec)
saving....
2024-03-10 14:00:55.043264------------------------------------------------------ Precision@1: 65.62% 

[65.62]

Epoch: 1
2024-03-10 14:00:55.312654 epoch: 1 step: 0 cls_loss= 0.28926 (112016 samples/sec)
2024-03-10 14:01:04.649487 epoch: 1 step: 100 cls_loss= 0.29445 (3214 samples/sec)
saving....
2024-03-10 14:01:14.584828------------------------------------------------------ Precision@1: 65.90% 

[65.62, 65.9]

Epoch: 2
2024-03-10 14:01:14.830608 epoch: 2 step: 0 cls_loss= 0.38176 (122858 samples/sec)
2024-03-10 14:01:24.174813 epoch: 2 step: 100 cls_loss= 0.38450 (3213 samples/sec)
saving....
2024-03-10 14:01:34.115735------------------------------------------------------ Precision@1: 65.85% 

[65.62, 65.9, 65.85]

Epoch: 3
2024-03-10 14:01:34.361812 epoch: 3 step: 0 cls_loss= 0.34431 (122690 samples/sec)
2024-03-10 14:01:43.687814 epoch: 3 step: 100 cls_loss= 0.31208 (3220 samples/sec)
saving....
2024-03-10 14:01:53.609608------------------------------------------------------ Precision@1: 65.76% 

[65.62, 65.9, 65.85, 65.76]

Epoch: 4
2024-03-10 14:01:53.852712 epoch: 4 step: 0 cls_loss= 0.39124 (124173 samples/sec)
2024-03-10 14:02:03.227130 epoch: 4 step: 100 cls_loss= 0.36438 (3203 samples/sec)
saving....
2024-03-10 14:02:13.153233------------------------------------------------------ Precision@1: 65.98% 

[65.62, 65.9, 65.85, 65.76, 65.98]

Epoch: 5
2024-03-10 14:02:13.397192 epoch: 5 step: 0 cls_loss= 0.32992 (123751 samples/sec)
2024-03-10 14:02:22.768317 epoch: 5 step: 100 cls_loss= 0.37191 (3204 samples/sec)
saving....
2024-03-10 14:02:32.751581------------------------------------------------------ Precision@1: 65.58% 

[65.62, 65.9, 65.85, 65.76, 65.98, 65.58]

Epoch: 6
2024-03-10 14:02:33.011635 epoch: 6 step: 0 cls_loss= 0.36711 (116093 samples/sec)
2024-03-10 14:02:42.374461 epoch: 6 step: 100 cls_loss= 0.36586 (3205 samples/sec)
saving....
2024-03-10 14:02:52.319980------------------------------------------------------ Precision@1: 65.66% 

[65.62, 65.9, 65.85, 65.76, 65.98, 65.58, 65.66]

Epoch: 7
2024-03-10 14:02:52.577700 epoch: 7 step: 0 cls_loss= 0.31781 (117085 samples/sec)
2024-03-10 14:03:01.911382 epoch: 7 step: 100 cls_loss= 0.28665 (3217 samples/sec)
saving....
2024-03-10 14:03:11.852077------------------------------------------------------ Precision@1: 65.64% 

[65.62, 65.9, 65.85, 65.76, 65.98, 65.58, 65.66, 65.64]

Epoch: 8
2024-03-10 14:03:12.108753 epoch: 8 step: 0 cls_loss= 0.29095 (117594 samples/sec)
2024-03-10 14:03:21.443501 epoch: 8 step: 100 cls_loss= 0.32812 (3215 samples/sec)
saving....
2024-03-10 14:03:31.404984------------------------------------------------------ Precision@1: 65.97% 

[65.62, 65.9, 65.85, 65.76, 65.98, 65.58, 65.66, 65.64, 65.97]

Epoch: 9
2024-03-10 14:03:31.680483 epoch: 9 step: 0 cls_loss= 0.33609 (109397 samples/sec)
2024-03-10 14:03:41.006103 epoch: 9 step: 100 cls_loss= 0.33832 (3220 samples/sec)
saving....
2024-03-10 14:03:50.938127------------------------------------------------------ Precision@1: 65.73% 

[65.62, 65.9, 65.85, 65.76, 65.98, 65.58, 65.66, 65.64, 65.97, 65.73]

Epoch: 10
2024-03-10 14:03:51.187933 epoch: 10 step: 0 cls_loss= 0.29555 (120664 samples/sec)
2024-03-10 14:04:00.536966 epoch: 10 step: 100 cls_loss= 0.34594 (3210 samples/sec)
saving....
2024-03-10 14:04:10.480326------------------------------------------------------ Precision@1: 66.06% 

[65.62, 65.9, 65.85, 65.76, 65.98, 65.58, 65.66, 65.64, 65.97, 65.73, 66.06]

Epoch: 11
2024-03-10 14:04:10.737160 epoch: 11 step: 0 cls_loss= 0.31816 (117364 samples/sec)
2024-03-10 14:04:20.095755 epoch: 11 step: 100 cls_loss= 0.31665 (3208 samples/sec)
saving....
2024-03-10 14:04:30.138085------------------------------------------------------ Precision@1: 65.77% 

[65.62, 65.9, 65.85, 65.76, 65.98, 65.58, 65.66, 65.64, 65.97, 65.73, 66.06, 65.77]

Epoch: 12
2024-03-10 14:04:30.390197 epoch: 12 step: 0 cls_loss= 0.30495 (119738 samples/sec)
2024-03-10 14:04:39.719819 epoch: 12 step: 100 cls_loss= 0.34291 (3218 samples/sec)
saving....
2024-03-10 14:04:49.681120------------------------------------------------------ Precision@1: 65.68% 

[65.62, 65.9, 65.85, 65.76, 65.98, 65.58, 65.66, 65.64, 65.97, 65.73, 66.06, 65.77, 65.68]

Epoch: 13
2024-03-10 14:04:49.951006 epoch: 13 step: 0 cls_loss= 0.33554 (111808 samples/sec)
2024-03-10 14:04:59.334434 epoch: 13 step: 100 cls_loss= 0.26499 (3200 samples/sec)
saving....
2024-03-10 14:05:09.273183------------------------------------------------------ Precision@1: 65.91% 

[65.62, 65.9, 65.85, 65.76, 65.98, 65.58, 65.66, 65.64, 65.97, 65.73, 66.06, 65.77, 65.68, 65.91]

Epoch: 14
2024-03-10 14:05:09.549424 epoch: 14 step: 0 cls_loss= 0.30994 (109142 samples/sec)
2024-03-10 14:05:18.863464 epoch: 14 step: 100 cls_loss= 0.31699 (3222 samples/sec)
saving....
2024-03-10 14:05:28.735999------------------------------------------------------ Precision@1: 65.70% 

[65.62, 65.9, 65.85, 65.76, 65.98, 65.58, 65.66, 65.64, 65.97, 65.73, 66.06, 65.77, 65.68, 65.91, 65.7]

Epoch: 15
2024-03-10 14:05:29.003146 epoch: 15 step: 0 cls_loss= 0.27671 (112986 samples/sec)
2024-03-10 14:05:38.312788 epoch: 15 step: 100 cls_loss= 0.30975 (3223 samples/sec)
saving....
2024-03-10 14:05:48.274831------------------------------------------------------ Precision@1: 65.87% 

[65.62, 65.9, 65.85, 65.76, 65.98, 65.58, 65.66, 65.64, 65.97, 65.73, 66.06, 65.77, 65.68, 65.91, 65.7, 65.87]

Epoch: 16
2024-03-10 14:05:48.515814 epoch: 16 step: 0 cls_loss= 0.26583 (125266 samples/sec)
2024-03-10 14:05:57.845777 epoch: 16 step: 100 cls_loss= 0.31631 (3218 samples/sec)
saving....
2024-03-10 14:06:07.820763------------------------------------------------------ Precision@1: 65.72% 

[65.62, 65.9, 65.85, 65.76, 65.98, 65.58, 65.66, 65.64, 65.97, 65.73, 66.06, 65.77, 65.68, 65.91, 65.7, 65.87, 65.72]

Epoch: 17
2024-03-10 14:06:08.090852 epoch: 17 step: 0 cls_loss= 0.33867 (111529 samples/sec)
2024-03-10 14:06:17.405550 epoch: 17 step: 100 cls_loss= 0.32189 (3222 samples/sec)
saving....
2024-03-10 14:06:27.303154------------------------------------------------------ Precision@1: 65.65% 

[65.62, 65.9, 65.85, 65.76, 65.98, 65.58, 65.66, 65.64, 65.97, 65.73, 66.06, 65.77, 65.68, 65.91, 65.7, 65.87, 65.72, 65.65]

Epoch: 18
2024-03-10 14:06:27.571498 epoch: 18 step: 0 cls_loss= 0.40748 (112445 samples/sec)
2024-03-10 14:06:36.956182 epoch: 18 step: 100 cls_loss= 0.33893 (3197 samples/sec)
saving....
2024-03-10 14:06:46.922079------------------------------------------------------ Precision@1: 65.81% 

[65.62, 65.9, 65.85, 65.76, 65.98, 65.58, 65.66, 65.64, 65.97, 65.73, 66.06, 65.77, 65.68, 65.91, 65.7, 65.87, 65.72, 65.65, 65.81]

Epoch: 19
2024-03-10 14:06:47.172352 epoch: 19 step: 0 cls_loss= 0.32878 (120652 samples/sec)
2024-03-10 14:06:56.555339 epoch: 19 step: 100 cls_loss= 0.35312 (3200 samples/sec)
saving....
2024-03-10 14:07:06.561315------------------------------------------------------ Precision@1: 65.74% 

[65.62, 65.9, 65.85, 65.76, 65.98, 65.58, 65.66, 65.64, 65.97, 65.73, 66.06, 65.77, 65.68, 65.91, 65.7, 65.87, 65.72, 65.65, 65.81, 65.74]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 14:07:09.444616 epoch: 0 step: 0 cls_loss= 0.33528 (40479 samples/sec)
2024-03-10 14:07:18.777239 epoch: 0 step: 100 cls_loss= 0.41113 (3214 samples/sec)
saving....
2024-03-10 14:07:28.926544------------------------------------------------------ Precision@1: 65.88% 

[65.88]

Epoch: 1
2024-03-10 14:07:29.189667 epoch: 1 step: 0 cls_loss= 0.30339 (114602 samples/sec)
2024-03-10 14:07:38.481154 epoch: 1 step: 100 cls_loss= 0.34934 (3231 samples/sec)
saving....
2024-03-10 14:07:48.334984------------------------------------------------------ Precision@1: 65.90% 

[65.88, 65.9]

Epoch: 2
2024-03-10 14:07:48.589150 epoch: 2 step: 0 cls_loss= 0.31879 (118741 samples/sec)
2024-03-10 14:07:57.877142 epoch: 2 step: 100 cls_loss= 0.27495 (3233 samples/sec)
saving....
2024-03-10 14:08:07.907896------------------------------------------------------ Precision@1: 65.63% 

[65.88, 65.9, 65.63]

Epoch: 3
2024-03-10 14:08:08.168189 epoch: 3 step: 0 cls_loss= 0.29560 (115963 samples/sec)
2024-03-10 14:08:17.463750 epoch: 3 step: 100 cls_loss= 0.31191 (3230 samples/sec)
saving....
2024-03-10 14:08:27.519718------------------------------------------------------ Precision@1: 65.74% 

[65.88, 65.9, 65.63, 65.74]

Epoch: 4
2024-03-10 14:08:27.804571 epoch: 4 step: 0 cls_loss= 0.27122 (105840 samples/sec)
2024-03-10 14:08:37.147834 epoch: 4 step: 100 cls_loss= 0.29154 (3214 samples/sec)
saving....
2024-03-10 14:08:47.181563------------------------------------------------------ Precision@1: 65.81% 

[65.88, 65.9, 65.63, 65.74, 65.81]

Epoch: 5
2024-03-10 14:08:47.449113 epoch: 5 step: 0 cls_loss= 0.32598 (112801 samples/sec)
2024-03-10 14:08:56.805097 epoch: 5 step: 100 cls_loss= 0.31103 (3209 samples/sec)
saving....
2024-03-10 14:09:06.748365------------------------------------------------------ Precision@1: 65.76% 

[65.88, 65.9, 65.63, 65.74, 65.81, 65.76]

Epoch: 6
2024-03-10 14:09:07.007205 epoch: 6 step: 0 cls_loss= 0.30518 (116619 samples/sec)
2024-03-10 14:09:16.342426 epoch: 6 step: 100 cls_loss= 0.34412 (3216 samples/sec)
saving....
2024-03-10 14:09:26.250218------------------------------------------------------ Precision@1: 65.75% 

[65.88, 65.9, 65.63, 65.74, 65.81, 65.76, 65.75]

Epoch: 7
2024-03-10 14:09:26.515582 epoch: 7 step: 0 cls_loss= 0.31237 (113692 samples/sec)
2024-03-10 14:09:35.885055 epoch: 7 step: 100 cls_loss= 0.32073 (3205 samples/sec)
saving....
2024-03-10 14:09:45.859247------------------------------------------------------ Precision@1: 65.76% 

[65.88, 65.9, 65.63, 65.74, 65.81, 65.76, 65.75, 65.76]

Epoch: 8
2024-03-10 14:09:46.125749 epoch: 8 step: 0 cls_loss= 0.30083 (113137 samples/sec)
2024-03-10 14:09:55.480387 epoch: 8 step: 100 cls_loss= 0.37373 (3210 samples/sec)
saving....
2024-03-10 14:10:05.436979------------------------------------------------------ Precision@1: 65.76% 

[65.88, 65.9, 65.63, 65.74, 65.81, 65.76, 65.75, 65.76, 65.76]

Epoch: 9
2024-03-10 14:10:05.685722 epoch: 9 step: 0 cls_loss= 0.32712 (121394 samples/sec)
2024-03-10 14:10:15.019061 epoch: 9 step: 100 cls_loss= 0.28962 (3217 samples/sec)
saving....
2024-03-10 14:10:24.940043------------------------------------------------------ Precision@1: 65.54% 

[65.88, 65.9, 65.63, 65.74, 65.81, 65.76, 65.75, 65.76, 65.76, 65.54]

Epoch: 10
2024-03-10 14:10:25.181500 epoch: 10 step: 0 cls_loss= 0.30189 (125101 samples/sec)
2024-03-10 14:10:34.501301 epoch: 10 step: 100 cls_loss= 0.34728 (3222 samples/sec)
saving....
2024-03-10 14:10:44.390087------------------------------------------------------ Precision@1: 65.54% 

[65.88, 65.9, 65.63, 65.74, 65.81, 65.76, 65.75, 65.76, 65.76, 65.54, 65.54]

Epoch: 11
2024-03-10 14:10:44.646015 epoch: 11 step: 0 cls_loss= 0.25730 (117955 samples/sec)
2024-03-10 14:10:53.960322 epoch: 11 step: 100 cls_loss= 0.34760 (3224 samples/sec)
saving....
2024-03-10 14:11:03.879728------------------------------------------------------ Precision@1: 65.90% 

[65.88, 65.9, 65.63, 65.74, 65.81, 65.76, 65.75, 65.76, 65.76, 65.54, 65.54, 65.9]

Epoch: 12
2024-03-10 14:11:04.144998 epoch: 12 step: 0 cls_loss= 0.27714 (113639 samples/sec)
2024-03-10 14:11:13.450191 epoch: 12 step: 100 cls_loss= 0.35069 (3227 samples/sec)
saving....
2024-03-10 14:11:23.365051------------------------------------------------------ Precision@1: 65.78% 

[65.88, 65.9, 65.63, 65.74, 65.81, 65.76, 65.75, 65.76, 65.76, 65.54, 65.54, 65.9, 65.78]

Epoch: 13
2024-03-10 14:11:23.602304 epoch: 13 step: 0 cls_loss= 0.28419 (127278 samples/sec)
2024-03-10 14:11:32.895687 epoch: 13 step: 100 cls_loss= 0.27753 (3231 samples/sec)
saving....
2024-03-10 14:11:42.774673------------------------------------------------------ Precision@1: 65.62% 

[65.88, 65.9, 65.63, 65.74, 65.81, 65.76, 65.75, 65.76, 65.76, 65.54, 65.54, 65.9, 65.78, 65.62]

Epoch: 14
2024-03-10 14:11:43.029385 epoch: 14 step: 0 cls_loss= 0.33717 (118577 samples/sec)
2024-03-10 14:11:52.347707 epoch: 14 step: 100 cls_loss= 0.24826 (3222 samples/sec)
saving....
2024-03-10 14:12:02.281850------------------------------------------------------ Precision@1: 65.74% 

[65.88, 65.9, 65.63, 65.74, 65.81, 65.76, 65.75, 65.76, 65.76, 65.54, 65.54, 65.9, 65.78, 65.62, 65.74]

Epoch: 15
2024-03-10 14:12:02.532184 epoch: 15 step: 0 cls_loss= 0.33326 (120666 samples/sec)
2024-03-10 14:12:11.819740 epoch: 15 step: 100 cls_loss= 0.28173 (3233 samples/sec)
saving....
2024-03-10 14:12:21.737223------------------------------------------------------ Precision@1: 65.66% 

[65.88, 65.9, 65.63, 65.74, 65.81, 65.76, 65.75, 65.76, 65.76, 65.54, 65.54, 65.9, 65.78, 65.62, 65.74, 65.66]

Epoch: 16
2024-03-10 14:12:22.002665 epoch: 16 step: 0 cls_loss= 0.32023 (113714 samples/sec)
2024-03-10 14:12:31.341675 epoch: 16 step: 100 cls_loss= 0.31925 (3213 samples/sec)
saving....
2024-03-10 14:12:41.278665------------------------------------------------------ Precision@1: 65.67% 

[65.88, 65.9, 65.63, 65.74, 65.81, 65.76, 65.75, 65.76, 65.76, 65.54, 65.54, 65.9, 65.78, 65.62, 65.74, 65.66, 65.67]

Epoch: 17
2024-03-10 14:12:41.532731 epoch: 17 step: 0 cls_loss= 0.35694 (118783 samples/sec)
2024-03-10 14:12:50.896231 epoch: 17 step: 100 cls_loss= 0.34408 (3207 samples/sec)
saving....
2024-03-10 14:13:00.823105------------------------------------------------------ Precision@1: 65.92% 

[65.88, 65.9, 65.63, 65.74, 65.81, 65.76, 65.75, 65.76, 65.76, 65.54, 65.54, 65.9, 65.78, 65.62, 65.74, 65.66, 65.67, 65.92]

Epoch: 18
2024-03-10 14:13:01.080917 epoch: 18 step: 0 cls_loss= 0.31793 (117030 samples/sec)
2024-03-10 14:13:10.397082 epoch: 18 step: 100 cls_loss= 0.36228 (3223 samples/sec)
saving....
2024-03-10 14:13:20.428886------------------------------------------------------ Precision@1: 65.66% 

[65.88, 65.9, 65.63, 65.74, 65.81, 65.76, 65.75, 65.76, 65.76, 65.54, 65.54, 65.9, 65.78, 65.62, 65.74, 65.66, 65.67, 65.92, 65.66]

Epoch: 19
2024-03-10 14:13:20.694315 epoch: 19 step: 0 cls_loss= 0.35929 (113624 samples/sec)
2024-03-10 14:13:30.029869 epoch: 19 step: 100 cls_loss= 0.30137 (3216 samples/sec)
saving....
2024-03-10 14:13:39.979866------------------------------------------------------ Precision@1: 65.82% 

[65.88, 65.9, 65.63, 65.74, 65.81, 65.76, 65.75, 65.76, 65.76, 65.54, 65.54, 65.9, 65.78, 65.62, 65.74, 65.66, 65.67, 65.92, 65.66, 65.82]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
optimizer => CustomSGD
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
./cifar100_train_eval.py:175: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1488.)
  d_p.add_(weight_decay, p.data)
2024-03-10 14:13:42.942633 epoch: 0 step: 0 cls_loss= 0.29168 (40013 samples/sec)
2024-03-10 14:13:52.257641 epoch: 0 step: 100 cls_loss= 0.36043 (3220 samples/sec)
saving....
2024-03-10 14:14:02.412920------------------------------------------------------ Precision@1: 65.73% 

[65.73]

Epoch: 1
2024-03-10 14:14:02.653808 epoch: 1 step: 0 cls_loss= 0.37394 (125342 samples/sec)
2024-03-10 14:14:11.979175 epoch: 1 step: 100 cls_loss= 0.29482 (3220 samples/sec)
saving....
2024-03-10 14:14:21.978697------------------------------------------------------ Precision@1: 65.82% 

[65.73, 65.82]

Epoch: 2
2024-03-10 14:14:22.226225 epoch: 2 step: 0 cls_loss= 0.33169 (121965 samples/sec)
2024-03-10 14:14:31.522535 epoch: 2 step: 100 cls_loss= 0.29434 (3230 samples/sec)
saving....
2024-03-10 14:14:41.414397------------------------------------------------------ Precision@1: 65.65% 

[65.73, 65.82, 65.65]

Epoch: 3
2024-03-10 14:14:41.665987 epoch: 3 step: 0 cls_loss= 0.30190 (119974 samples/sec)
2024-03-10 14:14:51.006019 epoch: 3 step: 100 cls_loss= 0.33334 (3215 samples/sec)
saving....
2024-03-10 14:15:00.943194------------------------------------------------------ Precision@1: 65.83% 

[65.73, 65.82, 65.65, 65.83]

Epoch: 4
2024-03-10 14:15:01.188206 epoch: 4 step: 0 cls_loss= 0.34796 (123206 samples/sec)
2024-03-10 14:15:10.487979 epoch: 4 step: 100 cls_loss= 0.42839 (3229 samples/sec)
saving....
2024-03-10 14:15:20.475704------------------------------------------------------ Precision@1: 65.81% 

[65.73, 65.82, 65.65, 65.83, 65.81]

Epoch: 5
2024-03-10 14:15:20.744715 epoch: 5 step: 0 cls_loss= 0.29619 (112094 samples/sec)
2024-03-10 14:15:30.048980 epoch: 5 step: 100 cls_loss= 0.33592 (3227 samples/sec)
saving....
2024-03-10 14:15:39.910056------------------------------------------------------ Precision@1: 65.73% 

[65.73, 65.82, 65.65, 65.83, 65.81, 65.73]

Epoch: 6
2024-03-10 14:15:40.155136 epoch: 6 step: 0 cls_loss= 0.33855 (123108 samples/sec)
2024-03-10 14:15:49.484245 epoch: 6 step: 100 cls_loss= 0.29158 (3218 samples/sec)
saving....
2024-03-10 14:15:59.370449------------------------------------------------------ Precision@1: 65.67% 

[65.73, 65.82, 65.65, 65.83, 65.81, 65.73, 65.67]

Epoch: 7
2024-03-10 14:15:59.624160 epoch: 7 step: 0 cls_loss= 0.30036 (118935 samples/sec)
2024-03-10 14:16:08.994950 epoch: 7 step: 100 cls_loss= 0.31740 (3204 samples/sec)
saving....
2024-03-10 14:16:18.876749------------------------------------------------------ Precision@1: 65.84% 

[65.73, 65.82, 65.65, 65.83, 65.81, 65.73, 65.67, 65.84]

Epoch: 8
2024-03-10 14:16:19.115219 epoch: 8 step: 0 cls_loss= 0.34085 (126632 samples/sec)
2024-03-10 14:16:28.399792 epoch: 8 step: 100 cls_loss= 0.29196 (3234 samples/sec)
saving....
2024-03-10 14:16:38.259971------------------------------------------------------ Precision@1: 65.75% 

[65.73, 65.82, 65.65, 65.83, 65.81, 65.73, 65.67, 65.84, 65.75]

Epoch: 9
2024-03-10 14:16:38.502090 epoch: 9 step: 0 cls_loss= 0.26926 (124728 samples/sec)
2024-03-10 14:16:47.815396 epoch: 9 step: 100 cls_loss= 0.44786 (3224 samples/sec)
saving....
2024-03-10 14:16:57.714801------------------------------------------------------ Precision@1: 65.71% 

[65.73, 65.82, 65.65, 65.83, 65.81, 65.73, 65.67, 65.84, 65.75, 65.71]

Epoch: 10
2024-03-10 14:16:57.976465 epoch: 10 step: 0 cls_loss= 0.32865 (115280 samples/sec)
2024-03-10 14:17:07.287389 epoch: 10 step: 100 cls_loss= 0.31817 (3225 samples/sec)
saving....
2024-03-10 14:17:17.166536------------------------------------------------------ Precision@1: 65.79% 

[65.73, 65.82, 65.65, 65.83, 65.81, 65.73, 65.67, 65.84, 65.75, 65.71, 65.79]

Epoch: 11
2024-03-10 14:17:17.425920 epoch: 11 step: 0 cls_loss= 0.32338 (116413 samples/sec)
2024-03-10 14:17:26.772986 epoch: 11 step: 100 cls_loss= 0.33440 (3212 samples/sec)
saving....
2024-03-10 14:17:36.632630------------------------------------------------------ Precision@1: 65.53% 

[65.73, 65.82, 65.65, 65.83, 65.81, 65.73, 65.67, 65.84, 65.75, 65.71, 65.79, 65.53]

Epoch: 12
2024-03-10 14:17:36.877946 epoch: 12 step: 0 cls_loss= 0.35061 (122971 samples/sec)
2024-03-10 14:17:46.212418 epoch: 12 step: 100 cls_loss= 0.37202 (3217 samples/sec)
saving....
2024-03-10 14:17:56.071105------------------------------------------------------ Precision@1: 65.65% 

[65.73, 65.82, 65.65, 65.83, 65.81, 65.73, 65.67, 65.84, 65.75, 65.71, 65.79, 65.53, 65.65]

Epoch: 13
2024-03-10 14:17:56.328650 epoch: 13 step: 0 cls_loss= 0.40284 (117221 samples/sec)
2024-03-10 14:18:05.673661 epoch: 13 step: 100 cls_loss= 0.36251 (3213 samples/sec)
saving....
2024-03-10 14:18:15.575038------------------------------------------------------ Precision@1: 65.81% 

[65.73, 65.82, 65.65, 65.83, 65.81, 65.73, 65.67, 65.84, 65.75, 65.71, 65.79, 65.53, 65.65, 65.81]

Epoch: 14
2024-03-10 14:18:15.837312 epoch: 14 step: 0 cls_loss= 0.31822 (115056 samples/sec)
2024-03-10 14:18:25.214593 epoch: 14 step: 100 cls_loss= 0.32143 (3202 samples/sec)
saving....
2024-03-10 14:18:35.081332------------------------------------------------------ Precision@1: 65.78% 

[65.73, 65.82, 65.65, 65.83, 65.81, 65.73, 65.67, 65.84, 65.75, 65.71, 65.79, 65.53, 65.65, 65.81, 65.78]

Epoch: 15
2024-03-10 14:18:35.321450 epoch: 15 step: 0 cls_loss= 0.30848 (125746 samples/sec)
2024-03-10 14:18:44.731080 epoch: 15 step: 100 cls_loss= 0.31434 (3191 samples/sec)
saving....
2024-03-10 14:18:54.648257------------------------------------------------------ Precision@1: 65.77% 

[65.73, 65.82, 65.65, 65.83, 65.81, 65.73, 65.67, 65.84, 65.75, 65.71, 65.79, 65.53, 65.65, 65.81, 65.78, 65.77]

Epoch: 16
2024-03-10 14:18:54.917099 epoch: 16 step: 0 cls_loss= 0.28663 (112275 samples/sec)
2024-03-10 14:19:04.249834 epoch: 16 step: 100 cls_loss= 0.30994 (3217 samples/sec)
saving....
2024-03-10 14:19:14.105727------------------------------------------------------ Precision@1: 65.66% 

[65.73, 65.82, 65.65, 65.83, 65.81, 65.73, 65.67, 65.84, 65.75, 65.71, 65.79, 65.53, 65.65, 65.81, 65.78, 65.77, 65.66]

Epoch: 17
2024-03-10 14:19:14.375655 epoch: 17 step: 0 cls_loss= 0.35140 (111793 samples/sec)
2024-03-10 14:19:23.704492 epoch: 17 step: 100 cls_loss= 0.27111 (3219 samples/sec)
saving....
2024-03-10 14:19:33.601682------------------------------------------------------ Precision@1: 65.79% 

[65.73, 65.82, 65.65, 65.83, 65.81, 65.73, 65.67, 65.84, 65.75, 65.71, 65.79, 65.53, 65.65, 65.81, 65.78, 65.77, 65.66, 65.79]

Epoch: 18
2024-03-10 14:19:33.857141 epoch: 18 step: 0 cls_loss= 0.38378 (118200 samples/sec)
2024-03-10 14:19:43.197184 epoch: 18 step: 100 cls_loss= 0.29578 (3213 samples/sec)
saving....
2024-03-10 14:19:53.077394------------------------------------------------------ Precision@1: 65.68% 

[65.73, 65.82, 65.65, 65.83, 65.81, 65.73, 65.67, 65.84, 65.75, 65.71, 65.79, 65.53, 65.65, 65.81, 65.78, 65.77, 65.66, 65.79, 65.68]

Epoch: 19
2024-03-10 14:19:53.336125 epoch: 19 step: 0 cls_loss= 0.31868 (116509 samples/sec)
2024-03-10 14:20:02.658314 epoch: 19 step: 100 cls_loss= 0.32404 (3221 samples/sec)
saving....
2024-03-10 14:20:12.605723------------------------------------------------------ Precision@1: 65.78% 

[65.73, 65.82, 65.65, 65.83, 65.81, 65.73, 65.67, 65.84, 65.75, 65.71, 65.79, 65.53, 65.65, 65.81, 65.78, 65.77, 65.66, 65.79, 65.68, 65.78]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 14:20:15.496334 epoch: 0 step: 0 cls_loss= 0.35631 (40713 samples/sec)
2024-03-10 14:20:25.019971 epoch: 0 step: 100 cls_loss= 0.34056 (3150 samples/sec)
saving....
2024-03-10 14:20:35.321199------------------------------------------------------ Precision@1: 65.68% 

[65.68]

Epoch: 1
2024-03-10 14:20:35.593648 epoch: 1 step: 0 cls_loss= 0.28256 (110661 samples/sec)
2024-03-10 14:20:45.048773 epoch: 1 step: 100 cls_loss= 0.28575 (3173 samples/sec)
saving....
2024-03-10 14:20:55.078515------------------------------------------------------ Precision@1: 65.76% 

[65.68, 65.76]

Epoch: 2
2024-03-10 14:20:55.345077 epoch: 2 step: 0 cls_loss= 0.35404 (113136 samples/sec)
2024-03-10 14:21:04.847864 epoch: 2 step: 100 cls_loss= 0.28444 (3157 samples/sec)
saving....
2024-03-10 14:21:14.891304------------------------------------------------------ Precision@1: 65.77% 

[65.68, 65.76, 65.77]

Epoch: 3
2024-03-10 14:21:15.146766 epoch: 3 step: 0 cls_loss= 0.28278 (118110 samples/sec)
2024-03-10 14:21:24.666904 epoch: 3 step: 100 cls_loss= 0.36591 (3151 samples/sec)
saving....
2024-03-10 14:21:34.676002------------------------------------------------------ Precision@1: 65.81% 

[65.68, 65.76, 65.77, 65.81]

Epoch: 4
2024-03-10 14:21:34.944896 epoch: 4 step: 0 cls_loss= 0.38547 (112251 samples/sec)
2024-03-10 14:21:44.434629 epoch: 4 step: 100 cls_loss= 0.36536 (3161 samples/sec)
saving....
2024-03-10 14:21:54.460195------------------------------------------------------ Precision@1: 65.93% 

[65.68, 65.76, 65.77, 65.81, 65.93]

Epoch: 5
2024-03-10 14:21:54.731450 epoch: 5 step: 0 cls_loss= 0.30366 (111212 samples/sec)
2024-03-10 14:22:04.211847 epoch: 5 step: 100 cls_loss= 0.34216 (3164 samples/sec)
saving....
2024-03-10 14:22:14.275589------------------------------------------------------ Precision@1: 65.70% 

[65.68, 65.76, 65.77, 65.81, 65.93, 65.7]

Epoch: 6
2024-03-10 14:22:14.532731 epoch: 6 step: 0 cls_loss= 0.32603 (117321 samples/sec)
2024-03-10 14:22:24.104648 epoch: 6 step: 100 cls_loss= 0.26418 (3134 samples/sec)
saving....
2024-03-10 14:22:34.237985------------------------------------------------------ Precision@1: 65.65% 

[65.68, 65.76, 65.77, 65.81, 65.93, 65.7, 65.65]

Epoch: 7
2024-03-10 14:22:34.493284 epoch: 7 step: 0 cls_loss= 0.37148 (118175 samples/sec)
2024-03-10 14:22:43.962985 epoch: 7 step: 100 cls_loss= 0.34122 (3168 samples/sec)
saving....
2024-03-10 14:22:53.998241------------------------------------------------------ Precision@1: 65.64% 

[65.68, 65.76, 65.77, 65.81, 65.93, 65.7, 65.65, 65.64]

Epoch: 8
2024-03-10 14:22:54.261042 epoch: 8 step: 0 cls_loss= 0.27694 (114845 samples/sec)
2024-03-10 14:23:03.807138 epoch: 8 step: 100 cls_loss= 0.28414 (3142 samples/sec)
saving....
2024-03-10 14:23:13.894644------------------------------------------------------ Precision@1: 65.75% 

[65.68, 65.76, 65.77, 65.81, 65.93, 65.7, 65.65, 65.64, 65.75]

Epoch: 9
2024-03-10 14:23:14.154039 epoch: 9 step: 0 cls_loss= 0.29505 (116354 samples/sec)
2024-03-10 14:23:23.705800 epoch: 9 step: 100 cls_loss= 0.28745 (3140 samples/sec)
saving....
2024-03-10 14:23:33.892479------------------------------------------------------ Precision@1: 65.44% 

[65.68, 65.76, 65.77, 65.81, 65.93, 65.7, 65.65, 65.64, 65.75, 65.44]

Epoch: 10
2024-03-10 14:23:34.159357 epoch: 10 step: 0 cls_loss= 0.33146 (113041 samples/sec)
2024-03-10 14:23:43.651522 epoch: 10 step: 100 cls_loss= 0.37232 (3160 samples/sec)
saving....
2024-03-10 14:23:53.716748------------------------------------------------------ Precision@1: 65.49% 

[65.68, 65.76, 65.77, 65.81, 65.93, 65.7, 65.65, 65.64, 65.75, 65.44, 65.49]

Epoch: 11
2024-03-10 14:23:53.995986 epoch: 11 step: 0 cls_loss= 0.32552 (107958 samples/sec)
2024-03-10 14:24:03.469345 epoch: 11 step: 100 cls_loss= 0.27229 (3166 samples/sec)
saving....
2024-03-10 14:24:13.538227------------------------------------------------------ Precision@1: 65.51% 

[65.68, 65.76, 65.77, 65.81, 65.93, 65.7, 65.65, 65.64, 65.75, 65.44, 65.49, 65.51]

Epoch: 12
2024-03-10 14:24:13.816711 epoch: 12 step: 0 cls_loss= 0.32013 (108334 samples/sec)
2024-03-10 14:24:23.278006 epoch: 12 step: 100 cls_loss= 0.24724 (3170 samples/sec)
saving....
2024-03-10 14:24:33.308200------------------------------------------------------ Precision@1: 65.79% 

[65.68, 65.76, 65.77, 65.81, 65.93, 65.7, 65.65, 65.64, 65.75, 65.44, 65.49, 65.51, 65.79]

Epoch: 13
2024-03-10 14:24:33.581898 epoch: 13 step: 0 cls_loss= 0.30078 (110161 samples/sec)
2024-03-10 14:24:43.053226 epoch: 13 step: 100 cls_loss= 0.23333 (3167 samples/sec)
saving....
2024-03-10 14:24:53.047079------------------------------------------------------ Precision@1: 65.63% 

[65.68, 65.76, 65.77, 65.81, 65.93, 65.7, 65.65, 65.64, 65.75, 65.44, 65.49, 65.51, 65.79, 65.63]

Epoch: 14
2024-03-10 14:24:53.315937 epoch: 14 step: 0 cls_loss= 0.30954 (112308 samples/sec)
2024-03-10 14:25:02.775902 epoch: 14 step: 100 cls_loss= 0.29000 (3171 samples/sec)
saving....
2024-03-10 14:25:12.824539------------------------------------------------------ Precision@1: 65.51% 

[65.68, 65.76, 65.77, 65.81, 65.93, 65.7, 65.65, 65.64, 65.75, 65.44, 65.49, 65.51, 65.79, 65.63, 65.51]

Epoch: 15
2024-03-10 14:25:13.098093 epoch: 15 step: 0 cls_loss= 0.27114 (110301 samples/sec)
2024-03-10 14:25:22.579414 epoch: 15 step: 100 cls_loss= 0.31599 (3164 samples/sec)
saving....
2024-03-10 14:25:32.617769------------------------------------------------------ Precision@1: 65.31% 

[65.68, 65.76, 65.77, 65.81, 65.93, 65.7, 65.65, 65.64, 65.75, 65.44, 65.49, 65.51, 65.79, 65.63, 65.51, 65.31]

Epoch: 16
2024-03-10 14:25:32.890825 epoch: 16 step: 0 cls_loss= 0.25452 (110468 samples/sec)
2024-03-10 14:25:42.370526 epoch: 16 step: 100 cls_loss= 0.34569 (3164 samples/sec)
saving....
2024-03-10 14:25:52.444574------------------------------------------------------ Precision@1: 65.51% 

[65.68, 65.76, 65.77, 65.81, 65.93, 65.7, 65.65, 65.64, 65.75, 65.44, 65.49, 65.51, 65.79, 65.63, 65.51, 65.31, 65.51]

Epoch: 17
2024-03-10 14:25:52.700686 epoch: 17 step: 0 cls_loss= 0.30538 (117866 samples/sec)
2024-03-10 14:26:02.276672 epoch: 17 step: 100 cls_loss= 0.25079 (3133 samples/sec)
saving....
2024-03-10 14:26:12.352714------------------------------------------------------ Precision@1: 65.44% 

[65.68, 65.76, 65.77, 65.81, 65.93, 65.7, 65.65, 65.64, 65.75, 65.44, 65.49, 65.51, 65.79, 65.63, 65.51, 65.31, 65.51, 65.44]

Epoch: 18
2024-03-10 14:26:12.611445 epoch: 18 step: 0 cls_loss= 0.25182 (116618 samples/sec)
2024-03-10 14:26:22.193080 epoch: 18 step: 100 cls_loss= 0.34513 (3131 samples/sec)
saving....
2024-03-10 14:26:32.261834------------------------------------------------------ Precision@1: 65.36% 

[65.68, 65.76, 65.77, 65.81, 65.93, 65.7, 65.65, 65.64, 65.75, 65.44, 65.49, 65.51, 65.79, 65.63, 65.51, 65.31, 65.51, 65.44, 65.36]

Epoch: 19
2024-03-10 14:26:32.542677 epoch: 19 step: 0 cls_loss= 0.28367 (107373 samples/sec)
2024-03-10 14:26:42.047269 epoch: 19 step: 100 cls_loss= 0.32026 (3156 samples/sec)
saving....
2024-03-10 14:26:52.137064------------------------------------------------------ Precision@1: 65.47% 

[65.68, 65.76, 65.77, 65.81, 65.93, 65.7, 65.65, 65.64, 65.75, 65.44, 65.49, 65.51, 65.79, 65.63, 65.51, 65.31, 65.51, 65.44, 65.36, 65.47]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 14:26:55.067056 epoch: 0 step: 0 cls_loss= 0.32979 (38520 samples/sec)
2024-03-10 14:27:04.545986 epoch: 0 step: 100 cls_loss= 0.31380 (3164 samples/sec)
saving....
2024-03-10 14:27:14.784906------------------------------------------------------ Precision@1: 65.74% 

[65.74]

Epoch: 1
2024-03-10 14:27:15.037821 epoch: 1 step: 0 cls_loss= 0.35503 (119378 samples/sec)
2024-03-10 14:27:24.698756 epoch: 1 step: 100 cls_loss= 0.29269 (3105 samples/sec)
saving....
2024-03-10 14:27:34.891600------------------------------------------------------ Precision@1: 65.77% 

[65.74, 65.77]

Epoch: 2
2024-03-10 14:27:35.188064 epoch: 2 step: 0 cls_loss= 0.29682 (101689 samples/sec)
2024-03-10 14:27:44.672743 epoch: 2 step: 100 cls_loss= 0.33965 (3163 samples/sec)
saving....
2024-03-10 14:27:54.769680------------------------------------------------------ Precision@1: 65.65% 

[65.74, 65.77, 65.65]

Epoch: 3
2024-03-10 14:27:55.039627 epoch: 3 step: 0 cls_loss= 0.33229 (111844 samples/sec)
2024-03-10 14:28:04.543870 epoch: 3 step: 100 cls_loss= 0.40098 (3156 samples/sec)
saving....
2024-03-10 14:28:14.618539------------------------------------------------------ Precision@1: 65.72% 

[65.74, 65.77, 65.65, 65.72]

Epoch: 4
2024-03-10 14:28:14.887366 epoch: 4 step: 0 cls_loss= 0.34081 (112301 samples/sec)
2024-03-10 14:28:24.369722 epoch: 4 step: 100 cls_loss= 0.35077 (3163 samples/sec)
saving....
2024-03-10 14:28:34.389349------------------------------------------------------ Precision@1: 65.85% 

[65.74, 65.77, 65.65, 65.72, 65.85]

Epoch: 5
2024-03-10 14:28:34.667723 epoch: 5 step: 0 cls_loss= 0.35417 (108401 samples/sec)
2024-03-10 14:28:44.160869 epoch: 5 step: 100 cls_loss= 0.31534 (3160 samples/sec)
saving....
2024-03-10 14:28:54.172810------------------------------------------------------ Precision@1: 65.64% 

[65.74, 65.77, 65.65, 65.72, 65.85, 65.64]

Epoch: 6
2024-03-10 14:28:54.444348 epoch: 6 step: 0 cls_loss= 0.27938 (111100 samples/sec)
2024-03-10 14:29:03.974099 epoch: 6 step: 100 cls_loss= 0.32494 (3148 samples/sec)
saving....
2024-03-10 14:29:14.063086------------------------------------------------------ Precision@1: 65.49% 

[65.74, 65.77, 65.65, 65.72, 65.85, 65.64, 65.49]

Epoch: 7
2024-03-10 14:29:14.325777 epoch: 7 step: 0 cls_loss= 0.32020 (114850 samples/sec)
2024-03-10 14:29:23.840377 epoch: 7 step: 100 cls_loss= 0.31926 (3153 samples/sec)
saving....
2024-03-10 14:29:33.844908------------------------------------------------------ Precision@1: 65.56% 

[65.74, 65.77, 65.65, 65.72, 65.85, 65.64, 65.49, 65.56]

Epoch: 8
2024-03-10 14:29:34.114070 epoch: 8 step: 0 cls_loss= 0.26741 (112165 samples/sec)
2024-03-10 14:29:43.649589 epoch: 8 step: 100 cls_loss= 0.28800 (3146 samples/sec)
saving....
2024-03-10 14:29:53.693551------------------------------------------------------ Precision@1: 65.17% 

[65.74, 65.77, 65.65, 65.72, 65.85, 65.64, 65.49, 65.56, 65.17]

Epoch: 9
2024-03-10 14:29:53.968672 epoch: 9 step: 0 cls_loss= 0.32192 (109667 samples/sec)
2024-03-10 14:30:03.478248 epoch: 9 step: 100 cls_loss= 0.34427 (3154 samples/sec)
saving....
2024-03-10 14:30:13.585611------------------------------------------------------ Precision@1: 65.34% 

[65.74, 65.77, 65.65, 65.72, 65.85, 65.64, 65.49, 65.56, 65.17, 65.34]

Epoch: 10
2024-03-10 14:30:13.845981 epoch: 10 step: 0 cls_loss= 0.31741 (115921 samples/sec)
2024-03-10 14:30:23.345686 epoch: 10 step: 100 cls_loss= 0.27637 (3158 samples/sec)
saving....
2024-03-10 14:30:33.429293------------------------------------------------------ Precision@1: 65.67% 

[65.74, 65.77, 65.65, 65.72, 65.85, 65.64, 65.49, 65.56, 65.17, 65.34, 65.67]

Epoch: 11
2024-03-10 14:30:33.697738 epoch: 11 step: 0 cls_loss= 0.29537 (112389 samples/sec)
2024-03-10 14:30:43.196663 epoch: 11 step: 100 cls_loss= 0.35606 (3158 samples/sec)
saving....
2024-03-10 14:30:53.221940------------------------------------------------------ Precision@1: 65.59% 

[65.74, 65.77, 65.65, 65.72, 65.85, 65.64, 65.49, 65.56, 65.17, 65.34, 65.67, 65.59]

Epoch: 12
2024-03-10 14:30:53.489208 epoch: 12 step: 0 cls_loss= 0.30464 (112928 samples/sec)
2024-03-10 14:31:03.007871 epoch: 12 step: 100 cls_loss= 0.26468 (3151 samples/sec)
saving....
2024-03-10 14:31:13.075270------------------------------------------------------ Precision@1: 65.58% 

[65.74, 65.77, 65.65, 65.72, 65.85, 65.64, 65.49, 65.56, 65.17, 65.34, 65.67, 65.59, 65.58]

Epoch: 13
2024-03-10 14:31:13.357978 epoch: 13 step: 0 cls_loss= 0.36458 (106734 samples/sec)
2024-03-10 14:31:22.876662 epoch: 13 step: 100 cls_loss= 0.33839 (3151 samples/sec)
saving....
2024-03-10 14:31:32.929332------------------------------------------------------ Precision@1: 65.56% 

[65.74, 65.77, 65.65, 65.72, 65.85, 65.64, 65.49, 65.56, 65.17, 65.34, 65.67, 65.59, 65.58, 65.56]

Epoch: 14
2024-03-10 14:31:33.194081 epoch: 14 step: 0 cls_loss= 0.35816 (113984 samples/sec)
2024-03-10 14:31:42.756145 epoch: 14 step: 100 cls_loss= 0.33874 (3137 samples/sec)
saving....
2024-03-10 14:31:52.811055------------------------------------------------------ Precision@1: 65.36% 

[65.74, 65.77, 65.65, 65.72, 65.85, 65.64, 65.49, 65.56, 65.17, 65.34, 65.67, 65.59, 65.58, 65.56, 65.36]

Epoch: 15
2024-03-10 14:31:53.106924 epoch: 15 step: 0 cls_loss= 0.24115 (101868 samples/sec)
2024-03-10 14:32:02.611361 epoch: 15 step: 100 cls_loss= 0.32615 (3156 samples/sec)
saving....
2024-03-10 14:32:12.631848------------------------------------------------------ Precision@1: 65.44% 

[65.74, 65.77, 65.65, 65.72, 65.85, 65.64, 65.49, 65.56, 65.17, 65.34, 65.67, 65.59, 65.58, 65.56, 65.36, 65.44]

Epoch: 16
2024-03-10 14:32:12.909941 epoch: 16 step: 0 cls_loss= 0.29386 (108504 samples/sec)
2024-03-10 14:32:22.413461 epoch: 16 step: 100 cls_loss= 0.26445 (3156 samples/sec)
saving....
2024-03-10 14:32:32.477858------------------------------------------------------ Precision@1: 65.52% 

[65.74, 65.77, 65.65, 65.72, 65.85, 65.64, 65.49, 65.56, 65.17, 65.34, 65.67, 65.59, 65.58, 65.56, 65.36, 65.44, 65.52]

Epoch: 17
2024-03-10 14:32:32.748194 epoch: 17 step: 0 cls_loss= 0.32749 (111644 samples/sec)
2024-03-10 14:32:42.255001 epoch: 17 step: 100 cls_loss= 0.33359 (3155 samples/sec)
saving....
2024-03-10 14:32:52.339142------------------------------------------------------ Precision@1: 65.32% 

[65.74, 65.77, 65.65, 65.72, 65.85, 65.64, 65.49, 65.56, 65.17, 65.34, 65.67, 65.59, 65.58, 65.56, 65.36, 65.44, 65.52, 65.32]

Epoch: 18
2024-03-10 14:32:52.600814 epoch: 18 step: 0 cls_loss= 0.27394 (115355 samples/sec)
2024-03-10 14:33:02.111259 epoch: 18 step: 100 cls_loss= 0.23218 (3154 samples/sec)
saving....
2024-03-10 14:33:12.267035------------------------------------------------------ Precision@1: 65.33% 

[65.74, 65.77, 65.65, 65.72, 65.85, 65.64, 65.49, 65.56, 65.17, 65.34, 65.67, 65.59, 65.58, 65.56, 65.36, 65.44, 65.52, 65.32, 65.33]

Epoch: 19
2024-03-10 14:33:12.533744 epoch: 19 step: 0 cls_loss= 0.31373 (113173 samples/sec)
2024-03-10 14:33:22.045445 epoch: 19 step: 100 cls_loss= 0.27454 (3154 samples/sec)
saving....
2024-03-10 14:33:32.055291------------------------------------------------------ Precision@1: 65.42% 

[65.74, 65.77, 65.65, 65.72, 65.85, 65.64, 65.49, 65.56, 65.17, 65.34, 65.67, 65.59, 65.58, 65.56, 65.36, 65.44, 65.52, 65.32, 65.33, 65.42]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 14:33:34.940277 epoch: 0 step: 0 cls_loss= 0.27062 (40220 samples/sec)
2024-03-10 14:33:44.394370 epoch: 0 step: 100 cls_loss= 0.30089 (3173 samples/sec)
saving....
2024-03-10 14:33:54.643605------------------------------------------------------ Precision@1: 65.83% 

[65.83]

Epoch: 1
2024-03-10 14:33:54.899588 epoch: 1 step: 0 cls_loss= 0.33085 (117866 samples/sec)
2024-03-10 14:34:04.359728 epoch: 1 step: 100 cls_loss= 0.29055 (3171 samples/sec)
saving....
2024-03-10 14:34:14.418715------------------------------------------------------ Precision@1: 65.64% 

[65.83, 65.64]

Epoch: 2
2024-03-10 14:34:14.690225 epoch: 2 step: 0 cls_loss= 0.31348 (111139 samples/sec)
2024-03-10 14:34:24.250524 epoch: 2 step: 100 cls_loss= 0.35001 (3138 samples/sec)
saving....
2024-03-10 14:34:34.305045------------------------------------------------------ Precision@1: 65.80% 

[65.83, 65.64, 65.8]

Epoch: 3
2024-03-10 14:34:34.560366 epoch: 3 step: 0 cls_loss= 0.33128 (118178 samples/sec)
2024-03-10 14:34:44.095722 epoch: 3 step: 100 cls_loss= 0.33020 (3146 samples/sec)
saving....
2024-03-10 14:34:54.156238------------------------------------------------------ Precision@1: 65.57% 

[65.83, 65.64, 65.8, 65.57]

Epoch: 4
2024-03-10 14:34:54.421211 epoch: 4 step: 0 cls_loss= 0.30222 (113906 samples/sec)
2024-03-10 14:35:03.879408 epoch: 4 step: 100 cls_loss= 0.26940 (3172 samples/sec)
saving....
2024-03-10 14:35:14.021291------------------------------------------------------ Precision@1: 65.73% 

[65.83, 65.64, 65.8, 65.57, 65.73]

Epoch: 5
2024-03-10 14:35:14.295596 epoch: 5 step: 0 cls_loss= 0.27178 (110045 samples/sec)
2024-03-10 14:35:23.789879 epoch: 5 step: 100 cls_loss= 0.31697 (3159 samples/sec)
saving....
2024-03-10 14:35:33.824107------------------------------------------------------ Precision@1: 65.67% 

[65.83, 65.64, 65.8, 65.57, 65.73, 65.67]

Epoch: 6
2024-03-10 14:35:34.099628 epoch: 6 step: 0 cls_loss= 0.31364 (109550 samples/sec)
2024-03-10 14:35:43.608419 epoch: 6 step: 100 cls_loss= 0.34311 (3155 samples/sec)
saving....
2024-03-10 14:35:53.632016------------------------------------------------------ Precision@1: 65.71% 

[65.83, 65.64, 65.8, 65.57, 65.73, 65.67, 65.71]

Epoch: 7
2024-03-10 14:35:53.893609 epoch: 7 step: 0 cls_loss= 0.31621 (115296 samples/sec)
2024-03-10 14:36:03.437202 epoch: 7 step: 100 cls_loss= 0.39505 (3143 samples/sec)
saving....
2024-03-10 14:36:13.434043------------------------------------------------------ Precision@1: 65.68% 

[65.83, 65.64, 65.8, 65.57, 65.73, 65.67, 65.71, 65.68]

Epoch: 8
2024-03-10 14:36:13.699245 epoch: 8 step: 0 cls_loss= 0.26079 (113783 samples/sec)
2024-03-10 14:36:23.161216 epoch: 8 step: 100 cls_loss= 0.29528 (3170 samples/sec)
saving....
2024-03-10 14:36:33.167349------------------------------------------------------ Precision@1: 65.43% 

[65.83, 65.64, 65.8, 65.57, 65.73, 65.67, 65.71, 65.68, 65.43]

Epoch: 9
2024-03-10 14:36:33.430996 epoch: 9 step: 0 cls_loss= 0.28927 (114472 samples/sec)
2024-03-10 14:36:42.883329 epoch: 9 step: 100 cls_loss= 0.32506 (3174 samples/sec)
saving....
2024-03-10 14:36:52.895889------------------------------------------------------ Precision@1: 65.35% 

[65.83, 65.64, 65.8, 65.57, 65.73, 65.67, 65.71, 65.68, 65.43, 65.35]

Epoch: 10
2024-03-10 14:36:53.187093 epoch: 10 step: 0 cls_loss= 0.36727 (103470 samples/sec)
2024-03-10 14:37:02.724338 epoch: 10 step: 100 cls_loss= 0.35258 (3145 samples/sec)
saving....
2024-03-10 14:37:12.773420------------------------------------------------------ Precision@1: 65.54% 

[65.83, 65.64, 65.8, 65.57, 65.73, 65.67, 65.71, 65.68, 65.43, 65.35, 65.54]

Epoch: 11
2024-03-10 14:37:13.020472 epoch: 11 step: 0 cls_loss= 0.27552 (122221 samples/sec)
2024-03-10 14:37:22.560692 epoch: 11 step: 100 cls_loss= 0.34779 (3144 samples/sec)
saving....
2024-03-10 14:37:32.630999------------------------------------------------------ Precision@1: 65.48% 

[65.83, 65.64, 65.8, 65.57, 65.73, 65.67, 65.71, 65.68, 65.43, 65.35, 65.54, 65.48]

Epoch: 12
2024-03-10 14:37:32.903329 epoch: 12 step: 0 cls_loss= 0.32714 (110798 samples/sec)
2024-03-10 14:37:42.397090 epoch: 12 step: 100 cls_loss= 0.24702 (3160 samples/sec)
saving....
2024-03-10 14:37:52.412127------------------------------------------------------ Precision@1: 65.60% 

[65.83, 65.64, 65.8, 65.57, 65.73, 65.67, 65.71, 65.68, 65.43, 65.35, 65.54, 65.48, 65.6]

Epoch: 13
2024-03-10 14:37:52.671246 epoch: 13 step: 0 cls_loss= 0.29521 (116490 samples/sec)
2024-03-10 14:38:02.202710 epoch: 13 step: 100 cls_loss= 0.30573 (3147 samples/sec)
saving....
2024-03-10 14:38:12.250808------------------------------------------------------ Precision@1: 65.72% 

[65.83, 65.64, 65.8, 65.57, 65.73, 65.67, 65.71, 65.68, 65.43, 65.35, 65.54, 65.48, 65.6, 65.72]

Epoch: 14
2024-03-10 14:38:12.510721 epoch: 14 step: 0 cls_loss= 0.27997 (116114 samples/sec)
2024-03-10 14:38:22.008064 epoch: 14 step: 100 cls_loss= 0.28337 (3158 samples/sec)
saving....
2024-03-10 14:38:32.009297------------------------------------------------------ Precision@1: 65.87% 

[65.83, 65.64, 65.8, 65.57, 65.73, 65.67, 65.71, 65.68, 65.43, 65.35, 65.54, 65.48, 65.6, 65.72, 65.87]

Epoch: 15
2024-03-10 14:38:32.282736 epoch: 15 step: 0 cls_loss= 0.30767 (110346 samples/sec)
2024-03-10 14:38:41.775780 epoch: 15 step: 100 cls_loss= 0.27993 (3160 samples/sec)
saving....
2024-03-10 14:38:51.854485------------------------------------------------------ Precision@1: 65.46% 

[65.83, 65.64, 65.8, 65.57, 65.73, 65.67, 65.71, 65.68, 65.43, 65.35, 65.54, 65.48, 65.6, 65.72, 65.87, 65.46]

Epoch: 16
2024-03-10 14:38:52.124820 epoch: 16 step: 0 cls_loss= 0.30031 (111590 samples/sec)
2024-03-10 14:39:01.600355 epoch: 16 step: 100 cls_loss= 0.30964 (3166 samples/sec)
saving....
2024-03-10 14:39:11.582907------------------------------------------------------ Precision@1: 65.55% 

[65.83, 65.64, 65.8, 65.57, 65.73, 65.67, 65.71, 65.68, 65.43, 65.35, 65.54, 65.48, 65.6, 65.72, 65.87, 65.46, 65.55]

Epoch: 17
2024-03-10 14:39:11.845999 epoch: 17 step: 0 cls_loss= 0.29528 (114746 samples/sec)
2024-03-10 14:39:21.363624 epoch: 17 step: 100 cls_loss= 0.31046 (3152 samples/sec)
saving....
2024-03-10 14:39:31.370993------------------------------------------------------ Precision@1: 65.28% 

[65.83, 65.64, 65.8, 65.57, 65.73, 65.67, 65.71, 65.68, 65.43, 65.35, 65.54, 65.48, 65.6, 65.72, 65.87, 65.46, 65.55, 65.28]

Epoch: 18
2024-03-10 14:39:31.659535 epoch: 18 step: 0 cls_loss= 0.28547 (104514 samples/sec)
2024-03-10 14:39:41.117525 epoch: 18 step: 100 cls_loss= 0.28236 (3172 samples/sec)
saving....
2024-03-10 14:39:51.111218------------------------------------------------------ Precision@1: 65.52% 

[65.83, 65.64, 65.8, 65.57, 65.73, 65.67, 65.71, 65.68, 65.43, 65.35, 65.54, 65.48, 65.6, 65.72, 65.87, 65.46, 65.55, 65.28, 65.52]

Epoch: 19
2024-03-10 14:39:51.379956 epoch: 19 step: 0 cls_loss= 0.29021 (112324 samples/sec)
2024-03-10 14:40:00.859785 epoch: 19 step: 100 cls_loss= 0.34274 (3164 samples/sec)
saving....
2024-03-10 14:40:10.875900------------------------------------------------------ Precision@1: 65.34% 

[65.83, 65.64, 65.8, 65.57, 65.73, 65.67, 65.71, 65.68, 65.43, 65.35, 65.54, 65.48, 65.6, 65.72, 65.87, 65.46, 65.55, 65.28, 65.52, 65.34]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 14:40:13.780823 epoch: 0 step: 0 cls_loss= 0.33812 (39065 samples/sec)
2024-03-10 14:40:23.236050 epoch: 0 step: 100 cls_loss= 0.38089 (3172 samples/sec)
saving....
2024-03-10 14:40:33.528263------------------------------------------------------ Precision@1: 65.64% 

[65.64]

Epoch: 1
2024-03-10 14:40:33.782173 epoch: 1 step: 0 cls_loss= 0.35161 (118907 samples/sec)
2024-03-10 14:40:43.251315 epoch: 1 step: 100 cls_loss= 0.35379 (3168 samples/sec)
saving....
2024-03-10 14:40:53.347906------------------------------------------------------ Precision@1: 65.77% 

[65.64, 65.77]

Epoch: 2
2024-03-10 14:40:53.606727 epoch: 2 step: 0 cls_loss= 0.35458 (116617 samples/sec)
2024-03-10 14:41:03.110953 epoch: 2 step: 100 cls_loss= 0.35279 (3156 samples/sec)
saving....
2024-03-10 14:41:13.169313------------------------------------------------------ Precision@1: 65.62% 

[65.64, 65.77, 65.62]

Epoch: 3
2024-03-10 14:41:13.431651 epoch: 3 step: 0 cls_loss= 0.35020 (114904 samples/sec)
2024-03-10 14:41:22.896497 epoch: 3 step: 100 cls_loss= 0.35067 (3169 samples/sec)
saving....
2024-03-10 14:41:32.935208------------------------------------------------------ Precision@1: 65.81% 

[65.64, 65.77, 65.62, 65.81]

Epoch: 4
2024-03-10 14:41:33.196543 epoch: 4 step: 0 cls_loss= 0.34287 (115489 samples/sec)
2024-03-10 14:41:42.793971 epoch: 4 step: 100 cls_loss= 0.28514 (3126 samples/sec)
saving....
2024-03-10 14:41:52.857235------------------------------------------------------ Precision@1: 65.90% 

[65.64, 65.77, 65.62, 65.81, 65.9]

Epoch: 5
2024-03-10 14:41:53.126887 epoch: 5 step: 0 cls_loss= 0.29270 (111961 samples/sec)
2024-03-10 14:42:02.691791 epoch: 5 step: 100 cls_loss= 0.32785 (3136 samples/sec)
saving....
2024-03-10 14:42:12.748528------------------------------------------------------ Precision@1: 65.57% 

[65.64, 65.77, 65.62, 65.81, 65.9, 65.57]

Epoch: 6
2024-03-10 14:42:13.006522 epoch: 6 step: 0 cls_loss= 0.29663 (116989 samples/sec)
2024-03-10 14:42:22.529322 epoch: 6 step: 100 cls_loss= 0.29972 (3150 samples/sec)
saving....
2024-03-10 14:42:32.613395------------------------------------------------------ Precision@1: 65.52% 

[65.64, 65.77, 65.62, 65.81, 65.9, 65.57, 65.52]

Epoch: 7
2024-03-10 14:42:32.880570 epoch: 7 step: 0 cls_loss= 0.32566 (112848 samples/sec)
2024-03-10 14:42:42.345543 epoch: 7 step: 100 cls_loss= 0.27858 (3169 samples/sec)
saving....
2024-03-10 14:42:52.361640------------------------------------------------------ Precision@1: 65.50% 

[65.64, 65.77, 65.62, 65.81, 65.9, 65.57, 65.52, 65.5]

Epoch: 8
2024-03-10 14:42:52.646312 epoch: 8 step: 0 cls_loss= 0.30888 (105910 samples/sec)
2024-03-10 14:43:02.164553 epoch: 8 step: 100 cls_loss= 0.29564 (3152 samples/sec)
saving....
2024-03-10 14:43:12.188315------------------------------------------------------ Precision@1: 65.49% 

[65.64, 65.77, 65.62, 65.81, 65.9, 65.57, 65.52, 65.5, 65.49]

Epoch: 9
2024-03-10 14:43:12.471710 epoch: 9 step: 0 cls_loss= 0.24082 (106383 samples/sec)
2024-03-10 14:43:21.966571 epoch: 9 step: 100 cls_loss= 0.31183 (3159 samples/sec)
saving....
2024-03-10 14:43:32.166850------------------------------------------------------ Precision@1: 65.46% 

[65.64, 65.77, 65.62, 65.81, 65.9, 65.57, 65.52, 65.5, 65.49, 65.46]

Epoch: 10
2024-03-10 14:43:32.441479 epoch: 10 step: 0 cls_loss= 0.33524 (109771 samples/sec)
2024-03-10 14:43:41.950348 epoch: 10 step: 100 cls_loss= 0.26417 (3155 samples/sec)
saving....
2024-03-10 14:43:52.070820------------------------------------------------------ Precision@1: 65.63% 

[65.64, 65.77, 65.62, 65.81, 65.9, 65.57, 65.52, 65.5, 65.49, 65.46, 65.63]

Epoch: 11
2024-03-10 14:43:52.328770 epoch: 11 step: 0 cls_loss= 0.28902 (117009 samples/sec)
2024-03-10 14:44:01.856635 epoch: 11 step: 100 cls_loss= 0.29765 (3148 samples/sec)
saving....
2024-03-10 14:44:11.922379------------------------------------------------------ Precision@1: 65.47% 

[65.64, 65.77, 65.62, 65.81, 65.9, 65.57, 65.52, 65.5, 65.49, 65.46, 65.63, 65.47]

Epoch: 12
2024-03-10 14:44:12.201749 epoch: 12 step: 0 cls_loss= 0.28006 (107833 samples/sec)
2024-03-10 14:44:21.666179 epoch: 12 step: 100 cls_loss= 0.28384 (3169 samples/sec)
saving....
2024-03-10 14:44:31.727277------------------------------------------------------ Precision@1: 65.58% 

[65.64, 65.77, 65.62, 65.81, 65.9, 65.57, 65.52, 65.5, 65.49, 65.46, 65.63, 65.47, 65.58]

Epoch: 13
2024-03-10 14:44:32.008863 epoch: 13 step: 0 cls_loss= 0.31744 (107195 samples/sec)
2024-03-10 14:44:41.575258 epoch: 13 step: 100 cls_loss= 0.30576 (3136 samples/sec)
saving....
2024-03-10 14:44:51.655578------------------------------------------------------ Precision@1: 65.54% 

[65.64, 65.77, 65.62, 65.81, 65.9, 65.57, 65.52, 65.5, 65.49, 65.46, 65.63, 65.47, 65.58, 65.54]

Epoch: 14
2024-03-10 14:44:51.923879 epoch: 14 step: 0 cls_loss= 0.33055 (112475 samples/sec)
2024-03-10 14:45:01.431171 epoch: 14 step: 100 cls_loss= 0.29425 (3155 samples/sec)
saving....
2024-03-10 14:45:11.475390------------------------------------------------------ Precision@1: 65.52% 

[65.64, 65.77, 65.62, 65.81, 65.9, 65.57, 65.52, 65.5, 65.49, 65.46, 65.63, 65.47, 65.58, 65.54, 65.52]

Epoch: 15
2024-03-10 14:45:11.735891 epoch: 15 step: 0 cls_loss= 0.25057 (115835 samples/sec)
2024-03-10 14:45:21.198046 epoch: 15 step: 100 cls_loss= 0.23803 (3170 samples/sec)
saving....
2024-03-10 14:45:31.224069------------------------------------------------------ Precision@1: 65.52% 

[65.64, 65.77, 65.62, 65.81, 65.9, 65.57, 65.52, 65.5, 65.49, 65.46, 65.63, 65.47, 65.58, 65.54, 65.52, 65.52]

Epoch: 16
2024-03-10 14:45:31.477240 epoch: 16 step: 0 cls_loss= 0.29076 (119236 samples/sec)
2024-03-10 14:45:40.995792 epoch: 16 step: 100 cls_loss= 0.28374 (3151 samples/sec)
saving....
2024-03-10 14:45:51.079501------------------------------------------------------ Precision@1: 65.28% 

[65.64, 65.77, 65.62, 65.81, 65.9, 65.57, 65.52, 65.5, 65.49, 65.46, 65.63, 65.47, 65.58, 65.54, 65.52, 65.52, 65.28]

Epoch: 17
2024-03-10 14:45:51.357309 epoch: 17 step: 0 cls_loss= 0.28672 (108594 samples/sec)
2024-03-10 14:46:00.836811 epoch: 17 step: 100 cls_loss= 0.25951 (3164 samples/sec)
saving....
2024-03-10 14:46:10.907323------------------------------------------------------ Precision@1: 65.37% 

[65.64, 65.77, 65.62, 65.81, 65.9, 65.57, 65.52, 65.5, 65.49, 65.46, 65.63, 65.47, 65.58, 65.54, 65.52, 65.52, 65.28, 65.37]

Epoch: 18
2024-03-10 14:46:11.162626 epoch: 18 step: 0 cls_loss= 0.23895 (118171 samples/sec)
2024-03-10 14:46:20.702553 epoch: 18 step: 100 cls_loss= 0.26298 (3144 samples/sec)
saving....
2024-03-10 14:46:30.925495------------------------------------------------------ Precision@1: 65.63% 

[65.64, 65.77, 65.62, 65.81, 65.9, 65.57, 65.52, 65.5, 65.49, 65.46, 65.63, 65.47, 65.58, 65.54, 65.52, 65.52, 65.28, 65.37, 65.63]

Epoch: 19
2024-03-10 14:46:31.199316 epoch: 19 step: 0 cls_loss= 0.31921 (110083 samples/sec)
2024-03-10 14:46:40.693267 epoch: 19 step: 100 cls_loss= 0.28542 (3160 samples/sec)
saving....
2024-03-10 14:46:50.736582------------------------------------------------------ Precision@1: 65.42% 

[65.64, 65.77, 65.62, 65.81, 65.9, 65.57, 65.52, 65.5, 65.49, 65.46, 65.63, 65.47, 65.58, 65.54, 65.52, 65.52, 65.28, 65.37, 65.63, 65.42]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 14:46:53.621382 epoch: 0 step: 0 cls_loss= 0.32985 (39770 samples/sec)
2024-03-10 14:47:03.120611 epoch: 0 step: 100 cls_loss= 0.26945 (3158 samples/sec)
saving....
2024-03-10 14:47:13.542262------------------------------------------------------ Precision@1: 65.74% 

[65.74]

Epoch: 1
2024-03-10 14:47:13.796397 epoch: 1 step: 0 cls_loss= 0.38397 (118773 samples/sec)
2024-03-10 14:47:23.290810 epoch: 1 step: 100 cls_loss= 0.28790 (3159 samples/sec)
saving....
2024-03-10 14:47:33.317945------------------------------------------------------ Precision@1: 65.88% 

[65.74, 65.88]

Epoch: 2
2024-03-10 14:47:33.583673 epoch: 2 step: 0 cls_loss= 0.36102 (113538 samples/sec)
2024-03-10 14:47:43.067245 epoch: 2 step: 100 cls_loss= 0.34339 (3163 samples/sec)
saving....
2024-03-10 14:47:53.078863------------------------------------------------------ Precision@1: 65.70% 

[65.74, 65.88, 65.7]

Epoch: 3
2024-03-10 14:47:53.344243 epoch: 3 step: 0 cls_loss= 0.29001 (113712 samples/sec)
2024-03-10 14:48:02.880208 epoch: 3 step: 100 cls_loss= 0.32490 (3146 samples/sec)
saving....
2024-03-10 14:48:12.899330------------------------------------------------------ Precision@1: 65.75% 

[65.74, 65.88, 65.7, 65.75]

Epoch: 4
2024-03-10 14:48:13.186343 epoch: 4 step: 0 cls_loss= 0.37019 (105073 samples/sec)
2024-03-10 14:48:22.686169 epoch: 4 step: 100 cls_loss= 0.33085 (3158 samples/sec)
saving....
2024-03-10 14:48:32.705117------------------------------------------------------ Precision@1: 65.65% 

[65.74, 65.88, 65.7, 65.75, 65.65]

Epoch: 5
2024-03-10 14:48:32.965637 epoch: 5 step: 0 cls_loss= 0.34903 (115911 samples/sec)
2024-03-10 14:48:42.554429 epoch: 5 step: 100 cls_loss= 0.40152 (3128 samples/sec)
saving....
2024-03-10 14:48:52.582253------------------------------------------------------ Precision@1: 65.69% 

[65.74, 65.88, 65.7, 65.75, 65.65, 65.69]

Epoch: 6
2024-03-10 14:48:52.859945 epoch: 6 step: 0 cls_loss= 0.30567 (108631 samples/sec)
2024-03-10 14:49:02.349959 epoch: 6 step: 100 cls_loss= 0.32080 (3161 samples/sec)
saving....
2024-03-10 14:49:12.381626------------------------------------------------------ Precision@1: 65.58% 

[65.74, 65.88, 65.7, 65.75, 65.65, 65.69, 65.58]

Epoch: 7
2024-03-10 14:49:12.647617 epoch: 7 step: 0 cls_loss= 0.31547 (113463 samples/sec)
2024-03-10 14:49:22.187001 epoch: 7 step: 100 cls_loss= 0.33255 (3145 samples/sec)
saving....
2024-03-10 14:49:32.230760------------------------------------------------------ Precision@1: 65.65% 

[65.74, 65.88, 65.7, 65.75, 65.65, 65.69, 65.58, 65.65]

Epoch: 8
2024-03-10 14:49:32.477329 epoch: 8 step: 0 cls_loss= 0.34144 (122480 samples/sec)
2024-03-10 14:49:41.998587 epoch: 8 step: 100 cls_loss= 0.30113 (3150 samples/sec)
saving....
2024-03-10 14:49:52.142607------------------------------------------------------ Precision@1: 65.42% 

[65.74, 65.88, 65.7, 65.75, 65.65, 65.69, 65.58, 65.65, 65.42]

Epoch: 9
2024-03-10 14:49:52.422408 epoch: 9 step: 0 cls_loss= 0.31533 (107649 samples/sec)
2024-03-10 14:50:01.922336 epoch: 9 step: 100 cls_loss= 0.34162 (3158 samples/sec)
saving....
2024-03-10 14:50:11.960431------------------------------------------------------ Precision@1: 65.26% 

[65.74, 65.88, 65.7, 65.75, 65.65, 65.69, 65.58, 65.65, 65.42, 65.26]

Epoch: 10
2024-03-10 14:50:12.217072 epoch: 10 step: 0 cls_loss= 0.25853 (117640 samples/sec)
2024-03-10 14:50:21.721193 epoch: 10 step: 100 cls_loss= 0.33958 (3156 samples/sec)
saving....
2024-03-10 14:50:31.755991------------------------------------------------------ Precision@1: 65.38% 

[65.74, 65.88, 65.7, 65.75, 65.65, 65.69, 65.58, 65.65, 65.42, 65.26, 65.38]

Epoch: 11
2024-03-10 14:50:32.017226 epoch: 11 step: 0 cls_loss= 0.29723 (115477 samples/sec)
2024-03-10 14:50:41.475641 epoch: 11 step: 100 cls_loss= 0.26507 (3171 samples/sec)
saving....
2024-03-10 14:50:51.473929------------------------------------------------------ Precision@1: 65.24% 

[65.74, 65.88, 65.7, 65.75, 65.65, 65.69, 65.58, 65.65, 65.42, 65.26, 65.38, 65.24]

Epoch: 12
2024-03-10 14:50:51.730852 epoch: 12 step: 0 cls_loss= 0.24373 (117496 samples/sec)
2024-03-10 14:51:01.357716 epoch: 12 step: 100 cls_loss= 0.39222 (3116 samples/sec)
saving....
2024-03-10 14:51:11.467570------------------------------------------------------ Precision@1: 65.42% 

[65.74, 65.88, 65.7, 65.75, 65.65, 65.69, 65.58, 65.65, 65.42, 65.26, 65.38, 65.24, 65.42]

Epoch: 13
2024-03-10 14:51:11.746610 epoch: 13 step: 0 cls_loss= 0.25114 (108113 samples/sec)
2024-03-10 14:51:21.313443 epoch: 13 step: 100 cls_loss= 0.28555 (3136 samples/sec)
saving....
2024-03-10 14:51:31.361211------------------------------------------------------ Precision@1: 65.27% 

[65.74, 65.88, 65.7, 65.75, 65.65, 65.69, 65.58, 65.65, 65.42, 65.26, 65.38, 65.24, 65.42, 65.27]

Epoch: 14
2024-03-10 14:51:31.642622 epoch: 14 step: 0 cls_loss= 0.33106 (107218 samples/sec)
2024-03-10 14:51:41.128872 epoch: 14 step: 100 cls_loss= 0.25906 (3162 samples/sec)
saving....
2024-03-10 14:51:51.151778------------------------------------------------------ Precision@1: 65.18% 

[65.74, 65.88, 65.7, 65.75, 65.65, 65.69, 65.58, 65.65, 65.42, 65.26, 65.38, 65.24, 65.42, 65.27, 65.18]

Epoch: 15
2024-03-10 14:51:51.435409 epoch: 15 step: 0 cls_loss= 0.29768 (106267 samples/sec)
2024-03-10 14:52:00.921452 epoch: 15 step: 100 cls_loss= 0.30841 (3162 samples/sec)
saving....
2024-03-10 14:52:10.964936------------------------------------------------------ Precision@1: 65.51% 

[65.74, 65.88, 65.7, 65.75, 65.65, 65.69, 65.58, 65.65, 65.42, 65.26, 65.38, 65.24, 65.42, 65.27, 65.18, 65.51]

Epoch: 16
2024-03-10 14:52:11.251672 epoch: 16 step: 0 cls_loss= 0.27805 (105045 samples/sec)
2024-03-10 14:52:20.739641 epoch: 16 step: 100 cls_loss= 0.33428 (3162 samples/sec)
saving....
2024-03-10 14:52:30.891511------------------------------------------------------ Precision@1: 65.55% 

[65.74, 65.88, 65.7, 65.75, 65.65, 65.69, 65.58, 65.65, 65.42, 65.26, 65.38, 65.24, 65.42, 65.27, 65.18, 65.51, 65.55]

Epoch: 17
2024-03-10 14:52:31.163061 epoch: 17 step: 0 cls_loss= 0.24045 (111085 samples/sec)
2024-03-10 14:52:40.662623 epoch: 17 step: 100 cls_loss= 0.30902 (3158 samples/sec)
saving....
2024-03-10 14:52:50.754698------------------------------------------------------ Precision@1: 65.56% 

[65.74, 65.88, 65.7, 65.75, 65.65, 65.69, 65.58, 65.65, 65.42, 65.26, 65.38, 65.24, 65.42, 65.27, 65.18, 65.51, 65.55, 65.56]

Epoch: 18
2024-03-10 14:52:51.032056 epoch: 18 step: 0 cls_loss= 0.29461 (108765 samples/sec)
2024-03-10 14:53:00.505576 epoch: 18 step: 100 cls_loss= 0.23931 (3166 samples/sec)
saving....
2024-03-10 14:53:10.521925------------------------------------------------------ Precision@1: 65.49% 

[65.74, 65.88, 65.7, 65.75, 65.65, 65.69, 65.58, 65.65, 65.42, 65.26, 65.38, 65.24, 65.42, 65.27, 65.18, 65.51, 65.55, 65.56, 65.49]

Epoch: 19
2024-03-10 14:53:10.779700 epoch: 19 step: 0 cls_loss= 0.27734 (117044 samples/sec)
2024-03-10 14:53:20.282467 epoch: 19 step: 100 cls_loss= 0.20994 (3157 samples/sec)
saving....
2024-03-10 14:53:30.311919------------------------------------------------------ Precision@1: 65.68% 

[65.74, 65.88, 65.7, 65.75, 65.65, 65.69, 65.58, 65.65, 65.42, 65.26, 65.38, 65.24, 65.42, 65.27, 65.18, 65.51, 65.55, 65.56, 65.49, 65.68]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 14:53:33.223481 epoch: 0 step: 0 cls_loss= 0.25953 (39239 samples/sec)
2024-03-10 14:53:42.731517 epoch: 0 step: 100 cls_loss= 0.30341 (3155 samples/sec)
saving....
2024-03-10 14:53:53.029730------------------------------------------------------ Precision@1: 65.59% 

[65.59]

Epoch: 1
2024-03-10 14:53:53.295252 epoch: 1 step: 0 cls_loss= 0.30972 (113684 samples/sec)
2024-03-10 14:54:02.816549 epoch: 1 step: 100 cls_loss= 0.36544 (3151 samples/sec)
saving....
2024-03-10 14:54:12.803129------------------------------------------------------ Precision@1: 65.52% 

[65.59, 65.52]

Epoch: 2
2024-03-10 14:54:13.066052 epoch: 2 step: 0 cls_loss= 0.34915 (114725 samples/sec)
2024-03-10 14:54:22.529610 epoch: 2 step: 100 cls_loss= 0.37782 (3170 samples/sec)
saving....
2024-03-10 14:54:32.513028------------------------------------------------------ Precision@1: 65.62% 

[65.59, 65.52, 65.62]

Epoch: 3
2024-03-10 14:54:32.771076 epoch: 3 step: 0 cls_loss= 0.38033 (116953 samples/sec)
2024-03-10 14:54:42.235703 epoch: 3 step: 100 cls_loss= 0.39360 (3169 samples/sec)
saving....
2024-03-10 14:54:52.228171------------------------------------------------------ Precision@1: 65.88% 

[65.59, 65.52, 65.62, 65.88]

Epoch: 4
2024-03-10 14:54:52.489584 epoch: 4 step: 0 cls_loss= 0.31897 (115387 samples/sec)
2024-03-10 14:55:01.957629 epoch: 4 step: 100 cls_loss= 0.25833 (3168 samples/sec)
saving....
2024-03-10 14:55:11.930423------------------------------------------------------ Precision@1: 65.58% 

[65.59, 65.52, 65.62, 65.88, 65.58]

Epoch: 5
2024-03-10 14:55:12.211745 epoch: 5 step: 0 cls_loss= 0.34894 (107119 samples/sec)
2024-03-10 14:55:21.687571 epoch: 5 step: 100 cls_loss= 0.26439 (3166 samples/sec)
saving....
2024-03-10 14:55:31.677150------------------------------------------------------ Precision@1: 65.73% 

[65.59, 65.52, 65.62, 65.88, 65.58, 65.73]

Epoch: 6
2024-03-10 14:55:31.939700 epoch: 6 step: 0 cls_loss= 0.33686 (114927 samples/sec)
2024-03-10 14:55:41.453458 epoch: 6 step: 100 cls_loss= 0.33325 (3153 samples/sec)
saving....
2024-03-10 14:55:51.408331------------------------------------------------------ Precision@1: 65.48% 

[65.59, 65.52, 65.62, 65.88, 65.58, 65.73, 65.48]

Epoch: 7
2024-03-10 14:55:51.669871 epoch: 7 step: 0 cls_loss= 0.32923 (115290 samples/sec)
2024-03-10 14:56:01.127176 epoch: 7 step: 100 cls_loss= 0.30734 (3172 samples/sec)
saving....
2024-03-10 14:56:11.100780------------------------------------------------------ Precision@1: 65.64% 

[65.59, 65.52, 65.62, 65.88, 65.58, 65.73, 65.48, 65.64]

Epoch: 8
2024-03-10 14:56:11.375577 epoch: 8 step: 0 cls_loss= 0.29539 (109813 samples/sec)
2024-03-10 14:56:20.842185 epoch: 8 step: 100 cls_loss= 0.29952 (3169 samples/sec)
saving....
2024-03-10 14:56:30.889798------------------------------------------------------ Precision@1: 65.59% 

[65.59, 65.52, 65.62, 65.88, 65.58, 65.73, 65.48, 65.64, 65.59]

Epoch: 9
2024-03-10 14:56:31.155902 epoch: 9 step: 0 cls_loss= 0.32002 (113495 samples/sec)
2024-03-10 14:56:40.619210 epoch: 9 step: 100 cls_loss= 0.35575 (3170 samples/sec)
saving....
2024-03-10 14:56:50.636057------------------------------------------------------ Precision@1: 65.79% 

[65.59, 65.52, 65.62, 65.88, 65.58, 65.73, 65.48, 65.64, 65.59, 65.79]

Epoch: 10
2024-03-10 14:56:50.888136 epoch: 10 step: 0 cls_loss= 0.30305 (119794 samples/sec)
2024-03-10 14:57:00.418632 epoch: 10 step: 100 cls_loss= 0.29698 (3147 samples/sec)
saving....
2024-03-10 14:57:10.480166------------------------------------------------------ Precision@1: 65.63% 

[65.59, 65.52, 65.62, 65.88, 65.58, 65.73, 65.48, 65.64, 65.59, 65.79, 65.63]

Epoch: 11
2024-03-10 14:57:10.756315 epoch: 11 step: 0 cls_loss= 0.39449 (109241 samples/sec)
2024-03-10 14:57:20.245199 epoch: 11 step: 100 cls_loss= 0.34597 (3161 samples/sec)
saving....
2024-03-10 14:57:30.256220------------------------------------------------------ Precision@1: 65.72% 

[65.59, 65.52, 65.62, 65.88, 65.58, 65.73, 65.48, 65.64, 65.59, 65.79, 65.63, 65.72]

Epoch: 12
2024-03-10 14:57:30.523462 epoch: 12 step: 0 cls_loss= 0.34260 (112893 samples/sec)
2024-03-10 14:57:39.994629 epoch: 12 step: 100 cls_loss= 0.37372 (3167 samples/sec)
saving....
2024-03-10 14:57:50.012795------------------------------------------------------ Precision@1: 65.37% 

[65.59, 65.52, 65.62, 65.88, 65.58, 65.73, 65.48, 65.64, 65.59, 65.79, 65.63, 65.72, 65.37]

Epoch: 13
2024-03-10 14:57:50.271633 epoch: 13 step: 0 cls_loss= 0.30479 (116649 samples/sec)
2024-03-10 14:57:59.805510 epoch: 13 step: 100 cls_loss= 0.31600 (3146 samples/sec)
saving....
2024-03-10 14:58:09.952730------------------------------------------------------ Precision@1: 65.46% 

[65.59, 65.52, 65.62, 65.88, 65.58, 65.73, 65.48, 65.64, 65.59, 65.79, 65.63, 65.72, 65.37, 65.46]

Epoch: 14
2024-03-10 14:58:10.241406 epoch: 14 step: 0 cls_loss= 0.30269 (104449 samples/sec)
2024-03-10 14:58:19.742769 epoch: 14 step: 100 cls_loss= 0.29343 (3157 samples/sec)
saving....
2024-03-10 14:58:29.800393------------------------------------------------------ Precision@1: 65.29% 

[65.59, 65.52, 65.62, 65.88, 65.58, 65.73, 65.48, 65.64, 65.59, 65.79, 65.63, 65.72, 65.37, 65.46, 65.29]

Epoch: 15
2024-03-10 14:58:30.066095 epoch: 15 step: 0 cls_loss= 0.27615 (113697 samples/sec)
2024-03-10 14:58:39.618054 epoch: 15 step: 100 cls_loss= 0.29305 (3140 samples/sec)
saving....
2024-03-10 14:58:49.654142------------------------------------------------------ Precision@1: 65.70% 

[65.59, 65.52, 65.62, 65.88, 65.58, 65.73, 65.48, 65.64, 65.59, 65.79, 65.63, 65.72, 65.37, 65.46, 65.29, 65.7]

Epoch: 16
2024-03-10 14:58:49.932212 epoch: 16 step: 0 cls_loss= 0.33293 (108440 samples/sec)
2024-03-10 14:58:59.385216 epoch: 16 step: 100 cls_loss= 0.26766 (3173 samples/sec)
saving....
2024-03-10 14:59:09.409330------------------------------------------------------ Precision@1: 65.41% 

[65.59, 65.52, 65.62, 65.88, 65.58, 65.73, 65.48, 65.64, 65.59, 65.79, 65.63, 65.72, 65.37, 65.46, 65.29, 65.7, 65.41]

Epoch: 17
2024-03-10 14:59:09.654506 epoch: 17 step: 0 cls_loss= 0.29492 (123167 samples/sec)
2024-03-10 14:59:19.144441 epoch: 17 step: 100 cls_loss= 0.24856 (3161 samples/sec)
saving....
2024-03-10 14:59:29.180601------------------------------------------------------ Precision@1: 65.29% 

[65.59, 65.52, 65.62, 65.88, 65.58, 65.73, 65.48, 65.64, 65.59, 65.79, 65.63, 65.72, 65.37, 65.46, 65.29, 65.7, 65.41, 65.29]

Epoch: 18
2024-03-10 14:59:29.452079 epoch: 18 step: 0 cls_loss= 0.27987 (111117 samples/sec)
2024-03-10 14:59:38.956649 epoch: 18 step: 100 cls_loss= 0.30644 (3156 samples/sec)
saving....
2024-03-10 14:59:49.133175------------------------------------------------------ Precision@1: 65.63% 

[65.59, 65.52, 65.62, 65.88, 65.58, 65.73, 65.48, 65.64, 65.59, 65.79, 65.63, 65.72, 65.37, 65.46, 65.29, 65.7, 65.41, 65.29, 65.63]

Epoch: 19
2024-03-10 14:59:49.408536 epoch: 19 step: 0 cls_loss= 0.29194 (109589 samples/sec)
2024-03-10 14:59:58.881041 epoch: 19 step: 100 cls_loss= 0.33301 (3167 samples/sec)
saving....
2024-03-10 15:00:08.899531------------------------------------------------------ Precision@1: 65.47% 

[65.59, 65.52, 65.62, 65.88, 65.58, 65.73, 65.48, 65.64, 65.59, 65.79, 65.63, 65.72, 65.37, 65.46, 65.29, 65.7, 65.41, 65.29, 65.63, 65.47]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 15:00:11.792498 epoch: 0 step: 0 cls_loss= 0.37803 (39876 samples/sec)
2024-03-10 15:00:21.277104 epoch: 0 step: 100 cls_loss= 0.28619 (3163 samples/sec)
saving....
2024-03-10 15:00:31.510195------------------------------------------------------ Precision@1: 65.83% 

[65.83]

Epoch: 1
2024-03-10 15:00:31.775539 epoch: 1 step: 0 cls_loss= 0.35052 (113743 samples/sec)
2024-03-10 15:00:41.225637 epoch: 1 step: 100 cls_loss= 0.33560 (3174 samples/sec)
saving....
2024-03-10 15:00:51.303748------------------------------------------------------ Precision@1: 65.80% 

[65.83, 65.8]

Epoch: 2
2024-03-10 15:00:51.576219 epoch: 2 step: 0 cls_loss= 0.25181 (110759 samples/sec)
2024-03-10 15:01:01.017053 epoch: 2 step: 100 cls_loss= 0.28489 (3177 samples/sec)
saving....
2024-03-10 15:01:10.993881------------------------------------------------------ Precision@1: 65.84% 

[65.83, 65.8, 65.84]

Epoch: 3
2024-03-10 15:01:11.266394 epoch: 3 step: 0 cls_loss= 0.33851 (110589 samples/sec)
2024-03-10 15:01:20.755554 epoch: 3 step: 100 cls_loss= 0.33632 (3161 samples/sec)
saving....
2024-03-10 15:01:30.746692------------------------------------------------------ Precision@1: 65.82% 

[65.83, 65.8, 65.84, 65.82]

Epoch: 4
2024-03-10 15:01:31.012233 epoch: 4 step: 0 cls_loss= 0.27106 (113579 samples/sec)
2024-03-10 15:01:40.469707 epoch: 4 step: 100 cls_loss= 0.34154 (3172 samples/sec)
saving....
2024-03-10 15:01:50.451587------------------------------------------------------ Precision@1: 65.43% 

[65.83, 65.8, 65.84, 65.82, 65.43]

Epoch: 5
2024-03-10 15:01:50.726833 epoch: 5 step: 0 cls_loss= 0.33432 (109646 samples/sec)
2024-03-10 15:02:00.220496 epoch: 5 step: 100 cls_loss= 0.29646 (3160 samples/sec)
saving....
2024-03-10 15:02:10.250123------------------------------------------------------ Precision@1: 65.89% 

[65.83, 65.8, 65.84, 65.82, 65.43, 65.89]

Epoch: 6
2024-03-10 15:02:10.516071 epoch: 6 step: 0 cls_loss= 0.25778 (113336 samples/sec)
2024-03-10 15:02:20.084599 epoch: 6 step: 100 cls_loss= 0.27960 (3135 samples/sec)
saving....
2024-03-10 15:02:30.173368------------------------------------------------------ Precision@1: 65.51% 

[65.83, 65.8, 65.84, 65.82, 65.43, 65.89, 65.51]

Epoch: 7
2024-03-10 15:02:30.444480 epoch: 7 step: 0 cls_loss= 0.27658 (111323 samples/sec)
2024-03-10 15:02:39.944518 epoch: 7 step: 100 cls_loss= 0.32122 (3158 samples/sec)
saving....
2024-03-10 15:02:49.952158------------------------------------------------------ Precision@1: 65.49% 

[65.83, 65.8, 65.84, 65.82, 65.43, 65.89, 65.51, 65.49]

Epoch: 8
2024-03-10 15:02:50.219901 epoch: 8 step: 0 cls_loss= 0.26190 (112748 samples/sec)
2024-03-10 15:02:59.777910 epoch: 8 step: 100 cls_loss= 0.27234 (3138 samples/sec)
saving....
2024-03-10 15:03:09.853853------------------------------------------------------ Precision@1: 65.57% 

[65.83, 65.8, 65.84, 65.82, 65.43, 65.89, 65.51, 65.49, 65.57]

Epoch: 9
2024-03-10 15:03:10.115624 epoch: 9 step: 0 cls_loss= 0.34955 (115279 samples/sec)
2024-03-10 15:03:19.587045 epoch: 9 step: 100 cls_loss= 0.36847 (3167 samples/sec)
saving....
2024-03-10 15:03:29.582224------------------------------------------------------ Precision@1: 65.43% 

[65.83, 65.8, 65.84, 65.82, 65.43, 65.89, 65.51, 65.49, 65.57, 65.43]

Epoch: 10
2024-03-10 15:03:29.846901 epoch: 10 step: 0 cls_loss= 0.30286 (113984 samples/sec)
2024-03-10 15:03:39.306452 epoch: 10 step: 100 cls_loss= 0.32608 (3171 samples/sec)
saving....
2024-03-10 15:03:49.303248------------------------------------------------------ Precision@1: 65.60% 

[65.83, 65.8, 65.84, 65.82, 65.43, 65.89, 65.51, 65.49, 65.57, 65.43, 65.6]

Epoch: 11
2024-03-10 15:03:49.565234 epoch: 11 step: 0 cls_loss= 0.27290 (115087 samples/sec)
2024-03-10 15:03:59.062086 epoch: 11 step: 100 cls_loss= 0.27700 (3159 samples/sec)
saving....
2024-03-10 15:04:09.048859------------------------------------------------------ Precision@1: 65.74% 

[65.83, 65.8, 65.84, 65.82, 65.43, 65.89, 65.51, 65.49, 65.57, 65.43, 65.6, 65.74]

Epoch: 12
2024-03-10 15:04:09.321422 epoch: 12 step: 0 cls_loss= 0.33027 (110642 samples/sec)
2024-03-10 15:04:18.809470 epoch: 12 step: 100 cls_loss= 0.29113 (3162 samples/sec)
saving....
2024-03-10 15:04:28.774309------------------------------------------------------ Precision@1: 65.36% 

[65.83, 65.8, 65.84, 65.82, 65.43, 65.89, 65.51, 65.49, 65.57, 65.43, 65.6, 65.74, 65.36]

Epoch: 13
2024-03-10 15:04:29.037883 epoch: 13 step: 0 cls_loss= 0.24228 (114547 samples/sec)
2024-03-10 15:04:38.531277 epoch: 13 step: 100 cls_loss= 0.33374 (3160 samples/sec)
saving....
2024-03-10 15:04:48.534260------------------------------------------------------ Precision@1: 65.49% 

[65.83, 65.8, 65.84, 65.82, 65.43, 65.89, 65.51, 65.49, 65.57, 65.43, 65.6, 65.74, 65.36, 65.49]

Epoch: 14
2024-03-10 15:04:48.810491 epoch: 14 step: 0 cls_loss= 0.30443 (109194 samples/sec)
2024-03-10 15:04:58.284881 epoch: 14 step: 100 cls_loss= 0.29848 (3166 samples/sec)
saving....
2024-03-10 15:05:08.292081------------------------------------------------------ Precision@1: 65.39% 

[65.83, 65.8, 65.84, 65.82, 65.43, 65.89, 65.51, 65.49, 65.57, 65.43, 65.6, 65.74, 65.36, 65.49, 65.39]

Epoch: 15
2024-03-10 15:05:08.566527 epoch: 15 step: 0 cls_loss= 0.31719 (109884 samples/sec)
2024-03-10 15:05:18.034134 epoch: 15 step: 100 cls_loss= 0.31389 (3168 samples/sec)
saving....
2024-03-10 15:05:28.006879------------------------------------------------------ Precision@1: 65.46% 

[65.83, 65.8, 65.84, 65.82, 65.43, 65.89, 65.51, 65.49, 65.57, 65.43, 65.6, 65.74, 65.36, 65.49, 65.39, 65.46]

Epoch: 16
2024-03-10 15:05:28.259805 epoch: 16 step: 0 cls_loss= 0.23852 (119418 samples/sec)
2024-03-10 15:05:37.705020 epoch: 16 step: 100 cls_loss= 0.35339 (3176 samples/sec)
saving....
2024-03-10 15:05:47.657504------------------------------------------------------ Precision@1: 65.51% 

[65.83, 65.8, 65.84, 65.82, 65.43, 65.89, 65.51, 65.49, 65.57, 65.43, 65.6, 65.74, 65.36, 65.49, 65.39, 65.46, 65.51]

Epoch: 17
2024-03-10 15:05:47.934727 epoch: 17 step: 0 cls_loss= 0.32995 (108854 samples/sec)
2024-03-10 15:05:57.416115 epoch: 17 step: 100 cls_loss= 0.30901 (3164 samples/sec)
saving....
2024-03-10 15:06:07.397331------------------------------------------------------ Precision@1: 65.43% 

[65.83, 65.8, 65.84, 65.82, 65.43, 65.89, 65.51, 65.49, 65.57, 65.43, 65.6, 65.74, 65.36, 65.49, 65.39, 65.46, 65.51, 65.43]

Epoch: 18
2024-03-10 15:06:07.676641 epoch: 18 step: 0 cls_loss= 0.25392 (107892 samples/sec)
2024-03-10 15:06:17.128949 epoch: 18 step: 100 cls_loss= 0.28830 (3173 samples/sec)
saving....
2024-03-10 15:06:27.104028------------------------------------------------------ Precision@1: 65.41% 

[65.83, 65.8, 65.84, 65.82, 65.43, 65.89, 65.51, 65.49, 65.57, 65.43, 65.6, 65.74, 65.36, 65.49, 65.39, 65.46, 65.51, 65.43, 65.41]

Epoch: 19
2024-03-10 15:06:27.382309 epoch: 19 step: 0 cls_loss= 0.25382 (108408 samples/sec)
2024-03-10 15:06:36.868138 epoch: 19 step: 100 cls_loss= 0.31546 (3162 samples/sec)
saving....
2024-03-10 15:06:46.862202------------------------------------------------------ Precision@1: 65.46% 

[65.83, 65.8, 65.84, 65.82, 65.43, 65.89, 65.51, 65.49, 65.57, 65.43, 65.6, 65.74, 65.36, 65.49, 65.39, 65.46, 65.51, 65.43, 65.41, 65.46]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 15:06:49.739007 epoch: 0 step: 0 cls_loss= 0.36918 (39874 samples/sec)
2024-03-10 15:06:59.223774 epoch: 0 step: 100 cls_loss= 0.41056 (3162 samples/sec)
saving....
2024-03-10 15:07:09.558626------------------------------------------------------ Precision@1: 65.76% 

[65.76]

Epoch: 1
2024-03-10 15:07:09.835877 epoch: 1 step: 0 cls_loss= 0.37951 (108878 samples/sec)
2024-03-10 15:07:19.381720 epoch: 1 step: 100 cls_loss= 0.36137 (3142 samples/sec)
saving....
2024-03-10 15:07:29.413811------------------------------------------------------ Precision@1: 65.74% 

[65.76, 65.74]

Epoch: 2
2024-03-10 15:07:29.683073 epoch: 2 step: 0 cls_loss= 0.36512 (112089 samples/sec)
2024-03-10 15:07:39.148545 epoch: 2 step: 100 cls_loss= 0.29610 (3169 samples/sec)
saving....
2024-03-10 15:07:49.183550------------------------------------------------------ Precision@1: 65.70% 

[65.76, 65.74, 65.7]

Epoch: 3
2024-03-10 15:07:49.448236 epoch: 3 step: 0 cls_loss= 0.30752 (113980 samples/sec)
2024-03-10 15:07:58.989995 epoch: 3 step: 100 cls_loss= 0.28838 (3144 samples/sec)
saving....
2024-03-10 15:08:09.008655------------------------------------------------------ Precision@1: 65.56% 

[65.76, 65.74, 65.7, 65.56]

Epoch: 4
2024-03-10 15:08:09.297144 epoch: 4 step: 0 cls_loss= 0.38515 (104528 samples/sec)
2024-03-10 15:08:18.810688 epoch: 4 step: 100 cls_loss= 0.34859 (3153 samples/sec)
saving....
2024-03-10 15:08:28.829801------------------------------------------------------ Precision@1: 65.57% 

[65.76, 65.74, 65.7, 65.56, 65.57]

Epoch: 5
2024-03-10 15:08:29.085162 epoch: 5 step: 0 cls_loss= 0.26252 (118206 samples/sec)
2024-03-10 15:08:38.570657 epoch: 5 step: 100 cls_loss= 0.24730 (3162 samples/sec)
saving....
2024-03-10 15:08:48.589424------------------------------------------------------ Precision@1: 65.62% 

[65.76, 65.74, 65.7, 65.56, 65.57, 65.62]

Epoch: 6
2024-03-10 15:08:48.859987 epoch: 6 step: 0 cls_loss= 0.35008 (111513 samples/sec)
2024-03-10 15:08:58.335141 epoch: 6 step: 100 cls_loss= 0.33398 (3166 samples/sec)
saving....
2024-03-10 15:09:08.351600------------------------------------------------------ Precision@1: 65.66% 

[65.76, 65.74, 65.7, 65.56, 65.57, 65.62, 65.66]

Epoch: 7
2024-03-10 15:09:08.617743 epoch: 7 step: 0 cls_loss= 0.34676 (113299 samples/sec)
2024-03-10 15:09:18.133535 epoch: 7 step: 100 cls_loss= 0.29223 (3152 samples/sec)
saving....
2024-03-10 15:09:28.197495------------------------------------------------------ Precision@1: 65.37% 

[65.76, 65.74, 65.7, 65.56, 65.57, 65.62, 65.66, 65.37]

Epoch: 8
2024-03-10 15:09:28.460829 epoch: 8 step: 0 cls_loss= 0.26784 (114585 samples/sec)
2024-03-10 15:09:37.973868 epoch: 8 step: 100 cls_loss= 0.31219 (3153 samples/sec)
saving....
2024-03-10 15:09:48.027216------------------------------------------------------ Precision@1: 65.76% 

[65.76, 65.74, 65.7, 65.56, 65.57, 65.62, 65.66, 65.37, 65.76]

Epoch: 9
2024-03-10 15:09:48.280476 epoch: 9 step: 0 cls_loss= 0.25838 (119206 samples/sec)
2024-03-10 15:09:57.770720 epoch: 9 step: 100 cls_loss= 0.31573 (3161 samples/sec)
saving....
2024-03-10 15:10:07.802869------------------------------------------------------ Precision@1: 65.70% 

[65.76, 65.74, 65.7, 65.56, 65.57, 65.62, 65.66, 65.37, 65.76, 65.7]

Epoch: 10
2024-03-10 15:10:08.089269 epoch: 10 step: 0 cls_loss= 0.31250 (105262 samples/sec)
2024-03-10 15:10:17.622075 epoch: 10 step: 100 cls_loss= 0.34786 (3147 samples/sec)
saving....
2024-03-10 15:10:27.674298------------------------------------------------------ Precision@1: 65.63% 

[65.76, 65.74, 65.7, 65.56, 65.57, 65.62, 65.66, 65.37, 65.76, 65.7, 65.63]

Epoch: 11
2024-03-10 15:10:27.946705 epoch: 11 step: 0 cls_loss= 0.31700 (110761 samples/sec)
2024-03-10 15:10:37.432084 epoch: 11 step: 100 cls_loss= 0.29059 (3162 samples/sec)
saving....
2024-03-10 15:10:47.421520------------------------------------------------------ Precision@1: 65.74% 

[65.76, 65.74, 65.7, 65.56, 65.57, 65.62, 65.66, 65.37, 65.76, 65.7, 65.63, 65.74]

Epoch: 12
2024-03-10 15:10:47.685510 epoch: 12 step: 0 cls_loss= 0.27431 (114306 samples/sec)
2024-03-10 15:10:57.201527 epoch: 12 step: 100 cls_loss= 0.31585 (3152 samples/sec)
saving....
2024-03-10 15:11:07.328936------------------------------------------------------ Precision@1: 65.55% 

[65.76, 65.74, 65.7, 65.56, 65.57, 65.62, 65.66, 65.37, 65.76, 65.7, 65.63, 65.74, 65.55]

Epoch: 13
2024-03-10 15:11:07.603245 epoch: 13 step: 0 cls_loss= 0.29301 (110011 samples/sec)
2024-03-10 15:11:17.123181 epoch: 13 step: 100 cls_loss= 0.26218 (3151 samples/sec)
saving....
2024-03-10 15:11:27.187507------------------------------------------------------ Precision@1: 65.52% 

[65.76, 65.74, 65.7, 65.56, 65.57, 65.62, 65.66, 65.37, 65.76, 65.7, 65.63, 65.74, 65.55, 65.52]

Epoch: 14
2024-03-10 15:11:27.455392 epoch: 14 step: 0 cls_loss= 0.27870 (112702 samples/sec)
2024-03-10 15:11:36.944672 epoch: 14 step: 100 cls_loss= 0.29262 (3161 samples/sec)
saving....
2024-03-10 15:11:47.033294------------------------------------------------------ Precision@1: 65.21% 

[65.76, 65.74, 65.7, 65.56, 65.57, 65.62, 65.66, 65.37, 65.76, 65.7, 65.63, 65.74, 65.55, 65.52, 65.21]

Epoch: 15
2024-03-10 15:11:47.296423 epoch: 15 step: 0 cls_loss= 0.29274 (114732 samples/sec)
2024-03-10 15:11:56.783834 epoch: 15 step: 100 cls_loss= 0.30857 (3162 samples/sec)
saving....
2024-03-10 15:12:06.862461------------------------------------------------------ Precision@1: 65.42% 

[65.76, 65.74, 65.7, 65.56, 65.57, 65.62, 65.66, 65.37, 65.76, 65.7, 65.63, 65.74, 65.55, 65.52, 65.21, 65.42]

Epoch: 16
2024-03-10 15:12:07.139384 epoch: 16 step: 0 cls_loss= 0.23295 (108852 samples/sec)
2024-03-10 15:12:16.706679 epoch: 16 step: 100 cls_loss= 0.26486 (3135 samples/sec)
saving....
2024-03-10 15:12:26.725669------------------------------------------------------ Precision@1: 65.61% 

[65.76, 65.74, 65.7, 65.56, 65.57, 65.62, 65.66, 65.37, 65.76, 65.7, 65.63, 65.74, 65.55, 65.52, 65.21, 65.42, 65.61]

Epoch: 17
2024-03-10 15:12:26.997961 epoch: 17 step: 0 cls_loss= 0.29935 (110850 samples/sec)
2024-03-10 15:12:36.543051 epoch: 17 step: 100 cls_loss= 0.30819 (3143 samples/sec)
saving....
2024-03-10 15:12:46.601324------------------------------------------------------ Precision@1: 65.74% 

[65.76, 65.74, 65.7, 65.56, 65.57, 65.62, 65.66, 65.37, 65.76, 65.7, 65.63, 65.74, 65.55, 65.52, 65.21, 65.42, 65.61, 65.74]

Epoch: 18
2024-03-10 15:12:46.873159 epoch: 18 step: 0 cls_loss= 0.30382 (110953 samples/sec)
2024-03-10 15:12:56.351421 epoch: 18 step: 100 cls_loss= 0.29719 (3165 samples/sec)
saving....
2024-03-10 15:13:06.394778------------------------------------------------------ Precision@1: 65.49% 

[65.76, 65.74, 65.7, 65.56, 65.57, 65.62, 65.66, 65.37, 65.76, 65.7, 65.63, 65.74, 65.55, 65.52, 65.21, 65.42, 65.61, 65.74, 65.49]

Epoch: 19
2024-03-10 15:13:06.674447 epoch: 19 step: 0 cls_loss= 0.27160 (107863 samples/sec)
2024-03-10 15:13:16.184674 epoch: 19 step: 100 cls_loss= 0.30843 (3154 samples/sec)
saving....
2024-03-10 15:13:26.255243------------------------------------------------------ Precision@1: 65.48% 

[65.76, 65.74, 65.7, 65.56, 65.57, 65.62, 65.66, 65.37, 65.76, 65.7, 65.63, 65.74, 65.55, 65.52, 65.21, 65.42, 65.61, 65.74, 65.49, 65.48]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 15:13:29.147469 epoch: 0 step: 0 cls_loss= 0.31958 (39951 samples/sec)
2024-03-10 15:13:38.722712 epoch: 0 step: 100 cls_loss= 0.37661 (3133 samples/sec)
saving....
2024-03-10 15:13:49.121376------------------------------------------------------ Precision@1: 65.74% 

[65.74]

Epoch: 1
2024-03-10 15:13:49.389753 epoch: 1 step: 0 cls_loss= 0.28365 (112454 samples/sec)
2024-03-10 15:13:58.899309 epoch: 1 step: 100 cls_loss= 0.31780 (3154 samples/sec)
saving....
2024-03-10 15:14:08.960445------------------------------------------------------ Precision@1: 65.70% 

[65.74, 65.7]

Epoch: 2
2024-03-10 15:14:09.229887 epoch: 2 step: 0 cls_loss= 0.31654 (112032 samples/sec)
2024-03-10 15:14:18.754689 epoch: 2 step: 100 cls_loss= 0.37335 (3149 samples/sec)
saving....
2024-03-10 15:14:28.942911------------------------------------------------------ Precision@1: 65.78% 

[65.74, 65.7, 65.78]

Epoch: 3
2024-03-10 15:14:29.210789 epoch: 3 step: 0 cls_loss= 0.33275 (112483 samples/sec)
2024-03-10 15:14:38.746201 epoch: 3 step: 100 cls_loss= 0.33774 (3146 samples/sec)
saving....
2024-03-10 15:14:48.851835------------------------------------------------------ Precision@1: 65.70% 

[65.74, 65.7, 65.78, 65.7]

Epoch: 4
2024-03-10 15:14:49.114365 epoch: 4 step: 0 cls_loss= 0.29699 (114993 samples/sec)
2024-03-10 15:14:58.632894 epoch: 4 step: 100 cls_loss= 0.41577 (3151 samples/sec)
saving....
2024-03-10 15:15:08.730223------------------------------------------------------ Precision@1: 65.79% 

[65.74, 65.7, 65.78, 65.7, 65.79]

Epoch: 5
2024-03-10 15:15:08.997446 epoch: 5 step: 0 cls_loss= 0.32574 (112916 samples/sec)
2024-03-10 15:15:18.517501 epoch: 5 step: 100 cls_loss= 0.33614 (3151 samples/sec)
saving....
2024-03-10 15:15:28.589974------------------------------------------------------ Precision@1: 65.67% 

[65.74, 65.7, 65.78, 65.7, 65.79, 65.67]

Epoch: 6
2024-03-10 15:15:28.854483 epoch: 6 step: 0 cls_loss= 0.28602 (114079 samples/sec)
2024-03-10 15:15:38.416896 epoch: 6 step: 100 cls_loss= 0.30864 (3137 samples/sec)
saving....
2024-03-10 15:15:48.502927------------------------------------------------------ Precision@1: 65.75% 

[65.74, 65.7, 65.78, 65.7, 65.79, 65.67, 65.75]

Epoch: 7
2024-03-10 15:15:48.765154 epoch: 7 step: 0 cls_loss= 0.32983 (115116 samples/sec)
2024-03-10 15:15:58.247262 epoch: 7 step: 100 cls_loss= 0.33801 (3164 samples/sec)
saving....
2024-03-10 15:16:08.399188------------------------------------------------------ Precision@1: 65.56% 

[65.74, 65.7, 65.78, 65.7, 65.79, 65.67, 65.75, 65.56]

Epoch: 8
2024-03-10 15:16:08.665643 epoch: 8 step: 0 cls_loss= 0.33756 (113299 samples/sec)
2024-03-10 15:16:18.158056 epoch: 8 step: 100 cls_loss= 0.26985 (3160 samples/sec)
saving....
2024-03-10 15:16:28.195614------------------------------------------------------ Precision@1: 65.24% 

[65.74, 65.7, 65.78, 65.7, 65.79, 65.67, 65.75, 65.56, 65.24]

Epoch: 9
2024-03-10 15:16:28.464472 epoch: 9 step: 0 cls_loss= 0.30001 (112161 samples/sec)
2024-03-10 15:16:38.073494 epoch: 9 step: 100 cls_loss= 0.25381 (3122 samples/sec)
saving....
2024-03-10 15:16:48.200789------------------------------------------------------ Precision@1: 65.43% 

[65.74, 65.7, 65.78, 65.7, 65.79, 65.67, 65.75, 65.56, 65.24, 65.43]

Epoch: 10
2024-03-10 15:16:48.469466 epoch: 10 step: 0 cls_loss= 0.30501 (112356 samples/sec)
2024-03-10 15:16:58.010420 epoch: 10 step: 100 cls_loss= 0.29277 (3144 samples/sec)
saving....
2024-03-10 15:17:08.162034------------------------------------------------------ Precision@1: 65.60% 

[65.74, 65.7, 65.78, 65.7, 65.79, 65.67, 65.75, 65.56, 65.24, 65.43, 65.6]

Epoch: 11
2024-03-10 15:17:08.426261 epoch: 11 step: 0 cls_loss= 0.24125 (114267 samples/sec)
2024-03-10 15:17:18.002072 epoch: 11 step: 100 cls_loss= 0.33166 (3133 samples/sec)
saving....
2024-03-10 15:17:28.141903------------------------------------------------------ Precision@1: 65.41% 

[65.74, 65.7, 65.78, 65.7, 65.79, 65.67, 65.75, 65.56, 65.24, 65.43, 65.6, 65.41]

Epoch: 12
2024-03-10 15:17:28.417187 epoch: 12 step: 0 cls_loss= 0.24997 (109640 samples/sec)
2024-03-10 15:17:38.019972 epoch: 12 step: 100 cls_loss= 0.29795 (3124 samples/sec)
saving....
2024-03-10 15:17:48.121404------------------------------------------------------ Precision@1: 65.53% 

[65.74, 65.7, 65.78, 65.7, 65.79, 65.67, 65.75, 65.56, 65.24, 65.43, 65.6, 65.41, 65.53]

Epoch: 13
2024-03-10 15:17:48.395302 epoch: 13 step: 0 cls_loss= 0.30222 (110183 samples/sec)
2024-03-10 15:17:57.885913 epoch: 13 step: 100 cls_loss= 0.32488 (3161 samples/sec)
saving....
2024-03-10 15:18:08.036409------------------------------------------------------ Precision@1: 65.50% 

[65.74, 65.7, 65.78, 65.7, 65.79, 65.67, 65.75, 65.56, 65.24, 65.43, 65.6, 65.41, 65.53, 65.5]

Epoch: 14
2024-03-10 15:18:08.307053 epoch: 14 step: 0 cls_loss= 0.25708 (111495 samples/sec)
2024-03-10 15:18:17.860683 epoch: 14 step: 100 cls_loss= 0.28717 (3140 samples/sec)
saving....
2024-03-10 15:18:28.058882------------------------------------------------------ Precision@1: 65.45% 

[65.74, 65.7, 65.78, 65.7, 65.79, 65.67, 65.75, 65.56, 65.24, 65.43, 65.6, 65.41, 65.53, 65.5, 65.45]

Epoch: 15
2024-03-10 15:18:28.325466 epoch: 15 step: 0 cls_loss= 0.31900 (113197 samples/sec)
2024-03-10 15:18:37.884671 epoch: 15 step: 100 cls_loss= 0.33336 (3138 samples/sec)
saving....
2024-03-10 15:18:47.971365------------------------------------------------------ Precision@1: 65.78% 

[65.74, 65.7, 65.78, 65.7, 65.79, 65.67, 65.75, 65.56, 65.24, 65.43, 65.6, 65.41, 65.53, 65.5, 65.45, 65.78]

Epoch: 16
2024-03-10 15:18:48.234191 epoch: 16 step: 0 cls_loss= 0.38154 (114858 samples/sec)
2024-03-10 15:18:57.753566 epoch: 16 step: 100 cls_loss= 0.29369 (3151 samples/sec)
saving....
2024-03-10 15:19:07.925546------------------------------------------------------ Precision@1: 65.21% 

[65.74, 65.7, 65.78, 65.7, 65.79, 65.67, 65.75, 65.56, 65.24, 65.43, 65.6, 65.41, 65.53, 65.5, 65.45, 65.78, 65.21]

Epoch: 17
2024-03-10 15:19:08.189963 epoch: 17 step: 0 cls_loss= 0.34318 (114185 samples/sec)
2024-03-10 15:19:17.721945 epoch: 17 step: 100 cls_loss= 0.31999 (3147 samples/sec)
saving....
2024-03-10 15:19:27.798514------------------------------------------------------ Precision@1: 65.37% 

[65.74, 65.7, 65.78, 65.7, 65.79, 65.67, 65.75, 65.56, 65.24, 65.43, 65.6, 65.41, 65.53, 65.5, 65.45, 65.78, 65.21, 65.37]

Epoch: 18
2024-03-10 15:19:28.079582 epoch: 18 step: 0 cls_loss= 0.28199 (107315 samples/sec)
2024-03-10 15:19:37.584223 epoch: 18 step: 100 cls_loss= 0.31737 (3156 samples/sec)
saving....
2024-03-10 15:19:47.781896------------------------------------------------------ Precision@1: 65.61% 

[65.74, 65.7, 65.78, 65.7, 65.79, 65.67, 65.75, 65.56, 65.24, 65.43, 65.6, 65.41, 65.53, 65.5, 65.45, 65.78, 65.21, 65.37, 65.61]

Epoch: 19
2024-03-10 15:19:48.042007 epoch: 19 step: 0 cls_loss= 0.34464 (116074 samples/sec)
2024-03-10 15:19:57.530693 epoch: 19 step: 100 cls_loss= 0.27844 (3161 samples/sec)
saving....
2024-03-10 15:20:07.605779------------------------------------------------------ Precision@1: 65.55% 

[65.74, 65.7, 65.78, 65.7, 65.79, 65.67, 65.75, 65.56, 65.24, 65.43, 65.6, 65.41, 65.53, 65.5, 65.45, 65.78, 65.21, 65.37, 65.61, 65.55]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-10 15:20:10.486549 epoch: 0 step: 0 cls_loss= 0.29718 (39506 samples/sec)
2024-03-10 15:20:20.009956 epoch: 0 step: 100 cls_loss= 0.32325 (3150 samples/sec)
saving....
2024-03-10 15:20:30.371544------------------------------------------------------ Precision@1: 65.67% 

[65.67]

Epoch: 1
2024-03-10 15:20:30.623416 epoch: 1 step: 0 cls_loss= 0.38822 (119923 samples/sec)
2024-03-10 15:20:40.286691 epoch: 1 step: 100 cls_loss= 0.32726 (3104 samples/sec)
saving....
2024-03-10 15:20:50.490974------------------------------------------------------ Precision@1: 65.80% 

[65.67, 65.8]

Epoch: 2
2024-03-10 15:20:50.768787 epoch: 2 step: 0 cls_loss= 0.30407 (108673 samples/sec)
2024-03-10 15:21:00.314031 epoch: 2 step: 100 cls_loss= 0.28605 (3143 samples/sec)
saving....
2024-03-10 15:21:10.426026------------------------------------------------------ Precision@1: 65.77% 

[65.67, 65.8, 65.77]

Epoch: 3
2024-03-10 15:21:10.685246 epoch: 3 step: 0 cls_loss= 0.30390 (116444 samples/sec)
2024-03-10 15:21:20.207459 epoch: 3 step: 100 cls_loss= 0.32144 (3150 samples/sec)
saving....
2024-03-10 15:21:30.306954------------------------------------------------------ Precision@1: 65.48% 

[65.67, 65.8, 65.77, 65.48]

Epoch: 4
2024-03-10 15:21:30.573963 epoch: 4 step: 0 cls_loss= 0.37433 (113056 samples/sec)
2024-03-10 15:21:40.128294 epoch: 4 step: 100 cls_loss= 0.33013 (3140 samples/sec)
saving....
2024-03-10 15:21:50.217606------------------------------------------------------ Precision@1: 65.72% 

[65.67, 65.8, 65.77, 65.48, 65.72]

Epoch: 5
2024-03-10 15:21:50.497420 epoch: 5 step: 0 cls_loss= 0.33240 (107829 samples/sec)
2024-03-10 15:22:00.057220 epoch: 5 step: 100 cls_loss= 0.29156 (3138 samples/sec)
saving....
2024-03-10 15:22:10.232866------------------------------------------------------ Precision@1: 65.92% 

[65.67, 65.8, 65.77, 65.48, 65.72, 65.92]

Epoch: 6
2024-03-10 15:22:10.478574 epoch: 6 step: 0 cls_loss= 0.31261 (122902 samples/sec)
2024-03-10 15:22:20.015075 epoch: 6 step: 100 cls_loss= 0.32340 (3145 samples/sec)
saving....
2024-03-10 15:22:30.150660------------------------------------------------------ Precision@1: 65.44% 

[65.67, 65.8, 65.77, 65.48, 65.72, 65.92, 65.44]

Epoch: 7
2024-03-10 15:22:30.417808 epoch: 7 step: 0 cls_loss= 0.31686 (112817 samples/sec)
2024-03-10 15:22:39.948840 epoch: 7 step: 100 cls_loss= 0.29424 (3147 samples/sec)
saving....
2024-03-10 15:22:50.115242------------------------------------------------------ Precision@1: 65.46% 

[65.67, 65.8, 65.77, 65.48, 65.72, 65.92, 65.44, 65.46]

Epoch: 8
2024-03-10 15:22:50.368865 epoch: 8 step: 0 cls_loss= 0.30305 (118997 samples/sec)
2024-03-10 15:22:59.889055 epoch: 8 step: 100 cls_loss= 0.30096 (3151 samples/sec)
saving....
2024-03-10 15:23:09.991460------------------------------------------------------ Precision@1: 65.51% 

[65.67, 65.8, 65.77, 65.48, 65.72, 65.92, 65.44, 65.46, 65.51]

Epoch: 9
2024-03-10 15:23:10.261895 epoch: 9 step: 0 cls_loss= 0.31509 (111526 samples/sec)
2024-03-10 15:23:19.778985 epoch: 9 step: 100 cls_loss= 0.29114 (3152 samples/sec)
saving....
2024-03-10 15:23:29.868928------------------------------------------------------ Precision@1: 65.73% 

[65.67, 65.8, 65.77, 65.48, 65.72, 65.92, 65.44, 65.46, 65.51, 65.73]

Epoch: 10
2024-03-10 15:23:30.132283 epoch: 10 step: 0 cls_loss= 0.30919 (114626 samples/sec)
2024-03-10 15:23:39.666133 epoch: 10 step: 100 cls_loss= 0.41180 (3146 samples/sec)
saving....
2024-03-10 15:23:49.817190------------------------------------------------------ Precision@1: 65.57% 

[65.67, 65.8, 65.77, 65.48, 65.72, 65.92, 65.44, 65.46, 65.51, 65.73, 65.57]

Epoch: 11
2024-03-10 15:23:50.090828 epoch: 11 step: 0 cls_loss= 0.27562 (110304 samples/sec)
2024-03-10 15:23:59.627115 epoch: 11 step: 100 cls_loss= 0.30539 (3146 samples/sec)
saving....
2024-03-10 15:24:09.723453------------------------------------------------------ Precision@1: 65.41% 

[65.67, 65.8, 65.77, 65.48, 65.72, 65.92, 65.44, 65.46, 65.51, 65.73, 65.57, 65.41]

Epoch: 12
2024-03-10 15:24:09.994564 epoch: 12 step: 0 cls_loss= 0.31377 (111306 samples/sec)
2024-03-10 15:24:19.542058 epoch: 12 step: 100 cls_loss= 0.29364 (3142 samples/sec)
saving....
2024-03-10 15:24:29.593519------------------------------------------------------ Precision@1: 65.50% 

[65.67, 65.8, 65.77, 65.48, 65.72, 65.92, 65.44, 65.46, 65.51, 65.73, 65.57, 65.41, 65.5]

Epoch: 13
2024-03-10 15:24:29.865549 epoch: 13 step: 0 cls_loss= 0.29883 (110960 samples/sec)
2024-03-10 15:24:39.443408 epoch: 13 step: 100 cls_loss= 0.30335 (3132 samples/sec)
saving....
2024-03-10 15:24:49.579652------------------------------------------------------ Precision@1: 65.52% 

[65.67, 65.8, 65.77, 65.48, 65.72, 65.92, 65.44, 65.46, 65.51, 65.73, 65.57, 65.41, 65.5, 65.52]

Epoch: 14
2024-03-10 15:24:49.842020 epoch: 14 step: 0 cls_loss= 0.35750 (115097 samples/sec)
2024-03-10 15:24:59.389225 epoch: 14 step: 100 cls_loss= 0.28376 (3142 samples/sec)
saving....
2024-03-10 15:25:09.498712------------------------------------------------------ Precision@1: 65.26% 

[65.67, 65.8, 65.77, 65.48, 65.72, 65.92, 65.44, 65.46, 65.51, 65.73, 65.57, 65.41, 65.5, 65.52, 65.26]

Epoch: 15
2024-03-10 15:25:09.747867 epoch: 15 step: 0 cls_loss= 0.28086 (121226 samples/sec)
2024-03-10 15:25:19.291824 epoch: 15 step: 100 cls_loss= 0.31375 (3143 samples/sec)
saving....
2024-03-10 15:25:29.386052------------------------------------------------------ Precision@1: 65.50% 

[65.67, 65.8, 65.77, 65.48, 65.72, 65.92, 65.44, 65.46, 65.51, 65.73, 65.57, 65.41, 65.5, 65.52, 65.26, 65.5]

Epoch: 16
2024-03-10 15:25:29.659298 epoch: 16 step: 0 cls_loss= 0.27147 (110477 samples/sec)
2024-03-10 15:25:39.185038 epoch: 16 step: 100 cls_loss= 0.35148 (3149 samples/sec)
saving....
2024-03-10 15:25:49.310357------------------------------------------------------ Precision@1: 65.32% 

[65.67, 65.8, 65.77, 65.48, 65.72, 65.92, 65.44, 65.46, 65.51, 65.73, 65.57, 65.41, 65.5, 65.52, 65.26, 65.5, 65.32]

Epoch: 17
2024-03-10 15:25:49.573178 epoch: 17 step: 0 cls_loss= 0.31234 (114813 samples/sec)
2024-03-10 15:25:59.080995 epoch: 17 step: 100 cls_loss= 0.25785 (3155 samples/sec)
saving....
2024-03-10 15:26:09.244407------------------------------------------------------ Precision@1: 65.53% 

[65.67, 65.8, 65.77, 65.48, 65.72, 65.92, 65.44, 65.46, 65.51, 65.73, 65.57, 65.41, 65.5, 65.52, 65.26, 65.5, 65.32, 65.53]

Epoch: 18
2024-03-10 15:26:09.504422 epoch: 18 step: 0 cls_loss= 0.33466 (116134 samples/sec)
2024-03-10 15:26:19.169127 epoch: 18 step: 100 cls_loss= 0.30226 (3104 samples/sec)
saving....
2024-03-10 15:26:29.319257------------------------------------------------------ Precision@1: 65.60% 

[65.67, 65.8, 65.77, 65.48, 65.72, 65.92, 65.44, 65.46, 65.51, 65.73, 65.57, 65.41, 65.5, 65.52, 65.26, 65.5, 65.32, 65.53, 65.6]

Epoch: 19
2024-03-10 15:26:29.588794 epoch: 19 step: 0 cls_loss= 0.32032 (111996 samples/sec)
2024-03-10 15:26:39.134222 epoch: 19 step: 100 cls_loss= 0.29263 (3143 samples/sec)
saving....
2024-03-10 15:26:49.400099------------------------------------------------------ Precision@1: 65.31% 

[65.67, 65.8, 65.77, 65.48, 65.72, 65.92, 65.44, 65.46, 65.51, 65.73, 65.57, 65.41, 65.5, 65.52, 65.26, 65.5, 65.32, 65.53, 65.6, 65.31]
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ 
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ 