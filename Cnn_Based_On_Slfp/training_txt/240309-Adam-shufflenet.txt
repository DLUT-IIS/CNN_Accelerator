Script started on 2024-03-09 21:33:48+08:00 [TERM="xterm-256color" TTY="/dev/pts/0" COLUMNS="214" LINES="18"]
]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ bash bash_train.sh 
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-09 21:33:58.914766 epoch: 0 step: 0 cls_loss= 0.34245 (40172 samples/sec)
2024-03-09 21:34:08.392921 epoch: 0 step: 100 cls_loss= 0.27960 (3165 samples/sec)
saving....
2024-03-09 21:34:18.576305------------------------------------------------------ Precision@1: 65.62% 

[65.62]
max acc : 65.62

Epoch: 1
2024-03-09 21:34:18.865001 epoch: 1 step: 0 cls_loss= 0.30905 (111602 samples/sec)
2024-03-09 21:34:28.319894 epoch: 1 step: 100 cls_loss= 0.32021 (3173 samples/sec)
saving....
2024-03-09 21:34:38.257177------------------------------------------------------ Precision@1: 65.67% 

[65.62, 65.67]
max acc : 65.67

Epoch: 2
2024-03-09 21:34:38.524622 epoch: 2 step: 0 cls_loss= 0.34942 (120481 samples/sec)
2024-03-09 21:34:47.965857 epoch: 2 step: 100 cls_loss= 0.27818 (3177 samples/sec)
saving....
2024-03-09 21:34:57.942575------------------------------------------------------ Precision@1: 65.58% 

[65.62, 65.67, 65.58]

Epoch: 3
2024-03-09 21:34:58.204734 epoch: 3 step: 0 cls_loss= 0.30335 (115163 samples/sec)
2024-03-09 21:35:07.667263 epoch: 3 step: 100 cls_loss= 0.40120 (3170 samples/sec)
saving....
2024-03-09 21:35:17.626221------------------------------------------------------ Precision@1: 65.72% 

[65.62, 65.67, 65.58, 65.72]
max acc : 65.72

Epoch: 4
2024-03-09 21:35:17.905465 epoch: 4 step: 0 cls_loss= 0.28742 (117364 samples/sec)
2024-03-09 21:35:27.402743 epoch: 4 step: 100 cls_loss= 0.29601 (3159 samples/sec)
saving....
2024-03-09 21:35:37.325681------------------------------------------------------ Precision@1: 65.82% 

[65.62, 65.67, 65.58, 65.72, 65.82]
max acc : 65.82

Epoch: 5
2024-03-09 21:35:37.602221 epoch: 5 step: 0 cls_loss= 0.35616 (118617 samples/sec)
2024-03-09 21:35:47.055438 epoch: 5 step: 100 cls_loss= 0.33707 (3173 samples/sec)
saving....
2024-03-09 21:35:56.961976------------------------------------------------------ Precision@1: 65.61% 

[65.62, 65.67, 65.58, 65.72, 65.82, 65.61]

Epoch: 6
2024-03-09 21:35:57.205600 epoch: 6 step: 0 cls_loss= 0.32694 (123736 samples/sec)
2024-03-09 21:36:06.648131 epoch: 6 step: 100 cls_loss= 0.32066 (3177 samples/sec)
saving....
2024-03-09 21:36:16.567489------------------------------------------------------ Precision@1: 65.60% 

[65.62, 65.67, 65.58, 65.72, 65.82, 65.61, 65.6]

Epoch: 7
2024-03-09 21:36:16.826592 epoch: 7 step: 0 cls_loss= 0.34504 (116450 samples/sec)
2024-03-09 21:36:26.280491 epoch: 7 step: 100 cls_loss= 0.31048 (3173 samples/sec)
saving....
2024-03-09 21:36:36.250551------------------------------------------------------ Precision@1: 65.39% 

[65.62, 65.67, 65.58, 65.72, 65.82, 65.61, 65.6, 65.39]

Epoch: 8
2024-03-09 21:36:36.517502 epoch: 8 step: 0 cls_loss= 0.29474 (113039 samples/sec)
2024-03-09 21:36:45.950741 epoch: 8 step: 100 cls_loss= 0.31042 (3180 samples/sec)
saving....
2024-03-09 21:36:55.859855------------------------------------------------------ Precision@1: 65.61% 

[65.62, 65.67, 65.58, 65.72, 65.82, 65.61, 65.6, 65.39, 65.61]

Epoch: 9
2024-03-09 21:36:56.116896 epoch: 9 step: 0 cls_loss= 0.31555 (117435 samples/sec)
2024-03-09 21:37:05.562641 epoch: 9 step: 100 cls_loss= 0.29376 (3176 samples/sec)
saving....
2024-03-09 21:37:15.531049------------------------------------------------------ Precision@1: 65.64% 

[65.62, 65.67, 65.58, 65.72, 65.82, 65.61, 65.6, 65.39, 65.61, 65.64]

Epoch: 10
2024-03-09 21:37:15.790673 epoch: 10 step: 0 cls_loss= 0.29047 (116223 samples/sec)
2024-03-09 21:37:25.236449 epoch: 10 step: 100 cls_loss= 0.29380 (3176 samples/sec)
saving....
2024-03-09 21:37:35.250414------------------------------------------------------ Precision@1: 65.50% 

[65.62, 65.67, 65.58, 65.72, 65.82, 65.61, 65.6, 65.39, 65.61, 65.64, 65.5]

Epoch: 11
2024-03-09 21:37:35.516979 epoch: 11 step: 0 cls_loss= 0.27304 (113201 samples/sec)
2024-03-09 21:37:44.944527 epoch: 11 step: 100 cls_loss= 0.29180 (3182 samples/sec)
saving....
2024-03-09 21:37:54.975617------------------------------------------------------ Precision@1: 65.47% 

[65.62, 65.67, 65.58, 65.72, 65.82, 65.61, 65.6, 65.39, 65.61, 65.64, 65.5, 65.47]

Epoch: 12
2024-03-09 21:37:55.241343 epoch: 12 step: 0 cls_loss= 0.27921 (113513 samples/sec)
2024-03-09 21:38:04.675912 epoch: 12 step: 100 cls_loss= 0.26241 (3180 samples/sec)
saving....
2024-03-09 21:38:14.576777------------------------------------------------------ Precision@1: 65.71% 

[65.62, 65.67, 65.58, 65.72, 65.82, 65.61, 65.6, 65.39, 65.61, 65.64, 65.5, 65.47, 65.71]

Epoch: 13
2024-03-09 21:38:14.842482 epoch: 13 step: 0 cls_loss= 0.28733 (113566 samples/sec)
2024-03-09 21:38:24.304871 epoch: 13 step: 100 cls_loss= 0.32157 (3170 samples/sec)
saving....
2024-03-09 21:38:34.220187------------------------------------------------------ Precision@1: 65.34% 

[65.62, 65.67, 65.58, 65.72, 65.82, 65.61, 65.6, 65.39, 65.61, 65.64, 65.5, 65.47, 65.71, 65.34]

Epoch: 14
2024-03-09 21:38:34.471476 epoch: 14 step: 0 cls_loss= 0.23676 (120081 samples/sec)
2024-03-09 21:38:43.931542 epoch: 14 step: 100 cls_loss= 0.36434 (3171 samples/sec)
saving....
2024-03-09 21:38:53.898222------------------------------------------------------ Precision@1: 65.44% 

[65.62, 65.67, 65.58, 65.72, 65.82, 65.61, 65.6, 65.39, 65.61, 65.64, 65.5, 65.47, 65.71, 65.34, 65.44]

Epoch: 15
2024-03-09 21:38:54.157958 epoch: 15 step: 0 cls_loss= 0.27105 (116130 samples/sec)
2024-03-09 21:39:03.637977 epoch: 15 step: 100 cls_loss= 0.26669 (3164 samples/sec)
saving....
2024-03-09 21:39:13.643665------------------------------------------------------ Precision@1: 65.37% 

[65.62, 65.67, 65.58, 65.72, 65.82, 65.61, 65.6, 65.39, 65.61, 65.64, 65.5, 65.47, 65.71, 65.34, 65.44, 65.37]

Epoch: 16
2024-03-09 21:39:13.904035 epoch: 16 step: 0 cls_loss= 0.34372 (115836 samples/sec)
2024-03-09 21:39:23.395396 epoch: 16 step: 100 cls_loss= 0.27603 (3160 samples/sec)
saving....
2024-03-09 21:39:33.529834------------------------------------------------------ Precision@1: 65.58% 

[65.62, 65.67, 65.58, 65.72, 65.82, 65.61, 65.6, 65.39, 65.61, 65.64, 65.5, 65.47, 65.71, 65.34, 65.44, 65.37, 65.58]

Epoch: 17
2024-03-09 21:39:33.795426 epoch: 17 step: 0 cls_loss= 0.27157 (113670 samples/sec)
2024-03-09 21:39:43.295193 epoch: 17 step: 100 cls_loss= 0.32679 (3158 samples/sec)
saving....
2024-03-09 21:39:53.275563------------------------------------------------------ Precision@1: 65.64% 

[65.62, 65.67, 65.58, 65.72, 65.82, 65.61, 65.6, 65.39, 65.61, 65.64, 65.5, 65.47, 65.71, 65.34, 65.44, 65.37, 65.58, 65.64]

Epoch: 18
2024-03-09 21:39:53.524889 epoch: 18 step: 0 cls_loss= 0.28763 (121079 samples/sec)
2024-03-09 21:40:02.980061 epoch: 18 step: 100 cls_loss= 0.34358 (3173 samples/sec)
saving....
2024-03-09 21:40:12.941563------------------------------------------------------ Precision@1: 65.65% 

[65.62, 65.67, 65.58, 65.72, 65.82, 65.61, 65.6, 65.39, 65.61, 65.64, 65.5, 65.47, 65.71, 65.34, 65.44, 65.37, 65.58, 65.64, 65.65]

Epoch: 19
2024-03-09 21:40:13.214498 epoch: 19 step: 0 cls_loss= 0.20125 (110512 samples/sec)
2024-03-09 21:40:22.639776 epoch: 19 step: 100 cls_loss= 0.24986 (3183 samples/sec)
saving....
2024-03-09 21:40:32.605305------------------------------------------------------ Precision@1: 65.39% 

[65.62, 65.67, 65.58, 65.72, 65.82, 65.61, 65.6, 65.39, 65.61, 65.64, 65.5, 65.47, 65.71, 65.34, 65.44, 65.37, 65.58, 65.64, 65.65, 65.39]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-09 21:40:35.510153 epoch: 0 step: 0 cls_loss= 0.29483 (40008 samples/sec)
2024-03-09 21:40:44.980979 epoch: 0 step: 100 cls_loss= 0.43473 (3167 samples/sec)
saving....
2024-03-09 21:40:55.298502------------------------------------------------------ Precision@1: 65.64% 

[65.64]
max acc : 65.64

Epoch: 1
2024-03-09 21:40:55.584458 epoch: 1 step: 0 cls_loss= 0.34381 (112909 samples/sec)
2024-03-09 21:41:05.090813 epoch: 1 step: 100 cls_loss= 0.25135 (3155 samples/sec)
saving....
2024-03-09 21:41:15.106393------------------------------------------------------ Precision@1: 65.68% 

[65.64, 65.68]
max acc : 65.68

Epoch: 2
2024-03-09 21:41:15.392667 epoch: 2 step: 0 cls_loss= 0.21053 (114396 samples/sec)
2024-03-09 21:41:24.878789 epoch: 2 step: 100 cls_loss= 0.33610 (3162 samples/sec)
saving....
2024-03-09 21:41:34.873266------------------------------------------------------ Precision@1: 65.59% 

[65.64, 65.68, 65.59]

Epoch: 3
2024-03-09 21:41:35.133953 epoch: 3 step: 0 cls_loss= 0.31252 (115685 samples/sec)
2024-03-09 21:41:44.673718 epoch: 3 step: 100 cls_loss= 0.36529 (3144 samples/sec)
saving....
2024-03-09 21:41:54.744190------------------------------------------------------ Precision@1: 65.44% 

[65.64, 65.68, 65.59, 65.44]

Epoch: 4
2024-03-09 21:41:55.019309 epoch: 4 step: 0 cls_loss= 0.32545 (109732 samples/sec)
2024-03-09 21:42:04.519093 epoch: 4 step: 100 cls_loss= 0.36052 (3158 samples/sec)
saving....
2024-03-09 21:42:14.519968------------------------------------------------------ Precision@1: 65.46% 

[65.64, 65.68, 65.59, 65.44, 65.46]

Epoch: 5
2024-03-09 21:42:14.788102 epoch: 5 step: 0 cls_loss= 0.31678 (112460 samples/sec)
2024-03-09 21:42:24.311963 epoch: 5 step: 100 cls_loss= 0.38639 (3150 samples/sec)
saving....
2024-03-09 21:42:34.335162------------------------------------------------------ Precision@1: 65.68% 

[65.64, 65.68, 65.59, 65.44, 65.46, 65.68]

Epoch: 6
2024-03-09 21:42:34.591843 epoch: 6 step: 0 cls_loss= 0.29807 (117466 samples/sec)
2024-03-09 21:42:44.097820 epoch: 6 step: 100 cls_loss= 0.33082 (3156 samples/sec)
saving....
2024-03-09 21:42:54.117361------------------------------------------------------ Precision@1: 65.67% 

[65.64, 65.68, 65.59, 65.44, 65.46, 65.68, 65.67]

Epoch: 7
2024-03-09 21:42:54.395866 epoch: 7 step: 0 cls_loss= 0.31522 (108332 samples/sec)
2024-03-09 21:43:03.913065 epoch: 7 step: 100 cls_loss= 0.29691 (3152 samples/sec)
saving....
2024-03-09 21:43:13.989488------------------------------------------------------ Precision@1: 65.50% 

[65.64, 65.68, 65.59, 65.44, 65.46, 65.68, 65.67, 65.5]

Epoch: 8
2024-03-09 21:43:14.259411 epoch: 8 step: 0 cls_loss= 0.31369 (111756 samples/sec)
2024-03-09 21:43:23.805998 epoch: 8 step: 100 cls_loss= 0.26348 (3142 samples/sec)
saving....
2024-03-09 21:43:33.878791------------------------------------------------------ Precision@1: 65.56% 

[65.64, 65.68, 65.59, 65.44, 65.46, 65.68, 65.67, 65.5, 65.56]

Epoch: 9
2024-03-09 21:43:34.143877 epoch: 9 step: 0 cls_loss= 0.21678 (113808 samples/sec)
2024-03-09 21:43:43.717247 epoch: 9 step: 100 cls_loss= 0.23285 (3133 samples/sec)
saving....
2024-03-09 21:43:53.867353------------------------------------------------------ Precision@1: 65.61% 

[65.64, 65.68, 65.59, 65.44, 65.46, 65.68, 65.67, 65.5, 65.56, 65.61]

Epoch: 10
2024-03-09 21:43:54.140919 epoch: 10 step: 0 cls_loss= 0.26848 (110261 samples/sec)
2024-03-09 21:44:03.614252 epoch: 10 step: 100 cls_loss= 0.24547 (3166 samples/sec)
saving....
2024-03-09 21:44:13.577992------------------------------------------------------ Precision@1: 65.21% 

[65.64, 65.68, 65.59, 65.44, 65.46, 65.68, 65.67, 65.5, 65.56, 65.61, 65.21]

Epoch: 11
2024-03-09 21:44:13.849845 epoch: 11 step: 0 cls_loss= 0.30437 (110929 samples/sec)
2024-03-09 21:44:23.387995 epoch: 11 step: 100 cls_loss= 0.26359 (3145 samples/sec)
saving....
2024-03-09 21:44:33.482975------------------------------------------------------ Precision@1: 65.32% 

[65.64, 65.68, 65.59, 65.44, 65.46, 65.68, 65.67, 65.5, 65.56, 65.61, 65.21, 65.32]

Epoch: 12
2024-03-09 21:44:33.725382 epoch: 12 step: 0 cls_loss= 0.28125 (124602 samples/sec)
2024-03-09 21:44:43.296917 epoch: 12 step: 100 cls_loss= 0.29003 (3134 samples/sec)
saving....
2024-03-09 21:44:53.385342------------------------------------------------------ Precision@1: 65.26% 

[65.64, 65.68, 65.59, 65.44, 65.46, 65.68, 65.67, 65.5, 65.56, 65.61, 65.21, 65.32, 65.26]

Epoch: 13
2024-03-09 21:44:53.644063 epoch: 13 step: 0 cls_loss= 0.22096 (116674 samples/sec)
2024-03-09 21:45:03.121775 epoch: 13 step: 100 cls_loss= 0.22966 (3165 samples/sec)
saving....
2024-03-09 21:45:13.165778------------------------------------------------------ Precision@1: 65.25% 

[65.64, 65.68, 65.59, 65.44, 65.46, 65.68, 65.67, 65.5, 65.56, 65.61, 65.21, 65.32, 65.26, 65.25]

Epoch: 14
2024-03-09 21:45:13.441458 epoch: 14 step: 0 cls_loss= 0.27132 (109432 samples/sec)
2024-03-09 21:45:22.915963 epoch: 14 step: 100 cls_loss= 0.27793 (3166 samples/sec)
saving....
2024-03-09 21:45:33.141702------------------------------------------------------ Precision@1: 65.43% 

[65.64, 65.68, 65.59, 65.44, 65.46, 65.68, 65.67, 65.5, 65.56, 65.61, 65.21, 65.32, 65.26, 65.25, 65.43]

Epoch: 15
2024-03-09 21:45:33.404534 epoch: 15 step: 0 cls_loss= 0.28068 (114735 samples/sec)
2024-03-09 21:45:42.917136 epoch: 15 step: 100 cls_loss= 0.30529 (3153 samples/sec)
saving....
2024-03-09 21:45:52.969462------------------------------------------------------ Precision@1: 65.56% 

[65.64, 65.68, 65.59, 65.44, 65.46, 65.68, 65.67, 65.5, 65.56, 65.61, 65.21, 65.32, 65.26, 65.25, 65.43, 65.56]

Epoch: 16
2024-03-09 21:45:53.234579 epoch: 16 step: 0 cls_loss= 0.27867 (113816 samples/sec)
2024-03-09 21:46:02.758473 epoch: 16 step: 100 cls_loss= 0.27545 (3150 samples/sec)
saving....
2024-03-09 21:46:12.781855------------------------------------------------------ Precision@1: 65.34% 

[65.64, 65.68, 65.59, 65.44, 65.46, 65.68, 65.67, 65.5, 65.56, 65.61, 65.21, 65.32, 65.26, 65.25, 65.43, 65.56, 65.34]

Epoch: 17
2024-03-09 21:46:13.049370 epoch: 17 step: 0 cls_loss= 0.29449 (112807 samples/sec)
2024-03-09 21:46:22.538363 epoch: 17 step: 100 cls_loss= 0.29317 (3161 samples/sec)
saving....
2024-03-09 21:46:32.596122------------------------------------------------------ Precision@1: 65.17% 

[65.64, 65.68, 65.59, 65.44, 65.46, 65.68, 65.67, 65.5, 65.56, 65.61, 65.21, 65.32, 65.26, 65.25, 65.43, 65.56, 65.34, 65.17]

Epoch: 18
2024-03-09 21:46:32.851395 epoch: 18 step: 0 cls_loss= 0.29774 (118257 samples/sec)
2024-03-09 21:46:42.344863 epoch: 18 step: 100 cls_loss= 0.26432 (3160 samples/sec)
saving....
2024-03-09 21:46:52.432028------------------------------------------------------ Precision@1: 65.29% 

[65.64, 65.68, 65.59, 65.44, 65.46, 65.68, 65.67, 65.5, 65.56, 65.61, 65.21, 65.32, 65.26, 65.25, 65.43, 65.56, 65.34, 65.17, 65.29]

Epoch: 19
2024-03-09 21:46:52.697338 epoch: 19 step: 0 cls_loss= 0.25122 (113721 samples/sec)
2024-03-09 21:47:02.184176 epoch: 19 step: 100 cls_loss= 0.29032 (3162 samples/sec)
saving....
2024-03-09 21:47:12.249486------------------------------------------------------ Precision@1: 65.31% 

[65.64, 65.68, 65.59, 65.44, 65.46, 65.68, 65.67, 65.5, 65.56, 65.61, 65.21, 65.32, 65.26, 65.25, 65.43, 65.56, 65.34, 65.17, 65.29, 65.31]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-09 21:47:15.118953 epoch: 0 step: 0 cls_loss= 0.35212 (39920 samples/sec)
2024-03-09 21:47:24.614885 epoch: 0 step: 100 cls_loss= 0.33935 (3159 samples/sec)
saving....
2024-03-09 21:47:34.946768------------------------------------------------------ Precision@1: 65.73% 

[65.73]
max acc : 65.73

Epoch: 1
2024-03-09 21:47:35.236643 epoch: 1 step: 0 cls_loss= 0.37678 (113391 samples/sec)
2024-03-09 21:47:44.717120 epoch: 1 step: 100 cls_loss= 0.36586 (3164 samples/sec)
saving....
2024-03-09 21:47:54.733543------------------------------------------------------ Precision@1: 65.41% 

[65.73, 65.41]

Epoch: 2
2024-03-09 21:47:54.979346 epoch: 2 step: 0 cls_loss= 0.33105 (122827 samples/sec)
2024-03-09 21:48:04.498404 epoch: 2 step: 100 cls_loss= 0.34522 (3151 samples/sec)
saving....
2024-03-09 21:48:14.520577------------------------------------------------------ Precision@1: 65.58% 

[65.73, 65.41, 65.58]

Epoch: 3
2024-03-09 21:48:14.781472 epoch: 3 step: 0 cls_loss= 0.28419 (115655 samples/sec)
2024-03-09 21:48:24.272144 epoch: 3 step: 100 cls_loss= 0.30739 (3161 samples/sec)
saving....
2024-03-09 21:48:34.252220------------------------------------------------------ Precision@1: 65.64% 

[65.73, 65.41, 65.58, 65.64]

Epoch: 4
2024-03-09 21:48:34.512916 epoch: 4 step: 0 cls_loss= 0.27787 (115756 samples/sec)
2024-03-09 21:48:44.009240 epoch: 4 step: 100 cls_loss= 0.30948 (3159 samples/sec)
saving....
2024-03-09 21:48:54.031882------------------------------------------------------ Precision@1: 65.43% 

[65.73, 65.41, 65.58, 65.64, 65.43]

Epoch: 5
2024-03-09 21:48:54.297372 epoch: 5 step: 0 cls_loss= 0.23489 (113658 samples/sec)
2024-03-09 21:49:03.770402 epoch: 5 step: 100 cls_loss= 0.29179 (3167 samples/sec)
saving....
2024-03-09 21:49:13.911553------------------------------------------------------ Precision@1: 65.20% 

[65.73, 65.41, 65.58, 65.64, 65.43, 65.2]

Epoch: 6
2024-03-09 21:49:14.180015 epoch: 6 step: 0 cls_loss= 0.22258 (112401 samples/sec)
2024-03-09 21:49:23.737134 epoch: 6 step: 100 cls_loss= 0.30308 (3139 samples/sec)
saving....
2024-03-09 21:49:33.897830------------------------------------------------------ Precision@1: 65.07% 

[65.73, 65.41, 65.58, 65.64, 65.43, 65.2, 65.07]

Epoch: 7
2024-03-09 21:49:34.156527 epoch: 7 step: 0 cls_loss= 0.31519 (116686 samples/sec)
2024-03-09 21:49:43.627826 epoch: 7 step: 100 cls_loss= 0.32349 (3167 samples/sec)
saving....
2024-03-09 21:49:53.774556------------------------------------------------------ Precision@1: 65.09% 

[65.73, 65.41, 65.58, 65.64, 65.43, 65.2, 65.07, 65.09]

Epoch: 8
2024-03-09 21:49:54.055543 epoch: 8 step: 0 cls_loss= 0.27932 (107395 samples/sec)
2024-03-09 21:50:03.543036 epoch: 8 step: 100 cls_loss= 0.32049 (3162 samples/sec)
saving....
2024-03-09 21:50:13.574636------------------------------------------------------ Precision@1: 65.10% 

[65.73, 65.41, 65.58, 65.64, 65.43, 65.2, 65.07, 65.09, 65.1]

Epoch: 9
2024-03-09 21:50:13.842164 epoch: 9 step: 0 cls_loss= 0.26984 (112804 samples/sec)
2024-03-09 21:50:23.348679 epoch: 9 step: 100 cls_loss= 0.28524 (3155 samples/sec)
saving....
2024-03-09 21:50:33.356718------------------------------------------------------ Precision@1: 65.17% 

[65.73, 65.41, 65.58, 65.64, 65.43, 65.2, 65.07, 65.09, 65.1, 65.17]

Epoch: 10
2024-03-09 21:50:33.625411 epoch: 10 step: 0 cls_loss= 0.28968 (112107 samples/sec)
2024-03-09 21:50:43.133429 epoch: 10 step: 100 cls_loss= 0.28512 (3155 samples/sec)
saving....
2024-03-09 21:50:53.117477------------------------------------------------------ Precision@1: 65.37% 

[65.73, 65.41, 65.58, 65.64, 65.43, 65.2, 65.07, 65.09, 65.1, 65.17, 65.37]

Epoch: 11
2024-03-09 21:50:53.377706 epoch: 11 step: 0 cls_loss= 0.26423 (116007 samples/sec)
2024-03-09 21:51:02.845881 epoch: 11 step: 100 cls_loss= 0.28988 (3168 samples/sec)
saving....
2024-03-09 21:51:12.996860------------------------------------------------------ Precision@1: 64.92% 

[65.73, 65.41, 65.58, 65.64, 65.43, 65.2, 65.07, 65.09, 65.1, 65.17, 65.37, 64.92]

Epoch: 12
2024-03-09 21:51:13.263717 epoch: 12 step: 0 cls_loss= 0.22325 (112937 samples/sec)
2024-03-09 21:51:22.786249 epoch: 12 step: 100 cls_loss= 0.26208 (3150 samples/sec)
saving....
2024-03-09 21:51:32.867495------------------------------------------------------ Precision@1: 65.08% 

[65.73, 65.41, 65.58, 65.64, 65.43, 65.2, 65.07, 65.09, 65.1, 65.17, 65.37, 64.92, 65.08]

Epoch: 13
2024-03-09 21:51:33.121630 epoch: 13 step: 0 cls_loss= 0.29042 (118746 samples/sec)
2024-03-09 21:51:42.607576 epoch: 13 step: 100 cls_loss= 0.30216 (3162 samples/sec)
saving....
2024-03-09 21:51:52.645441------------------------------------------------------ Precision@1: 65.01% 

[65.73, 65.41, 65.58, 65.64, 65.43, 65.2, 65.07, 65.09, 65.1, 65.17, 65.37, 64.92, 65.08, 65.01]

Epoch: 14
2024-03-09 21:51:52.912999 epoch: 14 step: 0 cls_loss= 0.25680 (112782 samples/sec)
2024-03-09 21:52:02.375411 epoch: 14 step: 100 cls_loss= 0.26229 (3170 samples/sec)
saving....
2024-03-09 21:52:12.411885------------------------------------------------------ Precision@1: 65.06% 

[65.73, 65.41, 65.58, 65.64, 65.43, 65.2, 65.07, 65.09, 65.1, 65.17, 65.37, 64.92, 65.08, 65.01, 65.06]

Epoch: 15
2024-03-09 21:52:12.667115 epoch: 15 step: 0 cls_loss= 0.30122 (118188 samples/sec)
2024-03-09 21:52:22.167295 epoch: 15 step: 100 cls_loss= 0.25823 (3158 samples/sec)
saving....
2024-03-09 21:52:32.213166------------------------------------------------------ Precision@1: 64.91% 

[65.73, 65.41, 65.58, 65.64, 65.43, 65.2, 65.07, 65.09, 65.1, 65.17, 65.37, 64.92, 65.08, 65.01, 65.06, 64.91]

Epoch: 16
2024-03-09 21:52:32.466828 epoch: 16 step: 0 cls_loss= 0.24869 (119012 samples/sec)
2024-03-09 21:52:41.923043 epoch: 16 step: 100 cls_loss= 0.28187 (3172 samples/sec)
saving....
2024-03-09 21:52:51.959352------------------------------------------------------ Precision@1: 64.97% 

[65.73, 65.41, 65.58, 65.64, 65.43, 65.2, 65.07, 65.09, 65.1, 65.17, 65.37, 64.92, 65.08, 65.01, 65.06, 64.91, 64.97]

Epoch: 17
2024-03-09 21:52:52.224171 epoch: 17 step: 0 cls_loss= 0.33112 (113894 samples/sec)
2024-03-09 21:53:01.709190 epoch: 17 step: 100 cls_loss= 0.23743 (3163 samples/sec)
saving....
2024-03-09 21:53:11.740934------------------------------------------------------ Precision@1: 65.15% 

[65.73, 65.41, 65.58, 65.64, 65.43, 65.2, 65.07, 65.09, 65.1, 65.17, 65.37, 64.92, 65.08, 65.01, 65.06, 64.91, 64.97, 65.15]

Epoch: 18
2024-03-09 21:53:12.005421 epoch: 18 step: 0 cls_loss= 0.23756 (114162 samples/sec)
2024-03-09 21:53:21.490659 epoch: 18 step: 100 cls_loss= 0.23770 (3163 samples/sec)
saving....
2024-03-09 21:53:31.505901------------------------------------------------------ Precision@1: 64.70% 

[65.73, 65.41, 65.58, 65.64, 65.43, 65.2, 65.07, 65.09, 65.1, 65.17, 65.37, 64.92, 65.08, 65.01, 65.06, 64.91, 64.97, 65.15, 64.7]

Epoch: 19
2024-03-09 21:53:31.765259 epoch: 19 step: 0 cls_loss= 0.24067 (116389 samples/sec)
2024-03-09 21:53:41.264232 epoch: 19 step: 100 cls_loss= 0.30024 (3158 samples/sec)
saving....
2024-03-09 21:53:51.281636------------------------------------------------------ Precision@1: 65.00% 

[65.73, 65.41, 65.58, 65.64, 65.43, 65.2, 65.07, 65.09, 65.1, 65.17, 65.37, 64.92, 65.08, 65.01, 65.06, 64.91, 64.97, 65.15, 64.7, 65.0]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-09 21:53:54.144651 epoch: 0 step: 0 cls_loss= 0.40478 (40853 samples/sec)
2024-03-09 21:54:03.626442 epoch: 0 step: 100 cls_loss= 0.39411 (3163 samples/sec)
saving....
2024-03-09 21:54:13.910806------------------------------------------------------ Precision@1: 65.44% 

[65.44]
max acc : 65.44

Epoch: 1
2024-03-09 21:54:14.178406 epoch: 1 step: 0 cls_loss= 0.31509 (123319 samples/sec)
2024-03-09 21:54:23.702608 epoch: 1 step: 100 cls_loss= 0.31457 (3150 samples/sec)
saving....
2024-03-09 21:54:33.745229------------------------------------------------------ Precision@1: 65.48% 

[65.44, 65.48]
max acc : 65.48

Epoch: 2
2024-03-09 21:54:34.035176 epoch: 2 step: 0 cls_loss= 0.28445 (113107 samples/sec)
2024-03-09 21:54:43.541309 epoch: 2 step: 100 cls_loss= 0.34807 (3156 samples/sec)
saving....
2024-03-09 21:54:53.569360------------------------------------------------------ Precision@1: 65.28% 

[65.44, 65.48, 65.28]

Epoch: 3
2024-03-09 21:54:53.838040 epoch: 3 step: 0 cls_loss= 0.31528 (112322 samples/sec)
2024-03-09 21:55:03.360533 epoch: 3 step: 100 cls_loss= 0.36800 (3150 samples/sec)
saving....
2024-03-09 21:55:13.378284------------------------------------------------------ Precision@1: 65.49% 

[65.44, 65.48, 65.28, 65.49]
max acc : 65.49

Epoch: 4
2024-03-09 21:55:13.649127 epoch: 4 step: 0 cls_loss= 0.31752 (119909 samples/sec)
2024-03-09 21:55:23.131528 epoch: 4 step: 100 cls_loss= 0.30569 (3163 samples/sec)
saving....
2024-03-09 21:55:33.161093------------------------------------------------------ Precision@1: 65.22% 

[65.44, 65.48, 65.28, 65.49, 65.22]

Epoch: 5
2024-03-09 21:55:33.413491 epoch: 5 step: 0 cls_loss= 0.34122 (119620 samples/sec)
2024-03-09 21:55:42.987370 epoch: 5 step: 100 cls_loss= 0.29451 (3133 samples/sec)
saving....
2024-03-09 21:55:53.018972------------------------------------------------------ Precision@1: 65.07% 

[65.44, 65.48, 65.28, 65.49, 65.22, 65.07]

Epoch: 6
2024-03-09 21:55:53.294177 epoch: 6 step: 0 cls_loss= 0.29456 (109612 samples/sec)
2024-03-09 21:56:02.780767 epoch: 6 step: 100 cls_loss= 0.33703 (3162 samples/sec)
saving....
2024-03-09 21:56:12.835951------------------------------------------------------ Precision@1: 65.31% 

[65.44, 65.48, 65.28, 65.49, 65.22, 65.07, 65.31]

Epoch: 7
2024-03-09 21:56:13.100919 epoch: 7 step: 0 cls_loss= 0.29200 (113850 samples/sec)
2024-03-09 21:56:22.589667 epoch: 7 step: 100 cls_loss= 0.29111 (3161 samples/sec)
saving....
2024-03-09 21:56:32.567830------------------------------------------------------ Precision@1: 65.27% 

[65.44, 65.48, 65.28, 65.49, 65.22, 65.07, 65.31, 65.27]

Epoch: 8
2024-03-09 21:56:32.826105 epoch: 8 step: 0 cls_loss= 0.22109 (116868 samples/sec)
2024-03-09 21:56:42.280553 epoch: 8 step: 100 cls_loss= 0.28727 (3173 samples/sec)
saving....
2024-03-09 21:56:52.291405------------------------------------------------------ Precision@1: 64.99% 

[65.44, 65.48, 65.28, 65.49, 65.22, 65.07, 65.31, 65.27, 64.99]

Epoch: 9
2024-03-09 21:56:52.548113 epoch: 9 step: 0 cls_loss= 0.27245 (117558 samples/sec)
2024-03-09 21:57:01.996691 epoch: 9 step: 100 cls_loss= 0.28488 (3175 samples/sec)
saving....
2024-03-09 21:57:12.015063------------------------------------------------------ Precision@1: 65.21% 

[65.44, 65.48, 65.28, 65.49, 65.22, 65.07, 65.31, 65.27, 64.99, 65.21]

Epoch: 10
2024-03-09 21:57:12.276649 epoch: 10 step: 0 cls_loss= 0.25725 (115374 samples/sec)
2024-03-09 21:57:21.742569 epoch: 10 step: 100 cls_loss= 0.28989 (3169 samples/sec)
saving....
2024-03-09 21:57:31.745042------------------------------------------------------ Precision@1: 65.02% 

[65.44, 65.48, 65.28, 65.49, 65.22, 65.07, 65.31, 65.27, 64.99, 65.21, 65.02]

Epoch: 11
2024-03-09 21:57:32.015057 epoch: 11 step: 0 cls_loss= 0.24566 (111741 samples/sec)
2024-03-09 21:57:41.502000 epoch: 11 step: 100 cls_loss= 0.32819 (3162 samples/sec)
saving....
2024-03-09 21:57:51.487821------------------------------------------------------ Precision@1: 65.26% 

[65.44, 65.48, 65.28, 65.49, 65.22, 65.07, 65.31, 65.27, 64.99, 65.21, 65.02, 65.26]

Epoch: 12
2024-03-09 21:57:51.775993 epoch: 12 step: 0 cls_loss= 0.27646 (104664 samples/sec)
2024-03-09 21:58:01.251763 epoch: 12 step: 100 cls_loss= 0.21573 (3166 samples/sec)
saving....
2024-03-09 21:58:11.214477------------------------------------------------------ Precision@1: 65.22% 

[65.44, 65.48, 65.28, 65.49, 65.22, 65.07, 65.31, 65.27, 64.99, 65.21, 65.02, 65.26, 65.22]

Epoch: 13
2024-03-09 21:58:11.477214 epoch: 13 step: 0 cls_loss= 0.23525 (114854 samples/sec)
2024-03-09 21:58:20.938574 epoch: 13 step: 100 cls_loss= 0.24254 (3170 samples/sec)
saving....
2024-03-09 21:58:30.908831------------------------------------------------------ Precision@1: 65.26% 

[65.44, 65.48, 65.28, 65.49, 65.22, 65.07, 65.31, 65.27, 64.99, 65.21, 65.02, 65.26, 65.22, 65.26]

Epoch: 14
2024-03-09 21:58:31.191715 epoch: 14 step: 0 cls_loss= 0.22517 (106618 samples/sec)
2024-03-09 21:58:40.662912 epoch: 14 step: 100 cls_loss= 0.25113 (3167 samples/sec)
saving....
2024-03-09 21:58:50.633568------------------------------------------------------ Precision@1: 65.15% 

[65.44, 65.48, 65.28, 65.49, 65.22, 65.07, 65.31, 65.27, 64.99, 65.21, 65.02, 65.26, 65.22, 65.26, 65.15]

Epoch: 15
2024-03-09 21:58:50.902645 epoch: 15 step: 0 cls_loss= 0.20957 (112157 samples/sec)
2024-03-09 21:59:00.361573 epoch: 15 step: 100 cls_loss= 0.29488 (3171 samples/sec)
saving....
2024-03-09 21:59:10.327928------------------------------------------------------ Precision@1: 65.15% 

[65.44, 65.48, 65.28, 65.49, 65.22, 65.07, 65.31, 65.27, 64.99, 65.21, 65.02, 65.26, 65.22, 65.26, 65.15, 65.15]

Epoch: 16
2024-03-09 21:59:10.604543 epoch: 16 step: 0 cls_loss= 0.29488 (109060 samples/sec)
2024-03-09 21:59:20.115474 epoch: 16 step: 100 cls_loss= 0.23601 (3154 samples/sec)
saving....
2024-03-09 21:59:30.136938------------------------------------------------------ Precision@1: 64.92% 

[65.44, 65.48, 65.28, 65.49, 65.22, 65.07, 65.31, 65.27, 64.99, 65.21, 65.02, 65.26, 65.22, 65.26, 65.15, 65.15, 64.92]

Epoch: 17
2024-03-09 21:59:30.400498 epoch: 17 step: 0 cls_loss= 0.22845 (114548 samples/sec)
2024-03-09 21:59:39.874620 epoch: 17 step: 100 cls_loss= 0.29679 (3166 samples/sec)
saving....
2024-03-09 21:59:49.878989------------------------------------------------------ Precision@1: 64.99% 

[65.44, 65.48, 65.28, 65.49, 65.22, 65.07, 65.31, 65.27, 64.99, 65.21, 65.02, 65.26, 65.22, 65.26, 65.15, 65.15, 64.92, 64.99]

Epoch: 18
2024-03-09 21:59:50.159676 epoch: 18 step: 0 cls_loss= 0.18816 (107537 samples/sec)
2024-03-09 21:59:59.645835 epoch: 18 step: 100 cls_loss= 0.23293 (3162 samples/sec)
saving....
2024-03-09 22:00:09.675008------------------------------------------------------ Precision@1: 64.63% 

[65.44, 65.48, 65.28, 65.49, 65.22, 65.07, 65.31, 65.27, 64.99, 65.21, 65.02, 65.26, 65.22, 65.26, 65.15, 65.15, 64.92, 64.99, 64.63]

Epoch: 19
2024-03-09 22:00:09.956413 epoch: 19 step: 0 cls_loss= 0.24585 (107167 samples/sec)
2024-03-09 22:00:19.416511 epoch: 19 step: 100 cls_loss= 0.24206 (3171 samples/sec)
saving....
2024-03-09 22:00:29.399036------------------------------------------------------ Precision@1: 65.06% 

[65.44, 65.48, 65.28, 65.49, 65.22, 65.07, 65.31, 65.27, 64.99, 65.21, 65.02, 65.26, 65.22, 65.26, 65.15, 65.15, 64.92, 64.99, 64.63, 65.06]
training CIFAR-100 !
==> Preparing data ..
Files already downloaded and verified
Files already downloaded and verified
Total number of samples in train_loader: 50100
=> creating model shufflenetv2_m1 ...
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:151: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

Epoch: 0
2024-03-09 22:00:32.318085 epoch: 0 step: 0 cls_loss= 0.30516 (39378 samples/sec)
2024-03-09 22:00:41.795158 epoch: 0 step: 100 cls_loss= 0.37214 (3165 samples/sec)
saving....
2024-03-09 22:00:52.039584------------------------------------------------------ Precision@1: 65.15% 

[65.15]
max acc : 65.15

Epoch: 1
2024-03-09 22:00:52.333687 epoch: 1 step: 0 cls_loss= 0.35323 (111133 samples/sec)
2024-03-09 22:01:01.838830 epoch: 1 step: 100 cls_loss= 0.27924 (3156 samples/sec)
saving....
2024-03-09 22:01:11.843537------------------------------------------------------ Precision@1: 65.09% 

[65.15, 65.09]

Epoch: 2
2024-03-09 22:01:12.101717 epoch: 2 step: 0 cls_loss= 0.29772 (116862 samples/sec)
2024-03-09 22:01:21.599088 epoch: 2 step: 100 cls_loss= 0.32138 (3158 samples/sec)
saving....
2024-03-09 22:01:31.557083------------------------------------------------------ Precision@1: 65.55% 

[65.15, 65.09, 65.55]
max acc : 65.55

Epoch: 3
2024-03-09 22:01:31.852105 epoch: 3 step: 0 cls_loss= 0.24944 (110769 samples/sec)
2024-03-09 22:01:41.360020 epoch: 3 step: 100 cls_loss= 0.30106 (3155 samples/sec)
saving....
2024-03-09 22:01:51.360379------------------------------------------------------ Precision@1: 65.08% 

[65.15, 65.09, 65.55, 65.08]

Epoch: 4
2024-03-09 22:01:51.619446 epoch: 4 step: 0 cls_loss= 0.31387 (116420 samples/sec)
2024-03-09 22:02:01.097691 epoch: 4 step: 100 cls_loss= 0.33055 (3165 samples/sec)
saving....
2024-03-09 22:02:11.077495------------------------------------------------------ Precision@1: 65.19% 

[65.15, 65.09, 65.55, 65.08, 65.19]

Epoch: 5
2024-03-09 22:02:11.356315 epoch: 5 step: 0 cls_loss= 0.30616 (108175 samples/sec)
2024-03-09 22:02:20.803877 epoch: 5 step: 100 cls_loss= 0.39373 (3175 samples/sec)
saving....
2024-03-09 22:02:30.808540------------------------------------------------------ Precision@1: 65.31% 

[65.15, 65.09, 65.55, 65.08, 65.19, 65.31]

Epoch: 6
2024-03-09 22:02:31.098505 epoch: 6 step: 0 cls_loss= 0.26412 (104039 samples/sec)
2024-03-09 22:02:40.603878 epoch: 6 step: 100 cls_loss= 0.33739 (3156 samples/sec)
saving....
2024-03-09 22:02:50.619134------------------------------------------------------ Precision@1: 64.83% 

[65.15, 65.09, 65.55, 65.08, 65.19, 65.31, 64.83]

Epoch: 7
2024-03-09 22:02:50.873022 epoch: 7 step: 0 cls_loss= 0.31606 (118798 samples/sec)
2024-03-09 22:03:00.364017 epoch: 7 step: 100 cls_loss= 0.33030 (3161 samples/sec)
saving....
2024-03-09 22:03:10.382306------------------------------------------------------ Precision@1: 65.01% 

[65.15, 65.09, 65.55, 65.08, 65.19, 65.31, 64.83, 65.01]

Epoch: 8
2024-03-09 22:03:10.651891 epoch: 8 step: 0 cls_loss= 0.24157 (111952 samples/sec)
2024-03-09 22:03:20.127757 epoch: 8 step: 100 cls_loss= 0.25855 (3166 samples/sec)
^CTraceback (most recent call last):
  File "./cifar100_train_eval.py", line 362, in <module>
    main()
  File "./cifar100_train_eval.py", line 347, in main
    train(epoch)
  File "./cifar100_train_eval.py", line 204, in train
    outputs = model(inputs.cuda())
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1480, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspaces/pytorch-dev/SLFP_CNNs/nets/cifar100_shufflenet_v2.py", line 172, in forward
    x = self.stage2(x)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1480, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1480, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspaces/pytorch-dev/SLFP_CNNs/nets/cifar100_shufflenet_v2.py", line 104, in forward
    residual = self.residual(residual)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1480, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1480, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspaces/pytorch-dev/SLFP_CNNs/utils/slfp_conv_shufflenetv2.py", line 20, in forward
    self.input_q = self.quantize_fn(input/self.Ka) #量化并缩放激活值
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1480, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspaces/pytorch-dev/SLFP_CNNs/utils/sfp_quant.py", line 84, in forward
    weight_q = self.uniform_q(x)  # W_sfp 
  File "/workspaces/pytorch-dev/SLFP_CNNs/utils/sfp_quant.py", line 40, in forward
    scaling_factor = pow(2, torch.floor(torch.log2(input)))
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor.py", line 40, in wrapped
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor.py", line 866, in __rpow__
    return torch.tensor(other, dtype=dtype, device=self.device) ** self
KeyboardInterrupt

]0;vscode@pytorch: /workspaces/pytorch-dev/SLFP_CNNs[01;32mvscode@pytorch[00m:[01;34m/workspaces/pytorch-dev/SLFP_CNNs[00m$ exit

Script done on 2024-03-09 22:05:15+08:00 [COMMAND_EXIT_CODE="130"]
